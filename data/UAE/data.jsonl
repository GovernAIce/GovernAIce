{"_id": "686959815520a4cc7989de75", "title": "UAE Strategy for Artificial Intelligence 2031", "source": "https://ai.gov.ae/wp-content/uploads/2021/07/UAE-National-Strategy-for-Artificial-Intelligence-2031.pdf", "text": "WE WILL TRANSFORM THE UAE INTO A\nWORLD LEADER IN AI BY INVESTING IN\nPEOPLE AND INDUSTRIES THAT ARE KEY\nTO OUR SUCCESS.\nUAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 3CONTENTS\nPage 6 MINISTERIAL FORWARD Page 32 OBJECTIVE 5: ATTRACT AND TRAIN TALENT FOR FUTURE JOBS\nENABLED BY AI\nPage 8 EXECUTIVE SUMMARY\nPage 33 Public AI Training\nPage 10 WHERE THE UAE HAS OPPORTUNITIES TO LEAD\nPage 33 Upskilling Students\nPage 10 Industry Assets & Emerging Sectors\nPage 33 Government Training\nPage 13 Smart Government\nPage 34 Professional Upskilling\nPage 13 Data Sharing and Governance\nPage 34 Employment Transition Support\nPage 14 New Generation of Regional Talent\nPage 36 OBJECTIVE 6: BRING WORLD-LEADING RESEARCH CAPABILITY TO\nPage 18 EIGHT STRATEGIC OBJECTIVES\nWORK WITH TARGET INDUSTRIES\nPage 20 OBJECTIVE 1: BUILD A REPUTATION AS AN AI DESTINATION\nPage 39 National Virtual AI Institute\nPage 20 UAI Brand\nPage 39 Key Thinkers Program\nPage 22 OBJECTIVE 2: INCREASE THE UAE COMPETITIVE ASSETS IN PRIORITY Page 39 AI Library\nSECTORS THROUGH DEPLOYMENT OF AI\nPage 40 OBJECTIVE 7: PROVIDE THE DATA AND SUPPORTING INFRASTRUCTURE\nPage 22 Existing Assets\nESSENTIAL TO BECOME A TEST BED FOR AI\nPage 23 Emerging Sectors\nPage 40 Data Sharing\nPage 25 Proof-of-Concept Support in Priority Sectors\nPage 40 Secure Data Infrastructure\nPage 26 OBJECTIVE 3: DEVELOP A FERTILE ECOSYSTEM FOR AI\nPage 42 OBJECTIVE 8: ENSURE STRONG GOVERNANCE AND\nPage 26 AI Network\nEFFECTIVE REGULATION\nPage 26 Applied AI Accelerator\nPage 42 National Governance Review\nPage 28 AI Incentive Scheme for Overseas Companies\nPage 43 Second Global Governance of Artificial Intelligence Roundtable\nPage 29 Business Support for UAE AI Firms\nPage 43 Intergovernmental Panel on Artificial Intelligence\nPage 30 OBJECTIVE 4: ADOPT AI ACROSS CUSTOMER SERVICES TO IMPROVE\nPage 44 CONCLUSION\nLIVES AND GOVERNMENT\nPage 45 REFERENCES\nPage 30 National AI Challenges\nPage 30 UAE Artificial Intelligence and Blockchain CouncilMINISTERIAL FORWARD\nThe role of any new minister is to by Google and Amazon, want to put\nset a direction for their tenure and AI at the core of their businesses, and\norchestrate the vision set by the the UAE hopes to do the same for an\nleadership. entire nation1.\u201d The UAE will build an AI\n\u201cWE WANT THE UAE TO BECOME THE\neconomy, not wait for one.\nWORLD\u2019S MOST PREPARED COUNTRY The appointment as the UAE\u2019s Minister The National AI Strategy \u2013 UAI \u2013 from AI\nof State for Artificial Intelligence has ready to AI leader.\nFOR ARTIFICIAL INTELLIGENCE.\u201d brought the opportunity of a new remit\nwithout pre-existing boundaries and The UAE has a vision to become one\nHis Highness Sheikh Mohammed bin Rashid Al Maktoum\nconstraints. of the leading nations in AI by 2031\nUAE Vice President and Prime Minister and Ruler of Dubai The challenge in this role is balancing in alignment with the UAE Centennial\na global context that is changing 2071, creating new economic,\nrapidly with a stable direction for our educational, and social opportunities\nnation. Technological and economic for citizens, governments and\nopportunities come thick and fast, businesses and generating up to AED\nas do other nations vying for global 335 billion in extra growth. This report\nleadership in different aspects of AI. breaks down our approach to this goal.\nDeveloping a roadmap for the UAE\u2019s At the annual World Government\nrole required us to contextualize Summit Meeting in February 2018, the\nglobal debates to the challenges and UAE announced key elements of its\nopportunities of the Middle East, strategy \u2013 a welcoming destination for\nparticularly the unique situation of developing AI products, new education\nyoung and ambitious Arab countries. programs and championing good\nWe also benefited from conversations governance.\nwith companies, politicians and leading\nexperts on AI from around the world: We have added more details on\nunderstanding what they look for when how the UAE could become a fast\ndeciding whom to work with and where adopter of emerging AI technologies\nto work. across Government, as well as attract\ntop AI talent to experiment with\nThe UAE government knows its new technologies and work in a\nstrength is in combining a strong vision sophisticated, secure ecosystem to\nwith active involvement \u2013 investment, solve complex problems.\nlegislation and testbeds - for\ntechnological innovation. Therefore, for With this foundation of talent, as well\nthis nation, being the most prepared as better governance of AI, we will\ncountry means a lot more than have the right conditions to develop\ndeveloping legislation that responds to new AI solutions here in the UAE in\nchanges in the world. Instead, it means the coming decade and beyond. These\nOmar Sultan Al Olama\nproactively changing the world first. A novel technologies have huge economic\nMinister of State for Artificial Intelligence recent article summarizes this attitude: potential, including licensing and export\n\u201cToday\u2019s biggest tech companies, led overseas.\nUAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 6 UAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 7EXECUTIVE SUMMARY\nThe UAE sets a clear vision through its AI \u2022 Build a reputation as an AI destination.\nStrategy, to become the world leader in\n\u2022 Increase the UAE competitive assets in\nAI by 2031. Implementing this vision on\npriority sectors through deployment\nthe ground requires rigorous dedication\nof AI.\nand clear steps that outline the path\n\u2022 Develop a fertile ecosystem for AI.\nfor success. Hence, it is essential to set\n\u2022 Adopt AI across customer services to\nthe foundation, the AI Strategy, with\nimprove lives and government.\nclear strategic objectives that outline\nthe initiatives that are essential in \u2022 Attract and train talent for future jobs\nachieving the milestones. It is notably enabled by AI.\nworth mentioning that the AI Strategy\n\u2022 Bring world-leading research capability\naligns with the UAE Centennial 2071,\nto work with target industries.\nto make the UAE the best country in\n\u2022 Provide the data and supporting\nthe world by 2071. The AI Strategy will\ninfrastructure essential to become a\ncontribute significantly in education,\ntest bed for AI.\neconomy, government development, and\ncommunity happiness through various \u2022 Ensure strong governance and effective\nAI technologies implementations in regulation.\ndifferent sectors to include energy,\ntourism, and education, to list few. The UAE has a strong foundation\nThe UAE AI and Blockchain Council will consisting of cohesive and diversified\noverlook the implementation of the AI multinational community that is a\nStrategy throughout all emirates but fast adapter to new and emerging\nultimately, the implementation will be a technologies. Therefore, it acts as a\nmulti-stakeholders effort and cooperation magnet that attracts the best talents\nfrom different local and federal entities in from the globe to conduct their\nthe UAE. experiments on AI solutions in the\nUAE and open the doors to practical\nThere are eight strategic objectives implementations in different sectors.\noutlined in the AI Strategy, namely:\nUAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 8 UAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 9WHERE THE UAE HAS\nOPPORTUNITIES TO LEAD\nThe UAE\u2019s vision to become a world 1. Industry Assets & Healthcare: a small sector with young and resource-rich economies\nleader in AI does not mean aiming opportunity to be world leading in of the Middle East. For example, 85%\nEmerging Sectors\nfor leadership across all technologies specific treatments, particularly in rare of gains in oil and gas are likely to\nThe UAE has set priority sectors \u2013 these\nand sectors. The country will focus diseases. be in performance rather than labor\nwill be the focus of initial activities.\non domains where it can have substitution. This is similar in the\nThis does not mean that the UAE will\nworld-leading assets and unique Cybersecurity: a strategic imperative, redesign of the automotive industry\nstop working on AI solutions in other\nopportunities. given the rise of AI, the UAE will also or the large changes we are seeing to\nsectors where AI can deliver other\nconcentrate on building robust systems consumer marketing techniques.\nbenefits to society. It is also likely that\nTherefore, the mission for this first for protection.\nthese priorities will change over time,\nMinisterial term is to transform the UAE Spending on AI is also a significant\nas the UAE economy matures and new\ninto a world leader in AI by investing in Economic Value of AI economic factor. International Data\nopportunities arise.\nthe people and industries that are key Further details on these choices Corporation estimates annual spending\nto the UAE\u2019s success. are in Objective 2. A core reason for on AI in the Middle East and Africa to\nBut in the first instance, the UAE will\nchoosing these sectors came down to reach AED 419.54 million by 2021,\nleverage physical and digital assets in\nThe UAE will begin through its the potential of AI deployment causing increasing 32% a year.\ntwo of its strongest existing sectors as\nexisting strengths: disruption as well as pure economics,\npart of adopting and trialing AI. Support\n1. Industry Assets & Emerging Sectors for instance the potential of AED Using the UAE\u2019s national statistics,\nwill also be given to developments\n2. Smart Government 136 billion gain in services and trade the absolute opportunity to increase\nin emerging sectors where the UAE\nsectors played a significant role in economic output based on current\nhas the strong potential economic\nAnd also focus on opportunities choosing Tourism as a priority sector. technology capabilities rather than\ngains and where there are pockets of\nwhere it can lead: AI in this growing consumer-facing focusing on annual productivity growth\nopportunity to lead globally.\n3. Data Sharing And Governance sector could likely have spillovers into was calculated3. Assuming automation\n4. New Generation Of Regional Talent other service sectors. AED 91 billion happens to the full extent it can in\nTherefore, the current priority sectors\nin resources and utilities contributed each industry, there is a potential\nare:\nBy 2031, the very best version of the to making energy a priority, as did the gain of AED 335 billion in increased\nUAE would package these strengths AED 19 billion in logistics. economic output for the UAE. This is\nResources & Energy: from existing\nand opportunities together. For the equivalent to 26% increase.\ntechnology in the extraction industry\nexample, early Government adoption of Estimates for global economic gains\nto renewable energy and innovation in\nAI will come with training for domestic from automation technologies - 0.3%\nutilities.\ntalent. Governance frameworks will to 2.2 % growth in compound annual\nbe evaluated by testing them in the productivity \u2013 are impressive2. Using\nLogistics & Transport: longstanding\nUAE\u2019s industry pilots. The existence of this kind of modeling of year on year\nair and sea hubs in the UAE make it\na strong government and government- productivity gains, PwC estimated that\na valuable location for piloting new\nowned commercial sector in the UAE AI will contribute AED 353 billion to\nsystems in the sector.\nprovides novel opportunities for trialing GDP by 2030 (13.6% of GDP).\ngovernance, education and product Gains from increased performance\nTourism & Hospitality: opportunity for\ninnovation in combination. outweigh those that come from\nglobally becoming first in customer-\nreplacing labor with machines in some\nsupport AI, creating integrated and\nsectors, which play a major role in the\npersonalized services for tourists in the\nUAE.\nUAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 10 UAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 11Estimated increase in output due to application of AI 2. Smart Government\nin industries (AED billion in current prices)3 The UAE public sector is already a leader in smart public service delivery:\n\u201cWe initiated electronic services 16 years ago and\ntoday we are launching a fresh stage relying on\nArtificial Intelligence\u2026 we are seeking to adopt\nall tools and methodologies related to Artificial\nOutput increase in current prices, by industry (Billion Dirhams)\n335 Intelligence to expedite and ensure more efficiency\nfor government services at all levels.\u201d\n103 His Highness Sheikh Mohammed bin Rashid Al Maktoum\nUAE Vice President, Prime Minister and Ruler of Dubai\n86\nThe UAE is already taking steps to apply AI in innovative ways across government\n39 39\n\u2013 dynamically adjusting transport timetables to respond to incidents, using AI\n33\nsensors for smart traffic, deploying facial recognition to monitor driver fatigue and\n19\nintroducing chatbots to improve customer service.\n18\nObjective 4 explains how the UAE will take steps to increase the amount of\n14 13\ngovernment experimentation with AI to improve the lives of its citizens.\n05\n03 02\nFinance, Mining Construction Retail Trade Logistics Manufacturing ICT Government Utilities Hospitality Agriculture Total\n3. Data Sharing and Governance\nProfessional Resources and Social\nand Other Services\nServices It is part of the UAE\u2019s ethos to turn ambitious visions into deliverable projects.\nThis connection between big ideas and practical implementation will, become\nan asset in AI policy discussions, that can fall easily into abstract or implausible\nscience fiction. Combining hands-on experience with new technologies and global\npolicy development is a strong way to develop a plausible, positive future for AI.\nIt is almost a coincidence that the PwC estimate for 2030 gain in GDP is similar to the overall\nHow will the UAE ensure AI is used for good?\nincrease in economic output that we estimated. The methods used to develop these estimates are\nPublic debates about AI often focus on whether or not it could take over\ndifferent: the GDP estimate is based on data so far on average annual growth from automation,\nwhereas we calculated the effects of specific technologies in different industries. Data from UAE important human decisions: from whether we go to war, to who receives\nFCSA National Accounts Estimates Tables 12, 13 and 15; Analyzed using the team\u2019s review of AI medication.\napplications and desk research.\nUAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 12 UAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 13There continues to be a range of views 4. New Generation of Out of the job functions reported Learning & Statistical Modeling tools\nabout the prospects of AI, and many on LinkedIn profiles, operations, like Neural Networks5.\nRegional Talent\npotential future scenarios for AI in the information technology and\nA young and growing regional\nUAE societies. There is still time to engineering are ranked 1, 4 and 6 The UAE is also globally competitive\npopulation is often described only\nchange what this future will look like, respectively3. when it comes to the proportion of\nin terms of unemployment. Youth\nmaking it one that more clearly reflects university graduates who study STEM\nunemployment in the Arab countries\nthe UAE\u2019s values. The actions we take There is some evidence that there is a subjects (22% compared to 16% in\nand Middle East was 30.6 % in 2016.\ntoday are still very much under human significant subset of these individuals the US). These graduates already have\nIt remains the highest of any region\ncontrol and can still reflect those already starting to combine technical the base foundational skills relevant\nglobally.\nvalues. skills and business operations. Data to AI (computer science, programming\nfrom jobseekers on the Middle East literacy, and statistical analysis) and\nBut the Middle East and North\nResponding to this opportunity, several and South Asia jobs website, Bayt. so can be rapidly upskilled to become\nAfrica has an unusual segment of\nof the initiatives in this strategy aim to com, shows that business analysts AI-ready.\nthe professional workforce. There is\ndevelop a values-driven approach to AI: with technical skills often already\na high proportion of professionals\nhave sophisticated, AI-relevant skills.\nalready involved in operations, IT and\nThe UAE Government will play a direct 21% of the skills identified among this\nengineering.\nrole in designing and enabling AI group were associated with business\nsystems that create the most value for intelligence software like IBM Cognos,\nIn fact there are giants in the field of\nsociety (objective 4). This will also give Microsoft Power BI or Qlik Sense.\nAI who have come from some of the\nthe UAE practical experience of how 20% were to do with data operations\nmost fragile states in the Middle East.\nthese systems operate and allow the experience, for example with Hadoop\nIyad Rahwan and Oussama Khatib were\ncountry to identify, ahead of time, any or Apache Pig. 8% were Machine\nboth born in Aleppo, Syria. Iyad is now\npotential unintentional consequences.\nthe director and principal investigator\nof the Scalable Cooperation group at 14\nThese schemes and the national pilots\nMIT Media Lab. Oussama is a professor\nin objective 2 will guide the approach\nof computer science at Stanford\nto the governance of AI (objective\nUniversity.\n8). This approach to governance -\nembedded in worked examples - will\nThe UAE offers access to world-leading\nhelp take the UAE beyond abstract\nuniversities and a safe hub for highly\nstatements to useable guidelines for\nskilled professionals to re-skill the\nvalues-driven AI.\nmost in-demand AI roles. The country\nneeds to leverage on its geographic\nThe schemes will advocate these\nposition, and this existing cohort of\nguidelines on a global stage, working\ntalent around it.\nwith other countries and international\ntechnological groups (also objective 8).\nFinally, research that keeps to these\nethical principles will be rewarded\n(objective 6).\nUAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 14 UAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 15\nsnoitarepO selaS pihsruenerpertnE noitacudE gnireenignE tnempoleveD\nssenisuB\nnoitamrofnI ygolonhceT ecnaniF evitartsinimdA ngiseD\n& strA\nsecivreS\nerachtlaeH\nsecruoseR\nnamuH\nlaicoS\n& ytinummoC\nsecivreS gnitnuoccA tcejorP\n& margorP\ntnemeganaM gnitekraM &\naideM\nnoitacinummoC hcraeseR troppuS gnitlusnoC lageL evitcetorP\n&\nyratiliM\nsecivreS etatsE\nlaeR\necnarussA\nytilauQ\ngnisahcruP tnemeganaM\ntcudorP\n2016\nShare of Job Functions in MENA (%)4\n12\n10\n8\n6\n4\n2\n0Share of national graduates that are in\nAI-relevant specializations\n(statistics, mathematics, ICT technologies\nand engineering) for selected countries.6\nUK\n26%\nIndia\nUSA 31%\n16%\nUAE\n22%\nAustralia 18\nEstonia 26\nFrance 25\nIndia 31\nAustralia\nOman 43\nRepublic of Korea 31 18%\nSaudi Arabia 24\nTurkey 20\nUnited Arab Emirates 22\nUnited Kingdom of Great Britain and 26\nNorthern Ireland\nTunisia 44\nUnited States of America 16\nUAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 16 UAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 17EIGHT STRATEGIC\nOBJECTIVES\nAlthough priorities are set out above, there will need to be a complete system\nof support to move from a nation that adopts AI to one that is building and\nexporting it.\nVision\nTo become one of the leading nations in AI by 2031\n1.\nBuild a\nreputation as\nan AI destination.\nEg. UAI brand.\nLeadership\n2. 3. 4.\nIncrease the UAE Develop Adopt AI\ncompetitive assets a fertile across\nin priority sectors ecosystem government\nthrough deployment for AI. services to\nof AI. improve lives. Over time, AI activities will include more significant programs \u2013 from\nEg. Proof-of-concept in Eg. Applied AI Eg. National AI\npriority sectors. accelerator. challenges. funding for proof-of-concepts to a domestic AI Accelerator. The\nAI Activity foundation will also start to grow a new generation of AI-ready talent,\ncomplemented by regular presence from leading global AI researchers\n5. 6. 7. 8.\nAttract and train Bring world-leading Provide the data Ensure\nin the UAE and through playing a leading role\ntalent for future research capability and supporting strong\njobs enabled to work with infracstructure governance in international governance initiatives.\nby AI. target industries. essential to become a and effective\ntest bed for AI. regulation.\nEg. Public AI basic Eg. Key Thinkers Eg. Secure data Eg. Intergovernmental panel Colleagues in the UAE Government, international bodies, educational\ntraining. program intrastructure. on artifical intelligence.\nFoundation institutions and global AI firms will play a significant role in achieving\nsome of these objectives. The Office of the Minister of State for\nArtificial Intelligence (henceforth \u2018AI Office\u2019) will help broker new\nThe UAE\u2019s first steps will build a strong brand through AI activities that partnerships, particularly in education and governance. In particular,\ndemonstrate the UAE as a testbed for AI technology. This will come through the AI Office aims to support other Ministries to make the most of\nbrokering agreements with international firms to base pilots in the UAE (under world-leading AI technologies in their projects and policies, as well as\nobjective 2); it means coordinating access to domestic data systems applications train a generation of AI-ready talent in the UAE.\n(objective 3); and it will require government to take the lead in providing AI- The second half of this report provides more detail of the direction\nenhanced services (objective 4). under each objective. This includes detail of initiatives that are already\nrunning, ones that to start over the next 3 years, as well as examples\nThere are also some early steps to be made in starting to build stronger of successful policies and projects from other nations that could\nfoundations in talent, research, data and governance. Publicly accessible provide templates for UAE policymakers.\nAI courses have already begun with large tech partners (objective 5); and\ninternational discussions on the positive use of AI provide an active platform\nfor better governance of these technologies (objective 8).\nUAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 18 UAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 19OBJECTIVE 1:\nBUILD A REPUTATION AS\nAN AI DESTINATION\nTo become a global AI leader, the UAE This includes a UAI mark recognizing\nneeds to compete with destinations high quality, ethical AI companies. It\naround the world that are also trying would reward safe, efficient, verified\nto attract scarce AI talent and grow AI AI technology with a \u2018UAI Seal of\ninvestment. Boston, London, Beijing, Approval\u2019.\nShenzhen, Toronto and many other\nplaces are all vying to be the \u2018next The UAI will consist of four levels of\nSilicon Valley\u2019 for AI. approval to include Public Sector Level,\nPrivate Sector Level, Institutional Level,\nAchieving this objective will require and Product Level.\na brand that is built on what\ndifferentiates the UAE: its established The certification system is based\nreputation as a bold innovator. on the highest level of world-wide\nstandards that will establish the core\nThis reputation already brings requirements in obtaining the UAE\ncompanies to the country. Seal of Approval. This robust, rigorous,\nSparkCognition, the world-leading AI and comprehensive certification\nfirm, recently announced their first methodology will ensure verifying the\ninternational office outside the US to entities with the best AI technology in\nbe based in Dubai. the region.\nThis objective relies heavily on The UAE is aiming to host key\nachieving the other seven. Those international conferences and forums\nobjectives are necessary but not on AI making it a hub for global experts\nsufficient to deliver the UAE as and entrepreneurs. With this, the UAE\na destination for AI talent and will become the center of AI startups in\ninvestment. the region.\nThere will also need to be a brand Singapore developed a successful\ncampaign that explains and illustrates brand campaign called \u2018Smart Nation\u2019.\nthe UAE AI offering in a compelling and It demonstrated the connection\nauthentic form. This brand will provide between digital innovation and\na practical means to communicate this national priorities, signaling that the\nto the rest of the world. This has been digital revolution is at the center of\nannounced as \u2018UAI\u2019. Singapore\u2019s national strategy. A brand\nidentity was backed up by regular5,\nsubstantive indicators of progress in\nUAI Brand\nthe field.\nThe UAE is developing a UAI brand\nand will use this to attract talent and\nbusiness from across the globe to come\nto the UAE to test and develop AI.\nUAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 20 UAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 21OBJECTIVE 2:\nINCREASE THE UAE\nCOMPETITIVE ASSETS IN\nPRIORITY SECTORS\nTHROUGH DEPLOYMENT OF AI\nAI has the potential to generate up to algorithms to support its operations. opportunity to make the most of these The opportunity in this sector is to\nAED 335 billion in the UAE economy As the UAE makes the transition assets \u2013 facilitating test beds for new create partnerships that can boost\nboosting this by supporting industry to renewable energy supplies and technologies in these locations by tourist numbers in the UAE, where\npilots in sectors where this kind of more efficient water desalination, deploying AI solutions for air traffic those tourists have AI-driven schedules\nintervention will create the most there is also an opportunity for AI management, baggage handling, and or use automated assistants during\neconomic or social value. systems to play a fundamental role in airplane boarding.. their stay.\nenergy sector innovation. There is an\nThis objective details our initial priority opportunity to open up this sector to Demonstrator projects that make the a. Healthcare: Government does not\nsectors. This effort is complemented more companies, and provide support most of these physical and digital own the healthcare industry in the\nby support for government services for proof-of-concept systems developed assets in logistics and transport will be UAE, but it plays a significant role\nin objective 4. In the medium to long first in the UAE. funded. in it. Dubai Health Authority\u2019s new\nterm, these priority sectors could Dubai Genomics program hopes to\nchange. The UAE is planning a proof-of-concept c. Tourism & Hospitality: Tourism bring population-scale whole-genome\nto utilize AI systems in order to focus is a highly visible, successful export sequencing to the Emirate. The aim is\nExisting Assets both internally, to make energy saving sector for the UAE. There is a particular to use the diverse genetic community\nThe largest economic gains from AI will decisions, and globally to understand opportunity to integrate services in the UAE as a resource for new\ncome in significant and mature sectors, supply and demand for oil. attracting tourists to the UAE and the scientific studies, which make it\nwhere the AI potential is also high \u2013 packages that are offered once they easier to predict risks associated with\nfinance, resources, construction and Energy supply and utilities is also an are here, including business travelers genetic-related illnesses. This kind\nretail trade. area of innovation. From smart grids and those on short stopovers. of study, and similar uses of patient\nto water recycling, there also needs data from UAE hospitals, could lead to\nGovernment has a role to play in to be support for small companies and The greatest opportunity in Tourism novel opportunities for digital health\nsupporting industry to achieve these utilities supplies to test and improve & Hospitality comes from innovations innovation based in the UAE.\ngains \u2013 helping industries to develop AI this infrastructure. that have potential for spillover into\nwhere a competitive advantage exists, other customer service sectors. AI can The opportunities will most likely\nincentivizing global AI firms to locate b. Logistics & Transport: The UAE be utilized to predict tourist\u2019s needs be low in number but some could\nin the UAE and local firms to connect is a globally competitive transit hub. and provide customized services. be world-leading or have significant\nglobally, and finally by supporting 60 million people pass through Dubai impact on the care of individuals with\nbusiness more generally with advice Airport each year; 26 million pass Emerging Sectors rare diseases. For example, testing\nto become successful in an AI enabled through Abu Dhabi Airport. Jebel Ali There are three other sectors, diagnostics in a clinic that has access\nworld. port is the largest marine terminal in where the UAE has different kinds to the latest monitoring technology,\nIn three sectors, there are additional the Middle East and provides market of advantage \u2013 these are not about detailed historical patient data and\nnational assets or need for innovation access to over 2 billion people. Airport existing scale but pockets of a diverse population could provide a\nthat make them priorities: and port management companies opportunity that are already visible. rare asset for healthcare companies.\nfrom the UAE continue to expand their There have smaller, but valuable data There is also an opportunity to focus\na. Resources & Energy: The UAE is management of overseas facilities. assets; fast-growth and entrepreneurial on diseases that are prevalent in the\nthe 5th largest exporter of oil in the The UAE\u2019s peninsular location between activities or areas where government region, which receive relatively little\nworld. The existing extraction industry South Asia and East Africa provides are taking a lead in the sector. attention from global pharmaceutical\nalready uses modelling software and an enduring advantage. There is an companies.\nUAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 22 UAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 23The AI Office is most interested in b. Cybersecurity: Historically, the UAE There will be more than 7.5 billion establish UAE\u2019s credentials as a world\nproviding access for companies and has attracted the regional hubs for Internet users by 2030 (90% of leader in AI and act as a catalyst to\nresearchers to hospital and national large technology companies like SAP the projected world population of attract further talent and investment.\ndatabases, where their work could or Microsoft, which often locate to free 8.5 billion). Like street crime, which Focusing efforts in industries with an\ndevelop specialist capability in zones within the city. More recently, historically grew in relation to obvious potential for AI development,\ndiagnostics that use AI, particularly for the UAE has grown or attracted smaller population growth, similar evolution commercialization and export exists\ncommon diseases in the region. cybersecurity firms. of cyber crime is being witnessed with will maximize the likelihood of success\nThere is significant potential benefit projected damage costs to hit USD and the return on investment.\nThe AI Office has funded research for government in developing better 6 trillion annually by 2021. Hence,\ninto developing an AI algorithm for cybersecurity for their own services cyber security is a big investment\ndetecting Tuberculosis in patients via and for making the UAE a secure that requires a priority considering\ndiagnosis of X-ray data and the pilot environment for business. There is also the global shift towards maintaining\nwas launched at the United Nations a strong entrepreneurial segment in safety. Over the next five years, the\nWorld Data Forum 2018. cybersecurity, which the government global spending on cyber security will\nwants to encourage. cumulatively exceed USD 1 trillion7.\nSupporting pilots that demonstrate\nnew cybersecurity approaches in the\nUAE first. There is also interest in\ncommunity-building and skills-building\nprograms for SMEs and local talent\nthat focus specifically on AI as a risk or\nopportunity for cybersecurity.\nProof-of-Concept Support\nin Priority Sectors\nWithin these five priority sectors,\nthe UAE government will fund or\nbroker pilot projects. These proofs-of-\nconcepts could be designed by public\nsector, private sector or consortia.\nFunding will depend on how well\nproposed pilots map onto the reasoning\nfor each priority sector as detailed\nabove. For example, the AI Office is\nworking with various private sector\ncompanies to develop pilots that use\nquantum computing to support health\ndiagnostics and global energy supply\nmanagement.\nDeveloping AI technology in the UAE\nwill help the UAE diversify its economy,\nenhance productivity and find new\nsources of growth. It will also firmly\nUAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 24 UAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 25OBJECTIVE 3:\nDEVELOP A FERTILE\nECOSYSTEM FOR AI\nA combination of funding, knowledge, fund and the UAE Artificial Intelligence\nand strategic support will be needed to and Blockchain Council (see objective\ndevelop a domestic AI ecosystem. This 4) could support companies that\nstarts with better access to local data need access to government data or\ninfrastructure and funding for projects partnerships with Government.\nthat make the most of this which will\nlead to opportunities for building new The AI market is estimated to grow\ncompanies. Once these elements are in to USD 60 billion by 2021, with China\nplace, there needs to be incentives for alone aiming to create a USD 150 Applied AI Accelerator\nworld-leading products and services to billion market by 20308. The UAE\nThe AI Office will support the\nbe developed in the UAE. will need to accelerate domestic AI\ndevelopment of a domestic AI startup\ncommercialization in order to capture\nand product development ecosystem\nThere are difficulties and uncertainties its share of this growing market. There\nthrough incubator funds, mentoring,\nin developing algorithmic services. AI are currently an estimated 2,600 AI-\nand publication of shared knowledge.\nsystems often require coordination focused startups globally, but the vast\nacross several locations, firms or majority of them are in America and\nZeroth.AI, a Hong Kong based AI\nindustries. They also require trusted China, along with economies like the UK\naccelerator, provides USD 120,000\npartners in order to automate products and Japan9.\nin seed capital to companies in the\nand services.\nprogram for exchange for 10% equity\nThe UAE has an opportunity to become\nstake. They also provide mentoring\nGovernments can play an important a competitive regional hub for AI\nand support to get a long-term visa in\ncoordinating role, providing access to entrepreneurs through providing a\nHong Kong. Another example, this time\nnetworks, data and finance that can supportive ecosystem. A developed\nof a government supporting industry\nhelp overcome these barriers. ecosystem of local startups will ensure\ndevelopment, is in AI Singapore\u2019s\nthat AI solutions are catering to the\n100 Experiments project. The project\nmarket needs of the UAE economy,\nAI Network funds researchers and academics\nrather than being reliant on adapting\nIn order to encourage more research, with up to USD 250,000 to work on\nimported ideas and products.\ncollaboration and commercialization industry specific problems for which\nlocal expertise will be aggregated AI technologies may be quickly built,\nthrough the establishment of a network but without the need for time and\nof researchers, industry experts and resource-consuming research.\npolicy experts from across the UAE.\nFunding for AI research and companies\ncould be provided according to priorities\nidentified by the group, backed by\nevidence from a survey of regional AI\nactivities.\nThe Mohammed bin Rashid Innovation\nFund has AED 2 billion to support local\ninnovators. Collaboration between the\nUAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 26 UAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 27AI Incentive Scheme for Overseas Companies Business Support for UAE AI Firms\nGreater FDI by foreign firms will be a key enabler of industry development, Once local systems and companies are in place, the next step is to form more\nbringing technology and skills to the UAE. The scope for greater FDI is real. 70% ambitious UAE presence in global markets.\nof global executives believe technological change will lead to an increase in global\nFDI10. While the UAE is seen as a promising source of FDI, it is not seen as a top Creating global markets often requires investments that are speculative (e.g.\ndestination for FDI investment. Planned relaxation of foreign investment laws and international trips, marketing campaigns) or require coordination\nimproving reputation for ease of doing business should facilitate greater FDI11. (e.g. trade missions, joint ventures). This can often be difficult for individual\nbusinesses to undertake. Similarly, investments in new products require large-\nIncentives will be developed to encourage UAE firms to partner with global AI scale investments, which can carry too much risk for any one investor.\ntechnology firms to foster greater links into global value chains and enable\ntechnology transfer from international firms. The incentives will also motivate Governments can help business solve this problem by offering guidance,\ninternational companies to set up regional offices in the UAE or relocate here. financial support, and by acting as a coordinator. Providing support and guidance\nFor example, a new cyber research center in Stuttgart and Tubingen, Germany will overcome knowledge barriers to developing AI solutions and strengthen\n(the Max Planck Society\u2019s Institute for Intelligence Systems) attracted foreign connections into international markets, increasing exports and growth.\ninvestment from Amazon leading to an estimated 100 jobs over the next five years\nand providing EUR 420,000 per year to fund research students. Foreign investors\nwere driven by locating near this known center of talent, which previously had\nnot engaged with industry partners. Although the UAE does not have this kind of\nstrong AI talent hub, it is building attractors that will grow the technical community\nhere quickly.\nUAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 28 UAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 29OBJECTIVE 4:\nADOPT AI ACROSS CUSTOMER\nSERVICES TO IMPROVE\nLIVES AND GOVERNMENT\nGovernment can play a strong role in ideas will receive funding, mentoring\nmaking sure AI delivers the greatest and access to data. Importantly, the\npublic value, by making citizens safer, program will have the potential to\nhealthier and happier. demonstrate the benefit of AI to the\nUAE population and inspire the nation\nThe UAE also faces significant social to embrace AI to make lives better.\nand economic challenges, where the\noutcomes for the population are poor\nUAE Artificial Intelligence\ncompared to other countries. For\nand Blockchain Council\nexample, high rates of obesity and\nAI can also be used to improve the\nheart disease, high rates of traffic\nexperience and cost of government\nfatalities, poor air quality and poor\ntransactions and services. There will be\neducation outcomes. Using AI to better\nfewer time-consuming administrative\nrespond to these challenges has huge\nprocesses, fewer errors, and more\npotential benefits. There is a role for\nconvenient services.\nGovernment in supercharging this \u2013\nproviding the focus, resources and drive\nBuilding on a successful generation\nto solve these challenges.\nof digital government initiatives,\nthe UAE has an opportunity for\nNational AI Challenges\nglobal leadership. But government\nA single program could be set up to\nentities need support from political\nsupport the best ideas from across\nleaders to move key services \u2013 such\ngovernment, universities and the\nas tax filings, applications, regulatory\nprivate sector, which solve the UAE\u2019s\ncompliance checks, payment of fines \u2013\nmost pressing challenges using AI. In\nto interoperable digital platforms, with\nAustralia, the government recently\nhigh-quality, complete and accessible\nlaunched a National Missions Program,\ndata.\nbeginning with making the nation the\nhealthiest in the world. This included\nThe UAE Artificial Intelligence\na step-change increase in investment\nand Blockchain Council includes\nin national genomics and personalized\nrepresentatives from all emirates\nmedicine capability and its integration\non both federal and local levels. The\ninto medical research and healthcare\nCouncil\u2019s main objective is to identify\nsystems.\nhow and where AI can be incorporated\nin government and what supporting\nIn similar fashion, a nationwide\ninfrastructure it requires.\nprogram to tackle the UAE\u2019s distinctive\nchallenges will be launched. The best\nUAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 30 UAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 31OBJECTIVE 5: Digital Skills in the UAE and UK\nUnskilled Novice Competent Expert\nATTRACT AND TRAIN\nUAE 15 45 35 05\nTALENT FOR FUTURE\nUK 07 37 46 10\nJOBS ENABLED BY AI Unskilled Novice Competent Expert\nA recent study commissioned for the was one in administration i.e. a role Public AI Training PhD places at any one time in AI and\n2018 World Government Summit that may not exist in the future. related disciplines, by 2025. In order\nFree courses are being run for UAE\nin Dubai argued that for six Middle to compete technically on a global\nresidents to raise awareness and\nEast countries, 45% of the existing These predictions could prove scale the UAE must also be ambitious\nunderstanding of AI technologies. The\nwork activities in the labor market inaccurate. Working practices in the in its targets, to that effect, at the\nUAE AI Summer Camp took place in the\nare automatable today based on UAE are often different to US job February 2018 World Government\nsummer of 2018 and it supports the\ncurrent technologies. This average is descriptions, which are the ones Summit, the Minister of State for\nefforts of future knowledge transfer\nslightly below the global average of used to estimate how work can be Artificial Intelligence announced that\nand building a generation capable of\n50%12. The same study shows that automated. At the same time, the the UAE has the intention to produce\nadopting advanced technologies in\nthis risk is higher in sectors when growing youth population in the region world-class AI talent. This will be\ndeveloping solutions for various future\nemployees perform routine tasks like and dominance of job-related visas done through upskilling 1/3 of the\nchallenges. Over 5000 UAE residents\nin manufacturing and transportation. in the UAE could have the greatest UAE\u2019s STEM graduates per year (2000\nreceived specialized training on the\nIn the arts, education and healthcare, effects on the underlying dynamics of students).\nfundamentals of AI with hands-on\nwhere human interaction or creativity is the workforce.\nexperience .\nmore important, the risk is much lower. Given the public sector is a major\nGiven this, there is a significant low- employer and potential user of AI in\nUpskilling Students\nFor the UAE, around 43% of existing skilled population whose job can easily the UAE, The AI Office has also started\nThere is a similar opportunity in the\nwork activities have the potential be changed by automation, but who specific training for government\nstudent population. The UAE has a\nto be automated across key sectors currently have few skills to make the employees.\nsmall student population, but a high\nsuch as administration, government, most of these changes.\npercentage (22%) are in core STEM\nmanufacturing and construction. With Government Training\nareas: ICT , engineering and natural\naround 70% of Emiratis employed in the 40% of the UAE workforce has good\nThe AI Office is offering more advanced\nsciences. Upskilling STEM graduates\npublic sector, retraining of government digital skills14. This is less than the\ncourses for Government employees\nwith specialist courses will provide\nworkers is particularly critical. It has 56% of people with good digital skills\nstarting Q4 2018, focused on skills\nthe fastest short-term solution to\nbeen estimated that almost 300,000 in the UK, the top rated-nation in the needed to work with them being the\nincreasing the number of AI experts.\njobs in the UAE in the Administrative AI-Readiness Index15. AI Experts (ambassadors) in their\nThis upskilling will also provide a\nand Support and Government sector entities. These require participants to\nstronger pipeline of students able to\nmay be impacted by automation, with For most of the population, developing complete a capstone project related\nundertake post-graduate training in AI\naround 125,000 of these jobs held by better digital skills and basic to their current job. The aim is to\nto develop the pool of UAE talent able\nUAE nationals13. This will have a major understanding of AI will help them ensure that 100% of senior leadership\nto build AI systems.\nimpact on the public sector workforce make better decisions in an economy in government - Director-General,\nand needs to be carefully managed, where automation technologies enter Ministerial and Senior-Ministerial levels\nThe United Kingdom has recently\nwith a 2016 survey of Emirati workers the workplace. - are trained and versed in AI, with\nstated an aggressive target of having\nfinding the ideal future role for 54% more junior government employees\nat least 1,000 government supported\nbeing trained on a more ad hoc basis.\nUAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 32 UAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 33This illustration summarizes how skills training Professional Upskilling\nThere is also an opportunity to take professionals with expert digital and analytic skills and\ncould fit together across different segments of\nprovide them with the training needed to become specialists in AI. In the New Generation of\nthe labor market. Regional Talent section of this AI Strategy, the strong segment of professionals in the region\nwith operational and analytical skills was highlighted. It is the AI Office\u2019s aim to help upskill\nthese individuals. Upskilling existing professional workers in the UAE could include specialist-\ntraining, secondments and study tours overseas.\nEmployment Transition Support\nSkills training for 60% of the workforce with low digital skills would benefit from more robust\ndata on current skills in the labor force and current job openings. The AI Office supports the\nAI experts (0.2%)\nWorkers with low Minister of State for Higher Education and Advanced Skills in their efforts to improve this data\ndigital skills Funding secondments collection, and champions efforts to develop a series of career advice tools and services to\nProfessionals\n(60% of the workforce) overseas help current and future workers make more informed choices.\nwith expert\nShort couses on\ndigital skills (5%)\nspecific techniques\nProfessionals with digital\ncompetence (35%)\nCareer advice and\nCourses on basic digital Project-based learning\ntraining services\nand computational opportunities using\nfor those at risk of\nconcepts computational concepts\nautomation\nApprenticeship programmes\nAdult Students\nCareer advice for students\nSummer short courses\nScholarships\nSchools\nUAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 34 UAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 35OBJECTIVE 6:\nBRING WORLD-LEADING\nRESEARCH CAPABILITY\nTO WORK WITH TARGET\nINDUSTRIES\nThis objective is concerned with without the overheads of establishing\nbuilding the wider knowledge a new university-scale institution. The\nproduction in the UAE, including AI Network in Objective 3 will help\nuniversity and commercial R&D. support similar coordination.\nThis will need to include increasing\ninvestment in research and encouraging Even in countries with a well-\nworld-class academics to work in the established research base, AI experts\nUAE. are in short supply and highly\nattractive to industry. For example,\nInvesting in AI R&D capability is a 65% of Google DeepMind research hires\nnecessary first step. The US, France, came from academia16. AI ideas are\nUK and China have embraced strategic still emerging and new technologies\nnational plans to boost AI\u2019s share are still finding their way to industry.\nof its R&D investments. The UAE is Governments are investing heavily in\nranked 35th in the world for overall R&D AI, to supplement but by no means\ninvestments. match significant investment by private\ncompanies.\nThere are researchers in UAE\ninstitutions developing or modifying The US and China are world-leaders in\nalgorithmic or automated technologies. developing domestic research capacity.\nTo provide a targeted boost to R&D in This dominance is visible in AI research\nAI, the focus will be on supporting and output, where the countries also\nexpanding the research of this small produce the most number of original\ncommunity. research papers on AI17.\nThe UK\u2019s national institute for data Countries with fewer researchers are\nscience and artificial intelligence was still able to have research impact by\ncreated as a partnership between building capacity in strategic areas.\ncenters of excellence at existing Countries like Canada and Spain have\nuniversities. Five founding universities already developed hubs in AI-related\n\u2013 Cambridge, Edinburgh, Oxford, UCL research.\nand Warwick \u2013 and the UK Engineering\nand Physical Sciences Research Council\ncreated The Alan Turing Institute in\n2017. It provides coordination and\nsupport for the research community,\nUAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 36 UAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 37Percentage of population working as\nfull time researchers in science,\nThe UAE is a young country that has Objective 3 will provide the platform for\ntechnology, and innovation18. not yet established a strong academic this R&D network.\ntradition to provide a pipeline of world\nclass researchers. It will need to look\nKey Thinkers Program\nfor other ways to access research\nKorea 0.69% A program to attract key AI thinkers\ntalent. With more than half the world\u2019s\nto visit the UAE will be initiated.\nSingapore 0.66% population just a five hour flight away, These key AI thinkers will participate\nthe UAE is in a prime position to attract\nin workshops and lectures with local\nNorway 0.59% global research talent to visit the UAE\nuniversities and businesses.\nto help build capacity and share their AI\nNetherlands 0.45% Key Thinkers will also be provided with\nknowledge.\nincentives to run research projects in\nUK 0.44% partnership with these local bodies.\nShort-term opportunities for leading AI\nGermany 0.43% professors to work and experience the\nIn line with objective 4, improving\npotential in the UAE may also support\nCanada 0.43% lives, and objective 8, good governance\nthe UAE to attract leading professors\nof AI, The AI Office will also want to\nUSA 0.42% in the medium to longer term, and recognize and reward AI research with\ndevelop UAE university capacity.\nthe greatest value to society. An\nAustralia 0.41% Saudi Arabia\u2019s Center for Complex\naward for programs with outstanding\nEngineering Systems is a partnership\nFrance 0.40% governance frameworks or the greatest\nbetween the Saudi Government\nsocial impact would be a helpful\nNew Zealand 0.38% and MIT, creating a flow of expert incentive.\nacademics to Saudi Arabia.\n0.32%\nGreece\nAI Library\nRussia 0.31% National Virtual AI The research gap can be closed if the\nInstitute benefit of research can be shared\nSpain 0.26%\nTo ensure this increase in investment with those who have an interest in AI.\nItaly 0.20% is well targeted, the AI Office will In order to boost further innovation,\nsurvey current local R&D capacity. This the AI Office will work on creating an\nUAE 0.19% will help identify options for what is open-access digital library of research\nrequired and how best to boost R&D and papers in both English and Arabic.\nChina 0.12%\nthat can be directly applied to industry, This will be a first-of-a-kind initiative\nBrazil 0.07% providing a medium-longer term to boost the research sector in the\nsolution to addressing the UAE\u2019s R&D region. The UAE will also endeavor to\nSouth Africa 0.04% gap. Following the survey, the UAE will create accessible summaries of UAE\nlaunch a National Virtual AI Institute government funded AI research and\n0.02%\nIndia\nwith stakeholder partners to aggregate programs in order to help encourage\nKuwait 0.01% the best local and global expertise the development of AI solutions. The\nin the region, and to encourage AI Library will be a joint collaboration\nmore R&D activity, collaboration and between academia, industry and\ncommercialisation. The AI Network in government.\nUAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 38 UAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 39OBJECTIVE 7:\nPROVIDE THE DATA\nAND SUPPORTING\nINFRASTRUCTURE\nESSENTIAL TO BECOME\nA TEST BED FOR AI\nGovernments around the world are invest into creating a robust data general practice and cancer registries.\nincreasingly recognizing the value of infrastructure. While designed to handle health data,\nthe vast data sets they collate. Machine The UAE\u2019s ambition is to create a it is now being used by other agencies\nlearning models need access to training data-sharing program, providing shared with sensitive data, e.g. the Australian\ndata sets, and open data can also be open and standardized AI-ready data, Taxation Office and the Australian\nused to test and improve AI systems\u2019 collected through a consistent data Department of Social Services. SURE\nperformance. standard. offers a data repository service, where\na user can purchase secure, hosted\nThe UAE has taken steps towards The X-Road platform in Estonia space for multiple datasets and projects\nopening data to improve transparency, supports access and combination of and set their own data governance\nbut still significantly lags behind other government and private databases, framework, if approved by SURE. SURE\ncountries in the number of open data setting the stage for the application also offers single project workspaces\nsets it releases. 537 datasets are of machine learning tools. The data where SURE manages both the data\ncurrently available, whereas Turkey solution saves citizens over 800 years governance and technical aspects\nhas shared 1,280 and Canada has over of working time per annum. of hosting. In these cases, a user\n10,000. must seek research ethics committee\nSecure Data approval for the research before a\nData Sharing workspace will be granted.\nInfrastructure\nThe UAE has an opportunity to become\nA secure data infrastructure will be\nBeyond national datasets, there\na leader in available open data for\nnecessary to facilitate data sharing,\nis also a need for data protection\ntraining and developing AI systems.\nand manage privacy concerns. Investing\nand authentication as part of good\nThe greatest advantage that the\nin a single AI data infrastructure makes\ncorporate practice in the UAE. As the\nUAE has is in its diversified culture,\nit easier to do this efficiently, and\nconsultative group under objective 8\nwith more than 200 nationalities\nmakes it simpler to access data relevant\ndevelops, they will begin to address\nresiding in the UAE. Given the unique\nto research or developing new products\nthese issues. Europe\u2019s General Data\nmixture of cultures in the UAE, the\nand services.\nProtection Regulation includes new\ndata sets that the country holds is\nrights for consumers; it provides\nimpeccable. This data in combination\nSome countries have already\nan opportunity to re-consider how\nwith machine learning can aid in\nexperimented with virtual data\nconsumer data is handled, even for\naccurately diagnosing diseases such\nlibraries. Australia\u2019s SURE (Secure\ncustomers who are not European\nas Tuberculosis (TB) using Artificial\nUser Research Environment) allows\ncitizens.\nIntelligence. The UAE realizes that\nresearchers to access data in hospitals,\nthe oil of the future is data and will\nUAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 40 UAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 41OBJECTIVE 8:\nENSURE STRONG\nGOVERNANCE AND\nEFFECTIVE REGULATION\nThe speed of developments in AI is a digital disruption. as well as ethics-by-design training Intergovernmental Panel\nchallenge for governance. With vast Cyberwarfare capability will continue for tech developers to build ethical\non Artificial Intelligence\nresearch efforts around the globe, it to grow, meaning that cybersecurity considerations into their projects.\nA natural evolution of the Roundtable\nis difficult to ensure this technology will become increasingly important.\nis in the formulation of an\nis developed in a safe and ethical In the absence of a coherent national Globally, the UAE has begun work on a\nintergovernmental body, dedicated\nenvironment. strategy, cybersecurity would be number of initiatives to help develop\nto providing a mechanism for\ndeveloped on an ad hoc basis. This responsible AI.\ngovernments and private companies to\nAs Governments and leading AI is inefficient and risks leaving gaps.\nbetter understand AI and its impact on\nthinkers around the world grapple This will be addressed through the During the World Government Summit\nsocieties in order to help give a solid\nwith this challenge, there is an governance review. in February 2018, over 100 leading\nframework for future regulation, in a\nopportunity for the UAE to learn from experts at the inaugural Global\nmore tangible and enforceable manner.\nthe best and collaborate with others National Governance Governance of Artificial Intelligence\nIn March 2018, President Macron, at\nto ensure effective governance and Roundtable were hosted. This\nReview the launch of the French Artificial\nregulation of AI, both domestically and collection of AI experts debated how\nThe UAE Artificial Intelligence and Intelligence Strategy, announced\ninternationally. governments could best navigate the\nBlockchain Council will add to its remit a desire to establish an \u201cIPCC for\nchallenges posed by the rapid rise of AI.\nto review national approaches to issues Artificial Intelligence\u201d \u2013 referring to the\nThe UAE has the ambition to take a\nsuch as data management, ethics and Intergovernmental Panel on Climate\nleading role in the development of Second Global\ncybersecurity. They will also review the Change. The UAE has expressed\nresponsible AI and advancing the\nlatest international best practices in Governance of Artificial a desire to work with France and\nregulation of AI. For example, the UAE\nhas an important contribution to make legislation and global risks from AI. Intelligence Roundtable other governments in creating the\nFurthermore, the Council will ultimately foundations for such a body. The AI\nto this global discussion by connecting For the 2019 World Government\noversee the implementation of the AI Office is actively working toward\nabstract discussions to pilots run by, or Summit, The AI Office is working with\nStrategy in the UAE. making this happen.\nin partnership with government. UNESCO, OECD, IEEE and the Council\non Extended Intelligence in identifying\nOther countries have developed\nThis will also mean working to make the foremost experts and themes to\nadvisory structures that combine\nsure the UAE has the legal environment explore. All of these working groups\nexpertise in technical fields and\nto support innovation in general will then present their outcomes at a\nregulation. The 2016 White House\nand the adoption of AI in particular. High Level Ministerial Panel composed\nArtificial Intelligence Strategy formed\nInnovations in AI technology often of the world\u2019s foremost Minister\u2019s of\na standing committee consisted of\nrequire rapid changes in regulatory Digital, Technology and ICT who are\nregulators and industry experts.\nsettings and can create risks to society. responsible for the development and\nCalifornia\u2019s Little Hoover Commission\nThe adoption of interconnected data use of AI in their countries.\nis currently studying the impact of\nsystems and the growing dependence\nAI on regulatory settings through a\nof major industries on software also\ncommittee of experts. France has\nmakes an economy more vulnerable to\ncreated a national AI ethics committee,\nUAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 42 UAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 43CONCLUSION REFERENCES\n1: Futurism.com, \u201cAn Inside Look at the First Nation With a State Minister for Artificial\n\u201cWe, in the UAE, have no such word as \u201cimpossible\u201d;\nIntelligence\u201d (2017).\nit does not exist in our lexicon. Such a word is used 2: McKinsey & Company, \u201cThe Future of Jobs in the Middle East\u201d (2018). SOURCES:\nNicholas Crafts, \u201cSteam as a general purpose technology: A growth accounting\nby the lazy and the weak, who fear challenges\nperspective,\u201d Economic Journal, volume 114, issue 495, April 2004; Mary O\u2019Mahony\nand progress. When one doubts his potential and and Marcel P. Timmer, \u201cOutput, input, and productivity measures at the industry level:\nThe EU KLEMS database,\u201d Economic Journal, volume 119, issue 538, June 2009; Georg\ncapabilities as well as his confidence, he will lose the Graetz and Guy Michaels, Robots at work, Centre for Economic Performance discussion\npaper 1335, March 2015; McKinsey Global Institute analysis.\ncompass that leads him to success and excellence,\n3: Data from UAE FCSA National Accounts Estimates Tables 12, 13 and 15.\nthus failing to achieve his goal.\u201d\n4: LinkedIn data, analyzed for: World Economic Forum, \u201cThe Future of Jobs and Skills in\nthe Middle East and North Africa Preparing the Region for the Fourth Industrial\nRevolution\u201d (2017).\nHis Highness Sheikh Mohammed bin Rashid Al Maktoum\nUAE Vice President, Prime Minister and Ruler of Dubai 5: Analysis by Bayt.com data librarians.\n6: UNESCO Institute for Statistics, latest data for each nation (2015 onwards).\n7: Cybersecurity Ventures Official Annual Cybercrime Report.\nThe UAE is unlike any other country in its diversified population, comprising of\nunique talents \u2013 we aim to give this human potential the best opportunities to 8: Forrester, \u201cPredictions 2017: Artificial Intelligence Will Drive The Insights Revolution\u201d\n(2016) and Government of China, \u201cA Next Generation Artificial Intelligence\nnourish and flourish. Given this human potential, the UAE has always aimed at not\nDevelopment Plan\u201d (2017).\njust being better, but to become the best.\n9: Tencent, \u201cGlobal AI Talent White Paper\u201d (2017).\nThe AI National Strategy is a cornerstone of the UAE Centennial 2071 and is a 10: UNCTAD, IPA survey (2017).\nmajor variable in the overall equation. It will bring transformation to a new level 11: World Bank, Ease of Doing Business Index (2018).\nby 2031 and set the foundations for future generations in the UAE to become the\n12: McKinsey & Company, \u201cThe Future of Jobs in the Middle East\u201d (2018).\nbest.\n13: Ibid\nAs one of the first movers in paving the path for AI nationally, a plethora of 14: Digital skills classifications based on our team\u2019s analysis of occupations and\nclassifications with data from UAE Labor Force Survey. Additional data from the UK\nchallenges is certain to arise, but we are true believers in that nothing is\nONS Labor Force Survey and Digital Skills Taskforce and McKinsey & Company, \u201cThe\nimpossible in the UAE. We are a country that is known for tackling challenges\nFuture of Jobs in the Middle East\u201d (2018). 2030 projection based on estimated\nhead on, creating new opportunities, and deploying innovative solutions. historical rate of change.\n15: Oxford Insights, AI readiness Index (2017).\nAs Minister of State for Artificial Intelligence, I aim to catalyze the responsible\n16: Nature, \u201cAI talent grab sparks excitement and concern\u201d (2016).\ndevelopment of AI within our country, in order to help us reach the UAE Centennial\n2071 \u2013 and to act as an inspiration for other nations to harness this technology 17: Based on analysis of Elsevier/ Scopus data. Times Higher Education, \u201cCountries and\nUniversities leading on AI research\u201d (2017).\nfor the betterment of humankind.\n18: Based on latest UNESCO data. 2015 data for majority of countries.\nUAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 44 UAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 45www.ai.gov.ae\nUAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 46", "metadata": {"country": "UAE", "year": "2017", "legally_binding": "no", "binding_proof": "None", "date": "01/10/2024", "regulator": "UAE Council for Artificial Intelligence and Blockchain, Ministry of Artificial Intelligence, Digital Economy and Remote Work Applications", "type": "national strategy", "status": "active", "language": "English, Arabic", "use_cases": "[1, 2, 3, 4, 5, 6]"}}
{"_id": "68695a475520a4cc7989de76", "title": "AI Ethics Principles and Guidelines", "source": "https://ai.gov.ae/wp-content/uploads/2023/03/MOCAI-AI-Ethics-EN-1.pdf", "text": "AI ETHICS\nARTIFICIAL INTELLIGENCE ETHICS\nPRINCIPLES & GUIDELINES\nPRINCIPLES & GUIDELINES\nDEC 2022Table of Content\nPage Page\n1. Overview 4 3.4 Explainable 29\n1.1 Introduction 5 3.4.1 Principle 29\nWe will make AI systems as explainable as technically possible\n1.2 Scope 6 3.4.2 Guidelines 29\n1.3 Strategic alignment 6 3.5 Robust, Safe and Secure AI 31\n1.4 Responsibility 8 3.5.1 Principles 31\nAI systems will be technically robust\n3.5.2 Guidelines 32\n1.5 Licensing 8\n3.6 Human Centred AI 34\n1.6 Toolkit 8\n3.6.1 Principles 34\nWe will give AI systems human values and make them beneficial to society\n3.6.2 Guidelines 35\n2. Definitions 10\n3.7 Sustainable and Environmentally Friendly AI 38\n3.7.1 Principle 38\n3. Principles and Guidelines 16\nWe will promote sustainabity and environemntly friendly AI 38\n3.7.2 Guidelines 38\n3.1 Fairness 17\n3.8 Privacy Preserving AI 38\n3.1.1 Principle 17\nWe will make AI systems fair\n3.8.1 Principle 40\n3.1.2 Guidelines 20\nWe will respect people\u2019s privacy\n3.8.2 Guidelines 40\n3.2 Accountable AI 20\n3.2.1 Principle 20\nWe will make AI systems accountable\n4. Bibliography 44\n3.2.2 Guidelines 20\n3.3 Transparent AI 26\n3.3.1 Principle 26\nWe will make AI systems transparent\n3.3.2 Guidelines 26\n2 301 1.1 Introduction\nAI\u2019s rapid advancement and innovation potential across a range of fields is incredibly exciting.\nYet a thorough and open discussion around AI ethics, and the principles organizations using this\nOverview\ntechnology must consider, is needed.\nThis document - AI Ethics: Principles and Guidelines \u2013 intends to meet this need to balance these\ntwo central considerations of AI. It is designed to offer detailed guidance to help AI actors adhere\nto eight principles of AI ethics.\nWe will make AI systems that are:\nexplainable transparent accountable fair\nprivacy preserving sustainable and human centered robust, safe and\nenvironmentally secure\nfriendly\nThe guidelines are non-binding, and are drafted as a collaborative multi-\nstakeholder effort, with full awareness of organizations\u2019 needs to innovate\nand protect their intellectual property. This is a collaborative process in\nwhich all stakeholders are invited to be part of an ongoing dialogue. We\nwould like to see the AI Ethics Guidelines evolve into a universal, practical\nand applicable framework informing ethical requirements for AI design and\nuse. The eventual goal is to reach widespread agreement and adoption of\ncommonly agreed policies to inform the ethical use of AI nationally and\naround the world.\n4 51.2 Scope Additionally, the AI Office is currently developing additional policies to be taken into consideration\nalong with this document:\nThis document gives non-mandatory guidelines for achieving the ethical design\nand deployment of AI systems in both the public and private sectors. AI already\nAI Seal AI Procurement Guidelines\nsurrounds us, but some applications are more visible and sensitive than others.\nThe UAE AI Seal brand (UAI) will be used The AI Procurement Guidelines will be\nThis document is applicable only to those AI systems which make or inform\nto attract talent and business from used to guide Federal Government entities\n\u2018significant decisions\u2019 \u2013 that is, those decisions which have the potential for\nacross the globe to come to the UAE to in the procurement of AI systems. They\nsignificant impact either on individuals or on society as a whole. They also apply to\ntest and develop AI. This includes a UAI will list the general principles, proposed\n\u2018critical decisions\u2019, which are a subset of significant decisions and are of especially\nmark recognizing high quality, ethical AI mechanisms for procurement and review\ncritical nature.\ncompanies. It would reward safe, efficient, of AI products between the federal\nverified AI technology with a \u2018UAI Seal government and vendors. The policy\nThis document guides the further developments of sector specific requirements\nof Approval\u2019. The UAE AI Seal brand is considers R&D for specific challenges\nand principles, hence every entity within its sector can further amend, use, or\ncurrently under development by the AI that do not currently have an AI solution\ncomplement its current frameworks to properly suit its context and the need of\nOffice. product that is market-ready. Based on\naffected stakeholders.\nglobal standards for AI Procurement, the\npolicy aims to incentivize AI adoption\nIt is a living document and will undergo further future reviews and enhancements.\nwithin government entities and promote\nAI R&D within the private sector for\nspecified government challenges. The\nprocurement policy is currently under\ndevelopment by the AI Office.\n1.3 Strategic alignment\nThis document comes as a fulfillment of the goals and aspirations set in the UAE National AI\nStrategy and the international Sustainable Development Goals (SDGs). The UAE has long been\nat the forefront of sustainable development. These shared global milestones have created global\nmovement and adoption in several disciplines and sectors. As an early adopter, the UAE has\nestablished a national committee on SDGs with a long-term realization of the UAE SDG agenda\n2030. These goals are further engrained and reflected in the AI Ethics Principles under the topics of\nfairness, inclusiveness, equality, human benefit, and rights.\n6 71.4 Responsibility\nThe AI Office will not be responsible for any misuse of the AI Ethics Principles and Guidelines. The\nuser bears all the consequences of their use.\n1.5 Licensing\nThis document is published under the terms of a Creative Commons Attribution 4.0 International\nLicense in order to facilitate its re-use by other governments and private sector organizations.\nIn summary this means you are free to share and adapt the material, including for commercial\npurposes, provided that you give appropriate credit to the UAE AI and Blockchain Council as its\nowner and do not suggest the Minister of State for Artificial Intelligence, Digital Economy and\nRemote Work Applications Office endorses your use.\n1.6 Toolkit\nThe document is part of a toolkit of self-governing nature that boosts awareness and enables\ngovernment and private institutions to pursue innovative use cases while maintaining human\nvalues and principles.\n8 92.1 AI developer organization\n02\nAn organization which does any of the following:\na determines the purpose of an AI system;\nDefinitions\nb designs an AI system;\nc builds an AI system, or:\nd performs technical maintenance or tuning on an AI system\nNote:\nThe definition applies regardless of whether the organization is the ultimate user of the system, or\nwhether they sell it on or give it away.\nExample: A company develops an artificially intelligent facial recognition system and\nsells it to a country\u2019s border force, who use it to identify suspicious personnel. The company\nis an AI developer organization and the border force is an AI operator organization.\n2.2 AI operator organization\nAn organization which does any of the following:\na uses AI systems in operations, backroom processes or decision-making;\nb uses an AI system to provide a service to an AI subject;\nc is a business owner of an AI system;\nd procures and treats data for use in an AI system; or\ne evaluates the use case for an AI system and decides whether to proceed.\nNotes:\n1. This definition applies regardless of whether the AI system was developed in-house or procured.\n2. It is possible for organizations to be both an AI developer organization and an AI operator organization.\n10 112.6 Critical decision\n2.3 Artificial Intelligence (also AI)\nThe capability of a functional unit to perform functions that are generally associated with human An individually significant decision which is deemed to either have a very large impact on an\nintelligence such as reasoning, learning and self-improvement1. individual or to have especially high stakes. These can be especially sensitive, have the potential\nto cause high loss or damage, be societally significant, or set an important precedent.\n2.4 Artificially Intelligent System (also AI system)\nNotes:\nThe types of decisions referred to here are the same as those in the definition of significant-at-\nA product, service, process or decision-making methodology whose operation or outcome is\nscale decisions, except in this case the effects are felt as a result of an individual decision rather\nmaterially influenced by artificially intelligent functional units.\nthan an aggregate of many decisions..\nNotes:\n1. It is not necessary for a system\u2019s outcome to be solely determined by artificially intelligent\nfunctional units in order for the system to be defined as an artificially intelligent system; and Example: A court determines whether a defendant is guilty of a criminal charge, with the\n2. A particular feature of AI systems is that they learn behavior and rules not explicitly punishment for guilt being a life sentence. This is a critical decision because it has a very\nprogrammed in. large impact on the life of the defendant and also sets precedent for similar cases in the\nfuture.\nExample 1: A small claims court uses Example 2: A government entity uses\n2.7 Functional Unit\nan artificially intelligent software package a chatbot which allows customers to ask\nto collect evidence pertaining to a case, routine questions, book appointments\nEntity of hardware or software, or both, capable of accomplishing a specified purpose3 .\ncompare it to similar cases in the past, and conduct minor financial transactions.\nand present a recommended decision The chatbot responds to customer\nto a judge. The judge determines the queries with pre-written responses and is\n2.8 Individually significant decision\nfinal outcome. This decision-making based on pre-programmed decision rules.\nA decision which has the potential for significant impact on at least one individual\u2019s\nmethodology is materially influenced Therefore, the chatbot is not an AI system.\ncircumstances, behavior or choices, or has legal or similarly significant effects on him or her.\nby an artificially intelligent functional If, however, the chatbot autonomously\nunit, and is therefore classified as an AI adjusted its treatment of customers\nsystem. based on the outcome of past cases, it\nwould be an AI system. Example: A company decides to make an employee redundant. This is an individually\nsignificant decision because of its potential impact on the employee\u2019s financial situation.\n2.5 Bias (of a system)\nInclination or prejudice for or against one person or group, especially in a way considered to be\nunfair2.\n2 Oxford Dictionary 2018, Oxford University Press, viewed online 4th October 2018, <https://en.oxforddictionaries.com>\n1 Consistent with ISO/IEC 2382:2015 3 From ISO/IEC 2382:2015\n12 132.9 Non-operational bias (of a system) 2.11 Significant decision\nBias that is either: A decision which is either individually significant or is part of a set of significant-at-scale\ndecisions.\na not a design feature; or\nb not important in achieving the stated purpose of the system. 2.12 Subject of an artificially intelligent system (also AI subject)\nA natural person who is any of the following:\n2.10 Set of significant-at-scale decisions\na an end-user of an AI system\nA set of decisions made by the same system or organization which, when taken in aggregate,\nb directly affected by the operation of or outcomes of an AI system, or:\nhave significant impact on society as a whole or groups within it.\nc a recipient of a service or recommendation provided by an AI system\nNotes:\n1. The decisions need not be individually significant in order to qualify, in aggregate, as a set of\nsignificant-at-scale decisions; and\n2. Examples of areas which have a large impact on society and which include but are not limited\nto: the large-scale allocation of resources or opportunities amongst groups; the structure of\ngovernment; the division of power between large entities or groups; the law, and its interpretation\nand enforcement; conflict and war; international relations, etc.\nExample: An AI system is used by a website to determine which content to show users.\nThis decision is not individually significant, since a user is not greatly affected by whether\na particular piece of media is shown to them. However, if the website is popular then the AI\nsystem may be making a set of significant-at-scale decisions, because any biases in the\nsystem will affect a large number of users.\n14 153.1 Fairness (Principle 1)\n03\n3.1.1 Principle\nPRINCIPLES AND We will make AI systems fair\nData ingested should, where possible, be accurate and representative of the affected\npopulation.\nGUIDELINES\nAlgorithms should avoid non-operational bias.\nSteps should be taken to mitigate and disclose the biases inherent in datasets.\nSignificant decisions should be provably fair.\nAll personnel involved in the development, deployment and use of AI systems have a role and\nresponsibility to operationalize AI fairness and should be educated accordingly.\n3.1.2 Guidelines\n3.1.2.1\nBenefits of AI systems should be available and accessible to all. AI systems should not discriminate\nagainst people or groups in a way that could have an adverse impact. Especially significant and\ncritical decisions made or assisted by AI should be provably fair.\na Considering that fairness can have many different definitions, an organization should\ndocument its own definition of fairness for the context that the AI system is going to be\nimplemented in. Organizations should document what the implemented fairness objective\nstands for and why this choice was considered the most suitable for the given scenario.\nb It is recommended to identify and document demographic groups that may be adversely\nimpacted and mitigate the risk where possible.\nc AI developers and AI operators could consider formal procedures such as Discrimination\nImpact Assessments as a means of ensuring fairness. This assessment should be\ndocumented as well as the mitigation measures that were implemented by the organization.\nThe impact assessment should be conducted pre-release and regularly after as an ongoing\nevaluation, results are advised to be documented.\n16 173.1.2.2 3.1.2.3\nConsideration should be given to whether the data ingested is accurate and representative Consideration should be given to whether decision-making processes introduce bias\nof the affected population\na When subjecting different groups to different decision-making processes, AI developers\nd Fairness has many different definitions in different cultures and for different contexts. should consider whether this will lead to non-operational bias.\nEncouraging a diverse and inclusive AI ecosystem is thus all the more crucial to ensure\nthat one definition of fairness does not contradict another, and that the process of defining b When evaluating the fairness of an AI system, AI developers and AI operators should\nfairness itself is fair, with under-represented groups represented in the discussion. consider whether AI subjects in the same circumstances receive equal treatment.\ne AI developers and AI operators should undertake reasonable data exploration and/or testing\nto identify potentially prejudicial decision-making tendencies in AI systems arising from\nExample: An organization uses an AI tool to automate the pre-screening of candidates\ndata inaccuracy and biases in the data.\nfor a job opening. It is trained on data from the company\u2019s existing employees, the majority\nof whom are from the same ethnic background. Therefore, the system learns to use name\nf AI developers and operators should evaluate all datasets to assess inclusiveness of and nationality as discriminating factors in filtering job applicants. This could have been\nidentified demographic groups and collect data to close any gaps.\nidentified through testing and rectified by, for example, balancing the training data or only\nusing relevant data fields for training.\ng AI developers and AI operators should refrain from training AI systems on data that is not\nlikely to be representative of the affected AI subjects, or is not likely to be accurate, whether\nthat be due to age, omission, method of collection, or other factors.\n3.1.2.4\nSignificant decisions informed by the use of AI should be fair\nExample: Following a natural disaster, a government relief agency uses an AI system to a AI developers and AI operators could consider formal procedures such as Discrimination\ndetect communities in greatest need by analyzing social media data from a range of websites. Impact Assessments as a means of ensuring fairness.\nHowever, those communities where smartphone penetration is lower having less presence on\n3.1.2.4\nsocial media, and so are at risk of receiving less attention. Therefore, the charity complements\nAI operators should consider whether their AI systems are accessible and usable in a fair\ntheir AI tool with traditional techniques to identify needy populations elsewhere.\nmanner across user groups\n3.1.2.5\nh AI developers should consider whether their AI systems can be expected to perform well Consideration should be given to the effect of diversity on the development and deployment\nwhen exposed to previously unseen data, especially when evaluating people who are not processes\nwell-represented in the training data.\nb Organizations should seek to include people from diverse demographic backgrounds across\nthe full lifecyle, to include design, development and deployment processes. Organizations\nshould seek to engage diverse internal and external groups also.\nc AI developers should consider whether the assumptions they make about AI subjects could\nbe wrong or are likely to lead to non-operational bias; if so, they should consider consulting\nthe AI subjects in a representative manner during the design, development and deployment\nto confirm these assumptions.\n18 193.2 Accountable AI (Principle 2)\n3.2.2.2\nPositive efforts should be made to identify and mitigate any significant risks inherent in the AI\nsystems designed\n3.2.1 Principle\na AI operators should only use AI systems that are backed by evidence-based academic or\nindustrial research, and AI developers should base their development on such research.\nWe will make AI systems accountable\nAccountability for the outcomes of an AI system lies not with the system itself but is\napportioned between those who design, develop and deploy it; Example: A foreign country has a government service which identifies parents who owe\nmoney in child maintenance. The data matching process is often incorrect due to misspelled\nDevelopers should make efforts to mitigate the risks inherent in the systems they design; names or missing data which results in some individuals being incorrectly targeted\nautomatically by the system with the result being a large bill, poor credit ratings and even\nAI systems should have built-in appeals procedures whereby users can challenge significant\nfreezing wages. The recourse for individuals who are incorrectly targeted is time-consuming\ndecisions;\nand not straightforward . If the potential impact of incorrect decisions had been assessed,\nmitigation measures such a user-friendly review procedure could have been set up.\nAI systems should be developed by diverse teams which include experts in the area in which\nthe system will be deployed; and\nAI systems should be subject to external audit and decision quality assurance. b AI operators should identify the likely impact of incorrect automated decisions on AI\nsubjects and, in the case where incorrect decisions are likely to cause significant cost or\ninconvenience, consider mitigating measures.\n3.2.2 Guidelines\nc AI operators should consider internal risk assessments or ethics frameworks as a means to\n3.2.2.1 facilitate the identification of risks and mitigating measures.\nAccountability for the outcomes of an AI system should not lie with the system itself\nd In designing AI systems to inform significant decisions, AI developers should consider\na Accountability for loss or damages resulting from the application of AI systems should not measures to maintain data accuracy over time, including:\nbe attributed to the system itself.\n\u2022 the completeness of the data.\nb AI operators and AI developers should consider designating individuals to be responsible for \u2022 timely update of the data, and.\ninvestigating and rectifying the cause of loss or damage arising from the deployment of AI\n\u2022 whether the context in which the data was collected affects its suitability for the\nsystems.\nintended use case.\nExample: A border camera scanning for predictors of risk may misinterpret a \u201ctic\u201d of an\nindividual with Tourette syndrome as suspicious. These can manifest in a diverse fashion, and\nshould not cause this person to undergo secondary inspection every time they pass through\nthe border . If the data is updated after the first case is encountered then it would avoid\ncausing inconvenience on subsequent visits.\n5 Government of Canada. (2018). Responsible AI in the Government of Canada. Digital Disruption White Paper Series. Version 2.0, p.26. Retrieved from:\nhttps://docs.google.com/document/d/1Sn-qBZUXEUG4dVk909eSg5qvfbpNlRhzIefWPtBwbxY/edit\n4 Cabinet Office (UK), Data Science Ethical Framework, Version 1.0, licensed under the Open Government License v3.0, p. 13 6 Stoica, I. et. al.,2017, A Berkeley View of Systems Challenges for AI, p. 2, https://arxiv.org/pdf/1712.05855.pdf\n20 21e AI developers and AI operators should tune AI models periodically to cater for changes to e Be mindful that there might be fundamental tensions between different principles and\ndata and/or models over time. requirements. Continuously identify, evaluate, document and communicate these trade-\noffs and their solutions.\nf AI operators should consider whether AI systems trained in a static environment will display\nmodel instability when deployed in dynamic environments. Adopt a Trustworthy AI assessment list when developing, deploying or using AI systems,\nf\nand adapt it to the specific use case in which the system is being applied.\nExample: AI systems will need to be able to adapt to the changes in the environment that g Keep in mind that such an assessment list will never be exhaustive. Ensuring Trustworthy\nthey are deployed in. For example, a self-driving car would need to quickly adapt to unexpected AI is not about ticking boxes, but about continuously identifying and implementing\nand dangerous road by learning in real time from other cars that have successfully dealt with requirements, evaluating solutions, ensuring improved outcomes throughout the AI\nthese conditions. In addition, such mission-critical applications must handle noisy inputs system\u2019s lifecycle, and involving stakeholders in this.\nand defend against malicious actors .\n3.2.2.4\nAI subjects should be able to challenge significant automated decisions concerning them and,\ng AI developers should collaborate with AI Operators to train models using historical data where appropriate, be able to opt out of such decisions\nfrom the Operator.\na AI operators using AI systems to inform significant decisions should provide procedures by\nwhich affected AI subjects can challenge a specific decision concerning them.\nh AI Operators should consider working with their vendors (AI developers) to continually\nmonitor performance.\nb AI operators should consider such procedures even for non-significant decisions.\ni AI Operators should subject AI systems informing significant decisions to quality checks\nat least as stringent as those that would be required of a human being taking the same c AI operators should make affected AI subjects aware of these procedures and should\ndesign them to be convenient and user-friendly.\ndecision.\nd AI operators should consider employing human case evaluators to review any such\n3.2.2.3\nchallenges and, when appropriate, overturn the challenged decision.\nAI systems informing critical decisions should be subject to appropriate external audit\na When AI systems are used for critical decisions, external auditing of the AI systems in\nquestion should be used as a means to ensure meaningful standards of transparency and Example: A bank allows customers to apply for a loan online by entering their data. The\naccountability are upheld. bank uses an AI system to automatically determine whether to give the loan and what the\ninterest rate should be. They provide users with an option to contest the decision and have\nb In the case that critical decisions are of civic interest, public release of the results of the it reviewed by a human . They also require that customers justify their challenge by filling in\naudit should be considered as a means of ensuring public processes remain accountable a form, which assists the case reviewer and deters customers from challenging a decision\nto those affected by them. without good reason.7\nIn the case that critical decision are life and death decisions, these decisions should be\nc\nsupported by further validation and verification via a human operator.\nd Facilitate traceability and auditability of AI systems, particularly in critical contexts or\nsituations.\n7 Adapted from EU Commission, Can I be subject to automated individual decision-making, including profiling? Retrieved from: https://ec.europa.eu/\ninfo/law/law-topic/data-protection/reform/rights-citizens/my-rights/can-i-be-subject-automated-individual-decision-making-including-profiling_\nen#example\n22 23e AI operators should consider instituting an opt-out mechanism for significant automated 3.2.2.7\ndecisions. AI operators should understand the AI systems they use sufficiently to assess their suitability\nfor the use case and to ensure accountability and transparency\nf AI operators could consider \u201ccrowd challenge\u201d mechanisms whereby a critical number of\ncomplaints triggers an investigation into the fairness and/or accuracy of a decision-making\na In the case of critical decisions, AI operators should avoid using AI systems that cannot be\nprocess as a whole.\nsubjected to meaningful standards of accountability and transparency.\n3.2.2.5 b AI developers should consider notifying customers and AI operators of the use cases for\nAI systems informing significant decisions should not attempt to make value judgements on which the system has been designed, and those for which it is not suitable.\npeople\u2019s behalf without prior consent\na When informing an AI subject about significant choices they will make, AI systems should\nnot unreasonably restrict the available options or otherwise attempt to influence their value\njudgements without the explicit consent of the AI subject in question.\n3.2.2.6\nAI systems informing significant decisions should be developed by diverse teams with\nappropriate backgrounds\na AI developers who develop AI systems which may be used to assist in making critical\ndecisions should involve in the process experts with a background in social science, policy,\nor another subject which prepares them to evaluate the broader societal impact of their\nwork.\nb Development of AI systems informing significant decisions should include consultation\nwith experts in the field in which the system will be deployed.\nExample: An app that uses AI to assess medical symptoms and has a large user base had\nto face regulatory scrutiny because of number of complaints from doctors. They warned\nthat the application can miss signs of serious illness. A number of different shortcomings\nwere identified by doctors, some of which the company could address and resolve.8\n8 Financial Times. 2018. High-profile health app under scrutiny after doctor\u2019s complaints. Retrieved from: https://www.ft.com/content/19dc6b7e-8529-\n11e896-dd-fa565ec55929\n24 253.3 Transparent AI (Principle 3)\nc To facilitate the above, AI developers and AI operators should consider documenting the\nfollowing information during the design, development and deployment phases, and retaining\n3.3.1 Principle this documentation for a length of time appropriate to the decision type or industry:\n\u2022 the provenance of the training data, the methods of collection and treatment, how the data was\nWe will make AI systems transparent\nmoved, and measures taken to maintain its accuracy over time;\n\u2022 the model design and algorithms employed; and\nDevelopers should build systems whose failures can be traced and diagnosed. \u2022 changes to the codebase, and authorship of those changes.\nPeople should be told when significant decisions about them are being made by AI. d Where possible given the model design, AI developers should consider building in a means\nby which the \u201cdecision journey\u201d of a specific outcome (i.e. the component decisions leading\nWithin the limits of privacy and the preservation of intellectual property, those who deploy to it) can be logged.\nAI systems should be transparent about the data and algorithms they use.\nResponsible disclosures should be provided in a timely manner, and provide reasonable\nExample: A technology company has a product which is designed to assist in medical\njustifications for AI systems outcomes. This includes information that helps people\ndiagnosis. It documents each stage of its reasoning and relates it back to the input data.9\nunderstand outcomes, like key factors used in decision making.\n3.3.2 Guidelines\n3.3.2.1\nTraceability should be considered for significant decisions, especially those that have the\npotential to result in loss, harm or damage.\na For AI systems which inform significant decisions, especially those with the potential to\ncause loss, harm or damage, AI developers should consider building in traceability (i.e. the\nability to trace the key factors leading to any specific decision).\nb Organizations should ensure that harms caused through AI systems are investigated and\nredressed, by enacting strong enforcement mechanisms and remedial actions, to make\ncertain that human rights and the rule of law are respected in the digital world and in the\nphysical world.\n9 See IBM WatsonPaths\n26 273.3.2.2 3.4 Explainable AI (Principle 4)\nPeople should be informed of the extent of their interaction with AI systems\n3.4.1 Principle\na AI operators should inform AI subjects when a significant decision affecting them has been\nmade by an AI system.\nWe will make AI systems as explainable as\ntechnically possible\nExample: A small claims court adjudicates minor civil matters such as debt collection and\nevictions. They introduce an AI system to suggest the outcome of a ruling. At the time of the\nDecisions and methodologies of AI systems which have a significant effect on individuals\nruling the plaintiff and defendant are notified that the decision was assisted by an AI system.\nshould be explainable to them, to the extent permitted by available technology.\nThe court also provides an explanation for the decision.\nIt should be possible to ascertain the key factors leading to any specific decision that\ncould have a significant effect on an individual.\nb If an AI system can convincingly impersonate a human being, it should do so only after\nIn the above situation we will provide channels through which people can request such\nnotifying the AI subject that it is an AI system.\nexplanations.\nExample: A technology company produces a conversational AI agent which can make 3.4.2 Guidelines\nsome phone calls on behalf of its users. Those who receive the calls may believe that they\n3.4.2.1\nare speaking to a human. Therefore, the company programs the agent to identify itself at\nAI operators could consider providing affected AI subjects with a high-level explanation of\nthe start of every conversation.\nhow their AI system works\na AI operators could consider informing the affected AI subjects in understandable, non-\ntechnical language of:\n\u2022 the data that is ingested by the system;\n\u2022 the types of algorithms employed;\n\u2022 the categories into which people can be placed; and\n\u2022 the most important features driving the outcomes of decisions\n\u2022 A comprehensive list of feature engineering and models that was considered during\nthe model building phase.\nExample: A person turned down for a credit card might be told that the algorithm took\ntheir credit history, age, and postcode into account, but not learn why their application was\nrejected10 .\n28 29b For non-sensitive public sector use cases designed for the common good, AI operators\ncould consider making source code, together with an explanation of the workings of the AI Example: The UK\u2019s NHS developed a tool called Predict, which allows women with breast\nsystem, available either publicly or upon request (this should be done only if there is low risk cancer to compare their case to other women who have had the same condition in the past,\nof people \u2018gaming the system\u2019). and visualize the expected survival rate under various treatment options. The website has an\nexplanation page which shows the weights behind various factors and contains a description\nc The AI operator should maintain necessary documentation that provides elaboration and of the underlying mathematics12 .\nclarification on how the algorithm works, for example documentation of processes, decision\nmaking flow charts of the system, and how the appeal process is embedded.\nd An individual should have the ability to contest and seek effective redress against decisions c In the case that such explanations are available, they should be easily and quickly accessible,\nmade by AI systems. These must be addressed by the group or team supporting these free of charge and user-friendly.\nmodels.\ne Develop appropriate impact indices for the evaluation of AI system technological 3.5 Robust, Safe and Secure AI (Principle 5)\ninterventions from multiple perspectives.\n3.5.1 Principles\n3.4.2.2\nAI operators should consider providing affected AI subjects with a means to request\nexplanations for specific significant decisions, to the extent possible given the state of present AI systems will be technically robust\nresearch and the choice of model\nAI systems should be technically robust with a preventative approach to risks which\na AI operators should consider providing a means by which people affected by a significant\noperates in a manner such that they reliably behave.\ndecision informed by AI can access the reasoning behind that decision.\nAI Developers should ensure AI systems will not cause any unintentional harm and\nadverse impacts.\nExample: The US Consumer Financial Protection Bureau requires that creditors in the US\nwho reject credit applications must explain to the applicant the principal reason(s) why their\nAI systems should be resilient to attacks and security such as data poisoning and\napplication was rejected (e.g. \u201clength of residence\u201d or \u201cage of automobile\u201d)11 . In particular,\nmodel leakage.\n\u201cstatements that the adverse action was based on the creditor`s internal standards or\npolicies or that the applicant, joint applicant, or similar party failed to achieve a qualifying AI systems should have safeguards that enable a fallback plan in case of problems.\nscore on the creditor`s credit scoring system are insufficient\u201d.\nAI system results should be reproducible.\nb Where such explainability is not possible given available technology, AI operators should\nconsider compromises such as counterfactual reasoning, or listing the most heavily\nweighted factors contributing to the decision.\n11 Consumer Financial Protection Bureau, 12 CFR Part 1002 - Equal Credit Opportunity Act (Regulation B), \u00a7 1002.9 Notifications, Retrieved from https://www.\n10 The Guardian. AI watchdog needed to regulate automated decision-making, say experts. Retrieved from: https://www.theguardian.com/technology/2017/ consumerfinance.gov/policy-compliance/rulemaking/regulations/1002/\njan/27/ai-artificial-intelligence-watchdog-needed-to-prevent-discriminatory-automated-decisions 12 Predict website, accessible at http://www.predict.nhs.uk/predict_v2.1/legal/algorithm\n30 313.5.2 Guidelines\n3.5.2.1 3.5.2.3\nAI Operators should continue conducting vulnerability assessments, verification of AI system AI Operators should assure end users of the system\u2019s reliability through documentation, and\ndifferent behaviors in unexpected situations and for any dual-use case, to include: operationalizing processes for testing and verification of desired outcomes , specifically:\na Putting measures to ensure integrity and resilience of IA system against potential attacks. a AI system should be designed with an approach to continue monitoring if the system meets\ngoals, purposes and intended application.\nb AI Operators should define an approach to ensure results are reproducible through clear\nExample: IBM researchers presented a novel approach to exploiting AI for malicious\nprocess and documentation.\npurposes. Their system, DeepLocker embeds AI capabilities within the malware itself in order\nto improve its evasion techniques. It uses an Artificial Intelligence model to identify its target\nc AI Operators should publish documentation to assure system robustness to end users.\nusing indicators like facial recognition, geolocation and voice recognition.\n3.5.2.4\nAI systems will be safe, secure and controllable by humans\nb Verifying how AI systems behave in unexpected situations and environments.\na Safety and security of people, be they operators, end-users or other parties, will be of\nc Taking preventative measures against any possible system dual-case use. paramount concern in the design of any AI system.\nb AI systems should be verifiably secure and controllable throughout their operational\n3.5.2.2\nlifetime, to the extent permitted by technology.\nAI operators should define a suitable fall back plan and test it to maintain readiness in\nunexpected situations and in high safety risks level, to include:\nc The continued security and privacy of users should be considered when decommissioning\na Developing a plan to measure and assess potential safety risks to you or any third party AI systems.\nfrom technology use accidental or malicious misuse.\nd AI systems that may directly impact people\u2019s lives in a significant way should receive\nb Define thresholds for system acceptable results and governance procure to fall back on commensurate care in their designs.\nalternative defined and tested plans.\ne Such systems should be able to be overridden or their decisions reversed by designated\nc Considering an insurance policy to mitigate risks arising from potential damages. individuals.\nExample: Adopting a risk-based approach to procurement and clearly communicating it to\nvendors can help address such issues by giving the procuring organization advance notice\nof the specific oversight capabilities it will need in future stages of the system lifecycle,\npreventing vendors from presenting intellectual property arguments against required testing,\nmonitoring and auditing of their AI systems going forward and rewarding vendors\u2014of all\nsizes\u2014that are more advanced and responsive in their responsible AI efforts.\n13 IBM. https://documents.trendmicro.com/assets/white_papers/wp-malicious-uses-and-abuses-of-artificial-intelligence.pdf\n32 333.6.2 Guidelines\n3.5.2.5\nAi systems should not be able to autonomously hurt, destroy or deceive humans\n3.6.2.1\na AI systems should be built to serve and inform, and not to deceive and manipulate.\nAI should be developed to align with human values, contribute to human flourishing and\nbenefit society.\nb Nations should collaborate to avoid an arms race in lethal autonomous weapons, and\nsuch weapons should be tightly controlled. a Society should be consulted in a representative fashion to inform the development of AI.\nc Active cooperation should be pursued to avoid corner-cutting on safety standards. b Stakeholders throughout society should be involved in the development of AI and its\ngovernance.\nSystems designed to inform significant decisions should do so impartially.\nd\nc Stakeholders should proactively engage in responsible stewardship of trustworthy AI in\npursuit of beneficial outcomes for people and the planet.\n3.6 Human Centered AI (Principle 6)\nd Organizations should prioritize having all their stakeholders learn about wellbeing metrics\nas a potential determinant of how they create, deploy, market and monitor their AI\n3.6.1 Principles technologies.\nWe will give AI systems human values and\nExample: IEEE P7010 recommended practice14 establishes wellbeing metrics relating to\nmake them beneficial to society human factors directly affected by intelligent and autonomous systems and establishes a\nbaseline for the types of objective and subjective data these systems should analyze and\nGovernment will support the research of the beneficial use of AI. include (in their programming and functioning) to proactively increase human wellbeing.\nStakeholders throughout society should be involved in the development of AI and its\ngovernance.\n3.6.2.2\nSystematic analyses that examine the ethics of designing affective systems to nudge human\nDesign AI systems to adopt, learn and follow the norms and values of the community\nbeings prior to deployment are needed.\nthey serve.\nSystematic analyses that examine the ethics of designing affective systems to nudge a Nudging in AI systems should have an opt-in system policy with explicit consent.\nhuman beings prior to deployment are needed.\nb We recommend that when appropriate, an affective system that nudges human beings\nDecisions related to lethal force, life and death should not be delegated to AI systems. should have the ability to accurately distinguish between users, including detecting\nRules and standards should be adopted to ensure effective human control over those characteristics such as whether the user is an adult or a child. Additional protections must\ndecisions. be put in place for vulnerable populations, such as children, when informed consent cannot\nbe obtained, or when it may not be a sufficient safeguard.\nc AI systems with nudging strategies must be carefully evaluated, monitored, and controlled.\n14 IEEE. https://sagroups.ieee.org/7010/\n34 353.6.2.3 f In situations where it is needed, human rights impact assessments and human rights due\nHumanity should retain the power to govern itself and make the final decision, with AI in an diligence, human determination codes of ethical conduct, or quality labels and certifications\nassisting role. intended to promote human centered values and fairness should be considered.\ng Mechanisms should be put into place to receive external feedback regarding AI systems\na Decisions related to lethal force, life and death should not be delegated to AI systems. Rules\nthat potentially infringe on fundamental rights.\nand standards should be adopted to ensure effective human control over those decisions.\nb Responsible parties (e.g., parents, nurse practitioners, social workers, and governments)\nshould be trained to detect the influence due to AI and ineffective mitigation techniques.\nIn the most extreme cases it should always be possible to shut down harmful AI system.\nc Those actions undertaken by an affective system that are most likely to generate an\nemotional response should be designed to be easily changed.\nd Users should be able to make informed autonomous decisions regarding AI systems. They\nshould be given appropriate knowledge and tools to comprehend and interact with AI\nsystems to satisfactory degree and, where possible to reasonably self-assess or challenge\nthe system.\n3.6.2.4\nAI systems should be designed in a way that respects the rule of law, human rights, society\nvalues, and should include appropriate safeguards to ensure a fair and just society.\na AI Developers should design AI systems to adopt, learn and follow the norms and values of\nthe community they serve.\nb Organizations could identify the norms of the specific community in which AI systems are\nto be deployed in. In particular, pay attention to the norms relevant to the kinds of tasks that\nthe AI systems are designed to perform. These could be documented as well as how these\nnorms are addressed by the AI system.\nc To respond to the dynamic change of norms in society the AI system could be able to adjust\nits existing norms and learn new ones, while being transparent about these changes.\nd Designers should consider forms and metrics for assessing an AI system\u2019s norm conformity\nover the lifetime of the system (e.g. human-machine agreement on moral decisions,\nverifiability of AI decisions, justified trust).\ne The norm identification process must document the similarities and differences between\nthe norms that humans apply to other humans and the norms they apply to AI systems.\nNorm implementations should be evaluated specifically against the norms that the\ncommunity expects the AI system to follow.\n36 373.7 Sustainable and Environmentally Friendly AI (Principle 7)\nd While there is no standardized means of reporting on the environmental impact of AI\nsystems, where possible, model development and commissioning should make carbon\n3.7.1 Principle impacts a core consideration alongside functional and business requirements.\ne Smaller models should be considered \u2013 shrinking down the model size and using fewer\nWe will promote sustainable and environmentally compute cycles (to balance financial and performance costs with the end performance of\nthe model).\nfriendly AI\nf Carbon awareness should be considered \u2013 adjusting the operational parameters of the AI\nAI systems should be used to benefit all human beings, including future generations. The\nsystem to dynamically select the best time and location for energy use from the grid can\nenvironment is fundamental to this.\nreduce its carbon footprint.\nThroughout the AI lifecycle, implementations should only be carried out on the basis of a full\nunderstanding and acknowledgement of implications for sustainability and environment.\nExample: In agriculture, AI can transform Example: Google`s DeepMind division\nAn AI system\u2019s development, deployment and use, should be assessed via critical examination production by better monitoring and has developed AI that teaches itself to\nof resource usage and energy consumption.\nmanaging environmental conditions minimize the use of energy to cool Google`s\nand crop yields. AI can help reduce both data centers. As a result, Google reduced\nMechanisms to measure environmental impact due to type of energy use and processing\nfertilizer and water, all while improving its data center energy requirements by\npower provided by data centers should be established.\ncrop yields. 40% 15.\n3.7.2 Guidelines\n3.7.2.1 Throughout the AI lifecycle, implementations should always be carried out after full\nunderstanding and acknowledgement of AI implications on sustainability and environment.\na The application of Artificial Intelligence to address sustainability challenges are well\nunderstood. Building and running green, sustainable AI systems is of increasing importance,\ngiven the large carbon footprint that they can generate and the wider context of addressing\nclimate change.\nb AI model development, and by extension commissioning, should therefore seek to balance\ntechnical performance with energy consumption and environmental impact.\nc Efforts should be made to estimate and understand sustainability and environmental\nimpact across the AI lifecycle (e.g. the energy consumption costs associated with model\ntraining, and CO2 emissions and cloud compute costs associated with the deployment\nand running of the system).\n15 https://www.deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-by-40\n38 393.8 Privacy Preserving AI (Principle 8)\n3.8.2.2\nEstablish an oversight mechanism for data collection, storage and processing and use across\nyour organization\n3.8.1 Principle\na Assess who can access data and under which conditions.\nWe will respect people\u2019s privacy\nb Assign specific responsibilities and role for Data Protection Officers.\nAI systems should respect privacy and use the minimum intrusion necessary.\nc Prevention of harm to privacy necessitates adequate data governance that covers the\nquality and integrity of the data used, its relevance in light of the domain in which the\nAI systems should uphold high standards of data governance and security,\nprotecting personal information. AI systems will be deployed, its access protocols and the capability to process data in a\nmanner that protects privacy.\nSurveillance or other AI-driven technologies should not be deployed to the extent\nof violating internationally and/or UAE\u2019s accepted standards of privacy and human d AI systems must guarantee privacy and data protection throughout a system\u2019s entire\ndignity and people rights. lifecycle. This includes the information initially provided by the user, as well as the\ninformation generated about the user over the course of their interaction with the system\nPrivacy by design should be embedded in AI systems and where possible AI\n(e.g. outputs that the AI systems generated for specific users or how users responded to\nalgorithm should have adequate privacy impact assessments.\nparticular recommendations). Digital records of human behavior may allow AI systems\nto infer not only individuals\u2019 preferences, but also their age, gender, religious or political\nAdequate data protection frameworks and governance mechanisms should be\nviews. To allow individuals to trust the data gathering process, it must be ensured that data\nestablished in a multi-stakeholder approach and ensured throughout the life cycle\ncollected about them will not be used to unlawfully or unfairly discriminate against them.\nof AI systems.\ne In any given organization that handles individuals\u2019 data (whether someone is a user of the\nAI developers and operators should strive to strike the balance between privacy\nsystem or not), data protocols governing data access should be put in place in line with\nrequirements, individual rights and innovation growth and society benefits.\nnational privacy legislation (be these universal or sector-specific). These protocols should\noutline who can access data and under which circumstances. Only duly qualified personnel\n3.8.2 Guidelines\nwith the competence and need to access individual\u2019s data should be allowed to do so.\n3.8.2.1\nEstablish mechanism for users to flag issues related to privacy or data protection\na AI Operators should review the system for proper consent logging, ability of users to revoke\npermission whenever applicable.\nb Consider training AI model without or with minimal user of potentially sensitive or personal\ndata.\nc Use measures to enhance privacy such as encryption, anonymization and aggregation.\n40 41f Algorithmic systems require adequate privacy impact assessments, which also include\nsocietal and ethical considerations of their use and an innovative use of the privacy by\ndesign approach. AI actors need to ensure that they are accountable for the design and\nimplementation of AI systems in such a way as to ensure that personal information is\nprotected throughout the life cycle of the AI system.\ng Establish data policies or equivalent frameworks, or reinforce existing ones, to ensure full\nsecurity for personal data and sensitive data, which, if disclosed, may cause exceptional\ndamage, injury or hardship to individuals. Examples include data relating to offences,\ncriminal proceedings and convictions, and related security measures; biometric, genetic\nand health data; and personal data such as that relating to race, descent, gender, age,\nlanguage, religion, political opinion, national origin, ethnic origin, social origin, economic or\nsocial condition of birth, or disability and any other characteristics.\nh Promote mechanisms, such as open repositories for publicly funded or publicly held data,\nsource code and data trusts, to support the safe, fair, legal and ethical sharing of data,\namong others.\ni Promote and facilitate the use of quality and robust datasets for training, development and\nuse of AI systems, and exercise vigilance in overseeing their collection and use.\nExample: with the help of AI & ML algorithms16 , Synthetic data can be created to enhance\nprivacy. The data created will have the same statistical characteristics for the testing\nenvironment and where a third party can have access.\n16 https://research.aimultiple.com/privacy-enhancing-technologies/\n42 43Microsoft. 2022. Microsoft Responsible AI Standard, v2. General Requirements. June 2022.\n04\nRetrieved from:\nhttps://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4ZPmV\nISO. 2022. Proposed ISO Standard on Risk Management of AI: What Businesses Should Know.\nBibliography\nMay 2022. Retrieved from:\nhttps://unesdoc.unesco.org/ark:/48223/pf0000381137/PDF/381137eng.pdf.multi\nGSMA 2022. The AI Ethics Playbook. Implementing ethical principles into everyday business.\nFebruary 2022. Retrieved from:\nhttps://www.gsma.com/betterfuture/wp-content/uploads/202201//The-Mobile-Industry-Ethics-\nPlaybook_Feb-2022.pdf\nOECD. 2022. AI Policy Observatory. Retrieved from:\nhttps://standards.ieee.org/wp-content/uploads/import/documents/other/ead_v2.pdf\nUNESCO. 2022. Recommendation on the Ethics of Artificial Intelligence. Adopted in November\n2021. Retrieved from:\nhttps://unesdoc.unesco.org/ark:/48223/pf0000381137/PDF/381137eng.pdf.multi\nEuropean Commission. 2021. Proposal for a Regulation of the European Parliament and of the\nCouncil Laying Down Harmonized Rules on Artificial Intelligence (Artificial Intelligence Act) and\nAmending Certain Union Legislative Acts. Retrieved from:\nhttps://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A52021PC0206\nEuropean Commission. High Level Expert Group on Artificial Intelligence ( AI HLEG ). Ethics\nGuidelines for Trustworthy AI . April 2019. Retrieved from:\nhttps://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai\nCentre for Data Ethics and Innovation. UK Government: The roadmap to an effective AI assurance\necosystem. December 2021. Retrieved from:\nhttps://www.gov.uk/government/publications/the-roadmap-to-an-effective-ai-assurance-\necosystem\nThe New York City Council. 2021. A Local Law to amend the administrative code of the city of New\nYork, in relation to automated employment decision tools. Retrieved from:\nhttps://legistar.council.nyc.gov/LegislationDetail.aspx?ID=4344524&GUID=%20B051915D-A9AC-\n451E-81F86596032-FA3F9&\nUNESCO, Recommendation on the Ethics of Artificial Intelligence. 2021. Retrieved from:\nhttps://unesdoc.unesco.org/ark:/48223/pf0000381137\n44 45AI NOW Institute. Algorithmic accountability for the public sector. August 2021. Retrieved from: CNIL. (2017). How can humans keep the upper hand? The ethical matters raised by algorithms\nhttps://www.opengovpartnership.org/documents/algorithmic-accountability-public-sector/ and artificial intelligence. Retrieved from:\nhttps://www.cnil.fr/sites/default/files/atoms/files/cnil_rapport_ai_gb_web.pdf\nIEEE 2019. Ethically Aligned Design. Version 2 \u2013 For Public Discussion. Retrieved from:\nhttps://standards.ieee.org/wp-content/uploads/import/documents/other/ead_v2.pdf Executive Office of the President of the United States. National Science and Technology Council.\nNetworking and Information Technology Research and Development Subcommittee. (2016). The\nnational artificial intelligence research and development strategic plan. Retrieved from:\nIEEE 2017. Ethically Aligned Designed : A vision for Prioritizing Human Well-being with\nhttps://www.nitrd.gov/PUBS/national_ai_rd_strategic_plan.pdf\nAutonomous and Intelligent Systems. Retrieved from:\nhttps://standards.ieee.org/wp-content/uploads/import/documents/other/ead_v2.pdf\nExecutive Office of the President of the United States National Science and Technology Council.\nCommittee on Technology. (2016). Preparing for the future of artificial intelligence. Retrieved from:\nGoogle. AI Principles 2021. Retrieved from:\nhttps://obamawhitehouse.archives.gov/sites/default/files/whitehouse_files/microsites/ostp\nhttps://ai.google/principles/\nNSTC/preparing_for_the_future_of_ai.pdf\nMicrosoft. Responsible AI Standard v2 (June 2022). Retrieved from:\nThe Headquarters for Japan\u2019s Economic Revitalization. (2015). New Robot Strategy. Japan\u2019s\nhttps://www.microsoft.com/en-us/ai/responsible-ai\nRobot Strategy. Vision, Strategy, Action Plan. Retrieved from:\nhttp://www.meti.go.jp/english/press/2015/pdf/0123_01b.pdf\nThe ECONOMIST Group 2022 . Pushing forward: the future of AI in the Middle East and North\nAfrica Report. Retrieved from:\nTreasury Board of Canada Secretariat. (2018). Responsible Artificial Intelligence in the\nhttps://impact.economist.com/perspectives/sites/default/files/google_ai_mena_report.pdf\nGovernment of Canada. Digital Disruption White Paper Series. Version 2.0.\nhttps://docs.google.com/document/d/1Sn-qBZUXEUG4dVk909eSg5qvfbpNlRhzIefWPtBwbxY/\nPDPC. (2018, June 5). Discussion paper on Artificial Intelligence (AI) and Personal Data.\nedit\nSingapore: Personal Data Protection Commission Singapore (PDPC). Retrieved from:\nhttps://www.pdpc.gov.sg/-/media/Files/PDPC/PDF-Files/Resource-for-Organisation/AI/\nThe Toronto Declaration: Protecting the right to equality and non-discrimination in machine\nDiscussion-Paper-on-AI-and-PD---050618.pdf\nlearning systems. Toronto, Canada: Amnesty International and Access Now. Retrieved from:\nhttps://www.accessnow.org/cms/assets/uploads/201808//The-Toronto-Declaration_ENG_08-\nITI. AI Policy Principles. Retrieved from:\n2018.pdf\nhttps://www.itic.org/public-policy/ITIAIPolicyPrinciplesFINAL.pdf\nhttp://data.parliament.uk/writtenevidence/committeeevidence.svc/evidencedocument/artificial-\nCabinet Office.(2016, May 19). Data Science Ethical Framework. Retrieved from: intelligence-committee/artificial-intelligence/oral/73546.html (house of lords select committee\nhttps://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/ oral evidence, Q61)oral evidence, Q61)\nfile/524298/Data_science_ethics_framework_v1.0_for_publication__1_.pdf\nHouse of Lords Select Committee on Artificial Intelligence. (2018). AI in the UK: ready, willing and\nEuropean Parliament. (2017, February 16). European Parliament resolution of 16 February 2017 able?\nwith recommendations to the Commission on Civil Law Rules on Robotics (20152103/(INL)). https://publications.parliament.uk/pa/ld201719/ldselect/ldai/100100/.pdf\nRetrieved from:\nhttp://www.europarl.europa.eu/sides/getDoc.do?pubRef=-//EP//TEXT+TA+P8-TA-2017- House of Commons Science and Technology Select Committee. Algorithms in Decision Making.\n0++0051DOC+XML+V0//EN https://publications.parliament.uk/pa/cm201719/cmselect/cmsctech/351351/.pdf\nVillani, C. (2018). For a meaningful Artificial Intelligence towards a French and European strategy.\nRetrieved from:\nhttps://www.aiforhumanity.fr/pdfs/MissionVillani_Report_ENG-VF.pdf\n46 47", "metadata": {"country": "UAE", "year": "2022", "legally_binding": "no", "binding_proof": "None", "date": "01/01/2023", "regulator": "Ministry of Artificial Intelligence, Digital Economy and Remote Work Applications", "type": "guidelines", "status": "active", "language": "English, Arabic", "use_cases": "[1, 3, 5, 6]"}}
{"_id": "68695b475520a4cc7989de77", "title": "UAE Charter for the Development and Use of Artificial Intelligence", "source": "https://uaelegislation.gov.ae/en/policy/details/the-uae-charter-for-the-development-and-use-of-artificial-intelligence", "text": "AI ETHICS\nARTIFICIAL INTELLIGENCE ETHICS\nPRINCIPLES & GUIDELINES\nPRINCIPLES & GUIDELINES\nDEC 2022Table of Content\nPage Page\n1. Overview 4 3.4 Explainable 29\n1.1 Introduction 5 3.4.1 Principle 29\nWe will make AI systems as explainable as technically possible\n1.2 Scope 6 3.4.2 Guidelines 29\n1.3 Strategic alignment 6 3.5 Robust, Safe and Secure AI 31\n1.4 Responsibility 8 3.5.1 Principles 31\nAI systems will be technically robust\n3.5.2 Guidelines 32\n1.5 Licensing 8\n3.6 Human Centred AI 34\n1.6 Toolkit 8\n3.6.1 Principles 34\nWe will give AI systems human values and make them beneficial to society\n3.6.2 Guidelines 35\n2. Definitions 10\n3.7 Sustainable and Environmentally Friendly AI 38\n3.7.1 Principle 38\n3. Principles and Guidelines 16\nWe will promote sustainabity and environemntly friendly AI 38\n3.7.2 Guidelines 38\n3.1 Fairness 17\n3.8 Privacy Preserving AI 38\n3.1.1 Principle 17\nWe will make AI systems fair\n3.8.1 Principle 40\n3.1.2 Guidelines 20\nWe will respect people\u2019s privacy\n3.8.2 Guidelines 40\n3.2 Accountable AI 20\n3.2.1 Principle 20\nWe will make AI systems accountable\n4. Bibliography 44\n3.2.2 Guidelines 20\n3.3 Transparent AI 26\n3.3.1 Principle 26\nWe will make AI systems transparent\n3.3.2 Guidelines 26\n2 301 1.1 Introduction\nAI\u2019s rapid advancement and innovation potential across a range of fields is incredibly exciting.\nYet a thorough and open discussion around AI ethics, and the principles organizations using this\nOverview\ntechnology must consider, is needed.\nThis document - AI Ethics: Principles and Guidelines \u2013 intends to meet this need to balance these\ntwo central considerations of AI. It is designed to offer detailed guidance to help AI actors adhere\nto eight principles of AI ethics.\nWe will make AI systems that are:\nexplainable transparent accountable fair\nprivacy preserving sustainable and human centered robust, safe and\nenvironmentally secure\nfriendly\nThe guidelines are non-binding, and are drafted as a collaborative multi-\nstakeholder effort, with full awareness of organizations\u2019 needs to innovate\nand protect their intellectual property. This is a collaborative process in\nwhich all stakeholders are invited to be part of an ongoing dialogue. We\nwould like to see the AI Ethics Guidelines evolve into a universal, practical\nand applicable framework informing ethical requirements for AI design and\nuse. The eventual goal is to reach widespread agreement and adoption of\ncommonly agreed policies to inform the ethical use of AI nationally and\naround the world.\n4 51.2 Scope Additionally, the AI Office is currently developing additional policies to be taken into consideration\nalong with this document:\nThis document gives non-mandatory guidelines for achieving the ethical design\nand deployment of AI systems in both the public and private sectors. AI already\nAI Seal AI Procurement Guidelines\nsurrounds us, but some applications are more visible and sensitive than others.\nThe UAE AI Seal brand (UAI) will be used The AI Procurement Guidelines will be\nThis document is applicable only to those AI systems which make or inform\nto attract talent and business from used to guide Federal Government entities\n\u2018significant decisions\u2019 \u2013 that is, those decisions which have the potential for\nacross the globe to come to the UAE to in the procurement of AI systems. They\nsignificant impact either on individuals or on society as a whole. They also apply to\ntest and develop AI. This includes a UAI will list the general principles, proposed\n\u2018critical decisions\u2019, which are a subset of significant decisions and are of especially\nmark recognizing high quality, ethical AI mechanisms for procurement and review\ncritical nature.\ncompanies. It would reward safe, efficient, of AI products between the federal\nverified AI technology with a \u2018UAI Seal government and vendors. The policy\nThis document guides the further developments of sector specific requirements\nof Approval\u2019. The UAE AI Seal brand is considers R&D for specific challenges\nand principles, hence every entity within its sector can further amend, use, or\ncurrently under development by the AI that do not currently have an AI solution\ncomplement its current frameworks to properly suit its context and the need of\nOffice. product that is market-ready. Based on\naffected stakeholders.\nglobal standards for AI Procurement, the\npolicy aims to incentivize AI adoption\nIt is a living document and will undergo further future reviews and enhancements.\nwithin government entities and promote\nAI R&D within the private sector for\nspecified government challenges. The\nprocurement policy is currently under\ndevelopment by the AI Office.\n1.3 Strategic alignment\nThis document comes as a fulfillment of the goals and aspirations set in the UAE National AI\nStrategy and the international Sustainable Development Goals (SDGs). The UAE has long been\nat the forefront of sustainable development. These shared global milestones have created global\nmovement and adoption in several disciplines and sectors. As an early adopter, the UAE has\nestablished a national committee on SDGs with a long-term realization of the UAE SDG agenda\n2030. These goals are further engrained and reflected in the AI Ethics Principles under the topics of\nfairness, inclusiveness, equality, human benefit, and rights.\n6 71.4 Responsibility\nThe AI Office will not be responsible for any misuse of the AI Ethics Principles and Guidelines. The\nuser bears all the consequences of their use.\n1.5 Licensing\nThis document is published under the terms of a Creative Commons Attribution 4.0 International\nLicense in order to facilitate its re-use by other governments and private sector organizations.\nIn summary this means you are free to share and adapt the material, including for commercial\npurposes, provided that you give appropriate credit to the UAE AI and Blockchain Council as its\nowner and do not suggest the Minister of State for Artificial Intelligence, Digital Economy and\nRemote Work Applications Office endorses your use.\n1.6 Toolkit\nThe document is part of a toolkit of self-governing nature that boosts awareness and enables\ngovernment and private institutions to pursue innovative use cases while maintaining human\nvalues and principles.\n8 92.1 AI developer organization\n02\nAn organization which does any of the following:\na determines the purpose of an AI system;\nDefinitions\nb designs an AI system;\nc builds an AI system, or:\nd performs technical maintenance or tuning on an AI system\nNote:\nThe definition applies regardless of whether the organization is the ultimate user of the system, or\nwhether they sell it on or give it away.\nExample: A company develops an artificially intelligent facial recognition system and\nsells it to a country\u2019s border force, who use it to identify suspicious personnel. The company\nis an AI developer organization and the border force is an AI operator organization.\n2.2 AI operator organization\nAn organization which does any of the following:\na uses AI systems in operations, backroom processes or decision-making;\nb uses an AI system to provide a service to an AI subject;\nc is a business owner of an AI system;\nd procures and treats data for use in an AI system; or\ne evaluates the use case for an AI system and decides whether to proceed.\nNotes:\n1. This definition applies regardless of whether the AI system was developed in-house or procured.\n2. It is possible for organizations to be both an AI developer organization and an AI operator organization.\n10 112.6 Critical decision\n2.3 Artificial Intelligence (also AI)\nThe capability of a functional unit to perform functions that are generally associated with human An individually significant decision which is deemed to either have a very large impact on an\nintelligence such as reasoning, learning and self-improvement1. individual or to have especially high stakes. These can be especially sensitive, have the potential\nto cause high loss or damage, be societally significant, or set an important precedent.\n2.4 Artificially Intelligent System (also AI system)\nNotes:\nThe types of decisions referred to here are the same as those in the definition of significant-at-\nA product, service, process or decision-making methodology whose operation or outcome is\nscale decisions, except in this case the effects are felt as a result of an individual decision rather\nmaterially influenced by artificially intelligent functional units.\nthan an aggregate of many decisions..\nNotes:\n1. It is not necessary for a system\u2019s outcome to be solely determined by artificially intelligent\nfunctional units in order for the system to be defined as an artificially intelligent system; and Example: A court determines whether a defendant is guilty of a criminal charge, with the\n2. A particular feature of AI systems is that they learn behavior and rules not explicitly punishment for guilt being a life sentence. This is a critical decision because it has a very\nprogrammed in. large impact on the life of the defendant and also sets precedent for similar cases in the\nfuture.\nExample 1: A small claims court uses Example 2: A government entity uses\n2.7 Functional Unit\nan artificially intelligent software package a chatbot which allows customers to ask\nto collect evidence pertaining to a case, routine questions, book appointments\nEntity of hardware or software, or both, capable of accomplishing a specified purpose3 .\ncompare it to similar cases in the past, and conduct minor financial transactions.\nand present a recommended decision The chatbot responds to customer\nto a judge. The judge determines the queries with pre-written responses and is\n2.8 Individually significant decision\nfinal outcome. This decision-making based on pre-programmed decision rules.\nA decision which has the potential for significant impact on at least one individual\u2019s\nmethodology is materially influenced Therefore, the chatbot is not an AI system.\ncircumstances, behavior or choices, or has legal or similarly significant effects on him or her.\nby an artificially intelligent functional If, however, the chatbot autonomously\nunit, and is therefore classified as an AI adjusted its treatment of customers\nsystem. based on the outcome of past cases, it\nwould be an AI system. Example: A company decides to make an employee redundant. This is an individually\nsignificant decision because of its potential impact on the employee\u2019s financial situation.\n2.5 Bias (of a system)\nInclination or prejudice for or against one person or group, especially in a way considered to be\nunfair2.\n2 Oxford Dictionary 2018, Oxford University Press, viewed online 4th October 2018, <https://en.oxforddictionaries.com>\n1 Consistent with ISO/IEC 2382:2015 3 From ISO/IEC 2382:2015\n12 132.9 Non-operational bias (of a system) 2.11 Significant decision\nBias that is either: A decision which is either individually significant or is part of a set of significant-at-scale\ndecisions.\na not a design feature; or\nb not important in achieving the stated purpose of the system. 2.12 Subject of an artificially intelligent system (also AI subject)\nA natural person who is any of the following:\n2.10 Set of significant-at-scale decisions\na an end-user of an AI system\nA set of decisions made by the same system or organization which, when taken in aggregate,\nb directly affected by the operation of or outcomes of an AI system, or:\nhave significant impact on society as a whole or groups within it.\nc a recipient of a service or recommendation provided by an AI system\nNotes:\n1. The decisions need not be individually significant in order to qualify, in aggregate, as a set of\nsignificant-at-scale decisions; and\n2. Examples of areas which have a large impact on society and which include but are not limited\nto: the large-scale allocation of resources or opportunities amongst groups; the structure of\ngovernment; the division of power between large entities or groups; the law, and its interpretation\nand enforcement; conflict and war; international relations, etc.\nExample: An AI system is used by a website to determine which content to show users.\nThis decision is not individually significant, since a user is not greatly affected by whether\na particular piece of media is shown to them. However, if the website is popular then the AI\nsystem may be making a set of significant-at-scale decisions, because any biases in the\nsystem will affect a large number of users.\n14 153.1 Fairness (Principle 1)\n03\n3.1.1 Principle\nPRINCIPLES AND We will make AI systems fair\nData ingested should, where possible, be accurate and representative of the affected\npopulation.\nGUIDELINES\nAlgorithms should avoid non-operational bias.\nSteps should be taken to mitigate and disclose the biases inherent in datasets.\nSignificant decisions should be provably fair.\nAll personnel involved in the development, deployment and use of AI systems have a role and\nresponsibility to operationalize AI fairness and should be educated accordingly.\n3.1.2 Guidelines\n3.1.2.1\nBenefits of AI systems should be available and accessible to all. AI systems should not discriminate\nagainst people or groups in a way that could have an adverse impact. Especially significant and\ncritical decisions made or assisted by AI should be provably fair.\na Considering that fairness can have many different definitions, an organization should\ndocument its own definition of fairness for the context that the AI system is going to be\nimplemented in. Organizations should document what the implemented fairness objective\nstands for and why this choice was considered the most suitable for the given scenario.\nb It is recommended to identify and document demographic groups that may be adversely\nimpacted and mitigate the risk where possible.\nc AI developers and AI operators could consider formal procedures such as Discrimination\nImpact Assessments as a means of ensuring fairness. This assessment should be\ndocumented as well as the mitigation measures that were implemented by the organization.\nThe impact assessment should be conducted pre-release and regularly after as an ongoing\nevaluation, results are advised to be documented.\n16 173.1.2.2 3.1.2.3\nConsideration should be given to whether the data ingested is accurate and representative Consideration should be given to whether decision-making processes introduce bias\nof the affected population\na When subjecting different groups to different decision-making processes, AI developers\nd Fairness has many different definitions in different cultures and for different contexts. should consider whether this will lead to non-operational bias.\nEncouraging a diverse and inclusive AI ecosystem is thus all the more crucial to ensure\nthat one definition of fairness does not contradict another, and that the process of defining b When evaluating the fairness of an AI system, AI developers and AI operators should\nfairness itself is fair, with under-represented groups represented in the discussion. consider whether AI subjects in the same circumstances receive equal treatment.\ne AI developers and AI operators should undertake reasonable data exploration and/or testing\nto identify potentially prejudicial decision-making tendencies in AI systems arising from\nExample: An organization uses an AI tool to automate the pre-screening of candidates\ndata inaccuracy and biases in the data.\nfor a job opening. It is trained on data from the company\u2019s existing employees, the majority\nof whom are from the same ethnic background. Therefore, the system learns to use name\nf AI developers and operators should evaluate all datasets to assess inclusiveness of and nationality as discriminating factors in filtering job applicants. This could have been\nidentified demographic groups and collect data to close any gaps.\nidentified through testing and rectified by, for example, balancing the training data or only\nusing relevant data fields for training.\ng AI developers and AI operators should refrain from training AI systems on data that is not\nlikely to be representative of the affected AI subjects, or is not likely to be accurate, whether\nthat be due to age, omission, method of collection, or other factors.\n3.1.2.4\nSignificant decisions informed by the use of AI should be fair\nExample: Following a natural disaster, a government relief agency uses an AI system to a AI developers and AI operators could consider formal procedures such as Discrimination\ndetect communities in greatest need by analyzing social media data from a range of websites. Impact Assessments as a means of ensuring fairness.\nHowever, those communities where smartphone penetration is lower having less presence on\n3.1.2.4\nsocial media, and so are at risk of receiving less attention. Therefore, the charity complements\nAI operators should consider whether their AI systems are accessible and usable in a fair\ntheir AI tool with traditional techniques to identify needy populations elsewhere.\nmanner across user groups\n3.1.2.5\nh AI developers should consider whether their AI systems can be expected to perform well Consideration should be given to the effect of diversity on the development and deployment\nwhen exposed to previously unseen data, especially when evaluating people who are not processes\nwell-represented in the training data.\nb Organizations should seek to include people from diverse demographic backgrounds across\nthe full lifecyle, to include design, development and deployment processes. Organizations\nshould seek to engage diverse internal and external groups also.\nc AI developers should consider whether the assumptions they make about AI subjects could\nbe wrong or are likely to lead to non-operational bias; if so, they should consider consulting\nthe AI subjects in a representative manner during the design, development and deployment\nto confirm these assumptions.\n18 193.2 Accountable AI (Principle 2)\n3.2.2.2\nPositive efforts should be made to identify and mitigate any significant risks inherent in the AI\nsystems designed\n3.2.1 Principle\na AI operators should only use AI systems that are backed by evidence-based academic or\nindustrial research, and AI developers should base their development on such research.\nWe will make AI systems accountable\nAccountability for the outcomes of an AI system lies not with the system itself but is\napportioned between those who design, develop and deploy it; Example: A foreign country has a government service which identifies parents who owe\nmoney in child maintenance. The data matching process is often incorrect due to misspelled\nDevelopers should make efforts to mitigate the risks inherent in the systems they design; names or missing data which results in some individuals being incorrectly targeted\nautomatically by the system with the result being a large bill, poor credit ratings and even\nAI systems should have built-in appeals procedures whereby users can challenge significant\nfreezing wages. The recourse for individuals who are incorrectly targeted is time-consuming\ndecisions;\nand not straightforward . If the potential impact of incorrect decisions had been assessed,\nmitigation measures such a user-friendly review procedure could have been set up.\nAI systems should be developed by diverse teams which include experts in the area in which\nthe system will be deployed; and\nAI systems should be subject to external audit and decision quality assurance. b AI operators should identify the likely impact of incorrect automated decisions on AI\nsubjects and, in the case where incorrect decisions are likely to cause significant cost or\ninconvenience, consider mitigating measures.\n3.2.2 Guidelines\nc AI operators should consider internal risk assessments or ethics frameworks as a means to\n3.2.2.1 facilitate the identification of risks and mitigating measures.\nAccountability for the outcomes of an AI system should not lie with the system itself\nd In designing AI systems to inform significant decisions, AI developers should consider\na Accountability for loss or damages resulting from the application of AI systems should not measures to maintain data accuracy over time, including:\nbe attributed to the system itself.\n\u2022 the completeness of the data.\nb AI operators and AI developers should consider designating individuals to be responsible for \u2022 timely update of the data, and.\ninvestigating and rectifying the cause of loss or damage arising from the deployment of AI\n\u2022 whether the context in which the data was collected affects its suitability for the\nsystems.\nintended use case.\nExample: A border camera scanning for predictors of risk may misinterpret a \u201ctic\u201d of an\nindividual with Tourette syndrome as suspicious. These can manifest in a diverse fashion, and\nshould not cause this person to undergo secondary inspection every time they pass through\nthe border . If the data is updated after the first case is encountered then it would avoid\ncausing inconvenience on subsequent visits.\n5 Government of Canada. (2018). Responsible AI in the Government of Canada. Digital Disruption White Paper Series. Version 2.0, p.26. Retrieved from:\nhttps://docs.google.com/document/d/1Sn-qBZUXEUG4dVk909eSg5qvfbpNlRhzIefWPtBwbxY/edit\n4 Cabinet Office (UK), Data Science Ethical Framework, Version 1.0, licensed under the Open Government License v3.0, p. 13 6 Stoica, I. et. al.,2017, A Berkeley View of Systems Challenges for AI, p. 2, https://arxiv.org/pdf/1712.05855.pdf\n20 21e AI developers and AI operators should tune AI models periodically to cater for changes to e Be mindful that there might be fundamental tensions between different principles and\ndata and/or models over time. requirements. Continuously identify, evaluate, document and communicate these trade-\noffs and their solutions.\nf AI operators should consider whether AI systems trained in a static environment will display\nmodel instability when deployed in dynamic environments. Adopt a Trustworthy AI assessment list when developing, deploying or using AI systems,\nf\nand adapt it to the specific use case in which the system is being applied.\nExample: AI systems will need to be able to adapt to the changes in the environment that g Keep in mind that such an assessment list will never be exhaustive. Ensuring Trustworthy\nthey are deployed in. For example, a self-driving car would need to quickly adapt to unexpected AI is not about ticking boxes, but about continuously identifying and implementing\nand dangerous road by learning in real time from other cars that have successfully dealt with requirements, evaluating solutions, ensuring improved outcomes throughout the AI\nthese conditions. In addition, such mission-critical applications must handle noisy inputs system\u2019s lifecycle, and involving stakeholders in this.\nand defend against malicious actors .\n3.2.2.4\nAI subjects should be able to challenge significant automated decisions concerning them and,\ng AI developers should collaborate with AI Operators to train models using historical data where appropriate, be able to opt out of such decisions\nfrom the Operator.\na AI operators using AI systems to inform significant decisions should provide procedures by\nwhich affected AI subjects can challenge a specific decision concerning them.\nh AI Operators should consider working with their vendors (AI developers) to continually\nmonitor performance.\nb AI operators should consider such procedures even for non-significant decisions.\ni AI Operators should subject AI systems informing significant decisions to quality checks\nat least as stringent as those that would be required of a human being taking the same c AI operators should make affected AI subjects aware of these procedures and should\ndesign them to be convenient and user-friendly.\ndecision.\nd AI operators should consider employing human case evaluators to review any such\n3.2.2.3\nchallenges and, when appropriate, overturn the challenged decision.\nAI systems informing critical decisions should be subject to appropriate external audit\na When AI systems are used for critical decisions, external auditing of the AI systems in\nquestion should be used as a means to ensure meaningful standards of transparency and Example: A bank allows customers to apply for a loan online by entering their data. The\naccountability are upheld. bank uses an AI system to automatically determine whether to give the loan and what the\ninterest rate should be. They provide users with an option to contest the decision and have\nb In the case that critical decisions are of civic interest, public release of the results of the it reviewed by a human . They also require that customers justify their challenge by filling in\naudit should be considered as a means of ensuring public processes remain accountable a form, which assists the case reviewer and deters customers from challenging a decision\nto those affected by them. without good reason.7\nIn the case that critical decision are life and death decisions, these decisions should be\nc\nsupported by further validation and verification via a human operator.\nd Facilitate traceability and auditability of AI systems, particularly in critical contexts or\nsituations.\n7 Adapted from EU Commission, Can I be subject to automated individual decision-making, including profiling? Retrieved from: https://ec.europa.eu/\ninfo/law/law-topic/data-protection/reform/rights-citizens/my-rights/can-i-be-subject-automated-individual-decision-making-including-profiling_\nen#example\n22 23e AI operators should consider instituting an opt-out mechanism for significant automated 3.2.2.7\ndecisions. AI operators should understand the AI systems they use sufficiently to assess their suitability\nfor the use case and to ensure accountability and transparency\nf AI operators could consider \u201ccrowd challenge\u201d mechanisms whereby a critical number of\ncomplaints triggers an investigation into the fairness and/or accuracy of a decision-making\na In the case of critical decisions, AI operators should avoid using AI systems that cannot be\nprocess as a whole.\nsubjected to meaningful standards of accountability and transparency.\n3.2.2.5 b AI developers should consider notifying customers and AI operators of the use cases for\nAI systems informing significant decisions should not attempt to make value judgements on which the system has been designed, and those for which it is not suitable.\npeople\u2019s behalf without prior consent\na When informing an AI subject about significant choices they will make, AI systems should\nnot unreasonably restrict the available options or otherwise attempt to influence their value\njudgements without the explicit consent of the AI subject in question.\n3.2.2.6\nAI systems informing significant decisions should be developed by diverse teams with\nappropriate backgrounds\na AI developers who develop AI systems which may be used to assist in making critical\ndecisions should involve in the process experts with a background in social science, policy,\nor another subject which prepares them to evaluate the broader societal impact of their\nwork.\nb Development of AI systems informing significant decisions should include consultation\nwith experts in the field in which the system will be deployed.\nExample: An app that uses AI to assess medical symptoms and has a large user base had\nto face regulatory scrutiny because of number of complaints from doctors. They warned\nthat the application can miss signs of serious illness. A number of different shortcomings\nwere identified by doctors, some of which the company could address and resolve.8\n8 Financial Times. 2018. High-profile health app under scrutiny after doctor\u2019s complaints. Retrieved from: https://www.ft.com/content/19dc6b7e-8529-\n11e896-dd-fa565ec55929\n24 253.3 Transparent AI (Principle 3)\nc To facilitate the above, AI developers and AI operators should consider documenting the\nfollowing information during the design, development and deployment phases, and retaining\n3.3.1 Principle this documentation for a length of time appropriate to the decision type or industry:\n\u2022 the provenance of the training data, the methods of collection and treatment, how the data was\nWe will make AI systems transparent\nmoved, and measures taken to maintain its accuracy over time;\n\u2022 the model design and algorithms employed; and\nDevelopers should build systems whose failures can be traced and diagnosed. \u2022 changes to the codebase, and authorship of those changes.\nPeople should be told when significant decisions about them are being made by AI. d Where possible given the model design, AI developers should consider building in a means\nby which the \u201cdecision journey\u201d of a specific outcome (i.e. the component decisions leading\nWithin the limits of privacy and the preservation of intellectual property, those who deploy to it) can be logged.\nAI systems should be transparent about the data and algorithms they use.\nResponsible disclosures should be provided in a timely manner, and provide reasonable\nExample: A technology company has a product which is designed to assist in medical\njustifications for AI systems outcomes. This includes information that helps people\ndiagnosis. It documents each stage of its reasoning and relates it back to the input data.9\nunderstand outcomes, like key factors used in decision making.\n3.3.2 Guidelines\n3.3.2.1\nTraceability should be considered for significant decisions, especially those that have the\npotential to result in loss, harm or damage.\na For AI systems which inform significant decisions, especially those with the potential to\ncause loss, harm or damage, AI developers should consider building in traceability (i.e. the\nability to trace the key factors leading to any specific decision).\nb Organizations should ensure that harms caused through AI systems are investigated and\nredressed, by enacting strong enforcement mechanisms and remedial actions, to make\ncertain that human rights and the rule of law are respected in the digital world and in the\nphysical world.\n9 See IBM WatsonPaths\n26 273.3.2.2 3.4 Explainable AI (Principle 4)\nPeople should be informed of the extent of their interaction with AI systems\n3.4.1 Principle\na AI operators should inform AI subjects when a significant decision affecting them has been\nmade by an AI system.\nWe will make AI systems as explainable as\ntechnically possible\nExample: A small claims court adjudicates minor civil matters such as debt collection and\nevictions. They introduce an AI system to suggest the outcome of a ruling. At the time of the\nDecisions and methodologies of AI systems which have a significant effect on individuals\nruling the plaintiff and defendant are notified that the decision was assisted by an AI system.\nshould be explainable to them, to the extent permitted by available technology.\nThe court also provides an explanation for the decision.\nIt should be possible to ascertain the key factors leading to any specific decision that\ncould have a significant effect on an individual.\nb If an AI system can convincingly impersonate a human being, it should do so only after\nIn the above situation we will provide channels through which people can request such\nnotifying the AI subject that it is an AI system.\nexplanations.\nExample: A technology company produces a conversational AI agent which can make 3.4.2 Guidelines\nsome phone calls on behalf of its users. Those who receive the calls may believe that they\n3.4.2.1\nare speaking to a human. Therefore, the company programs the agent to identify itself at\nAI operators could consider providing affected AI subjects with a high-level explanation of\nthe start of every conversation.\nhow their AI system works\na AI operators could consider informing the affected AI subjects in understandable, non-\ntechnical language of:\n\u2022 the data that is ingested by the system;\n\u2022 the types of algorithms employed;\n\u2022 the categories into which people can be placed; and\n\u2022 the most important features driving the outcomes of decisions\n\u2022 A comprehensive list of feature engineering and models that was considered during\nthe model building phase.\nExample: A person turned down for a credit card might be told that the algorithm took\ntheir credit history, age, and postcode into account, but not learn why their application was\nrejected10 .\n28 29b For non-sensitive public sector use cases designed for the common good, AI operators\ncould consider making source code, together with an explanation of the workings of the AI Example: The UK\u2019s NHS developed a tool called Predict, which allows women with breast\nsystem, available either publicly or upon request (this should be done only if there is low risk cancer to compare their case to other women who have had the same condition in the past,\nof people \u2018gaming the system\u2019). and visualize the expected survival rate under various treatment options. The website has an\nexplanation page which shows the weights behind various factors and contains a description\nc The AI operator should maintain necessary documentation that provides elaboration and of the underlying mathematics12 .\nclarification on how the algorithm works, for example documentation of processes, decision\nmaking flow charts of the system, and how the appeal process is embedded.\nd An individual should have the ability to contest and seek effective redress against decisions c In the case that such explanations are available, they should be easily and quickly accessible,\nmade by AI systems. These must be addressed by the group or team supporting these free of charge and user-friendly.\nmodels.\ne Develop appropriate impact indices for the evaluation of AI system technological 3.5 Robust, Safe and Secure AI (Principle 5)\ninterventions from multiple perspectives.\n3.5.1 Principles\n3.4.2.2\nAI operators should consider providing affected AI subjects with a means to request\nexplanations for specific significant decisions, to the extent possible given the state of present AI systems will be technically robust\nresearch and the choice of model\nAI systems should be technically robust with a preventative approach to risks which\na AI operators should consider providing a means by which people affected by a significant\noperates in a manner such that they reliably behave.\ndecision informed by AI can access the reasoning behind that decision.\nAI Developers should ensure AI systems will not cause any unintentional harm and\nadverse impacts.\nExample: The US Consumer Financial Protection Bureau requires that creditors in the US\nwho reject credit applications must explain to the applicant the principal reason(s) why their\nAI systems should be resilient to attacks and security such as data poisoning and\napplication was rejected (e.g. \u201clength of residence\u201d or \u201cage of automobile\u201d)11 . In particular,\nmodel leakage.\n\u201cstatements that the adverse action was based on the creditor`s internal standards or\npolicies or that the applicant, joint applicant, or similar party failed to achieve a qualifying AI systems should have safeguards that enable a fallback plan in case of problems.\nscore on the creditor`s credit scoring system are insufficient\u201d.\nAI system results should be reproducible.\nb Where such explainability is not possible given available technology, AI operators should\nconsider compromises such as counterfactual reasoning, or listing the most heavily\nweighted factors contributing to the decision.\n11 Consumer Financial Protection Bureau, 12 CFR Part 1002 - Equal Credit Opportunity Act (Regulation B), \u00a7 1002.9 Notifications, Retrieved from https://www.\n10 The Guardian. AI watchdog needed to regulate automated decision-making, say experts. Retrieved from: https://www.theguardian.com/technology/2017/ consumerfinance.gov/policy-compliance/rulemaking/regulations/1002/\njan/27/ai-artificial-intelligence-watchdog-needed-to-prevent-discriminatory-automated-decisions 12 Predict website, accessible at http://www.predict.nhs.uk/predict_v2.1/legal/algorithm\n30 313.5.2 Guidelines\n3.5.2.1 3.5.2.3\nAI Operators should continue conducting vulnerability assessments, verification of AI system AI Operators should assure end users of the system\u2019s reliability through documentation, and\ndifferent behaviors in unexpected situations and for any dual-use case, to include: operationalizing processes for testing and verification of desired outcomes , specifically:\na Putting measures to ensure integrity and resilience of IA system against potential attacks. a AI system should be designed with an approach to continue monitoring if the system meets\ngoals, purposes and intended application.\nb AI Operators should define an approach to ensure results are reproducible through clear\nExample: IBM researchers presented a novel approach to exploiting AI for malicious\nprocess and documentation.\npurposes. Their system, DeepLocker embeds AI capabilities within the malware itself in order\nto improve its evasion techniques. It uses an Artificial Intelligence model to identify its target\nc AI Operators should publish documentation to assure system robustness to end users.\nusing indicators like facial recognition, geolocation and voice recognition.\n3.5.2.4\nAI systems will be safe, secure and controllable by humans\nb Verifying how AI systems behave in unexpected situations and environments.\na Safety and security of people, be they operators, end-users or other parties, will be of\nc Taking preventative measures against any possible system dual-case use. paramount concern in the design of any AI system.\nb AI systems should be verifiably secure and controllable throughout their operational\n3.5.2.2\nlifetime, to the extent permitted by technology.\nAI operators should define a suitable fall back plan and test it to maintain readiness in\nunexpected situations and in high safety risks level, to include:\nc The continued security and privacy of users should be considered when decommissioning\na Developing a plan to measure and assess potential safety risks to you or any third party AI systems.\nfrom technology use accidental or malicious misuse.\nd AI systems that may directly impact people\u2019s lives in a significant way should receive\nb Define thresholds for system acceptable results and governance procure to fall back on commensurate care in their designs.\nalternative defined and tested plans.\ne Such systems should be able to be overridden or their decisions reversed by designated\nc Considering an insurance policy to mitigate risks arising from potential damages. individuals.\nExample: Adopting a risk-based approach to procurement and clearly communicating it to\nvendors can help address such issues by giving the procuring organization advance notice\nof the specific oversight capabilities it will need in future stages of the system lifecycle,\npreventing vendors from presenting intellectual property arguments against required testing,\nmonitoring and auditing of their AI systems going forward and rewarding vendors\u2014of all\nsizes\u2014that are more advanced and responsive in their responsible AI efforts.\n13 IBM. https://documents.trendmicro.com/assets/white_papers/wp-malicious-uses-and-abuses-of-artificial-intelligence.pdf\n32 333.6.2 Guidelines\n3.5.2.5\nAi systems should not be able to autonomously hurt, destroy or deceive humans\n3.6.2.1\na AI systems should be built to serve and inform, and not to deceive and manipulate.\nAI should be developed to align with human values, contribute to human flourishing and\nbenefit society.\nb Nations should collaborate to avoid an arms race in lethal autonomous weapons, and\nsuch weapons should be tightly controlled. a Society should be consulted in a representative fashion to inform the development of AI.\nc Active cooperation should be pursued to avoid corner-cutting on safety standards. b Stakeholders throughout society should be involved in the development of AI and its\ngovernance.\nSystems designed to inform significant decisions should do so impartially.\nd\nc Stakeholders should proactively engage in responsible stewardship of trustworthy AI in\npursuit of beneficial outcomes for people and the planet.\n3.6 Human Centered AI (Principle 6)\nd Organizations should prioritize having all their stakeholders learn about wellbeing metrics\nas a potential determinant of how they create, deploy, market and monitor their AI\n3.6.1 Principles technologies.\nWe will give AI systems human values and\nExample: IEEE P7010 recommended practice14 establishes wellbeing metrics relating to\nmake them beneficial to society human factors directly affected by intelligent and autonomous systems and establishes a\nbaseline for the types of objective and subjective data these systems should analyze and\nGovernment will support the research of the beneficial use of AI. include (in their programming and functioning) to proactively increase human wellbeing.\nStakeholders throughout society should be involved in the development of AI and its\ngovernance.\n3.6.2.2\nSystematic analyses that examine the ethics of designing affective systems to nudge human\nDesign AI systems to adopt, learn and follow the norms and values of the community\nbeings prior to deployment are needed.\nthey serve.\nSystematic analyses that examine the ethics of designing affective systems to nudge a Nudging in AI systems should have an opt-in system policy with explicit consent.\nhuman beings prior to deployment are needed.\nb We recommend that when appropriate, an affective system that nudges human beings\nDecisions related to lethal force, life and death should not be delegated to AI systems. should have the ability to accurately distinguish between users, including detecting\nRules and standards should be adopted to ensure effective human control over those characteristics such as whether the user is an adult or a child. Additional protections must\ndecisions. be put in place for vulnerable populations, such as children, when informed consent cannot\nbe obtained, or when it may not be a sufficient safeguard.\nc AI systems with nudging strategies must be carefully evaluated, monitored, and controlled.\n14 IEEE. https://sagroups.ieee.org/7010/\n34 353.6.2.3 f In situations where it is needed, human rights impact assessments and human rights due\nHumanity should retain the power to govern itself and make the final decision, with AI in an diligence, human determination codes of ethical conduct, or quality labels and certifications\nassisting role. intended to promote human centered values and fairness should be considered.\ng Mechanisms should be put into place to receive external feedback regarding AI systems\na Decisions related to lethal force, life and death should not be delegated to AI systems. Rules\nthat potentially infringe on fundamental rights.\nand standards should be adopted to ensure effective human control over those decisions.\nb Responsible parties (e.g., parents, nurse practitioners, social workers, and governments)\nshould be trained to detect the influence due to AI and ineffective mitigation techniques.\nIn the most extreme cases it should always be possible to shut down harmful AI system.\nc Those actions undertaken by an affective system that are most likely to generate an\nemotional response should be designed to be easily changed.\nd Users should be able to make informed autonomous decisions regarding AI systems. They\nshould be given appropriate knowledge and tools to comprehend and interact with AI\nsystems to satisfactory degree and, where possible to reasonably self-assess or challenge\nthe system.\n3.6.2.4\nAI systems should be designed in a way that respects the rule of law, human rights, society\nvalues, and should include appropriate safeguards to ensure a fair and just society.\na AI Developers should design AI systems to adopt, learn and follow the norms and values of\nthe community they serve.\nb Organizations could identify the norms of the specific community in which AI systems are\nto be deployed in. In particular, pay attention to the norms relevant to the kinds of tasks that\nthe AI systems are designed to perform. These could be documented as well as how these\nnorms are addressed by the AI system.\nc To respond to the dynamic change of norms in society the AI system could be able to adjust\nits existing norms and learn new ones, while being transparent about these changes.\nd Designers should consider forms and metrics for assessing an AI system\u2019s norm conformity\nover the lifetime of the system (e.g. human-machine agreement on moral decisions,\nverifiability of AI decisions, justified trust).\ne The norm identification process must document the similarities and differences between\nthe norms that humans apply to other humans and the norms they apply to AI systems.\nNorm implementations should be evaluated specifically against the norms that the\ncommunity expects the AI system to follow.\n36 373.7 Sustainable and Environmentally Friendly AI (Principle 7)\nd While there is no standardized means of reporting on the environmental impact of AI\nsystems, where possible, model development and commissioning should make carbon\n3.7.1 Principle impacts a core consideration alongside functional and business requirements.\ne Smaller models should be considered \u2013 shrinking down the model size and using fewer\nWe will promote sustainable and environmentally compute cycles (to balance financial and performance costs with the end performance of\nthe model).\nfriendly AI\nf Carbon awareness should be considered \u2013 adjusting the operational parameters of the AI\nAI systems should be used to benefit all human beings, including future generations. The\nsystem to dynamically select the best time and location for energy use from the grid can\nenvironment is fundamental to this.\nreduce its carbon footprint.\nThroughout the AI lifecycle, implementations should only be carried out on the basis of a full\nunderstanding and acknowledgement of implications for sustainability and environment.\nExample: In agriculture, AI can transform Example: Google`s DeepMind division\nAn AI system\u2019s development, deployment and use, should be assessed via critical examination production by better monitoring and has developed AI that teaches itself to\nof resource usage and energy consumption.\nmanaging environmental conditions minimize the use of energy to cool Google`s\nand crop yields. AI can help reduce both data centers. As a result, Google reduced\nMechanisms to measure environmental impact due to type of energy use and processing\nfertilizer and water, all while improving its data center energy requirements by\npower provided by data centers should be established.\ncrop yields. 40% 15.\n3.7.2 Guidelines\n3.7.2.1 Throughout the AI lifecycle, implementations should always be carried out after full\nunderstanding and acknowledgement of AI implications on sustainability and environment.\na The application of Artificial Intelligence to address sustainability challenges are well\nunderstood. Building and running green, sustainable AI systems is of increasing importance,\ngiven the large carbon footprint that they can generate and the wider context of addressing\nclimate change.\nb AI model development, and by extension commissioning, should therefore seek to balance\ntechnical performance with energy consumption and environmental impact.\nc Efforts should be made to estimate and understand sustainability and environmental\nimpact across the AI lifecycle (e.g. the energy consumption costs associated with model\ntraining, and CO2 emissions and cloud compute costs associated with the deployment\nand running of the system).\n15 https://www.deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-by-40\n38 393.8 Privacy Preserving AI (Principle 8)\n3.8.2.2\nEstablish an oversight mechanism for data collection, storage and processing and use across\nyour organization\n3.8.1 Principle\na Assess who can access data and under which conditions.\nWe will respect people\u2019s privacy\nb Assign specific responsibilities and role for Data Protection Officers.\nAI systems should respect privacy and use the minimum intrusion necessary.\nc Prevention of harm to privacy necessitates adequate data governance that covers the\nquality and integrity of the data used, its relevance in light of the domain in which the\nAI systems should uphold high standards of data governance and security,\nprotecting personal information. AI systems will be deployed, its access protocols and the capability to process data in a\nmanner that protects privacy.\nSurveillance or other AI-driven technologies should not be deployed to the extent\nof violating internationally and/or UAE\u2019s accepted standards of privacy and human d AI systems must guarantee privacy and data protection throughout a system\u2019s entire\ndignity and people rights. lifecycle. This includes the information initially provided by the user, as well as the\ninformation generated about the user over the course of their interaction with the system\nPrivacy by design should be embedded in AI systems and where possible AI\n(e.g. outputs that the AI systems generated for specific users or how users responded to\nalgorithm should have adequate privacy impact assessments.\nparticular recommendations). Digital records of human behavior may allow AI systems\nto infer not only individuals\u2019 preferences, but also their age, gender, religious or political\nAdequate data protection frameworks and governance mechanisms should be\nviews. To allow individuals to trust the data gathering process, it must be ensured that data\nestablished in a multi-stakeholder approach and ensured throughout the life cycle\ncollected about them will not be used to unlawfully or unfairly discriminate against them.\nof AI systems.\ne In any given organization that handles individuals\u2019 data (whether someone is a user of the\nAI developers and operators should strive to strike the balance between privacy\nsystem or not), data protocols governing data access should be put in place in line with\nrequirements, individual rights and innovation growth and society benefits.\nnational privacy legislation (be these universal or sector-specific). These protocols should\noutline who can access data and under which circumstances. Only duly qualified personnel\n3.8.2 Guidelines\nwith the competence and need to access individual\u2019s data should be allowed to do so.\n3.8.2.1\nEstablish mechanism for users to flag issues related to privacy or data protection\na AI Operators should review the system for proper consent logging, ability of users to revoke\npermission whenever applicable.\nb Consider training AI model without or with minimal user of potentially sensitive or personal\ndata.\nc Use measures to enhance privacy such as encryption, anonymization and aggregation.\n40 41f Algorithmic systems require adequate privacy impact assessments, which also include\nsocietal and ethical considerations of their use and an innovative use of the privacy by\ndesign approach. AI actors need to ensure that they are accountable for the design and\nimplementation of AI systems in such a way as to ensure that personal information is\nprotected throughout the life cycle of the AI system.\ng Establish data policies or equivalent frameworks, or reinforce existing ones, to ensure full\nsecurity for personal data and sensitive data, which, if disclosed, may cause exceptional\ndamage, injury or hardship to individuals. Examples include data relating to offences,\ncriminal proceedings and convictions, and related security measures; biometric, genetic\nand health data; and personal data such as that relating to race, descent, gender, age,\nlanguage, religion, political opinion, national origin, ethnic origin, social origin, economic or\nsocial condition of birth, or disability and any other characteristics.\nh Promote mechanisms, such as open repositories for publicly funded or publicly held data,\nsource code and data trusts, to support the safe, fair, legal and ethical sharing of data,\namong others.\ni Promote and facilitate the use of quality and robust datasets for training, development and\nuse of AI systems, and exercise vigilance in overseeing their collection and use.\nExample: with the help of AI & ML algorithms16 , Synthetic data can be created to enhance\nprivacy. The data created will have the same statistical characteristics for the testing\nenvironment and where a third party can have access.\n16 https://research.aimultiple.com/privacy-enhancing-technologies/\n42 43Microsoft. 2022. Microsoft Responsible AI Standard, v2. General Requirements. June 2022.\n04\nRetrieved from:\nhttps://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4ZPmV\nISO. 2022. Proposed ISO Standard on Risk Management of AI: What Businesses Should Know.\nBibliography\nMay 2022. Retrieved from:\nhttps://unesdoc.unesco.org/ark:/48223/pf0000381137/PDF/381137eng.pdf.multi\nGSMA 2022. The AI Ethics Playbook. Implementing ethical principles into everyday business.\nFebruary 2022. Retrieved from:\nhttps://www.gsma.com/betterfuture/wp-content/uploads/202201//The-Mobile-Industry-Ethics-\nPlaybook_Feb-2022.pdf\nOECD. 2022. AI Policy Observatory. Retrieved from:\nhttps://standards.ieee.org/wp-content/uploads/import/documents/other/ead_v2.pdf\nUNESCO. 2022. Recommendation on the Ethics of Artificial Intelligence. Adopted in November\n2021. Retrieved from:\nhttps://unesdoc.unesco.org/ark:/48223/pf0000381137/PDF/381137eng.pdf.multi\nEuropean Commission. 2021. Proposal for a Regulation of the European Parliament and of the\nCouncil Laying Down Harmonized Rules on Artificial Intelligence (Artificial Intelligence Act) and\nAmending Certain Union Legislative Acts. Retrieved from:\nhttps://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A52021PC0206\nEuropean Commission. High Level Expert Group on Artificial Intelligence ( AI HLEG ). Ethics\nGuidelines for Trustworthy AI . April 2019. Retrieved from:\nhttps://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai\nCentre for Data Ethics and Innovation. UK Government: The roadmap to an effective AI assurance\necosystem. December 2021. Retrieved from:\nhttps://www.gov.uk/government/publications/the-roadmap-to-an-effective-ai-assurance-\necosystem\nThe New York City Council. 2021. A Local Law to amend the administrative code of the city of New\nYork, in relation to automated employment decision tools. Retrieved from:\nhttps://legistar.council.nyc.gov/LegislationDetail.aspx?ID=4344524&GUID=%20B051915D-A9AC-\n451E-81F86596032-FA3F9&\nUNESCO, Recommendation on the Ethics of Artificial Intelligence. 2021. Retrieved from:\nhttps://unesdoc.unesco.org/ark:/48223/pf0000381137\n44 45AI NOW Institute. Algorithmic accountability for the public sector. August 2021. Retrieved from: CNIL. (2017). How can humans keep the upper hand? The ethical matters raised by algorithms\nhttps://www.opengovpartnership.org/documents/algorithmic-accountability-public-sector/ and artificial intelligence. Retrieved from:\nhttps://www.cnil.fr/sites/default/files/atoms/files/cnil_rapport_ai_gb_web.pdf\nIEEE 2019. Ethically Aligned Design. Version 2 \u2013 For Public Discussion. Retrieved from:\nhttps://standards.ieee.org/wp-content/uploads/import/documents/other/ead_v2.pdf Executive Office of the President of the United States. National Science and Technology Council.\nNetworking and Information Technology Research and Development Subcommittee. (2016). The\nnational artificial intelligence research and development strategic plan. Retrieved from:\nIEEE 2017. Ethically Aligned Designed : A vision for Prioritizing Human Well-being with\nhttps://www.nitrd.gov/PUBS/national_ai_rd_strategic_plan.pdf\nAutonomous and Intelligent Systems. Retrieved from:\nhttps://standards.ieee.org/wp-content/uploads/import/documents/other/ead_v2.pdf\nExecutive Office of the President of the United States National Science and Technology Council.\nCommittee on Technology. (2016). Preparing for the future of artificial intelligence. Retrieved from:\nGoogle. AI Principles 2021. Retrieved from:\nhttps://obamawhitehouse.archives.gov/sites/default/files/whitehouse_files/microsites/ostp\nhttps://ai.google/principles/\nNSTC/preparing_for_the_future_of_ai.pdf\nMicrosoft. Responsible AI Standard v2 (June 2022). Retrieved from:\nThe Headquarters for Japan\u2019s Economic Revitalization. (2015). New Robot Strategy. Japan\u2019s\nhttps://www.microsoft.com/en-us/ai/responsible-ai\nRobot Strategy. Vision, Strategy, Action Plan. Retrieved from:\nhttp://www.meti.go.jp/english/press/2015/pdf/0123_01b.pdf\nThe ECONOMIST Group 2022 . Pushing forward: the future of AI in the Middle East and North\nAfrica Report. Retrieved from:\nTreasury Board of Canada Secretariat. (2018). Responsible Artificial Intelligence in the\nhttps://impact.economist.com/perspectives/sites/default/files/google_ai_mena_report.pdf\nGovernment of Canada. Digital Disruption White Paper Series. Version 2.0.\nhttps://docs.google.com/document/d/1Sn-qBZUXEUG4dVk909eSg5qvfbpNlRhzIefWPtBwbxY/\nPDPC. (2018, June 5). Discussion paper on Artificial Intelligence (AI) and Personal Data.\nedit\nSingapore: Personal Data Protection Commission Singapore (PDPC). Retrieved from:\nhttps://www.pdpc.gov.sg/-/media/Files/PDPC/PDF-Files/Resource-for-Organisation/AI/\nThe Toronto Declaration: Protecting the right to equality and non-discrimination in machine\nDiscussion-Paper-on-AI-and-PD---050618.pdf\nlearning systems. Toronto, Canada: Amnesty International and Access Now. Retrieved from:\nhttps://www.accessnow.org/cms/assets/uploads/201808//The-Toronto-Declaration_ENG_08-\nITI. AI Policy Principles. Retrieved from:\n2018.pdf\nhttps://www.itic.org/public-policy/ITIAIPolicyPrinciplesFINAL.pdf\nhttp://data.parliament.uk/writtenevidence/committeeevidence.svc/evidencedocument/artificial-\nCabinet Office.(2016, May 19). Data Science Ethical Framework. Retrieved from: intelligence-committee/artificial-intelligence/oral/73546.html (house of lords select committee\nhttps://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/ oral evidence, Q61)oral evidence, Q61)\nfile/524298/Data_science_ethics_framework_v1.0_for_publication__1_.pdf\nHouse of Lords Select Committee on Artificial Intelligence. (2018). AI in the UK: ready, willing and\nEuropean Parliament. (2017, February 16). European Parliament resolution of 16 February 2017 able?\nwith recommendations to the Commission on Civil Law Rules on Robotics (20152103/(INL)). https://publications.parliament.uk/pa/ld201719/ldselect/ldai/100100/.pdf\nRetrieved from:\nhttp://www.europarl.europa.eu/sides/getDoc.do?pubRef=-//EP//TEXT+TA+P8-TA-2017- House of Commons Science and Technology Select Committee. Algorithms in Decision Making.\n0++0051DOC+XML+V0//EN https://publications.parliament.uk/pa/cm201719/cmselect/cmsctech/351351/.pdf\nVillani, C. (2018). For a meaningful Artificial Intelligence towards a French and European strategy.\nRetrieved from:\nhttps://www.aiforhumanity.fr/pdfs/MissionVillani_Report_ENG-VF.pdf\n46 47", "metadata": {"country": "UAE", "year": "2022", "legally_binding": "no", "binding_proof": "None", "date": "01/10/2024", "regulator": "UAE Council for Artificial Intelligence and Blockchain", "type": "policy framework", "status": "active", "language": "English, Arabic", "use_cases": "[1, 3, 5, 6]"}}
{"_id": "68695b565520a4cc7989de78", "title": "Law No. 3 of 2024 Establishing the Artificial Intelligence and Advanced Technology Council (AIATC)", "source": "https://www.scribd.com/document/867231117/Abu-Dhabi-Law-No-3-2024-On-Establishing-the-AI-and-Advanced-Technology-Council", "text": "AI ETHICS\nARTIFICIAL INTELLIGENCE ETHICS\nPRINCIPLES & GUIDELINES\nPRINCIPLES & GUIDELINES\nDEC 2022Table of Content\nPage Page\n1. Overview 4 3.4 Explainable 29\n1.1 Introduction 5 3.4.1 Principle 29\nWe will make AI systems as explainable as technically possible\n1.2 Scope 6 3.4.2 Guidelines 29\n1.3 Strategic alignment 6 3.5 Robust, Safe and Secure AI 31\n1.4 Responsibility 8 3.5.1 Principles 31\nAI systems will be technically robust\n3.5.2 Guidelines 32\n1.5 Licensing 8\n3.6 Human Centred AI 34\n1.6 Toolkit 8\n3.6.1 Principles 34\nWe will give AI systems human values and make them beneficial to society\n3.6.2 Guidelines 35\n2. Definitions 10\n3.7 Sustainable and Environmentally Friendly AI 38\n3.7.1 Principle 38\n3. Principles and Guidelines 16\nWe will promote sustainabity and environemntly friendly AI 38\n3.7.2 Guidelines 38\n3.1 Fairness 17\n3.8 Privacy Preserving AI 38\n3.1.1 Principle 17\nWe will make AI systems fair\n3.8.1 Principle 40\n3.1.2 Guidelines 20\nWe will respect people\u2019s privacy\n3.8.2 Guidelines 40\n3.2 Accountable AI 20\n3.2.1 Principle 20\nWe will make AI systems accountable\n4. Bibliography 44\n3.2.2 Guidelines 20\n3.3 Transparent AI 26\n3.3.1 Principle 26\nWe will make AI systems transparent\n3.3.2 Guidelines 26\n2 301 1.1 Introduction\nAI\u2019s rapid advancement and innovation potential across a range of fields is incredibly exciting.\nYet a thorough and open discussion around AI ethics, and the principles organizations using this\nOverview\ntechnology must consider, is needed.\nThis document - AI Ethics: Principles and Guidelines \u2013 intends to meet this need to balance these\ntwo central considerations of AI. It is designed to offer detailed guidance to help AI actors adhere\nto eight principles of AI ethics.\nWe will make AI systems that are:\nexplainable transparent accountable fair\nprivacy preserving sustainable and human centered robust, safe and\nenvironmentally secure\nfriendly\nThe guidelines are non-binding, and are drafted as a collaborative multi-\nstakeholder effort, with full awareness of organizations\u2019 needs to innovate\nand protect their intellectual property. This is a collaborative process in\nwhich all stakeholders are invited to be part of an ongoing dialogue. We\nwould like to see the AI Ethics Guidelines evolve into a universal, practical\nand applicable framework informing ethical requirements for AI design and\nuse. The eventual goal is to reach widespread agreement and adoption of\ncommonly agreed policies to inform the ethical use of AI nationally and\naround the world.\n4 51.2 Scope Additionally, the AI Office is currently developing additional policies to be taken into consideration\nalong with this document:\nThis document gives non-mandatory guidelines for achieving the ethical design\nand deployment of AI systems in both the public and private sectors. AI already\nAI Seal AI Procurement Guidelines\nsurrounds us, but some applications are more visible and sensitive than others.\nThe UAE AI Seal brand (UAI) will be used The AI Procurement Guidelines will be\nThis document is applicable only to those AI systems which make or inform\nto attract talent and business from used to guide Federal Government entities\n\u2018significant decisions\u2019 \u2013 that is, those decisions which have the potential for\nacross the globe to come to the UAE to in the procurement of AI systems. They\nsignificant impact either on individuals or on society as a whole. They also apply to\ntest and develop AI. This includes a UAI will list the general principles, proposed\n\u2018critical decisions\u2019, which are a subset of significant decisions and are of especially\nmark recognizing high quality, ethical AI mechanisms for procurement and review\ncritical nature.\ncompanies. It would reward safe, efficient, of AI products between the federal\nverified AI technology with a \u2018UAI Seal government and vendors. The policy\nThis document guides the further developments of sector specific requirements\nof Approval\u2019. The UAE AI Seal brand is considers R&D for specific challenges\nand principles, hence every entity within its sector can further amend, use, or\ncurrently under development by the AI that do not currently have an AI solution\ncomplement its current frameworks to properly suit its context and the need of\nOffice. product that is market-ready. Based on\naffected stakeholders.\nglobal standards for AI Procurement, the\npolicy aims to incentivize AI adoption\nIt is a living document and will undergo further future reviews and enhancements.\nwithin government entities and promote\nAI R&D within the private sector for\nspecified government challenges. The\nprocurement policy is currently under\ndevelopment by the AI Office.\n1.3 Strategic alignment\nThis document comes as a fulfillment of the goals and aspirations set in the UAE National AI\nStrategy and the international Sustainable Development Goals (SDGs). The UAE has long been\nat the forefront of sustainable development. These shared global milestones have created global\nmovement and adoption in several disciplines and sectors. As an early adopter, the UAE has\nestablished a national committee on SDGs with a long-term realization of the UAE SDG agenda\n2030. These goals are further engrained and reflected in the AI Ethics Principles under the topics of\nfairness, inclusiveness, equality, human benefit, and rights.\n6 71.4 Responsibility\nThe AI Office will not be responsible for any misuse of the AI Ethics Principles and Guidelines. The\nuser bears all the consequences of their use.\n1.5 Licensing\nThis document is published under the terms of a Creative Commons Attribution 4.0 International\nLicense in order to facilitate its re-use by other governments and private sector organizations.\nIn summary this means you are free to share and adapt the material, including for commercial\npurposes, provided that you give appropriate credit to the UAE AI and Blockchain Council as its\nowner and do not suggest the Minister of State for Artificial Intelligence, Digital Economy and\nRemote Work Applications Office endorses your use.\n1.6 Toolkit\nThe document is part of a toolkit of self-governing nature that boosts awareness and enables\ngovernment and private institutions to pursue innovative use cases while maintaining human\nvalues and principles.\n8 92.1 AI developer organization\n02\nAn organization which does any of the following:\na determines the purpose of an AI system;\nDefinitions\nb designs an AI system;\nc builds an AI system, or:\nd performs technical maintenance or tuning on an AI system\nNote:\nThe definition applies regardless of whether the organization is the ultimate user of the system, or\nwhether they sell it on or give it away.\nExample: A company develops an artificially intelligent facial recognition system and\nsells it to a country\u2019s border force, who use it to identify suspicious personnel. The company\nis an AI developer organization and the border force is an AI operator organization.\n2.2 AI operator organization\nAn organization which does any of the following:\na uses AI systems in operations, backroom processes or decision-making;\nb uses an AI system to provide a service to an AI subject;\nc is a business owner of an AI system;\nd procures and treats data for use in an AI system; or\ne evaluates the use case for an AI system and decides whether to proceed.\nNotes:\n1. This definition applies regardless of whether the AI system was developed in-house or procured.\n2. It is possible for organizations to be both an AI developer organization and an AI operator organization.\n10 112.6 Critical decision\n2.3 Artificial Intelligence (also AI)\nThe capability of a functional unit to perform functions that are generally associated with human An individually significant decision which is deemed to either have a very large impact on an\nintelligence such as reasoning, learning and self-improvement1. individual or to have especially high stakes. These can be especially sensitive, have the potential\nto cause high loss or damage, be societally significant, or set an important precedent.\n2.4 Artificially Intelligent System (also AI system)\nNotes:\nThe types of decisions referred to here are the same as those in the definition of significant-at-\nA product, service, process or decision-making methodology whose operation or outcome is\nscale decisions, except in this case the effects are felt as a result of an individual decision rather\nmaterially influenced by artificially intelligent functional units.\nthan an aggregate of many decisions..\nNotes:\n1. It is not necessary for a system\u2019s outcome to be solely determined by artificially intelligent\nfunctional units in order for the system to be defined as an artificially intelligent system; and Example: A court determines whether a defendant is guilty of a criminal charge, with the\n2. A particular feature of AI systems is that they learn behavior and rules not explicitly punishment for guilt being a life sentence. This is a critical decision because it has a very\nprogrammed in. large impact on the life of the defendant and also sets precedent for similar cases in the\nfuture.\nExample 1: A small claims court uses Example 2: A government entity uses\n2.7 Functional Unit\nan artificially intelligent software package a chatbot which allows customers to ask\nto collect evidence pertaining to a case, routine questions, book appointments\nEntity of hardware or software, or both, capable of accomplishing a specified purpose3 .\ncompare it to similar cases in the past, and conduct minor financial transactions.\nand present a recommended decision The chatbot responds to customer\nto a judge. The judge determines the queries with pre-written responses and is\n2.8 Individually significant decision\nfinal outcome. This decision-making based on pre-programmed decision rules.\nA decision which has the potential for significant impact on at least one individual\u2019s\nmethodology is materially influenced Therefore, the chatbot is not an AI system.\ncircumstances, behavior or choices, or has legal or similarly significant effects on him or her.\nby an artificially intelligent functional If, however, the chatbot autonomously\nunit, and is therefore classified as an AI adjusted its treatment of customers\nsystem. based on the outcome of past cases, it\nwould be an AI system. Example: A company decides to make an employee redundant. This is an individually\nsignificant decision because of its potential impact on the employee\u2019s financial situation.\n2.5 Bias (of a system)\nInclination or prejudice for or against one person or group, especially in a way considered to be\nunfair2.\n2 Oxford Dictionary 2018, Oxford University Press, viewed online 4th October 2018, <https://en.oxforddictionaries.com>\n1 Consistent with ISO/IEC 2382:2015 3 From ISO/IEC 2382:2015\n12 132.9 Non-operational bias (of a system) 2.11 Significant decision\nBias that is either: A decision which is either individually significant or is part of a set of significant-at-scale\ndecisions.\na not a design feature; or\nb not important in achieving the stated purpose of the system. 2.12 Subject of an artificially intelligent system (also AI subject)\nA natural person who is any of the following:\n2.10 Set of significant-at-scale decisions\na an end-user of an AI system\nA set of decisions made by the same system or organization which, when taken in aggregate,\nb directly affected by the operation of or outcomes of an AI system, or:\nhave significant impact on society as a whole or groups within it.\nc a recipient of a service or recommendation provided by an AI system\nNotes:\n1. The decisions need not be individually significant in order to qualify, in aggregate, as a set of\nsignificant-at-scale decisions; and\n2. Examples of areas which have a large impact on society and which include but are not limited\nto: the large-scale allocation of resources or opportunities amongst groups; the structure of\ngovernment; the division of power between large entities or groups; the law, and its interpretation\nand enforcement; conflict and war; international relations, etc.\nExample: An AI system is used by a website to determine which content to show users.\nThis decision is not individually significant, since a user is not greatly affected by whether\na particular piece of media is shown to them. However, if the website is popular then the AI\nsystem may be making a set of significant-at-scale decisions, because any biases in the\nsystem will affect a large number of users.\n14 153.1 Fairness (Principle 1)\n03\n3.1.1 Principle\nPRINCIPLES AND We will make AI systems fair\nData ingested should, where possible, be accurate and representative of the affected\npopulation.\nGUIDELINES\nAlgorithms should avoid non-operational bias.\nSteps should be taken to mitigate and disclose the biases inherent in datasets.\nSignificant decisions should be provably fair.\nAll personnel involved in the development, deployment and use of AI systems have a role and\nresponsibility to operationalize AI fairness and should be educated accordingly.\n3.1.2 Guidelines\n3.1.2.1\nBenefits of AI systems should be available and accessible to all. AI systems should not discriminate\nagainst people or groups in a way that could have an adverse impact. Especially significant and\ncritical decisions made or assisted by AI should be provably fair.\na Considering that fairness can have many different definitions, an organization should\ndocument its own definition of fairness for the context that the AI system is going to be\nimplemented in. Organizations should document what the implemented fairness objective\nstands for and why this choice was considered the most suitable for the given scenario.\nb It is recommended to identify and document demographic groups that may be adversely\nimpacted and mitigate the risk where possible.\nc AI developers and AI operators could consider formal procedures such as Discrimination\nImpact Assessments as a means of ensuring fairness. This assessment should be\ndocumented as well as the mitigation measures that were implemented by the organization.\nThe impact assessment should be conducted pre-release and regularly after as an ongoing\nevaluation, results are advised to be documented.\n16 173.1.2.2 3.1.2.3\nConsideration should be given to whether the data ingested is accurate and representative Consideration should be given to whether decision-making processes introduce bias\nof the affected population\na When subjecting different groups to different decision-making processes, AI developers\nd Fairness has many different definitions in different cultures and for different contexts. should consider whether this will lead to non-operational bias.\nEncouraging a diverse and inclusive AI ecosystem is thus all the more crucial to ensure\nthat one definition of fairness does not contradict another, and that the process of defining b When evaluating the fairness of an AI system, AI developers and AI operators should\nfairness itself is fair, with under-represented groups represented in the discussion. consider whether AI subjects in the same circumstances receive equal treatment.\ne AI developers and AI operators should undertake reasonable data exploration and/or testing\nto identify potentially prejudicial decision-making tendencies in AI systems arising from\nExample: An organization uses an AI tool to automate the pre-screening of candidates\ndata inaccuracy and biases in the data.\nfor a job opening. It is trained on data from the company\u2019s existing employees, the majority\nof whom are from the same ethnic background. Therefore, the system learns to use name\nf AI developers and operators should evaluate all datasets to assess inclusiveness of and nationality as discriminating factors in filtering job applicants. This could have been\nidentified demographic groups and collect data to close any gaps.\nidentified through testing and rectified by, for example, balancing the training data or only\nusing relevant data fields for training.\ng AI developers and AI operators should refrain from training AI systems on data that is not\nlikely to be representative of the affected AI subjects, or is not likely to be accurate, whether\nthat be due to age, omission, method of collection, or other factors.\n3.1.2.4\nSignificant decisions informed by the use of AI should be fair\nExample: Following a natural disaster, a government relief agency uses an AI system to a AI developers and AI operators could consider formal procedures such as Discrimination\ndetect communities in greatest need by analyzing social media data from a range of websites. Impact Assessments as a means of ensuring fairness.\nHowever, those communities where smartphone penetration is lower having less presence on\n3.1.2.4\nsocial media, and so are at risk of receiving less attention. Therefore, the charity complements\nAI operators should consider whether their AI systems are accessible and usable in a fair\ntheir AI tool with traditional techniques to identify needy populations elsewhere.\nmanner across user groups\n3.1.2.5\nh AI developers should consider whether their AI systems can be expected to perform well Consideration should be given to the effect of diversity on the development and deployment\nwhen exposed to previously unseen data, especially when evaluating people who are not processes\nwell-represented in the training data.\nb Organizations should seek to include people from diverse demographic backgrounds across\nthe full lifecyle, to include design, development and deployment processes. Organizations\nshould seek to engage diverse internal and external groups also.\nc AI developers should consider whether the assumptions they make about AI subjects could\nbe wrong or are likely to lead to non-operational bias; if so, they should consider consulting\nthe AI subjects in a representative manner during the design, development and deployment\nto confirm these assumptions.\n18 193.2 Accountable AI (Principle 2)\n3.2.2.2\nPositive efforts should be made to identify and mitigate any significant risks inherent in the AI\nsystems designed\n3.2.1 Principle\na AI operators should only use AI systems that are backed by evidence-based academic or\nindustrial research, and AI developers should base their development on such research.\nWe will make AI systems accountable\nAccountability for the outcomes of an AI system lies not with the system itself but is\napportioned between those who design, develop and deploy it; Example: A foreign country has a government service which identifies parents who owe\nmoney in child maintenance. The data matching process is often incorrect due to misspelled\nDevelopers should make efforts to mitigate the risks inherent in the systems they design; names or missing data which results in some individuals being incorrectly targeted\nautomatically by the system with the result being a large bill, poor credit ratings and even\nAI systems should have built-in appeals procedures whereby users can challenge significant\nfreezing wages. The recourse for individuals who are incorrectly targeted is time-consuming\ndecisions;\nand not straightforward . If the potential impact of incorrect decisions had been assessed,\nmitigation measures such a user-friendly review procedure could have been set up.\nAI systems should be developed by diverse teams which include experts in the area in which\nthe system will be deployed; and\nAI systems should be subject to external audit and decision quality assurance. b AI operators should identify the likely impact of incorrect automated decisions on AI\nsubjects and, in the case where incorrect decisions are likely to cause significant cost or\ninconvenience, consider mitigating measures.\n3.2.2 Guidelines\nc AI operators should consider internal risk assessments or ethics frameworks as a means to\n3.2.2.1 facilitate the identification of risks and mitigating measures.\nAccountability for the outcomes of an AI system should not lie with the system itself\nd In designing AI systems to inform significant decisions, AI developers should consider\na Accountability for loss or damages resulting from the application of AI systems should not measures to maintain data accuracy over time, including:\nbe attributed to the system itself.\n\u2022 the completeness of the data.\nb AI operators and AI developers should consider designating individuals to be responsible for \u2022 timely update of the data, and.\ninvestigating and rectifying the cause of loss or damage arising from the deployment of AI\n\u2022 whether the context in which the data was collected affects its suitability for the\nsystems.\nintended use case.\nExample: A border camera scanning for predictors of risk may misinterpret a \u201ctic\u201d of an\nindividual with Tourette syndrome as suspicious. These can manifest in a diverse fashion, and\nshould not cause this person to undergo secondary inspection every time they pass through\nthe border . If the data is updated after the first case is encountered then it would avoid\ncausing inconvenience on subsequent visits.\n5 Government of Canada. (2018). Responsible AI in the Government of Canada. Digital Disruption White Paper Series. Version 2.0, p.26. Retrieved from:\nhttps://docs.google.com/document/d/1Sn-qBZUXEUG4dVk909eSg5qvfbpNlRhzIefWPtBwbxY/edit\n4 Cabinet Office (UK), Data Science Ethical Framework, Version 1.0, licensed under the Open Government License v3.0, p. 13 6 Stoica, I. et. al.,2017, A Berkeley View of Systems Challenges for AI, p. 2, https://arxiv.org/pdf/1712.05855.pdf\n20 21e AI developers and AI operators should tune AI models periodically to cater for changes to e Be mindful that there might be fundamental tensions between different principles and\ndata and/or models over time. requirements. Continuously identify, evaluate, document and communicate these trade-\noffs and their solutions.\nf AI operators should consider whether AI systems trained in a static environment will display\nmodel instability when deployed in dynamic environments. Adopt a Trustworthy AI assessment list when developing, deploying or using AI systems,\nf\nand adapt it to the specific use case in which the system is being applied.\nExample: AI systems will need to be able to adapt to the changes in the environment that g Keep in mind that such an assessment list will never be exhaustive. Ensuring Trustworthy\nthey are deployed in. For example, a self-driving car would need to quickly adapt to unexpected AI is not about ticking boxes, but about continuously identifying and implementing\nand dangerous road by learning in real time from other cars that have successfully dealt with requirements, evaluating solutions, ensuring improved outcomes throughout the AI\nthese conditions. In addition, such mission-critical applications must handle noisy inputs system\u2019s lifecycle, and involving stakeholders in this.\nand defend against malicious actors .\n3.2.2.4\nAI subjects should be able to challenge significant automated decisions concerning them and,\ng AI developers should collaborate with AI Operators to train models using historical data where appropriate, be able to opt out of such decisions\nfrom the Operator.\na AI operators using AI systems to inform significant decisions should provide procedures by\nwhich affected AI subjects can challenge a specific decision concerning them.\nh AI Operators should consider working with their vendors (AI developers) to continually\nmonitor performance.\nb AI operators should consider such procedures even for non-significant decisions.\ni AI Operators should subject AI systems informing significant decisions to quality checks\nat least as stringent as those that would be required of a human being taking the same c AI operators should make affected AI subjects aware of these procedures and should\ndesign them to be convenient and user-friendly.\ndecision.\nd AI operators should consider employing human case evaluators to review any such\n3.2.2.3\nchallenges and, when appropriate, overturn the challenged decision.\nAI systems informing critical decisions should be subject to appropriate external audit\na When AI systems are used for critical decisions, external auditing of the AI systems in\nquestion should be used as a means to ensure meaningful standards of transparency and Example: A bank allows customers to apply for a loan online by entering their data. The\naccountability are upheld. bank uses an AI system to automatically determine whether to give the loan and what the\ninterest rate should be. They provide users with an option to contest the decision and have\nb In the case that critical decisions are of civic interest, public release of the results of the it reviewed by a human . They also require that customers justify their challenge by filling in\naudit should be considered as a means of ensuring public processes remain accountable a form, which assists the case reviewer and deters customers from challenging a decision\nto those affected by them. without good reason.7\nIn the case that critical decision are life and death decisions, these decisions should be\nc\nsupported by further validation and verification via a human operator.\nd Facilitate traceability and auditability of AI systems, particularly in critical contexts or\nsituations.\n7 Adapted from EU Commission, Can I be subject to automated individual decision-making, including profiling? Retrieved from: https://ec.europa.eu/\ninfo/law/law-topic/data-protection/reform/rights-citizens/my-rights/can-i-be-subject-automated-individual-decision-making-including-profiling_\nen#example\n22 23e AI operators should consider instituting an opt-out mechanism for significant automated 3.2.2.7\ndecisions. AI operators should understand the AI systems they use sufficiently to assess their suitability\nfor the use case and to ensure accountability and transparency\nf AI operators could consider \u201ccrowd challenge\u201d mechanisms whereby a critical number of\ncomplaints triggers an investigation into the fairness and/or accuracy of a decision-making\na In the case of critical decisions, AI operators should avoid using AI systems that cannot be\nprocess as a whole.\nsubjected to meaningful standards of accountability and transparency.\n3.2.2.5 b AI developers should consider notifying customers and AI operators of the use cases for\nAI systems informing significant decisions should not attempt to make value judgements on which the system has been designed, and those for which it is not suitable.\npeople\u2019s behalf without prior consent\na When informing an AI subject about significant choices they will make, AI systems should\nnot unreasonably restrict the available options or otherwise attempt to influence their value\njudgements without the explicit consent of the AI subject in question.\n3.2.2.6\nAI systems informing significant decisions should be developed by diverse teams with\nappropriate backgrounds\na AI developers who develop AI systems which may be used to assist in making critical\ndecisions should involve in the process experts with a background in social science, policy,\nor another subject which prepares them to evaluate the broader societal impact of their\nwork.\nb Development of AI systems informing significant decisions should include consultation\nwith experts in the field in which the system will be deployed.\nExample: An app that uses AI to assess medical symptoms and has a large user base had\nto face regulatory scrutiny because of number of complaints from doctors. They warned\nthat the application can miss signs of serious illness. A number of different shortcomings\nwere identified by doctors, some of which the company could address and resolve.8\n8 Financial Times. 2018. High-profile health app under scrutiny after doctor\u2019s complaints. Retrieved from: https://www.ft.com/content/19dc6b7e-8529-\n11e896-dd-fa565ec55929\n24 253.3 Transparent AI (Principle 3)\nc To facilitate the above, AI developers and AI operators should consider documenting the\nfollowing information during the design, development and deployment phases, and retaining\n3.3.1 Principle this documentation for a length of time appropriate to the decision type or industry:\n\u2022 the provenance of the training data, the methods of collection and treatment, how the data was\nWe will make AI systems transparent\nmoved, and measures taken to maintain its accuracy over time;\n\u2022 the model design and algorithms employed; and\nDevelopers should build systems whose failures can be traced and diagnosed. \u2022 changes to the codebase, and authorship of those changes.\nPeople should be told when significant decisions about them are being made by AI. d Where possible given the model design, AI developers should consider building in a means\nby which the \u201cdecision journey\u201d of a specific outcome (i.e. the component decisions leading\nWithin the limits of privacy and the preservation of intellectual property, those who deploy to it) can be logged.\nAI systems should be transparent about the data and algorithms they use.\nResponsible disclosures should be provided in a timely manner, and provide reasonable\nExample: A technology company has a product which is designed to assist in medical\njustifications for AI systems outcomes. This includes information that helps people\ndiagnosis. It documents each stage of its reasoning and relates it back to the input data.9\nunderstand outcomes, like key factors used in decision making.\n3.3.2 Guidelines\n3.3.2.1\nTraceability should be considered for significant decisions, especially those that have the\npotential to result in loss, harm or damage.\na For AI systems which inform significant decisions, especially those with the potential to\ncause loss, harm or damage, AI developers should consider building in traceability (i.e. the\nability to trace the key factors leading to any specific decision).\nb Organizations should ensure that harms caused through AI systems are investigated and\nredressed, by enacting strong enforcement mechanisms and remedial actions, to make\ncertain that human rights and the rule of law are respected in the digital world and in the\nphysical world.\n9 See IBM WatsonPaths\n26 273.3.2.2 3.4 Explainable AI (Principle 4)\nPeople should be informed of the extent of their interaction with AI systems\n3.4.1 Principle\na AI operators should inform AI subjects when a significant decision affecting them has been\nmade by an AI system.\nWe will make AI systems as explainable as\ntechnically possible\nExample: A small claims court adjudicates minor civil matters such as debt collection and\nevictions. They introduce an AI system to suggest the outcome of a ruling. At the time of the\nDecisions and methodologies of AI systems which have a significant effect on individuals\nruling the plaintiff and defendant are notified that the decision was assisted by an AI system.\nshould be explainable to them, to the extent permitted by available technology.\nThe court also provides an explanation for the decision.\nIt should be possible to ascertain the key factors leading to any specific decision that\ncould have a significant effect on an individual.\nb If an AI system can convincingly impersonate a human being, it should do so only after\nIn the above situation we will provide channels through which people can request such\nnotifying the AI subject that it is an AI system.\nexplanations.\nExample: A technology company produces a conversational AI agent which can make 3.4.2 Guidelines\nsome phone calls on behalf of its users. Those who receive the calls may believe that they\n3.4.2.1\nare speaking to a human. Therefore, the company programs the agent to identify itself at\nAI operators could consider providing affected AI subjects with a high-level explanation of\nthe start of every conversation.\nhow their AI system works\na AI operators could consider informing the affected AI subjects in understandable, non-\ntechnical language of:\n\u2022 the data that is ingested by the system;\n\u2022 the types of algorithms employed;\n\u2022 the categories into which people can be placed; and\n\u2022 the most important features driving the outcomes of decisions\n\u2022 A comprehensive list of feature engineering and models that was considered during\nthe model building phase.\nExample: A person turned down for a credit card might be told that the algorithm took\ntheir credit history, age, and postcode into account, but not learn why their application was\nrejected10 .\n28 29b For non-sensitive public sector use cases designed for the common good, AI operators\ncould consider making source code, together with an explanation of the workings of the AI Example: The UK\u2019s NHS developed a tool called Predict, which allows women with breast\nsystem, available either publicly or upon request (this should be done only if there is low risk cancer to compare their case to other women who have had the same condition in the past,\nof people \u2018gaming the system\u2019). and visualize the expected survival rate under various treatment options. The website has an\nexplanation page which shows the weights behind various factors and contains a description\nc The AI operator should maintain necessary documentation that provides elaboration and of the underlying mathematics12 .\nclarification on how the algorithm works, for example documentation of processes, decision\nmaking flow charts of the system, and how the appeal process is embedded.\nd An individual should have the ability to contest and seek effective redress against decisions c In the case that such explanations are available, they should be easily and quickly accessible,\nmade by AI systems. These must be addressed by the group or team supporting these free of charge and user-friendly.\nmodels.\ne Develop appropriate impact indices for the evaluation of AI system technological 3.5 Robust, Safe and Secure AI (Principle 5)\ninterventions from multiple perspectives.\n3.5.1 Principles\n3.4.2.2\nAI operators should consider providing affected AI subjects with a means to request\nexplanations for specific significant decisions, to the extent possible given the state of present AI systems will be technically robust\nresearch and the choice of model\nAI systems should be technically robust with a preventative approach to risks which\na AI operators should consider providing a means by which people affected by a significant\noperates in a manner such that they reliably behave.\ndecision informed by AI can access the reasoning behind that decision.\nAI Developers should ensure AI systems will not cause any unintentional harm and\nadverse impacts.\nExample: The US Consumer Financial Protection Bureau requires that creditors in the US\nwho reject credit applications must explain to the applicant the principal reason(s) why their\nAI systems should be resilient to attacks and security such as data poisoning and\napplication was rejected (e.g. \u201clength of residence\u201d or \u201cage of automobile\u201d)11 . In particular,\nmodel leakage.\n\u201cstatements that the adverse action was based on the creditor`s internal standards or\npolicies or that the applicant, joint applicant, or similar party failed to achieve a qualifying AI systems should have safeguards that enable a fallback plan in case of problems.\nscore on the creditor`s credit scoring system are insufficient\u201d.\nAI system results should be reproducible.\nb Where such explainability is not possible given available technology, AI operators should\nconsider compromises such as counterfactual reasoning, or listing the most heavily\nweighted factors contributing to the decision.\n11 Consumer Financial Protection Bureau, 12 CFR Part 1002 - Equal Credit Opportunity Act (Regulation B), \u00a7 1002.9 Notifications, Retrieved from https://www.\n10 The Guardian. AI watchdog needed to regulate automated decision-making, say experts. Retrieved from: https://www.theguardian.com/technology/2017/ consumerfinance.gov/policy-compliance/rulemaking/regulations/1002/\njan/27/ai-artificial-intelligence-watchdog-needed-to-prevent-discriminatory-automated-decisions 12 Predict website, accessible at http://www.predict.nhs.uk/predict_v2.1/legal/algorithm\n30 313.5.2 Guidelines\n3.5.2.1 3.5.2.3\nAI Operators should continue conducting vulnerability assessments, verification of AI system AI Operators should assure end users of the system\u2019s reliability through documentation, and\ndifferent behaviors in unexpected situations and for any dual-use case, to include: operationalizing processes for testing and verification of desired outcomes , specifically:\na Putting measures to ensure integrity and resilience of IA system against potential attacks. a AI system should be designed with an approach to continue monitoring if the system meets\ngoals, purposes and intended application.\nb AI Operators should define an approach to ensure results are reproducible through clear\nExample: IBM researchers presented a novel approach to exploiting AI for malicious\nprocess and documentation.\npurposes. Their system, DeepLocker embeds AI capabilities within the malware itself in order\nto improve its evasion techniques. It uses an Artificial Intelligence model to identify its target\nc AI Operators should publish documentation to assure system robustness to end users.\nusing indicators like facial recognition, geolocation and voice recognition.\n3.5.2.4\nAI systems will be safe, secure and controllable by humans\nb Verifying how AI systems behave in unexpected situations and environments.\na Safety and security of people, be they operators, end-users or other parties, will be of\nc Taking preventative measures against any possible system dual-case use. paramount concern in the design of any AI system.\nb AI systems should be verifiably secure and controllable throughout their operational\n3.5.2.2\nlifetime, to the extent permitted by technology.\nAI operators should define a suitable fall back plan and test it to maintain readiness in\nunexpected situations and in high safety risks level, to include:\nc The continued security and privacy of users should be considered when decommissioning\na Developing a plan to measure and assess potential safety risks to you or any third party AI systems.\nfrom technology use accidental or malicious misuse.\nd AI systems that may directly impact people\u2019s lives in a significant way should receive\nb Define thresholds for system acceptable results and governance procure to fall back on commensurate care in their designs.\nalternative defined and tested plans.\ne Such systems should be able to be overridden or their decisions reversed by designated\nc Considering an insurance policy to mitigate risks arising from potential damages. individuals.\nExample: Adopting a risk-based approach to procurement and clearly communicating it to\nvendors can help address such issues by giving the procuring organization advance notice\nof the specific oversight capabilities it will need in future stages of the system lifecycle,\npreventing vendors from presenting intellectual property arguments against required testing,\nmonitoring and auditing of their AI systems going forward and rewarding vendors\u2014of all\nsizes\u2014that are more advanced and responsive in their responsible AI efforts.\n13 IBM. https://documents.trendmicro.com/assets/white_papers/wp-malicious-uses-and-abuses-of-artificial-intelligence.pdf\n32 333.6.2 Guidelines\n3.5.2.5\nAi systems should not be able to autonomously hurt, destroy or deceive humans\n3.6.2.1\na AI systems should be built to serve and inform, and not to deceive and manipulate.\nAI should be developed to align with human values, contribute to human flourishing and\nbenefit society.\nb Nations should collaborate to avoid an arms race in lethal autonomous weapons, and\nsuch weapons should be tightly controlled. a Society should be consulted in a representative fashion to inform the development of AI.\nc Active cooperation should be pursued to avoid corner-cutting on safety standards. b Stakeholders throughout society should be involved in the development of AI and its\ngovernance.\nSystems designed to inform significant decisions should do so impartially.\nd\nc Stakeholders should proactively engage in responsible stewardship of trustworthy AI in\npursuit of beneficial outcomes for people and the planet.\n3.6 Human Centered AI (Principle 6)\nd Organizations should prioritize having all their stakeholders learn about wellbeing metrics\nas a potential determinant of how they create, deploy, market and monitor their AI\n3.6.1 Principles technologies.\nWe will give AI systems human values and\nExample: IEEE P7010 recommended practice14 establishes wellbeing metrics relating to\nmake them beneficial to society human factors directly affected by intelligent and autonomous systems and establishes a\nbaseline for the types of objective and subjective data these systems should analyze and\nGovernment will support the research of the beneficial use of AI. include (in their programming and functioning) to proactively increase human wellbeing.\nStakeholders throughout society should be involved in the development of AI and its\ngovernance.\n3.6.2.2\nSystematic analyses that examine the ethics of designing affective systems to nudge human\nDesign AI systems to adopt, learn and follow the norms and values of the community\nbeings prior to deployment are needed.\nthey serve.\nSystematic analyses that examine the ethics of designing affective systems to nudge a Nudging in AI systems should have an opt-in system policy with explicit consent.\nhuman beings prior to deployment are needed.\nb We recommend that when appropriate, an affective system that nudges human beings\nDecisions related to lethal force, life and death should not be delegated to AI systems. should have the ability to accurately distinguish between users, including detecting\nRules and standards should be adopted to ensure effective human control over those characteristics such as whether the user is an adult or a child. Additional protections must\ndecisions. be put in place for vulnerable populations, such as children, when informed consent cannot\nbe obtained, or when it may not be a sufficient safeguard.\nc AI systems with nudging strategies must be carefully evaluated, monitored, and controlled.\n14 IEEE. https://sagroups.ieee.org/7010/\n34 353.6.2.3 f In situations where it is needed, human rights impact assessments and human rights due\nHumanity should retain the power to govern itself and make the final decision, with AI in an diligence, human determination codes of ethical conduct, or quality labels and certifications\nassisting role. intended to promote human centered values and fairness should be considered.\ng Mechanisms should be put into place to receive external feedback regarding AI systems\na Decisions related to lethal force, life and death should not be delegated to AI systems. Rules\nthat potentially infringe on fundamental rights.\nand standards should be adopted to ensure effective human control over those decisions.\nb Responsible parties (e.g., parents, nurse practitioners, social workers, and governments)\nshould be trained to detect the influence due to AI and ineffective mitigation techniques.\nIn the most extreme cases it should always be possible to shut down harmful AI system.\nc Those actions undertaken by an affective system that are most likely to generate an\nemotional response should be designed to be easily changed.\nd Users should be able to make informed autonomous decisions regarding AI systems. They\nshould be given appropriate knowledge and tools to comprehend and interact with AI\nsystems to satisfactory degree and, where possible to reasonably self-assess or challenge\nthe system.\n3.6.2.4\nAI systems should be designed in a way that respects the rule of law, human rights, society\nvalues, and should include appropriate safeguards to ensure a fair and just society.\na AI Developers should design AI systems to adopt, learn and follow the norms and values of\nthe community they serve.\nb Organizations could identify the norms of the specific community in which AI systems are\nto be deployed in. In particular, pay attention to the norms relevant to the kinds of tasks that\nthe AI systems are designed to perform. These could be documented as well as how these\nnorms are addressed by the AI system.\nc To respond to the dynamic change of norms in society the AI system could be able to adjust\nits existing norms and learn new ones, while being transparent about these changes.\nd Designers should consider forms and metrics for assessing an AI system\u2019s norm conformity\nover the lifetime of the system (e.g. human-machine agreement on moral decisions,\nverifiability of AI decisions, justified trust).\ne The norm identification process must document the similarities and differences between\nthe norms that humans apply to other humans and the norms they apply to AI systems.\nNorm implementations should be evaluated specifically against the norms that the\ncommunity expects the AI system to follow.\n36 373.7 Sustainable and Environmentally Friendly AI (Principle 7)\nd While there is no standardized means of reporting on the environmental impact of AI\nsystems, where possible, model development and commissioning should make carbon\n3.7.1 Principle impacts a core consideration alongside functional and business requirements.\ne Smaller models should be considered \u2013 shrinking down the model size and using fewer\nWe will promote sustainable and environmentally compute cycles (to balance financial and performance costs with the end performance of\nthe model).\nfriendly AI\nf Carbon awareness should be considered \u2013 adjusting the operational parameters of the AI\nAI systems should be used to benefit all human beings, including future generations. The\nsystem to dynamically select the best time and location for energy use from the grid can\nenvironment is fundamental to this.\nreduce its carbon footprint.\nThroughout the AI lifecycle, implementations should only be carried out on the basis of a full\nunderstanding and acknowledgement of implications for sustainability and environment.\nExample: In agriculture, AI can transform Example: Google`s DeepMind division\nAn AI system\u2019s development, deployment and use, should be assessed via critical examination production by better monitoring and has developed AI that teaches itself to\nof resource usage and energy consumption.\nmanaging environmental conditions minimize the use of energy to cool Google`s\nand crop yields. AI can help reduce both data centers. As a result, Google reduced\nMechanisms to measure environmental impact due to type of energy use and processing\nfertilizer and water, all while improving its data center energy requirements by\npower provided by data centers should be established.\ncrop yields. 40% 15.\n3.7.2 Guidelines\n3.7.2.1 Throughout the AI lifecycle, implementations should always be carried out after full\nunderstanding and acknowledgement of AI implications on sustainability and environment.\na The application of Artificial Intelligence to address sustainability challenges are well\nunderstood. Building and running green, sustainable AI systems is of increasing importance,\ngiven the large carbon footprint that they can generate and the wider context of addressing\nclimate change.\nb AI model development, and by extension commissioning, should therefore seek to balance\ntechnical performance with energy consumption and environmental impact.\nc Efforts should be made to estimate and understand sustainability and environmental\nimpact across the AI lifecycle (e.g. the energy consumption costs associated with model\ntraining, and CO2 emissions and cloud compute costs associated with the deployment\nand running of the system).\n15 https://www.deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-by-40\n38 393.8 Privacy Preserving AI (Principle 8)\n3.8.2.2\nEstablish an oversight mechanism for data collection, storage and processing and use across\nyour organization\n3.8.1 Principle\na Assess who can access data and under which conditions.\nWe will respect people\u2019s privacy\nb Assign specific responsibilities and role for Data Protection Officers.\nAI systems should respect privacy and use the minimum intrusion necessary.\nc Prevention of harm to privacy necessitates adequate data governance that covers the\nquality and integrity of the data used, its relevance in light of the domain in which the\nAI systems should uphold high standards of data governance and security,\nprotecting personal information. AI systems will be deployed, its access protocols and the capability to process data in a\nmanner that protects privacy.\nSurveillance or other AI-driven technologies should not be deployed to the extent\nof violating internationally and/or UAE\u2019s accepted standards of privacy and human d AI systems must guarantee privacy and data protection throughout a system\u2019s entire\ndignity and people rights. lifecycle. This includes the information initially provided by the user, as well as the\ninformation generated about the user over the course of their interaction with the system\nPrivacy by design should be embedded in AI systems and where possible AI\n(e.g. outputs that the AI systems generated for specific users or how users responded to\nalgorithm should have adequate privacy impact assessments.\nparticular recommendations). Digital records of human behavior may allow AI systems\nto infer not only individuals\u2019 preferences, but also their age, gender, religious or political\nAdequate data protection frameworks and governance mechanisms should be\nviews. To allow individuals to trust the data gathering process, it must be ensured that data\nestablished in a multi-stakeholder approach and ensured throughout the life cycle\ncollected about them will not be used to unlawfully or unfairly discriminate against them.\nof AI systems.\ne In any given organization that handles individuals\u2019 data (whether someone is a user of the\nAI developers and operators should strive to strike the balance between privacy\nsystem or not), data protocols governing data access should be put in place in line with\nrequirements, individual rights and innovation growth and society benefits.\nnational privacy legislation (be these universal or sector-specific). These protocols should\noutline who can access data and under which circumstances. Only duly qualified personnel\n3.8.2 Guidelines\nwith the competence and need to access individual\u2019s data should be allowed to do so.\n3.8.2.1\nEstablish mechanism for users to flag issues related to privacy or data protection\na AI Operators should review the system for proper consent logging, ability of users to revoke\npermission whenever applicable.\nb Consider training AI model without or with minimal user of potentially sensitive or personal\ndata.\nc Use measures to enhance privacy such as encryption, anonymization and aggregation.\n40 41f Algorithmic systems require adequate privacy impact assessments, which also include\nsocietal and ethical considerations of their use and an innovative use of the privacy by\ndesign approach. AI actors need to ensure that they are accountable for the design and\nimplementation of AI systems in such a way as to ensure that personal information is\nprotected throughout the life cycle of the AI system.\ng Establish data policies or equivalent frameworks, or reinforce existing ones, to ensure full\nsecurity for personal data and sensitive data, which, if disclosed, may cause exceptional\ndamage, injury or hardship to individuals. Examples include data relating to offences,\ncriminal proceedings and convictions, and related security measures; biometric, genetic\nand health data; and personal data such as that relating to race, descent, gender, age,\nlanguage, religion, political opinion, national origin, ethnic origin, social origin, economic or\nsocial condition of birth, or disability and any other characteristics.\nh Promote mechanisms, such as open repositories for publicly funded or publicly held data,\nsource code and data trusts, to support the safe, fair, legal and ethical sharing of data,\namong others.\ni Promote and facilitate the use of quality and robust datasets for training, development and\nuse of AI systems, and exercise vigilance in overseeing their collection and use.\nExample: with the help of AI & ML algorithms16 , Synthetic data can be created to enhance\nprivacy. The data created will have the same statistical characteristics for the testing\nenvironment and where a third party can have access.\n16 https://research.aimultiple.com/privacy-enhancing-technologies/\n42 43Microsoft. 2022. Microsoft Responsible AI Standard, v2. General Requirements. June 2022.\n04\nRetrieved from:\nhttps://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4ZPmV\nISO. 2022. Proposed ISO Standard on Risk Management of AI: What Businesses Should Know.\nBibliography\nMay 2022. Retrieved from:\nhttps://unesdoc.unesco.org/ark:/48223/pf0000381137/PDF/381137eng.pdf.multi\nGSMA 2022. The AI Ethics Playbook. Implementing ethical principles into everyday business.\nFebruary 2022. Retrieved from:\nhttps://www.gsma.com/betterfuture/wp-content/uploads/202201//The-Mobile-Industry-Ethics-\nPlaybook_Feb-2022.pdf\nOECD. 2022. AI Policy Observatory. Retrieved from:\nhttps://standards.ieee.org/wp-content/uploads/import/documents/other/ead_v2.pdf\nUNESCO. 2022. Recommendation on the Ethics of Artificial Intelligence. Adopted in November\n2021. Retrieved from:\nhttps://unesdoc.unesco.org/ark:/48223/pf0000381137/PDF/381137eng.pdf.multi\nEuropean Commission. 2021. Proposal for a Regulation of the European Parliament and of the\nCouncil Laying Down Harmonized Rules on Artificial Intelligence (Artificial Intelligence Act) and\nAmending Certain Union Legislative Acts. Retrieved from:\nhttps://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A52021PC0206\nEuropean Commission. High Level Expert Group on Artificial Intelligence ( AI HLEG ). Ethics\nGuidelines for Trustworthy AI . April 2019. Retrieved from:\nhttps://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai\nCentre for Data Ethics and Innovation. UK Government: The roadmap to an effective AI assurance\necosystem. December 2021. Retrieved from:\nhttps://www.gov.uk/government/publications/the-roadmap-to-an-effective-ai-assurance-\necosystem\nThe New York City Council. 2021. A Local Law to amend the administrative code of the city of New\nYork, in relation to automated employment decision tools. Retrieved from:\nhttps://legistar.council.nyc.gov/LegislationDetail.aspx?ID=4344524&GUID=%20B051915D-A9AC-\n451E-81F86596032-FA3F9&\nUNESCO, Recommendation on the Ethics of Artificial Intelligence. 2021. Retrieved from:\nhttps://unesdoc.unesco.org/ark:/48223/pf0000381137\n44 45AI NOW Institute. Algorithmic accountability for the public sector. August 2021. Retrieved from: CNIL. (2017). How can humans keep the upper hand? The ethical matters raised by algorithms\nhttps://www.opengovpartnership.org/documents/algorithmic-accountability-public-sector/ and artificial intelligence. Retrieved from:\nhttps://www.cnil.fr/sites/default/files/atoms/files/cnil_rapport_ai_gb_web.pdf\nIEEE 2019. Ethically Aligned Design. Version 2 \u2013 For Public Discussion. Retrieved from:\nhttps://standards.ieee.org/wp-content/uploads/import/documents/other/ead_v2.pdf Executive Office of the President of the United States. National Science and Technology Council.\nNetworking and Information Technology Research and Development Subcommittee. (2016). The\nnational artificial intelligence research and development strategic plan. Retrieved from:\nIEEE 2017. Ethically Aligned Designed : A vision for Prioritizing Human Well-being with\nhttps://www.nitrd.gov/PUBS/national_ai_rd_strategic_plan.pdf\nAutonomous and Intelligent Systems. Retrieved from:\nhttps://standards.ieee.org/wp-content/uploads/import/documents/other/ead_v2.pdf\nExecutive Office of the President of the United States National Science and Technology Council.\nCommittee on Technology. (2016). Preparing for the future of artificial intelligence. Retrieved from:\nGoogle. AI Principles 2021. Retrieved from:\nhttps://obamawhitehouse.archives.gov/sites/default/files/whitehouse_files/microsites/ostp\nhttps://ai.google/principles/\nNSTC/preparing_for_the_future_of_ai.pdf\nMicrosoft. Responsible AI Standard v2 (June 2022). Retrieved from:\nThe Headquarters for Japan\u2019s Economic Revitalization. (2015). New Robot Strategy. Japan\u2019s\nhttps://www.microsoft.com/en-us/ai/responsible-ai\nRobot Strategy. Vision, Strategy, Action Plan. Retrieved from:\nhttp://www.meti.go.jp/english/press/2015/pdf/0123_01b.pdf\nThe ECONOMIST Group 2022 . Pushing forward: the future of AI in the Middle East and North\nAfrica Report. Retrieved from:\nTreasury Board of Canada Secretariat. (2018). Responsible Artificial Intelligence in the\nhttps://impact.economist.com/perspectives/sites/default/files/google_ai_mena_report.pdf\nGovernment of Canada. Digital Disruption White Paper Series. Version 2.0.\nhttps://docs.google.com/document/d/1Sn-qBZUXEUG4dVk909eSg5qvfbpNlRhzIefWPtBwbxY/\nPDPC. (2018, June 5). Discussion paper on Artificial Intelligence (AI) and Personal Data.\nedit\nSingapore: Personal Data Protection Commission Singapore (PDPC). Retrieved from:\nhttps://www.pdpc.gov.sg/-/media/Files/PDPC/PDF-Files/Resource-for-Organisation/AI/\nThe Toronto Declaration: Protecting the right to equality and non-discrimination in machine\nDiscussion-Paper-on-AI-and-PD---050618.pdf\nlearning systems. Toronto, Canada: Amnesty International and Access Now. Retrieved from:\nhttps://www.accessnow.org/cms/assets/uploads/201808//The-Toronto-Declaration_ENG_08-\nITI. AI Policy Principles. Retrieved from:\n2018.pdf\nhttps://www.itic.org/public-policy/ITIAIPolicyPrinciplesFINAL.pdf\nhttp://data.parliament.uk/writtenevidence/committeeevidence.svc/evidencedocument/artificial-\nCabinet Office.(2016, May 19). Data Science Ethical Framework. Retrieved from: intelligence-committee/artificial-intelligence/oral/73546.html (house of lords select committee\nhttps://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/ oral evidence, Q61)oral evidence, Q61)\nfile/524298/Data_science_ethics_framework_v1.0_for_publication__1_.pdf\nHouse of Lords Select Committee on Artificial Intelligence. (2018). AI in the UK: ready, willing and\nEuropean Parliament. (2017, February 16). European Parliament resolution of 16 February 2017 able?\nwith recommendations to the Commission on Civil Law Rules on Robotics (20152103/(INL)). https://publications.parliament.uk/pa/ld201719/ldselect/ldai/100100/.pdf\nRetrieved from:\nhttp://www.europarl.europa.eu/sides/getDoc.do?pubRef=-//EP//TEXT+TA+P8-TA-2017- House of Commons Science and Technology Select Committee. Algorithms in Decision Making.\n0++0051DOC+XML+V0//EN https://publications.parliament.uk/pa/cm201719/cmselect/cmsctech/351351/.pdf\nVillani, C. (2018). For a meaningful Artificial Intelligence towards a French and European strategy.\nRetrieved from:\nhttps://www.aiforhumanity.fr/pdfs/MissionVillani_Report_ENG-VF.pdf\n46 47", "metadata": {"country": "UAE", "year": "2024", "legally_binding": "yes", "binding_proof": "Law", "date": "01/15/2024", "regulator": "Government of Abu Dhabi", "type": "legislation", "status": "active", "language": "English, Arabic", "use_cases": "[3, 6]"}}
{"_id": "68695b615520a4cc7989de79", "title": "AI Legislative Intelligence Office and Regulatory Intelligence Ecosystem", "source": "https://www.middleeastainews.com/p/uae-cabinet-new-ai-legal-system", "text": "AI ETHICS\nARTIFICIAL INTELLIGENCE ETHICS\nPRINCIPLES & GUIDELINES\nPRINCIPLES & GUIDELINES\nDEC 2022Table of Content\nPage Page\n1. Overview 4 3.4 Explainable 29\n1.1 Introduction 5 3.4.1 Principle 29\nWe will make AI systems as explainable as technically possible\n1.2 Scope 6 3.4.2 Guidelines 29\n1.3 Strategic alignment 6 3.5 Robust, Safe and Secure AI 31\n1.4 Responsibility 8 3.5.1 Principles 31\nAI systems will be technically robust\n3.5.2 Guidelines 32\n1.5 Licensing 8\n3.6 Human Centred AI 34\n1.6 Toolkit 8\n3.6.1 Principles 34\nWe will give AI systems human values and make them beneficial to society\n3.6.2 Guidelines 35\n2. Definitions 10\n3.7 Sustainable and Environmentally Friendly AI 38\n3.7.1 Principle 38\n3. Principles and Guidelines 16\nWe will promote sustainabity and environemntly friendly AI 38\n3.7.2 Guidelines 38\n3.1 Fairness 17\n3.8 Privacy Preserving AI 38\n3.1.1 Principle 17\nWe will make AI systems fair\n3.8.1 Principle 40\n3.1.2 Guidelines 20\nWe will respect people\u2019s privacy\n3.8.2 Guidelines 40\n3.2 Accountable AI 20\n3.2.1 Principle 20\nWe will make AI systems accountable\n4. Bibliography 44\n3.2.2 Guidelines 20\n3.3 Transparent AI 26\n3.3.1 Principle 26\nWe will make AI systems transparent\n3.3.2 Guidelines 26\n2 301 1.1 Introduction\nAI\u2019s rapid advancement and innovation potential across a range of fields is incredibly exciting.\nYet a thorough and open discussion around AI ethics, and the principles organizations using this\nOverview\ntechnology must consider, is needed.\nThis document - AI Ethics: Principles and Guidelines \u2013 intends to meet this need to balance these\ntwo central considerations of AI. It is designed to offer detailed guidance to help AI actors adhere\nto eight principles of AI ethics.\nWe will make AI systems that are:\nexplainable transparent accountable fair\nprivacy preserving sustainable and human centered robust, safe and\nenvironmentally secure\nfriendly\nThe guidelines are non-binding, and are drafted as a collaborative multi-\nstakeholder effort, with full awareness of organizations\u2019 needs to innovate\nand protect their intellectual property. This is a collaborative process in\nwhich all stakeholders are invited to be part of an ongoing dialogue. We\nwould like to see the AI Ethics Guidelines evolve into a universal, practical\nand applicable framework informing ethical requirements for AI design and\nuse. The eventual goal is to reach widespread agreement and adoption of\ncommonly agreed policies to inform the ethical use of AI nationally and\naround the world.\n4 51.2 Scope Additionally, the AI Office is currently developing additional policies to be taken into consideration\nalong with this document:\nThis document gives non-mandatory guidelines for achieving the ethical design\nand deployment of AI systems in both the public and private sectors. AI already\nAI Seal AI Procurement Guidelines\nsurrounds us, but some applications are more visible and sensitive than others.\nThe UAE AI Seal brand (UAI) will be used The AI Procurement Guidelines will be\nThis document is applicable only to those AI systems which make or inform\nto attract talent and business from used to guide Federal Government entities\n\u2018significant decisions\u2019 \u2013 that is, those decisions which have the potential for\nacross the globe to come to the UAE to in the procurement of AI systems. They\nsignificant impact either on individuals or on society as a whole. They also apply to\ntest and develop AI. This includes a UAI will list the general principles, proposed\n\u2018critical decisions\u2019, which are a subset of significant decisions and are of especially\nmark recognizing high quality, ethical AI mechanisms for procurement and review\ncritical nature.\ncompanies. It would reward safe, efficient, of AI products between the federal\nverified AI technology with a \u2018UAI Seal government and vendors. The policy\nThis document guides the further developments of sector specific requirements\nof Approval\u2019. The UAE AI Seal brand is considers R&D for specific challenges\nand principles, hence every entity within its sector can further amend, use, or\ncurrently under development by the AI that do not currently have an AI solution\ncomplement its current frameworks to properly suit its context and the need of\nOffice. product that is market-ready. Based on\naffected stakeholders.\nglobal standards for AI Procurement, the\npolicy aims to incentivize AI adoption\nIt is a living document and will undergo further future reviews and enhancements.\nwithin government entities and promote\nAI R&D within the private sector for\nspecified government challenges. The\nprocurement policy is currently under\ndevelopment by the AI Office.\n1.3 Strategic alignment\nThis document comes as a fulfillment of the goals and aspirations set in the UAE National AI\nStrategy and the international Sustainable Development Goals (SDGs). The UAE has long been\nat the forefront of sustainable development. These shared global milestones have created global\nmovement and adoption in several disciplines and sectors. As an early adopter, the UAE has\nestablished a national committee on SDGs with a long-term realization of the UAE SDG agenda\n2030. These goals are further engrained and reflected in the AI Ethics Principles under the topics of\nfairness, inclusiveness, equality, human benefit, and rights.\n6 71.4 Responsibility\nThe AI Office will not be responsible for any misuse of the AI Ethics Principles and Guidelines. The\nuser bears all the consequences of their use.\n1.5 Licensing\nThis document is published under the terms of a Creative Commons Attribution 4.0 International\nLicense in order to facilitate its re-use by other governments and private sector organizations.\nIn summary this means you are free to share and adapt the material, including for commercial\npurposes, provided that you give appropriate credit to the UAE AI and Blockchain Council as its\nowner and do not suggest the Minister of State for Artificial Intelligence, Digital Economy and\nRemote Work Applications Office endorses your use.\n1.6 Toolkit\nThe document is part of a toolkit of self-governing nature that boosts awareness and enables\ngovernment and private institutions to pursue innovative use cases while maintaining human\nvalues and principles.\n8 92.1 AI developer organization\n02\nAn organization which does any of the following:\na determines the purpose of an AI system;\nDefinitions\nb designs an AI system;\nc builds an AI system, or:\nd performs technical maintenance or tuning on an AI system\nNote:\nThe definition applies regardless of whether the organization is the ultimate user of the system, or\nwhether they sell it on or give it away.\nExample: A company develops an artificially intelligent facial recognition system and\nsells it to a country\u2019s border force, who use it to identify suspicious personnel. The company\nis an AI developer organization and the border force is an AI operator organization.\n2.2 AI operator organization\nAn organization which does any of the following:\na uses AI systems in operations, backroom processes or decision-making;\nb uses an AI system to provide a service to an AI subject;\nc is a business owner of an AI system;\nd procures and treats data for use in an AI system; or\ne evaluates the use case for an AI system and decides whether to proceed.\nNotes:\n1. This definition applies regardless of whether the AI system was developed in-house or procured.\n2. It is possible for organizations to be both an AI developer organization and an AI operator organization.\n10 112.6 Critical decision\n2.3 Artificial Intelligence (also AI)\nThe capability of a functional unit to perform functions that are generally associated with human An individually significant decision which is deemed to either have a very large impact on an\nintelligence such as reasoning, learning and self-improvement1. individual or to have especially high stakes. These can be especially sensitive, have the potential\nto cause high loss or damage, be societally significant, or set an important precedent.\n2.4 Artificially Intelligent System (also AI system)\nNotes:\nThe types of decisions referred to here are the same as those in the definition of significant-at-\nA product, service, process or decision-making methodology whose operation or outcome is\nscale decisions, except in this case the effects are felt as a result of an individual decision rather\nmaterially influenced by artificially intelligent functional units.\nthan an aggregate of many decisions..\nNotes:\n1. It is not necessary for a system\u2019s outcome to be solely determined by artificially intelligent\nfunctional units in order for the system to be defined as an artificially intelligent system; and Example: A court determines whether a defendant is guilty of a criminal charge, with the\n2. A particular feature of AI systems is that they learn behavior and rules not explicitly punishment for guilt being a life sentence. This is a critical decision because it has a very\nprogrammed in. large impact on the life of the defendant and also sets precedent for similar cases in the\nfuture.\nExample 1: A small claims court uses Example 2: A government entity uses\n2.7 Functional Unit\nan artificially intelligent software package a chatbot which allows customers to ask\nto collect evidence pertaining to a case, routine questions, book appointments\nEntity of hardware or software, or both, capable of accomplishing a specified purpose3 .\ncompare it to similar cases in the past, and conduct minor financial transactions.\nand present a recommended decision The chatbot responds to customer\nto a judge. The judge determines the queries with pre-written responses and is\n2.8 Individually significant decision\nfinal outcome. This decision-making based on pre-programmed decision rules.\nA decision which has the potential for significant impact on at least one individual\u2019s\nmethodology is materially influenced Therefore, the chatbot is not an AI system.\ncircumstances, behavior or choices, or has legal or similarly significant effects on him or her.\nby an artificially intelligent functional If, however, the chatbot autonomously\nunit, and is therefore classified as an AI adjusted its treatment of customers\nsystem. based on the outcome of past cases, it\nwould be an AI system. Example: A company decides to make an employee redundant. This is an individually\nsignificant decision because of its potential impact on the employee\u2019s financial situation.\n2.5 Bias (of a system)\nInclination or prejudice for or against one person or group, especially in a way considered to be\nunfair2.\n2 Oxford Dictionary 2018, Oxford University Press, viewed online 4th October 2018, <https://en.oxforddictionaries.com>\n1 Consistent with ISO/IEC 2382:2015 3 From ISO/IEC 2382:2015\n12 132.9 Non-operational bias (of a system) 2.11 Significant decision\nBias that is either: A decision which is either individually significant or is part of a set of significant-at-scale\ndecisions.\na not a design feature; or\nb not important in achieving the stated purpose of the system. 2.12 Subject of an artificially intelligent system (also AI subject)\nA natural person who is any of the following:\n2.10 Set of significant-at-scale decisions\na an end-user of an AI system\nA set of decisions made by the same system or organization which, when taken in aggregate,\nb directly affected by the operation of or outcomes of an AI system, or:\nhave significant impact on society as a whole or groups within it.\nc a recipient of a service or recommendation provided by an AI system\nNotes:\n1. The decisions need not be individually significant in order to qualify, in aggregate, as a set of\nsignificant-at-scale decisions; and\n2. Examples of areas which have a large impact on society and which include but are not limited\nto: the large-scale allocation of resources or opportunities amongst groups; the structure of\ngovernment; the division of power between large entities or groups; the law, and its interpretation\nand enforcement; conflict and war; international relations, etc.\nExample: An AI system is used by a website to determine which content to show users.\nThis decision is not individually significant, since a user is not greatly affected by whether\na particular piece of media is shown to them. However, if the website is popular then the AI\nsystem may be making a set of significant-at-scale decisions, because any biases in the\nsystem will affect a large number of users.\n14 153.1 Fairness (Principle 1)\n03\n3.1.1 Principle\nPRINCIPLES AND We will make AI systems fair\nData ingested should, where possible, be accurate and representative of the affected\npopulation.\nGUIDELINES\nAlgorithms should avoid non-operational bias.\nSteps should be taken to mitigate and disclose the biases inherent in datasets.\nSignificant decisions should be provably fair.\nAll personnel involved in the development, deployment and use of AI systems have a role and\nresponsibility to operationalize AI fairness and should be educated accordingly.\n3.1.2 Guidelines\n3.1.2.1\nBenefits of AI systems should be available and accessible to all. AI systems should not discriminate\nagainst people or groups in a way that could have an adverse impact. Especially significant and\ncritical decisions made or assisted by AI should be provably fair.\na Considering that fairness can have many different definitions, an organization should\ndocument its own definition of fairness for the context that the AI system is going to be\nimplemented in. Organizations should document what the implemented fairness objective\nstands for and why this choice was considered the most suitable for the given scenario.\nb It is recommended to identify and document demographic groups that may be adversely\nimpacted and mitigate the risk where possible.\nc AI developers and AI operators could consider formal procedures such as Discrimination\nImpact Assessments as a means of ensuring fairness. This assessment should be\ndocumented as well as the mitigation measures that were implemented by the organization.\nThe impact assessment should be conducted pre-release and regularly after as an ongoing\nevaluation, results are advised to be documented.\n16 173.1.2.2 3.1.2.3\nConsideration should be given to whether the data ingested is accurate and representative Consideration should be given to whether decision-making processes introduce bias\nof the affected population\na When subjecting different groups to different decision-making processes, AI developers\nd Fairness has many different definitions in different cultures and for different contexts. should consider whether this will lead to non-operational bias.\nEncouraging a diverse and inclusive AI ecosystem is thus all the more crucial to ensure\nthat one definition of fairness does not contradict another, and that the process of defining b When evaluating the fairness of an AI system, AI developers and AI operators should\nfairness itself is fair, with under-represented groups represented in the discussion. consider whether AI subjects in the same circumstances receive equal treatment.\ne AI developers and AI operators should undertake reasonable data exploration and/or testing\nto identify potentially prejudicial decision-making tendencies in AI systems arising from\nExample: An organization uses an AI tool to automate the pre-screening of candidates\ndata inaccuracy and biases in the data.\nfor a job opening. It is trained on data from the company\u2019s existing employees, the majority\nof whom are from the same ethnic background. Therefore, the system learns to use name\nf AI developers and operators should evaluate all datasets to assess inclusiveness of and nationality as discriminating factors in filtering job applicants. This could have been\nidentified demographic groups and collect data to close any gaps.\nidentified through testing and rectified by, for example, balancing the training data or only\nusing relevant data fields for training.\ng AI developers and AI operators should refrain from training AI systems on data that is not\nlikely to be representative of the affected AI subjects, or is not likely to be accurate, whether\nthat be due to age, omission, method of collection, or other factors.\n3.1.2.4\nSignificant decisions informed by the use of AI should be fair\nExample: Following a natural disaster, a government relief agency uses an AI system to a AI developers and AI operators could consider formal procedures such as Discrimination\ndetect communities in greatest need by analyzing social media data from a range of websites. Impact Assessments as a means of ensuring fairness.\nHowever, those communities where smartphone penetration is lower having less presence on\n3.1.2.4\nsocial media, and so are at risk of receiving less attention. Therefore, the charity complements\nAI operators should consider whether their AI systems are accessible and usable in a fair\ntheir AI tool with traditional techniques to identify needy populations elsewhere.\nmanner across user groups\n3.1.2.5\nh AI developers should consider whether their AI systems can be expected to perform well Consideration should be given to the effect of diversity on the development and deployment\nwhen exposed to previously unseen data, especially when evaluating people who are not processes\nwell-represented in the training data.\nb Organizations should seek to include people from diverse demographic backgrounds across\nthe full lifecyle, to include design, development and deployment processes. Organizations\nshould seek to engage diverse internal and external groups also.\nc AI developers should consider whether the assumptions they make about AI subjects could\nbe wrong or are likely to lead to non-operational bias; if so, they should consider consulting\nthe AI subjects in a representative manner during the design, development and deployment\nto confirm these assumptions.\n18 193.2 Accountable AI (Principle 2)\n3.2.2.2\nPositive efforts should be made to identify and mitigate any significant risks inherent in the AI\nsystems designed\n3.2.1 Principle\na AI operators should only use AI systems that are backed by evidence-based academic or\nindustrial research, and AI developers should base their development on such research.\nWe will make AI systems accountable\nAccountability for the outcomes of an AI system lies not with the system itself but is\napportioned between those who design, develop and deploy it; Example: A foreign country has a government service which identifies parents who owe\nmoney in child maintenance. The data matching process is often incorrect due to misspelled\nDevelopers should make efforts to mitigate the risks inherent in the systems they design; names or missing data which results in some individuals being incorrectly targeted\nautomatically by the system with the result being a large bill, poor credit ratings and even\nAI systems should have built-in appeals procedures whereby users can challenge significant\nfreezing wages. The recourse for individuals who are incorrectly targeted is time-consuming\ndecisions;\nand not straightforward . If the potential impact of incorrect decisions had been assessed,\nmitigation measures such a user-friendly review procedure could have been set up.\nAI systems should be developed by diverse teams which include experts in the area in which\nthe system will be deployed; and\nAI systems should be subject to external audit and decision quality assurance. b AI operators should identify the likely impact of incorrect automated decisions on AI\nsubjects and, in the case where incorrect decisions are likely to cause significant cost or\ninconvenience, consider mitigating measures.\n3.2.2 Guidelines\nc AI operators should consider internal risk assessments or ethics frameworks as a means to\n3.2.2.1 facilitate the identification of risks and mitigating measures.\nAccountability for the outcomes of an AI system should not lie with the system itself\nd In designing AI systems to inform significant decisions, AI developers should consider\na Accountability for loss or damages resulting from the application of AI systems should not measures to maintain data accuracy over time, including:\nbe attributed to the system itself.\n\u2022 the completeness of the data.\nb AI operators and AI developers should consider designating individuals to be responsible for \u2022 timely update of the data, and.\ninvestigating and rectifying the cause of loss or damage arising from the deployment of AI\n\u2022 whether the context in which the data was collected affects its suitability for the\nsystems.\nintended use case.\nExample: A border camera scanning for predictors of risk may misinterpret a \u201ctic\u201d of an\nindividual with Tourette syndrome as suspicious. These can manifest in a diverse fashion, and\nshould not cause this person to undergo secondary inspection every time they pass through\nthe border . If the data is updated after the first case is encountered then it would avoid\ncausing inconvenience on subsequent visits.\n5 Government of Canada. (2018). Responsible AI in the Government of Canada. Digital Disruption White Paper Series. Version 2.0, p.26. Retrieved from:\nhttps://docs.google.com/document/d/1Sn-qBZUXEUG4dVk909eSg5qvfbpNlRhzIefWPtBwbxY/edit\n4 Cabinet Office (UK), Data Science Ethical Framework, Version 1.0, licensed under the Open Government License v3.0, p. 13 6 Stoica, I. et. al.,2017, A Berkeley View of Systems Challenges for AI, p. 2, https://arxiv.org/pdf/1712.05855.pdf\n20 21e AI developers and AI operators should tune AI models periodically to cater for changes to e Be mindful that there might be fundamental tensions between different principles and\ndata and/or models over time. requirements. Continuously identify, evaluate, document and communicate these trade-\noffs and their solutions.\nf AI operators should consider whether AI systems trained in a static environment will display\nmodel instability when deployed in dynamic environments. Adopt a Trustworthy AI assessment list when developing, deploying or using AI systems,\nf\nand adapt it to the specific use case in which the system is being applied.\nExample: AI systems will need to be able to adapt to the changes in the environment that g Keep in mind that such an assessment list will never be exhaustive. Ensuring Trustworthy\nthey are deployed in. For example, a self-driving car would need to quickly adapt to unexpected AI is not about ticking boxes, but about continuously identifying and implementing\nand dangerous road by learning in real time from other cars that have successfully dealt with requirements, evaluating solutions, ensuring improved outcomes throughout the AI\nthese conditions. In addition, such mission-critical applications must handle noisy inputs system\u2019s lifecycle, and involving stakeholders in this.\nand defend against malicious actors .\n3.2.2.4\nAI subjects should be able to challenge significant automated decisions concerning them and,\ng AI developers should collaborate with AI Operators to train models using historical data where appropriate, be able to opt out of such decisions\nfrom the Operator.\na AI operators using AI systems to inform significant decisions should provide procedures by\nwhich affected AI subjects can challenge a specific decision concerning them.\nh AI Operators should consider working with their vendors (AI developers) to continually\nmonitor performance.\nb AI operators should consider such procedures even for non-significant decisions.\ni AI Operators should subject AI systems informing significant decisions to quality checks\nat least as stringent as those that would be required of a human being taking the same c AI operators should make affected AI subjects aware of these procedures and should\ndesign them to be convenient and user-friendly.\ndecision.\nd AI operators should consider employing human case evaluators to review any such\n3.2.2.3\nchallenges and, when appropriate, overturn the challenged decision.\nAI systems informing critical decisions should be subject to appropriate external audit\na When AI systems are used for critical decisions, external auditing of the AI systems in\nquestion should be used as a means to ensure meaningful standards of transparency and Example: A bank allows customers to apply for a loan online by entering their data. The\naccountability are upheld. bank uses an AI system to automatically determine whether to give the loan and what the\ninterest rate should be. They provide users with an option to contest the decision and have\nb In the case that critical decisions are of civic interest, public release of the results of the it reviewed by a human . They also require that customers justify their challenge by filling in\naudit should be considered as a means of ensuring public processes remain accountable a form, which assists the case reviewer and deters customers from challenging a decision\nto those affected by them. without good reason.7\nIn the case that critical decision are life and death decisions, these decisions should be\nc\nsupported by further validation and verification via a human operator.\nd Facilitate traceability and auditability of AI systems, particularly in critical contexts or\nsituations.\n7 Adapted from EU Commission, Can I be subject to automated individual decision-making, including profiling? Retrieved from: https://ec.europa.eu/\ninfo/law/law-topic/data-protection/reform/rights-citizens/my-rights/can-i-be-subject-automated-individual-decision-making-including-profiling_\nen#example\n22 23e AI operators should consider instituting an opt-out mechanism for significant automated 3.2.2.7\ndecisions. AI operators should understand the AI systems they use sufficiently to assess their suitability\nfor the use case and to ensure accountability and transparency\nf AI operators could consider \u201ccrowd challenge\u201d mechanisms whereby a critical number of\ncomplaints triggers an investigation into the fairness and/or accuracy of a decision-making\na In the case of critical decisions, AI operators should avoid using AI systems that cannot be\nprocess as a whole.\nsubjected to meaningful standards of accountability and transparency.\n3.2.2.5 b AI developers should consider notifying customers and AI operators of the use cases for\nAI systems informing significant decisions should not attempt to make value judgements on which the system has been designed, and those for which it is not suitable.\npeople\u2019s behalf without prior consent\na When informing an AI subject about significant choices they will make, AI systems should\nnot unreasonably restrict the available options or otherwise attempt to influence their value\njudgements without the explicit consent of the AI subject in question.\n3.2.2.6\nAI systems informing significant decisions should be developed by diverse teams with\nappropriate backgrounds\na AI developers who develop AI systems which may be used to assist in making critical\ndecisions should involve in the process experts with a background in social science, policy,\nor another subject which prepares them to evaluate the broader societal impact of their\nwork.\nb Development of AI systems informing significant decisions should include consultation\nwith experts in the field in which the system will be deployed.\nExample: An app that uses AI to assess medical symptoms and has a large user base had\nto face regulatory scrutiny because of number of complaints from doctors. They warned\nthat the application can miss signs of serious illness. A number of different shortcomings\nwere identified by doctors, some of which the company could address and resolve.8\n8 Financial Times. 2018. High-profile health app under scrutiny after doctor\u2019s complaints. Retrieved from: https://www.ft.com/content/19dc6b7e-8529-\n11e896-dd-fa565ec55929\n24 253.3 Transparent AI (Principle 3)\nc To facilitate the above, AI developers and AI operators should consider documenting the\nfollowing information during the design, development and deployment phases, and retaining\n3.3.1 Principle this documentation for a length of time appropriate to the decision type or industry:\n\u2022 the provenance of the training data, the methods of collection and treatment, how the data was\nWe will make AI systems transparent\nmoved, and measures taken to maintain its accuracy over time;\n\u2022 the model design and algorithms employed; and\nDevelopers should build systems whose failures can be traced and diagnosed. \u2022 changes to the codebase, and authorship of those changes.\nPeople should be told when significant decisions about them are being made by AI. d Where possible given the model design, AI developers should consider building in a means\nby which the \u201cdecision journey\u201d of a specific outcome (i.e. the component decisions leading\nWithin the limits of privacy and the preservation of intellectual property, those who deploy to it) can be logged.\nAI systems should be transparent about the data and algorithms they use.\nResponsible disclosures should be provided in a timely manner, and provide reasonable\nExample: A technology company has a product which is designed to assist in medical\njustifications for AI systems outcomes. This includes information that helps people\ndiagnosis. It documents each stage of its reasoning and relates it back to the input data.9\nunderstand outcomes, like key factors used in decision making.\n3.3.2 Guidelines\n3.3.2.1\nTraceability should be considered for significant decisions, especially those that have the\npotential to result in loss, harm or damage.\na For AI systems which inform significant decisions, especially those with the potential to\ncause loss, harm or damage, AI developers should consider building in traceability (i.e. the\nability to trace the key factors leading to any specific decision).\nb Organizations should ensure that harms caused through AI systems are investigated and\nredressed, by enacting strong enforcement mechanisms and remedial actions, to make\ncertain that human rights and the rule of law are respected in the digital world and in the\nphysical world.\n9 See IBM WatsonPaths\n26 273.3.2.2 3.4 Explainable AI (Principle 4)\nPeople should be informed of the extent of their interaction with AI systems\n3.4.1 Principle\na AI operators should inform AI subjects when a significant decision affecting them has been\nmade by an AI system.\nWe will make AI systems as explainable as\ntechnically possible\nExample: A small claims court adjudicates minor civil matters such as debt collection and\nevictions. They introduce an AI system to suggest the outcome of a ruling. At the time of the\nDecisions and methodologies of AI systems which have a significant effect on individuals\nruling the plaintiff and defendant are notified that the decision was assisted by an AI system.\nshould be explainable to them, to the extent permitted by available technology.\nThe court also provides an explanation for the decision.\nIt should be possible to ascertain the key factors leading to any specific decision that\ncould have a significant effect on an individual.\nb If an AI system can convincingly impersonate a human being, it should do so only after\nIn the above situation we will provide channels through which people can request such\nnotifying the AI subject that it is an AI system.\nexplanations.\nExample: A technology company produces a conversational AI agent which can make 3.4.2 Guidelines\nsome phone calls on behalf of its users. Those who receive the calls may believe that they\n3.4.2.1\nare speaking to a human. Therefore, the company programs the agent to identify itself at\nAI operators could consider providing affected AI subjects with a high-level explanation of\nthe start of every conversation.\nhow their AI system works\na AI operators could consider informing the affected AI subjects in understandable, non-\ntechnical language of:\n\u2022 the data that is ingested by the system;\n\u2022 the types of algorithms employed;\n\u2022 the categories into which people can be placed; and\n\u2022 the most important features driving the outcomes of decisions\n\u2022 A comprehensive list of feature engineering and models that was considered during\nthe model building phase.\nExample: A person turned down for a credit card might be told that the algorithm took\ntheir credit history, age, and postcode into account, but not learn why their application was\nrejected10 .\n28 29b For non-sensitive public sector use cases designed for the common good, AI operators\ncould consider making source code, together with an explanation of the workings of the AI Example: The UK\u2019s NHS developed a tool called Predict, which allows women with breast\nsystem, available either publicly or upon request (this should be done only if there is low risk cancer to compare their case to other women who have had the same condition in the past,\nof people \u2018gaming the system\u2019). and visualize the expected survival rate under various treatment options. The website has an\nexplanation page which shows the weights behind various factors and contains a description\nc The AI operator should maintain necessary documentation that provides elaboration and of the underlying mathematics12 .\nclarification on how the algorithm works, for example documentation of processes, decision\nmaking flow charts of the system, and how the appeal process is embedded.\nd An individual should have the ability to contest and seek effective redress against decisions c In the case that such explanations are available, they should be easily and quickly accessible,\nmade by AI systems. These must be addressed by the group or team supporting these free of charge and user-friendly.\nmodels.\ne Develop appropriate impact indices for the evaluation of AI system technological 3.5 Robust, Safe and Secure AI (Principle 5)\ninterventions from multiple perspectives.\n3.5.1 Principles\n3.4.2.2\nAI operators should consider providing affected AI subjects with a means to request\nexplanations for specific significant decisions, to the extent possible given the state of present AI systems will be technically robust\nresearch and the choice of model\nAI systems should be technically robust with a preventative approach to risks which\na AI operators should consider providing a means by which people affected by a significant\noperates in a manner such that they reliably behave.\ndecision informed by AI can access the reasoning behind that decision.\nAI Developers should ensure AI systems will not cause any unintentional harm and\nadverse impacts.\nExample: The US Consumer Financial Protection Bureau requires that creditors in the US\nwho reject credit applications must explain to the applicant the principal reason(s) why their\nAI systems should be resilient to attacks and security such as data poisoning and\napplication was rejected (e.g. \u201clength of residence\u201d or \u201cage of automobile\u201d)11 . In particular,\nmodel leakage.\n\u201cstatements that the adverse action was based on the creditor`s internal standards or\npolicies or that the applicant, joint applicant, or similar party failed to achieve a qualifying AI systems should have safeguards that enable a fallback plan in case of problems.\nscore on the creditor`s credit scoring system are insufficient\u201d.\nAI system results should be reproducible.\nb Where such explainability is not possible given available technology, AI operators should\nconsider compromises such as counterfactual reasoning, or listing the most heavily\nweighted factors contributing to the decision.\n11 Consumer Financial Protection Bureau, 12 CFR Part 1002 - Equal Credit Opportunity Act (Regulation B), \u00a7 1002.9 Notifications, Retrieved from https://www.\n10 The Guardian. AI watchdog needed to regulate automated decision-making, say experts. Retrieved from: https://www.theguardian.com/technology/2017/ consumerfinance.gov/policy-compliance/rulemaking/regulations/1002/\njan/27/ai-artificial-intelligence-watchdog-needed-to-prevent-discriminatory-automated-decisions 12 Predict website, accessible at http://www.predict.nhs.uk/predict_v2.1/legal/algorithm\n30 313.5.2 Guidelines\n3.5.2.1 3.5.2.3\nAI Operators should continue conducting vulnerability assessments, verification of AI system AI Operators should assure end users of the system\u2019s reliability through documentation, and\ndifferent behaviors in unexpected situations and for any dual-use case, to include: operationalizing processes for testing and verification of desired outcomes , specifically:\na Putting measures to ensure integrity and resilience of IA system against potential attacks. a AI system should be designed with an approach to continue monitoring if the system meets\ngoals, purposes and intended application.\nb AI Operators should define an approach to ensure results are reproducible through clear\nExample: IBM researchers presented a novel approach to exploiting AI for malicious\nprocess and documentation.\npurposes. Their system, DeepLocker embeds AI capabilities within the malware itself in order\nto improve its evasion techniques. It uses an Artificial Intelligence model to identify its target\nc AI Operators should publish documentation to assure system robustness to end users.\nusing indicators like facial recognition, geolocation and voice recognition.\n3.5.2.4\nAI systems will be safe, secure and controllable by humans\nb Verifying how AI systems behave in unexpected situations and environments.\na Safety and security of people, be they operators, end-users or other parties, will be of\nc Taking preventative measures against any possible system dual-case use. paramount concern in the design of any AI system.\nb AI systems should be verifiably secure and controllable throughout their operational\n3.5.2.2\nlifetime, to the extent permitted by technology.\nAI operators should define a suitable fall back plan and test it to maintain readiness in\nunexpected situations and in high safety risks level, to include:\nc The continued security and privacy of users should be considered when decommissioning\na Developing a plan to measure and assess potential safety risks to you or any third party AI systems.\nfrom technology use accidental or malicious misuse.\nd AI systems that may directly impact people\u2019s lives in a significant way should receive\nb Define thresholds for system acceptable results and governance procure to fall back on commensurate care in their designs.\nalternative defined and tested plans.\ne Such systems should be able to be overridden or their decisions reversed by designated\nc Considering an insurance policy to mitigate risks arising from potential damages. individuals.\nExample: Adopting a risk-based approach to procurement and clearly communicating it to\nvendors can help address such issues by giving the procuring organization advance notice\nof the specific oversight capabilities it will need in future stages of the system lifecycle,\npreventing vendors from presenting intellectual property arguments against required testing,\nmonitoring and auditing of their AI systems going forward and rewarding vendors\u2014of all\nsizes\u2014that are more advanced and responsive in their responsible AI efforts.\n13 IBM. https://documents.trendmicro.com/assets/white_papers/wp-malicious-uses-and-abuses-of-artificial-intelligence.pdf\n32 333.6.2 Guidelines\n3.5.2.5\nAi systems should not be able to autonomously hurt, destroy or deceive humans\n3.6.2.1\na AI systems should be built to serve and inform, and not to deceive and manipulate.\nAI should be developed to align with human values, contribute to human flourishing and\nbenefit society.\nb Nations should collaborate to avoid an arms race in lethal autonomous weapons, and\nsuch weapons should be tightly controlled. a Society should be consulted in a representative fashion to inform the development of AI.\nc Active cooperation should be pursued to avoid corner-cutting on safety standards. b Stakeholders throughout society should be involved in the development of AI and its\ngovernance.\nSystems designed to inform significant decisions should do so impartially.\nd\nc Stakeholders should proactively engage in responsible stewardship of trustworthy AI in\npursuit of beneficial outcomes for people and the planet.\n3.6 Human Centered AI (Principle 6)\nd Organizations should prioritize having all their stakeholders learn about wellbeing metrics\nas a potential determinant of how they create, deploy, market and monitor their AI\n3.6.1 Principles technologies.\nWe will give AI systems human values and\nExample: IEEE P7010 recommended practice14 establishes wellbeing metrics relating to\nmake them beneficial to society human factors directly affected by intelligent and autonomous systems and establishes a\nbaseline for the types of objective and subjective data these systems should analyze and\nGovernment will support the research of the beneficial use of AI. include (in their programming and functioning) to proactively increase human wellbeing.\nStakeholders throughout society should be involved in the development of AI and its\ngovernance.\n3.6.2.2\nSystematic analyses that examine the ethics of designing affective systems to nudge human\nDesign AI systems to adopt, learn and follow the norms and values of the community\nbeings prior to deployment are needed.\nthey serve.\nSystematic analyses that examine the ethics of designing affective systems to nudge a Nudging in AI systems should have an opt-in system policy with explicit consent.\nhuman beings prior to deployment are needed.\nb We recommend that when appropriate, an affective system that nudges human beings\nDecisions related to lethal force, life and death should not be delegated to AI systems. should have the ability to accurately distinguish between users, including detecting\nRules and standards should be adopted to ensure effective human control over those characteristics such as whether the user is an adult or a child. Additional protections must\ndecisions. be put in place for vulnerable populations, such as children, when informed consent cannot\nbe obtained, or when it may not be a sufficient safeguard.\nc AI systems with nudging strategies must be carefully evaluated, monitored, and controlled.\n14 IEEE. https://sagroups.ieee.org/7010/\n34 353.6.2.3 f In situations where it is needed, human rights impact assessments and human rights due\nHumanity should retain the power to govern itself and make the final decision, with AI in an diligence, human determination codes of ethical conduct, or quality labels and certifications\nassisting role. intended to promote human centered values and fairness should be considered.\ng Mechanisms should be put into place to receive external feedback regarding AI systems\na Decisions related to lethal force, life and death should not be delegated to AI systems. Rules\nthat potentially infringe on fundamental rights.\nand standards should be adopted to ensure effective human control over those decisions.\nb Responsible parties (e.g., parents, nurse practitioners, social workers, and governments)\nshould be trained to detect the influence due to AI and ineffective mitigation techniques.\nIn the most extreme cases it should always be possible to shut down harmful AI system.\nc Those actions undertaken by an affective system that are most likely to generate an\nemotional response should be designed to be easily changed.\nd Users should be able to make informed autonomous decisions regarding AI systems. They\nshould be given appropriate knowledge and tools to comprehend and interact with AI\nsystems to satisfactory degree and, where possible to reasonably self-assess or challenge\nthe system.\n3.6.2.4\nAI systems should be designed in a way that respects the rule of law, human rights, society\nvalues, and should include appropriate safeguards to ensure a fair and just society.\na AI Developers should design AI systems to adopt, learn and follow the norms and values of\nthe community they serve.\nb Organizations could identify the norms of the specific community in which AI systems are\nto be deployed in. In particular, pay attention to the norms relevant to the kinds of tasks that\nthe AI systems are designed to perform. These could be documented as well as how these\nnorms are addressed by the AI system.\nc To respond to the dynamic change of norms in society the AI system could be able to adjust\nits existing norms and learn new ones, while being transparent about these changes.\nd Designers should consider forms and metrics for assessing an AI system\u2019s norm conformity\nover the lifetime of the system (e.g. human-machine agreement on moral decisions,\nverifiability of AI decisions, justified trust).\ne The norm identification process must document the similarities and differences between\nthe norms that humans apply to other humans and the norms they apply to AI systems.\nNorm implementations should be evaluated specifically against the norms that the\ncommunity expects the AI system to follow.\n36 373.7 Sustainable and Environmentally Friendly AI (Principle 7)\nd While there is no standardized means of reporting on the environmental impact of AI\nsystems, where possible, model development and commissioning should make carbon\n3.7.1 Principle impacts a core consideration alongside functional and business requirements.\ne Smaller models should be considered \u2013 shrinking down the model size and using fewer\nWe will promote sustainable and environmentally compute cycles (to balance financial and performance costs with the end performance of\nthe model).\nfriendly AI\nf Carbon awareness should be considered \u2013 adjusting the operational parameters of the AI\nAI systems should be used to benefit all human beings, including future generations. The\nsystem to dynamically select the best time and location for energy use from the grid can\nenvironment is fundamental to this.\nreduce its carbon footprint.\nThroughout the AI lifecycle, implementations should only be carried out on the basis of a full\nunderstanding and acknowledgement of implications for sustainability and environment.\nExample: In agriculture, AI can transform Example: Google`s DeepMind division\nAn AI system\u2019s development, deployment and use, should be assessed via critical examination production by better monitoring and has developed AI that teaches itself to\nof resource usage and energy consumption.\nmanaging environmental conditions minimize the use of energy to cool Google`s\nand crop yields. AI can help reduce both data centers. As a result, Google reduced\nMechanisms to measure environmental impact due to type of energy use and processing\nfertilizer and water, all while improving its data center energy requirements by\npower provided by data centers should be established.\ncrop yields. 40% 15.\n3.7.2 Guidelines\n3.7.2.1 Throughout the AI lifecycle, implementations should always be carried out after full\nunderstanding and acknowledgement of AI implications on sustainability and environment.\na The application of Artificial Intelligence to address sustainability challenges are well\nunderstood. Building and running green, sustainable AI systems is of increasing importance,\ngiven the large carbon footprint that they can generate and the wider context of addressing\nclimate change.\nb AI model development, and by extension commissioning, should therefore seek to balance\ntechnical performance with energy consumption and environmental impact.\nc Efforts should be made to estimate and understand sustainability and environmental\nimpact across the AI lifecycle (e.g. the energy consumption costs associated with model\ntraining, and CO2 emissions and cloud compute costs associated with the deployment\nand running of the system).\n15 https://www.deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-by-40\n38 393.8 Privacy Preserving AI (Principle 8)\n3.8.2.2\nEstablish an oversight mechanism for data collection, storage and processing and use across\nyour organization\n3.8.1 Principle\na Assess who can access data and under which conditions.\nWe will respect people\u2019s privacy\nb Assign specific responsibilities and role for Data Protection Officers.\nAI systems should respect privacy and use the minimum intrusion necessary.\nc Prevention of harm to privacy necessitates adequate data governance that covers the\nquality and integrity of the data used, its relevance in light of the domain in which the\nAI systems should uphold high standards of data governance and security,\nprotecting personal information. AI systems will be deployed, its access protocols and the capability to process data in a\nmanner that protects privacy.\nSurveillance or other AI-driven technologies should not be deployed to the extent\nof violating internationally and/or UAE\u2019s accepted standards of privacy and human d AI systems must guarantee privacy and data protection throughout a system\u2019s entire\ndignity and people rights. lifecycle. This includes the information initially provided by the user, as well as the\ninformation generated about the user over the course of their interaction with the system\nPrivacy by design should be embedded in AI systems and where possible AI\n(e.g. outputs that the AI systems generated for specific users or how users responded to\nalgorithm should have adequate privacy impact assessments.\nparticular recommendations). Digital records of human behavior may allow AI systems\nto infer not only individuals\u2019 preferences, but also their age, gender, religious or political\nAdequate data protection frameworks and governance mechanisms should be\nviews. To allow individuals to trust the data gathering process, it must be ensured that data\nestablished in a multi-stakeholder approach and ensured throughout the life cycle\ncollected about them will not be used to unlawfully or unfairly discriminate against them.\nof AI systems.\ne In any given organization that handles individuals\u2019 data (whether someone is a user of the\nAI developers and operators should strive to strike the balance between privacy\nsystem or not), data protocols governing data access should be put in place in line with\nrequirements, individual rights and innovation growth and society benefits.\nnational privacy legislation (be these universal or sector-specific). These protocols should\noutline who can access data and under which circumstances. Only duly qualified personnel\n3.8.2 Guidelines\nwith the competence and need to access individual\u2019s data should be allowed to do so.\n3.8.2.1\nEstablish mechanism for users to flag issues related to privacy or data protection\na AI Operators should review the system for proper consent logging, ability of users to revoke\npermission whenever applicable.\nb Consider training AI model without or with minimal user of potentially sensitive or personal\ndata.\nc Use measures to enhance privacy such as encryption, anonymization and aggregation.\n40 41f Algorithmic systems require adequate privacy impact assessments, which also include\nsocietal and ethical considerations of their use and an innovative use of the privacy by\ndesign approach. AI actors need to ensure that they are accountable for the design and\nimplementation of AI systems in such a way as to ensure that personal information is\nprotected throughout the life cycle of the AI system.\ng Establish data policies or equivalent frameworks, or reinforce existing ones, to ensure full\nsecurity for personal data and sensitive data, which, if disclosed, may cause exceptional\ndamage, injury or hardship to individuals. Examples include data relating to offences,\ncriminal proceedings and convictions, and related security measures; biometric, genetic\nand health data; and personal data such as that relating to race, descent, gender, age,\nlanguage, religion, political opinion, national origin, ethnic origin, social origin, economic or\nsocial condition of birth, or disability and any other characteristics.\nh Promote mechanisms, such as open repositories for publicly funded or publicly held data,\nsource code and data trusts, to support the safe, fair, legal and ethical sharing of data,\namong others.\ni Promote and facilitate the use of quality and robust datasets for training, development and\nuse of AI systems, and exercise vigilance in overseeing their collection and use.\nExample: with the help of AI & ML algorithms16 , Synthetic data can be created to enhance\nprivacy. The data created will have the same statistical characteristics for the testing\nenvironment and where a third party can have access.\n16 https://research.aimultiple.com/privacy-enhancing-technologies/\n42 43Microsoft. 2022. Microsoft Responsible AI Standard, v2. General Requirements. June 2022.\n04\nRetrieved from:\nhttps://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4ZPmV\nISO. 2022. Proposed ISO Standard on Risk Management of AI: What Businesses Should Know.\nBibliography\nMay 2022. Retrieved from:\nhttps://unesdoc.unesco.org/ark:/48223/pf0000381137/PDF/381137eng.pdf.multi\nGSMA 2022. The AI Ethics Playbook. Implementing ethical principles into everyday business.\nFebruary 2022. Retrieved from:\nhttps://www.gsma.com/betterfuture/wp-content/uploads/202201//The-Mobile-Industry-Ethics-\nPlaybook_Feb-2022.pdf\nOECD. 2022. AI Policy Observatory. Retrieved from:\nhttps://standards.ieee.org/wp-content/uploads/import/documents/other/ead_v2.pdf\nUNESCO. 2022. Recommendation on the Ethics of Artificial Intelligence. Adopted in November\n2021. Retrieved from:\nhttps://unesdoc.unesco.org/ark:/48223/pf0000381137/PDF/381137eng.pdf.multi\nEuropean Commission. 2021. Proposal for a Regulation of the European Parliament and of the\nCouncil Laying Down Harmonized Rules on Artificial Intelligence (Artificial Intelligence Act) and\nAmending Certain Union Legislative Acts. Retrieved from:\nhttps://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A52021PC0206\nEuropean Commission. High Level Expert Group on Artificial Intelligence ( AI HLEG ). Ethics\nGuidelines for Trustworthy AI . April 2019. Retrieved from:\nhttps://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai\nCentre for Data Ethics and Innovation. UK Government: The roadmap to an effective AI assurance\necosystem. December 2021. Retrieved from:\nhttps://www.gov.uk/government/publications/the-roadmap-to-an-effective-ai-assurance-\necosystem\nThe New York City Council. 2021. A Local Law to amend the administrative code of the city of New\nYork, in relation to automated employment decision tools. Retrieved from:\nhttps://legistar.council.nyc.gov/LegislationDetail.aspx?ID=4344524&GUID=%20B051915D-A9AC-\n451E-81F86596032-FA3F9&\nUNESCO, Recommendation on the Ethics of Artificial Intelligence. 2021. Retrieved from:\nhttps://unesdoc.unesco.org/ark:/48223/pf0000381137\n44 45AI NOW Institute. Algorithmic accountability for the public sector. August 2021. Retrieved from: CNIL. (2017). How can humans keep the upper hand? The ethical matters raised by algorithms\nhttps://www.opengovpartnership.org/documents/algorithmic-accountability-public-sector/ and artificial intelligence. Retrieved from:\nhttps://www.cnil.fr/sites/default/files/atoms/files/cnil_rapport_ai_gb_web.pdf\nIEEE 2019. Ethically Aligned Design. Version 2 \u2013 For Public Discussion. Retrieved from:\nhttps://standards.ieee.org/wp-content/uploads/import/documents/other/ead_v2.pdf Executive Office of the President of the United States. National Science and Technology Council.\nNetworking and Information Technology Research and Development Subcommittee. (2016). The\nnational artificial intelligence research and development strategic plan. Retrieved from:\nIEEE 2017. Ethically Aligned Designed : A vision for Prioritizing Human Well-being with\nhttps://www.nitrd.gov/PUBS/national_ai_rd_strategic_plan.pdf\nAutonomous and Intelligent Systems. Retrieved from:\nhttps://standards.ieee.org/wp-content/uploads/import/documents/other/ead_v2.pdf\nExecutive Office of the President of the United States National Science and Technology Council.\nCommittee on Technology. (2016). Preparing for the future of artificial intelligence. Retrieved from:\nGoogle. AI Principles 2021. Retrieved from:\nhttps://obamawhitehouse.archives.gov/sites/default/files/whitehouse_files/microsites/ostp\nhttps://ai.google/principles/\nNSTC/preparing_for_the_future_of_ai.pdf\nMicrosoft. Responsible AI Standard v2 (June 2022). Retrieved from:\nThe Headquarters for Japan\u2019s Economic Revitalization. (2015). New Robot Strategy. Japan\u2019s\nhttps://www.microsoft.com/en-us/ai/responsible-ai\nRobot Strategy. Vision, Strategy, Action Plan. Retrieved from:\nhttp://www.meti.go.jp/english/press/2015/pdf/0123_01b.pdf\nThe ECONOMIST Group 2022 . Pushing forward: the future of AI in the Middle East and North\nAfrica Report. Retrieved from:\nTreasury Board of Canada Secretariat. (2018). Responsible Artificial Intelligence in the\nhttps://impact.economist.com/perspectives/sites/default/files/google_ai_mena_report.pdf\nGovernment of Canada. Digital Disruption White Paper Series. Version 2.0.\nhttps://docs.google.com/document/d/1Sn-qBZUXEUG4dVk909eSg5qvfbpNlRhzIefWPtBwbxY/\nPDPC. (2018, June 5). Discussion paper on Artificial Intelligence (AI) and Personal Data.\nedit\nSingapore: Personal Data Protection Commission Singapore (PDPC). Retrieved from:\nhttps://www.pdpc.gov.sg/-/media/Files/PDPC/PDF-Files/Resource-for-Organisation/AI/\nThe Toronto Declaration: Protecting the right to equality and non-discrimination in machine\nDiscussion-Paper-on-AI-and-PD---050618.pdf\nlearning systems. Toronto, Canada: Amnesty International and Access Now. Retrieved from:\nhttps://www.accessnow.org/cms/assets/uploads/201808//The-Toronto-Declaration_ENG_08-\nITI. AI Policy Principles. Retrieved from:\n2018.pdf\nhttps://www.itic.org/public-policy/ITIAIPolicyPrinciplesFINAL.pdf\nhttp://data.parliament.uk/writtenevidence/committeeevidence.svc/evidencedocument/artificial-\nCabinet Office.(2016, May 19). Data Science Ethical Framework. Retrieved from: intelligence-committee/artificial-intelligence/oral/73546.html (house of lords select committee\nhttps://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/ oral evidence, Q61)oral evidence, Q61)\nfile/524298/Data_science_ethics_framework_v1.0_for_publication__1_.pdf\nHouse of Lords Select Committee on Artificial Intelligence. (2018). AI in the UK: ready, willing and\nEuropean Parliament. (2017, February 16). European Parliament resolution of 16 February 2017 able?\nwith recommendations to the Commission on Civil Law Rules on Robotics (20152103/(INL)). https://publications.parliament.uk/pa/ld201719/ldselect/ldai/100100/.pdf\nRetrieved from:\nhttp://www.europarl.europa.eu/sides/getDoc.do?pubRef=-//EP//TEXT+TA+P8-TA-2017- House of Commons Science and Technology Select Committee. Algorithms in Decision Making.\n0++0051DOC+XML+V0//EN https://publications.parliament.uk/pa/cm201719/cmselect/cmsctech/351351/.pdf\nVillani, C. (2018). For a meaningful Artificial Intelligence towards a French and European strategy.\nRetrieved from:\nhttps://www.aiforhumanity.fr/pdfs/MissionVillani_Report_ENG-VF.pdf\n46 47", "metadata": {"country": "UAE", "year": "2025", "legally_binding": "no", "binding_proof": "None", "date": "04/14/2025", "regulator": "UAE Cabinet", "type": "policy framework", "status": "active", "language": "English, Arabic", "use_cases": "[1, 4, 6]"}}
{"_id": "68695b8a5520a4cc7989de7a", "title": "UAE AI Ethics Guidelines", "source": "https://ai.gov.ae/", "text": "AI ETHICS\nARTIFICIAL INTELLIGENCE ETHICS\nPRINCIPLES & GUIDELINES\nPRINCIPLES & GUIDELINES\nDEC 2022Table of Content\nPage Page\n1. Overview 4 3.4 Explainable 29\n1.1 Introduction 5 3.4.1 Principle 29\nWe will make AI systems as explainable as technically possible\n1.2 Scope 6 3.4.2 Guidelines 29\n1.3 Strategic alignment 6 3.5 Robust, Safe and Secure AI 31\n1.4 Responsibility 8 3.5.1 Principles 31\nAI systems will be technically robust\n3.5.2 Guidelines 32\n1.5 Licensing 8\n3.6 Human Centred AI 34\n1.6 Toolkit 8\n3.6.1 Principles 34\nWe will give AI systems human values and make them beneficial to society\n3.6.2 Guidelines 35\n2. Definitions 10\n3.7 Sustainable and Environmentally Friendly AI 38\n3.7.1 Principle 38\n3. Principles and Guidelines 16\nWe will promote sustainabity and environemntly friendly AI 38\n3.7.2 Guidelines 38\n3.1 Fairness 17\n3.8 Privacy Preserving AI 38\n3.1.1 Principle 17\nWe will make AI systems fair\n3.8.1 Principle 40\n3.1.2 Guidelines 20\nWe will respect people\u2019s privacy\n3.8.2 Guidelines 40\n3.2 Accountable AI 20\n3.2.1 Principle 20\nWe will make AI systems accountable\n4. Bibliography 44\n3.2.2 Guidelines 20\n3.3 Transparent AI 26\n3.3.1 Principle 26\nWe will make AI systems transparent\n3.3.2 Guidelines 26\n2 301 1.1 Introduction\nAI\u2019s rapid advancement and innovation potential across a range of fields is incredibly exciting.\nYet a thorough and open discussion around AI ethics, and the principles organizations using this\nOverview\ntechnology must consider, is needed.\nThis document - AI Ethics: Principles and Guidelines \u2013 intends to meet this need to balance these\ntwo central considerations of AI. It is designed to offer detailed guidance to help AI actors adhere\nto eight principles of AI ethics.\nWe will make AI systems that are:\nexplainable transparent accountable fair\nprivacy preserving sustainable and human centered robust, safe and\nenvironmentally secure\nfriendly\nThe guidelines are non-binding, and are drafted as a collaborative multi-\nstakeholder effort, with full awareness of organizations\u2019 needs to innovate\nand protect their intellectual property. This is a collaborative process in\nwhich all stakeholders are invited to be part of an ongoing dialogue. We\nwould like to see the AI Ethics Guidelines evolve into a universal, practical\nand applicable framework informing ethical requirements for AI design and\nuse. The eventual goal is to reach widespread agreement and adoption of\ncommonly agreed policies to inform the ethical use of AI nationally and\naround the world.\n4 51.2 Scope Additionally, the AI Office is currently developing additional policies to be taken into consideration\nalong with this document:\nThis document gives non-mandatory guidelines for achieving the ethical design\nand deployment of AI systems in both the public and private sectors. AI already\nAI Seal AI Procurement Guidelines\nsurrounds us, but some applications are more visible and sensitive than others.\nThe UAE AI Seal brand (UAI) will be used The AI Procurement Guidelines will be\nThis document is applicable only to those AI systems which make or inform\nto attract talent and business from used to guide Federal Government entities\n\u2018significant decisions\u2019 \u2013 that is, those decisions which have the potential for\nacross the globe to come to the UAE to in the procurement of AI systems. They\nsignificant impact either on individuals or on society as a whole. They also apply to\ntest and develop AI. This includes a UAI will list the general principles, proposed\n\u2018critical decisions\u2019, which are a subset of significant decisions and are of especially\nmark recognizing high quality, ethical AI mechanisms for procurement and review\ncritical nature.\ncompanies. It would reward safe, efficient, of AI products between the federal\nverified AI technology with a \u2018UAI Seal government and vendors. The policy\nThis document guides the further developments of sector specific requirements\nof Approval\u2019. The UAE AI Seal brand is considers R&D for specific challenges\nand principles, hence every entity within its sector can further amend, use, or\ncurrently under development by the AI that do not currently have an AI solution\ncomplement its current frameworks to properly suit its context and the need of\nOffice. product that is market-ready. Based on\naffected stakeholders.\nglobal standards for AI Procurement, the\npolicy aims to incentivize AI adoption\nIt is a living document and will undergo further future reviews and enhancements.\nwithin government entities and promote\nAI R&D within the private sector for\nspecified government challenges. The\nprocurement policy is currently under\ndevelopment by the AI Office.\n1.3 Strategic alignment\nThis document comes as a fulfillment of the goals and aspirations set in the UAE National AI\nStrategy and the international Sustainable Development Goals (SDGs). The UAE has long been\nat the forefront of sustainable development. These shared global milestones have created global\nmovement and adoption in several disciplines and sectors. As an early adopter, the UAE has\nestablished a national committee on SDGs with a long-term realization of the UAE SDG agenda\n2030. These goals are further engrained and reflected in the AI Ethics Principles under the topics of\nfairness, inclusiveness, equality, human benefit, and rights.\n6 71.4 Responsibility\nThe AI Office will not be responsible for any misuse of the AI Ethics Principles and Guidelines. The\nuser bears all the consequences of their use.\n1.5 Licensing\nThis document is published under the terms of a Creative Commons Attribution 4.0 International\nLicense in order to facilitate its re-use by other governments and private sector organizations.\nIn summary this means you are free to share and adapt the material, including for commercial\npurposes, provided that you give appropriate credit to the UAE AI and Blockchain Council as its\nowner and do not suggest the Minister of State for Artificial Intelligence, Digital Economy and\nRemote Work Applications Office endorses your use.\n1.6 Toolkit\nThe document is part of a toolkit of self-governing nature that boosts awareness and enables\ngovernment and private institutions to pursue innovative use cases while maintaining human\nvalues and principles.\n8 92.1 AI developer organization\n02\nAn organization which does any of the following:\na determines the purpose of an AI system;\nDefinitions\nb designs an AI system;\nc builds an AI system, or:\nd performs technical maintenance or tuning on an AI system\nNote:\nThe definition applies regardless of whether the organization is the ultimate user of the system, or\nwhether they sell it on or give it away.\nExample: A company develops an artificially intelligent facial recognition system and\nsells it to a country\u2019s border force, who use it to identify suspicious personnel. The company\nis an AI developer organization and the border force is an AI operator organization.\n2.2 AI operator organization\nAn organization which does any of the following:\na uses AI systems in operations, backroom processes or decision-making;\nb uses an AI system to provide a service to an AI subject;\nc is a business owner of an AI system;\nd procures and treats data for use in an AI system; or\ne evaluates the use case for an AI system and decides whether to proceed.\nNotes:\n1. This definition applies regardless of whether the AI system was developed in-house or procured.\n2. It is possible for organizations to be both an AI developer organization and an AI operator organization.\n10 112.6 Critical decision\n2.3 Artificial Intelligence (also AI)\nThe capability of a functional unit to perform functions that are generally associated with human An individually significant decision which is deemed to either have a very large impact on an\nintelligence such as reasoning, learning and self-improvement1. individual or to have especially high stakes. These can be especially sensitive, have the potential\nto cause high loss or damage, be societally significant, or set an important precedent.\n2.4 Artificially Intelligent System (also AI system)\nNotes:\nThe types of decisions referred to here are the same as those in the definition of significant-at-\nA product, service, process or decision-making methodology whose operation or outcome is\nscale decisions, except in this case the effects are felt as a result of an individual decision rather\nmaterially influenced by artificially intelligent functional units.\nthan an aggregate of many decisions..\nNotes:\n1. It is not necessary for a system\u2019s outcome to be solely determined by artificially intelligent\nfunctional units in order for the system to be defined as an artificially intelligent system; and Example: A court determines whether a defendant is guilty of a criminal charge, with the\n2. A particular feature of AI systems is that they learn behavior and rules not explicitly punishment for guilt being a life sentence. This is a critical decision because it has a very\nprogrammed in. large impact on the life of the defendant and also sets precedent for similar cases in the\nfuture.\nExample 1: A small claims court uses Example 2: A government entity uses\n2.7 Functional Unit\nan artificially intelligent software package a chatbot which allows customers to ask\nto collect evidence pertaining to a case, routine questions, book appointments\nEntity of hardware or software, or both, capable of accomplishing a specified purpose3 .\ncompare it to similar cases in the past, and conduct minor financial transactions.\nand present a recommended decision The chatbot responds to customer\nto a judge. The judge determines the queries with pre-written responses and is\n2.8 Individually significant decision\nfinal outcome. This decision-making based on pre-programmed decision rules.\nA decision which has the potential for significant impact on at least one individual\u2019s\nmethodology is materially influenced Therefore, the chatbot is not an AI system.\ncircumstances, behavior or choices, or has legal or similarly significant effects on him or her.\nby an artificially intelligent functional If, however, the chatbot autonomously\nunit, and is therefore classified as an AI adjusted its treatment of customers\nsystem. based on the outcome of past cases, it\nwould be an AI system. Example: A company decides to make an employee redundant. This is an individually\nsignificant decision because of its potential impact on the employee\u2019s financial situation.\n2.5 Bias (of a system)\nInclination or prejudice for or against one person or group, especially in a way considered to be\nunfair2.\n2 Oxford Dictionary 2018, Oxford University Press, viewed online 4th October 2018, <https://en.oxforddictionaries.com>\n1 Consistent with ISO/IEC 2382:2015 3 From ISO/IEC 2382:2015\n12 132.9 Non-operational bias (of a system) 2.11 Significant decision\nBias that is either: A decision which is either individually significant or is part of a set of significant-at-scale\ndecisions.\na not a design feature; or\nb not important in achieving the stated purpose of the system. 2.12 Subject of an artificially intelligent system (also AI subject)\nA natural person who is any of the following:\n2.10 Set of significant-at-scale decisions\na an end-user of an AI system\nA set of decisions made by the same system or organization which, when taken in aggregate,\nb directly affected by the operation of or outcomes of an AI system, or:\nhave significant impact on society as a whole or groups within it.\nc a recipient of a service or recommendation provided by an AI system\nNotes:\n1. The decisions need not be individually significant in order to qualify, in aggregate, as a set of\nsignificant-at-scale decisions; and\n2. Examples of areas which have a large impact on society and which include but are not limited\nto: the large-scale allocation of resources or opportunities amongst groups; the structure of\ngovernment; the division of power between large entities or groups; the law, and its interpretation\nand enforcement; conflict and war; international relations, etc.\nExample: An AI system is used by a website to determine which content to show users.\nThis decision is not individually significant, since a user is not greatly affected by whether\na particular piece of media is shown to them. However, if the website is popular then the AI\nsystem may be making a set of significant-at-scale decisions, because any biases in the\nsystem will affect a large number of users.\n14 153.1 Fairness (Principle 1)\n03\n3.1.1 Principle\nPRINCIPLES AND We will make AI systems fair\nData ingested should, where possible, be accurate and representative of the affected\npopulation.\nGUIDELINES\nAlgorithms should avoid non-operational bias.\nSteps should be taken to mitigate and disclose the biases inherent in datasets.\nSignificant decisions should be provably fair.\nAll personnel involved in the development, deployment and use of AI systems have a role and\nresponsibility to operationalize AI fairness and should be educated accordingly.\n3.1.2 Guidelines\n3.1.2.1\nBenefits of AI systems should be available and accessible to all. AI systems should not discriminate\nagainst people or groups in a way that could have an adverse impact. Especially significant and\ncritical decisions made or assisted by AI should be provably fair.\na Considering that fairness can have many different definitions, an organization should\ndocument its own definition of fairness for the context that the AI system is going to be\nimplemented in. Organizations should document what the implemented fairness objective\nstands for and why this choice was considered the most suitable for the given scenario.\nb It is recommended to identify and document demographic groups that may be adversely\nimpacted and mitigate the risk where possible.\nc AI developers and AI operators could consider formal procedures such as Discrimination\nImpact Assessments as a means of ensuring fairness. This assessment should be\ndocumented as well as the mitigation measures that were implemented by the organization.\nThe impact assessment should be conducted pre-release and regularly after as an ongoing\nevaluation, results are advised to be documented.\n16 173.1.2.2 3.1.2.3\nConsideration should be given to whether the data ingested is accurate and representative Consideration should be given to whether decision-making processes introduce bias\nof the affected population\na When subjecting different groups to different decision-making processes, AI developers\nd Fairness has many different definitions in different cultures and for different contexts. should consider whether this will lead to non-operational bias.\nEncouraging a diverse and inclusive AI ecosystem is thus all the more crucial to ensure\nthat one definition of fairness does not contradict another, and that the process of defining b When evaluating the fairness of an AI system, AI developers and AI operators should\nfairness itself is fair, with under-represented groups represented in the discussion. consider whether AI subjects in the same circumstances receive equal treatment.\ne AI developers and AI operators should undertake reasonable data exploration and/or testing\nto identify potentially prejudicial decision-making tendencies in AI systems arising from\nExample: An organization uses an AI tool to automate the pre-screening of candidates\ndata inaccuracy and biases in the data.\nfor a job opening. It is trained on data from the company\u2019s existing employees, the majority\nof whom are from the same ethnic background. Therefore, the system learns to use name\nf AI developers and operators should evaluate all datasets to assess inclusiveness of and nationality as discriminating factors in filtering job applicants. This could have been\nidentified demographic groups and collect data to close any gaps.\nidentified through testing and rectified by, for example, balancing the training data or only\nusing relevant data fields for training.\ng AI developers and AI operators should refrain from training AI systems on data that is not\nlikely to be representative of the affected AI subjects, or is not likely to be accurate, whether\nthat be due to age, omission, method of collection, or other factors.\n3.1.2.4\nSignificant decisions informed by the use of AI should be fair\nExample: Following a natural disaster, a government relief agency uses an AI system to a AI developers and AI operators could consider formal procedures such as Discrimination\ndetect communities in greatest need by analyzing social media data from a range of websites. Impact Assessments as a means of ensuring fairness.\nHowever, those communities where smartphone penetration is lower having less presence on\n3.1.2.4\nsocial media, and so are at risk of receiving less attention. Therefore, the charity complements\nAI operators should consider whether their AI systems are accessible and usable in a fair\ntheir AI tool with traditional techniques to identify needy populations elsewhere.\nmanner across user groups\n3.1.2.5\nh AI developers should consider whether their AI systems can be expected to perform well Consideration should be given to the effect of diversity on the development and deployment\nwhen exposed to previously unseen data, especially when evaluating people who are not processes\nwell-represented in the training data.\nb Organizations should seek to include people from diverse demographic backgrounds across\nthe full lifecyle, to include design, development and deployment processes. Organizations\nshould seek to engage diverse internal and external groups also.\nc AI developers should consider whether the assumptions they make about AI subjects could\nbe wrong or are likely to lead to non-operational bias; if so, they should consider consulting\nthe AI subjects in a representative manner during the design, development and deployment\nto confirm these assumptions.\n18 193.2 Accountable AI (Principle 2)\n3.2.2.2\nPositive efforts should be made to identify and mitigate any significant risks inherent in the AI\nsystems designed\n3.2.1 Principle\na AI operators should only use AI systems that are backed by evidence-based academic or\nindustrial research, and AI developers should base their development on such research.\nWe will make AI systems accountable\nAccountability for the outcomes of an AI system lies not with the system itself but is\napportioned between those who design, develop and deploy it; Example: A foreign country has a government service which identifies parents who owe\nmoney in child maintenance. The data matching process is often incorrect due to misspelled\nDevelopers should make efforts to mitigate the risks inherent in the systems they design; names or missing data which results in some individuals being incorrectly targeted\nautomatically by the system with the result being a large bill, poor credit ratings and even\nAI systems should have built-in appeals procedures whereby users can challenge significant\nfreezing wages. The recourse for individuals who are incorrectly targeted is time-consuming\ndecisions;\nand not straightforward . If the potential impact of incorrect decisions had been assessed,\nmitigation measures such a user-friendly review procedure could have been set up.\nAI systems should be developed by diverse teams which include experts in the area in which\nthe system will be deployed; and\nAI systems should be subject to external audit and decision quality assurance. b AI operators should identify the likely impact of incorrect automated decisions on AI\nsubjects and, in the case where incorrect decisions are likely to cause significant cost or\ninconvenience, consider mitigating measures.\n3.2.2 Guidelines\nc AI operators should consider internal risk assessments or ethics frameworks as a means to\n3.2.2.1 facilitate the identification of risks and mitigating measures.\nAccountability for the outcomes of an AI system should not lie with the system itself\nd In designing AI systems to inform significant decisions, AI developers should consider\na Accountability for loss or damages resulting from the application of AI systems should not measures to maintain data accuracy over time, including:\nbe attributed to the system itself.\n\u2022 the completeness of the data.\nb AI operators and AI developers should consider designating individuals to be responsible for \u2022 timely update of the data, and.\ninvestigating and rectifying the cause of loss or damage arising from the deployment of AI\n\u2022 whether the context in which the data was collected affects its suitability for the\nsystems.\nintended use case.\nExample: A border camera scanning for predictors of risk may misinterpret a \u201ctic\u201d of an\nindividual with Tourette syndrome as suspicious. These can manifest in a diverse fashion, and\nshould not cause this person to undergo secondary inspection every time they pass through\nthe border . If the data is updated after the first case is encountered then it would avoid\ncausing inconvenience on subsequent visits.\n5 Government of Canada. (2018). Responsible AI in the Government of Canada. Digital Disruption White Paper Series. Version 2.0, p.26. Retrieved from:\nhttps://docs.google.com/document/d/1Sn-qBZUXEUG4dVk909eSg5qvfbpNlRhzIefWPtBwbxY/edit\n4 Cabinet Office (UK), Data Science Ethical Framework, Version 1.0, licensed under the Open Government License v3.0, p. 13 6 Stoica, I. et. al.,2017, A Berkeley View of Systems Challenges for AI, p. 2, https://arxiv.org/pdf/1712.05855.pdf\n20 21e AI developers and AI operators should tune AI models periodically to cater for changes to e Be mindful that there might be fundamental tensions between different principles and\ndata and/or models over time. requirements. Continuously identify, evaluate, document and communicate these trade-\noffs and their solutions.\nf AI operators should consider whether AI systems trained in a static environment will display\nmodel instability when deployed in dynamic environments. Adopt a Trustworthy AI assessment list when developing, deploying or using AI systems,\nf\nand adapt it to the specific use case in which the system is being applied.\nExample: AI systems will need to be able to adapt to the changes in the environment that g Keep in mind that such an assessment list will never be exhaustive. Ensuring Trustworthy\nthey are deployed in. For example, a self-driving car would need to quickly adapt to unexpected AI is not about ticking boxes, but about continuously identifying and implementing\nand dangerous road by learning in real time from other cars that have successfully dealt with requirements, evaluating solutions, ensuring improved outcomes throughout the AI\nthese conditions. In addition, such mission-critical applications must handle noisy inputs system\u2019s lifecycle, and involving stakeholders in this.\nand defend against malicious actors .\n3.2.2.4\nAI subjects should be able to challenge significant automated decisions concerning them and,\ng AI developers should collaborate with AI Operators to train models using historical data where appropriate, be able to opt out of such decisions\nfrom the Operator.\na AI operators using AI systems to inform significant decisions should provide procedures by\nwhich affected AI subjects can challenge a specific decision concerning them.\nh AI Operators should consider working with their vendors (AI developers) to continually\nmonitor performance.\nb AI operators should consider such procedures even for non-significant decisions.\ni AI Operators should subject AI systems informing significant decisions to quality checks\nat least as stringent as those that would be required of a human being taking the same c AI operators should make affected AI subjects aware of these procedures and should\ndesign them to be convenient and user-friendly.\ndecision.\nd AI operators should consider employing human case evaluators to review any such\n3.2.2.3\nchallenges and, when appropriate, overturn the challenged decision.\nAI systems informing critical decisions should be subject to appropriate external audit\na When AI systems are used for critical decisions, external auditing of the AI systems in\nquestion should be used as a means to ensure meaningful standards of transparency and Example: A bank allows customers to apply for a loan online by entering their data. The\naccountability are upheld. bank uses an AI system to automatically determine whether to give the loan and what the\ninterest rate should be. They provide users with an option to contest the decision and have\nb In the case that critical decisions are of civic interest, public release of the results of the it reviewed by a human . They also require that customers justify their challenge by filling in\naudit should be considered as a means of ensuring public processes remain accountable a form, which assists the case reviewer and deters customers from challenging a decision\nto those affected by them. without good reason.7\nIn the case that critical decision are life and death decisions, these decisions should be\nc\nsupported by further validation and verification via a human operator.\nd Facilitate traceability and auditability of AI systems, particularly in critical contexts or\nsituations.\n7 Adapted from EU Commission, Can I be subject to automated individual decision-making, including profiling? Retrieved from: https://ec.europa.eu/\ninfo/law/law-topic/data-protection/reform/rights-citizens/my-rights/can-i-be-subject-automated-individual-decision-making-including-profiling_\nen#example\n22 23e AI operators should consider instituting an opt-out mechanism for significant automated 3.2.2.7\ndecisions. AI operators should understand the AI systems they use sufficiently to assess their suitability\nfor the use case and to ensure accountability and transparency\nf AI operators could consider \u201ccrowd challenge\u201d mechanisms whereby a critical number of\ncomplaints triggers an investigation into the fairness and/or accuracy of a decision-making\na In the case of critical decisions, AI operators should avoid using AI systems that cannot be\nprocess as a whole.\nsubjected to meaningful standards of accountability and transparency.\n3.2.2.5 b AI developers should consider notifying customers and AI operators of the use cases for\nAI systems informing significant decisions should not attempt to make value judgements on which the system has been designed, and those for which it is not suitable.\npeople\u2019s behalf without prior consent\na When informing an AI subject about significant choices they will make, AI systems should\nnot unreasonably restrict the available options or otherwise attempt to influence their value\njudgements without the explicit consent of the AI subject in question.\n3.2.2.6\nAI systems informing significant decisions should be developed by diverse teams with\nappropriate backgrounds\na AI developers who develop AI systems which may be used to assist in making critical\ndecisions should involve in the process experts with a background in social science, policy,\nor another subject which prepares them to evaluate the broader societal impact of their\nwork.\nb Development of AI systems informing significant decisions should include consultation\nwith experts in the field in which the system will be deployed.\nExample: An app that uses AI to assess medical symptoms and has a large user base had\nto face regulatory scrutiny because of number of complaints from doctors. They warned\nthat the application can miss signs of serious illness. A number of different shortcomings\nwere identified by doctors, some of which the company could address and resolve.8\n8 Financial Times. 2018. High-profile health app under scrutiny after doctor\u2019s complaints. Retrieved from: https://www.ft.com/content/19dc6b7e-8529-\n11e896-dd-fa565ec55929\n24 253.3 Transparent AI (Principle 3)\nc To facilitate the above, AI developers and AI operators should consider documenting the\nfollowing information during the design, development and deployment phases, and retaining\n3.3.1 Principle this documentation for a length of time appropriate to the decision type or industry:\n\u2022 the provenance of the training data, the methods of collection and treatment, how the data was\nWe will make AI systems transparent\nmoved, and measures taken to maintain its accuracy over time;\n\u2022 the model design and algorithms employed; and\nDevelopers should build systems whose failures can be traced and diagnosed. \u2022 changes to the codebase, and authorship of those changes.\nPeople should be told when significant decisions about them are being made by AI. d Where possible given the model design, AI developers should consider building in a means\nby which the \u201cdecision journey\u201d of a specific outcome (i.e. the component decisions leading\nWithin the limits of privacy and the preservation of intellectual property, those who deploy to it) can be logged.\nAI systems should be transparent about the data and algorithms they use.\nResponsible disclosures should be provided in a timely manner, and provide reasonable\nExample: A technology company has a product which is designed to assist in medical\njustifications for AI systems outcomes. This includes information that helps people\ndiagnosis. It documents each stage of its reasoning and relates it back to the input data.9\nunderstand outcomes, like key factors used in decision making.\n3.3.2 Guidelines\n3.3.2.1\nTraceability should be considered for significant decisions, especially those that have the\npotential to result in loss, harm or damage.\na For AI systems which inform significant decisions, especially those with the potential to\ncause loss, harm or damage, AI developers should consider building in traceability (i.e. the\nability to trace the key factors leading to any specific decision).\nb Organizations should ensure that harms caused through AI systems are investigated and\nredressed, by enacting strong enforcement mechanisms and remedial actions, to make\ncertain that human rights and the rule of law are respected in the digital world and in the\nphysical world.\n9 See IBM WatsonPaths\n26 273.3.2.2 3.4 Explainable AI (Principle 4)\nPeople should be informed of the extent of their interaction with AI systems\n3.4.1 Principle\na AI operators should inform AI subjects when a significant decision affecting them has been\nmade by an AI system.\nWe will make AI systems as explainable as\ntechnically possible\nExample: A small claims court adjudicates minor civil matters such as debt collection and\nevictions. They introduce an AI system to suggest the outcome of a ruling. At the time of the\nDecisions and methodologies of AI systems which have a significant effect on individuals\nruling the plaintiff and defendant are notified that the decision was assisted by an AI system.\nshould be explainable to them, to the extent permitted by available technology.\nThe court also provides an explanation for the decision.\nIt should be possible to ascertain the key factors leading to any specific decision that\ncould have a significant effect on an individual.\nb If an AI system can convincingly impersonate a human being, it should do so only after\nIn the above situation we will provide channels through which people can request such\nnotifying the AI subject that it is an AI system.\nexplanations.\nExample: A technology company produces a conversational AI agent which can make 3.4.2 Guidelines\nsome phone calls on behalf of its users. Those who receive the calls may believe that they\n3.4.2.1\nare speaking to a human. Therefore, the company programs the agent to identify itself at\nAI operators could consider providing affected AI subjects with a high-level explanation of\nthe start of every conversation.\nhow their AI system works\na AI operators could consider informing the affected AI subjects in understandable, non-\ntechnical language of:\n\u2022 the data that is ingested by the system;\n\u2022 the types of algorithms employed;\n\u2022 the categories into which people can be placed; and\n\u2022 the most important features driving the outcomes of decisions\n\u2022 A comprehensive list of feature engineering and models that was considered during\nthe model building phase.\nExample: A person turned down for a credit card might be told that the algorithm took\ntheir credit history, age, and postcode into account, but not learn why their application was\nrejected10 .\n28 29b For non-sensitive public sector use cases designed for the common good, AI operators\ncould consider making source code, together with an explanation of the workings of the AI Example: The UK\u2019s NHS developed a tool called Predict, which allows women with breast\nsystem, available either publicly or upon request (this should be done only if there is low risk cancer to compare their case to other women who have had the same condition in the past,\nof people \u2018gaming the system\u2019). and visualize the expected survival rate under various treatment options. The website has an\nexplanation page which shows the weights behind various factors and contains a description\nc The AI operator should maintain necessary documentation that provides elaboration and of the underlying mathematics12 .\nclarification on how the algorithm works, for example documentation of processes, decision\nmaking flow charts of the system, and how the appeal process is embedded.\nd An individual should have the ability to contest and seek effective redress against decisions c In the case that such explanations are available, they should be easily and quickly accessible,\nmade by AI systems. These must be addressed by the group or team supporting these free of charge and user-friendly.\nmodels.\ne Develop appropriate impact indices for the evaluation of AI system technological 3.5 Robust, Safe and Secure AI (Principle 5)\ninterventions from multiple perspectives.\n3.5.1 Principles\n3.4.2.2\nAI operators should consider providing affected AI subjects with a means to request\nexplanations for specific significant decisions, to the extent possible given the state of present AI systems will be technically robust\nresearch and the choice of model\nAI systems should be technically robust with a preventative approach to risks which\na AI operators should consider providing a means by which people affected by a significant\noperates in a manner such that they reliably behave.\ndecision informed by AI can access the reasoning behind that decision.\nAI Developers should ensure AI systems will not cause any unintentional harm and\nadverse impacts.\nExample: The US Consumer Financial Protection Bureau requires that creditors in the US\nwho reject credit applications must explain to the applicant the principal reason(s) why their\nAI systems should be resilient to attacks and security such as data poisoning and\napplication was rejected (e.g. \u201clength of residence\u201d or \u201cage of automobile\u201d)11 . In particular,\nmodel leakage.\n\u201cstatements that the adverse action was based on the creditor`s internal standards or\npolicies or that the applicant, joint applicant, or similar party failed to achieve a qualifying AI systems should have safeguards that enable a fallback plan in case of problems.\nscore on the creditor`s credit scoring system are insufficient\u201d.\nAI system results should be reproducible.\nb Where such explainability is not possible given available technology, AI operators should\nconsider compromises such as counterfactual reasoning, or listing the most heavily\nweighted factors contributing to the decision.\n11 Consumer Financial Protection Bureau, 12 CFR Part 1002 - Equal Credit Opportunity Act (Regulation B), \u00a7 1002.9 Notifications, Retrieved from https://www.\n10 The Guardian. AI watchdog needed to regulate automated decision-making, say experts. Retrieved from: https://www.theguardian.com/technology/2017/ consumerfinance.gov/policy-compliance/rulemaking/regulations/1002/\njan/27/ai-artificial-intelligence-watchdog-needed-to-prevent-discriminatory-automated-decisions 12 Predict website, accessible at http://www.predict.nhs.uk/predict_v2.1/legal/algorithm\n30 313.5.2 Guidelines\n3.5.2.1 3.5.2.3\nAI Operators should continue conducting vulnerability assessments, verification of AI system AI Operators should assure end users of the system\u2019s reliability through documentation, and\ndifferent behaviors in unexpected situations and for any dual-use case, to include: operationalizing processes for testing and verification of desired outcomes , specifically:\na Putting measures to ensure integrity and resilience of IA system against potential attacks. a AI system should be designed with an approach to continue monitoring if the system meets\ngoals, purposes and intended application.\nb AI Operators should define an approach to ensure results are reproducible through clear\nExample: IBM researchers presented a novel approach to exploiting AI for malicious\nprocess and documentation.\npurposes. Their system, DeepLocker embeds AI capabilities within the malware itself in order\nto improve its evasion techniques. It uses an Artificial Intelligence model to identify its target\nc AI Operators should publish documentation to assure system robustness to end users.\nusing indicators like facial recognition, geolocation and voice recognition.\n3.5.2.4\nAI systems will be safe, secure and controllable by humans\nb Verifying how AI systems behave in unexpected situations and environments.\na Safety and security of people, be they operators, end-users or other parties, will be of\nc Taking preventative measures against any possible system dual-case use. paramount concern in the design of any AI system.\nb AI systems should be verifiably secure and controllable throughout their operational\n3.5.2.2\nlifetime, to the extent permitted by technology.\nAI operators should define a suitable fall back plan and test it to maintain readiness in\nunexpected situations and in high safety risks level, to include:\nc The continued security and privacy of users should be considered when decommissioning\na Developing a plan to measure and assess potential safety risks to you or any third party AI systems.\nfrom technology use accidental or malicious misuse.\nd AI systems that may directly impact people\u2019s lives in a significant way should receive\nb Define thresholds for system acceptable results and governance procure to fall back on commensurate care in their designs.\nalternative defined and tested plans.\ne Such systems should be able to be overridden or their decisions reversed by designated\nc Considering an insurance policy to mitigate risks arising from potential damages. individuals.\nExample: Adopting a risk-based approach to procurement and clearly communicating it to\nvendors can help address such issues by giving the procuring organization advance notice\nof the specific oversight capabilities it will need in future stages of the system lifecycle,\npreventing vendors from presenting intellectual property arguments against required testing,\nmonitoring and auditing of their AI systems going forward and rewarding vendors\u2014of all\nsizes\u2014that are more advanced and responsive in their responsible AI efforts.\n13 IBM. https://documents.trendmicro.com/assets/white_papers/wp-malicious-uses-and-abuses-of-artificial-intelligence.pdf\n32 333.6.2 Guidelines\n3.5.2.5\nAi systems should not be able to autonomously hurt, destroy or deceive humans\n3.6.2.1\na AI systems should be built to serve and inform, and not to deceive and manipulate.\nAI should be developed to align with human values, contribute to human flourishing and\nbenefit society.\nb Nations should collaborate to avoid an arms race in lethal autonomous weapons, and\nsuch weapons should be tightly controlled. a Society should be consulted in a representative fashion to inform the development of AI.\nc Active cooperation should be pursued to avoid corner-cutting on safety standards. b Stakeholders throughout society should be involved in the development of AI and its\ngovernance.\nSystems designed to inform significant decisions should do so impartially.\nd\nc Stakeholders should proactively engage in responsible stewardship of trustworthy AI in\npursuit of beneficial outcomes for people and the planet.\n3.6 Human Centered AI (Principle 6)\nd Organizations should prioritize having all their stakeholders learn about wellbeing metrics\nas a potential determinant of how they create, deploy, market and monitor their AI\n3.6.1 Principles technologies.\nWe will give AI systems human values and\nExample: IEEE P7010 recommended practice14 establishes wellbeing metrics relating to\nmake them beneficial to society human factors directly affected by intelligent and autonomous systems and establishes a\nbaseline for the types of objective and subjective data these systems should analyze and\nGovernment will support the research of the beneficial use of AI. include (in their programming and functioning) to proactively increase human wellbeing.\nStakeholders throughout society should be involved in the development of AI and its\ngovernance.\n3.6.2.2\nSystematic analyses that examine the ethics of designing affective systems to nudge human\nDesign AI systems to adopt, learn and follow the norms and values of the community\nbeings prior to deployment are needed.\nthey serve.\nSystematic analyses that examine the ethics of designing affective systems to nudge a Nudging in AI systems should have an opt-in system policy with explicit consent.\nhuman beings prior to deployment are needed.\nb We recommend that when appropriate, an affective system that nudges human beings\nDecisions related to lethal force, life and death should not be delegated to AI systems. should have the ability to accurately distinguish between users, including detecting\nRules and standards should be adopted to ensure effective human control over those characteristics such as whether the user is an adult or a child. Additional protections must\ndecisions. be put in place for vulnerable populations, such as children, when informed consent cannot\nbe obtained, or when it may not be a sufficient safeguard.\nc AI systems with nudging strategies must be carefully evaluated, monitored, and controlled.\n14 IEEE. https://sagroups.ieee.org/7010/\n34 353.6.2.3 f In situations where it is needed, human rights impact assessments and human rights due\nHumanity should retain the power to govern itself and make the final decision, with AI in an diligence, human determination codes of ethical conduct, or quality labels and certifications\nassisting role. intended to promote human centered values and fairness should be considered.\ng Mechanisms should be put into place to receive external feedback regarding AI systems\na Decisions related to lethal force, life and death should not be delegated to AI systems. Rules\nthat potentially infringe on fundamental rights.\nand standards should be adopted to ensure effective human control over those decisions.\nb Responsible parties (e.g., parents, nurse practitioners, social workers, and governments)\nshould be trained to detect the influence due to AI and ineffective mitigation techniques.\nIn the most extreme cases it should always be possible to shut down harmful AI system.\nc Those actions undertaken by an affective system that are most likely to generate an\nemotional response should be designed to be easily changed.\nd Users should be able to make informed autonomous decisions regarding AI systems. They\nshould be given appropriate knowledge and tools to comprehend and interact with AI\nsystems to satisfactory degree and, where possible to reasonably self-assess or challenge\nthe system.\n3.6.2.4\nAI systems should be designed in a way that respects the rule of law, human rights, society\nvalues, and should include appropriate safeguards to ensure a fair and just society.\na AI Developers should design AI systems to adopt, learn and follow the norms and values of\nthe community they serve.\nb Organizations could identify the norms of the specific community in which AI systems are\nto be deployed in. In particular, pay attention to the norms relevant to the kinds of tasks that\nthe AI systems are designed to perform. These could be documented as well as how these\nnorms are addressed by the AI system.\nc To respond to the dynamic change of norms in society the AI system could be able to adjust\nits existing norms and learn new ones, while being transparent about these changes.\nd Designers should consider forms and metrics for assessing an AI system\u2019s norm conformity\nover the lifetime of the system (e.g. human-machine agreement on moral decisions,\nverifiability of AI decisions, justified trust).\ne The norm identification process must document the similarities and differences between\nthe norms that humans apply to other humans and the norms they apply to AI systems.\nNorm implementations should be evaluated specifically against the norms that the\ncommunity expects the AI system to follow.\n36 373.7 Sustainable and Environmentally Friendly AI (Principle 7)\nd While there is no standardized means of reporting on the environmental impact of AI\nsystems, where possible, model development and commissioning should make carbon\n3.7.1 Principle impacts a core consideration alongside functional and business requirements.\ne Smaller models should be considered \u2013 shrinking down the model size and using fewer\nWe will promote sustainable and environmentally compute cycles (to balance financial and performance costs with the end performance of\nthe model).\nfriendly AI\nf Carbon awareness should be considered \u2013 adjusting the operational parameters of the AI\nAI systems should be used to benefit all human beings, including future generations. The\nsystem to dynamically select the best time and location for energy use from the grid can\nenvironment is fundamental to this.\nreduce its carbon footprint.\nThroughout the AI lifecycle, implementations should only be carried out on the basis of a full\nunderstanding and acknowledgement of implications for sustainability and environment.\nExample: In agriculture, AI can transform Example: Google`s DeepMind division\nAn AI system\u2019s development, deployment and use, should be assessed via critical examination production by better monitoring and has developed AI that teaches itself to\nof resource usage and energy consumption.\nmanaging environmental conditions minimize the use of energy to cool Google`s\nand crop yields. AI can help reduce both data centers. As a result, Google reduced\nMechanisms to measure environmental impact due to type of energy use and processing\nfertilizer and water, all while improving its data center energy requirements by\npower provided by data centers should be established.\ncrop yields. 40% 15.\n3.7.2 Guidelines\n3.7.2.1 Throughout the AI lifecycle, implementations should always be carried out after full\nunderstanding and acknowledgement of AI implications on sustainability and environment.\na The application of Artificial Intelligence to address sustainability challenges are well\nunderstood. Building and running green, sustainable AI systems is of increasing importance,\ngiven the large carbon footprint that they can generate and the wider context of addressing\nclimate change.\nb AI model development, and by extension commissioning, should therefore seek to balance\ntechnical performance with energy consumption and environmental impact.\nc Efforts should be made to estimate and understand sustainability and environmental\nimpact across the AI lifecycle (e.g. the energy consumption costs associated with model\ntraining, and CO2 emissions and cloud compute costs associated with the deployment\nand running of the system).\n15 https://www.deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-by-40\n38 393.8 Privacy Preserving AI (Principle 8)\n3.8.2.2\nEstablish an oversight mechanism for data collection, storage and processing and use across\nyour organization\n3.8.1 Principle\na Assess who can access data and under which conditions.\nWe will respect people\u2019s privacy\nb Assign specific responsibilities and role for Data Protection Officers.\nAI systems should respect privacy and use the minimum intrusion necessary.\nc Prevention of harm to privacy necessitates adequate data governance that covers the\nquality and integrity of the data used, its relevance in light of the domain in which the\nAI systems should uphold high standards of data governance and security,\nprotecting personal information. AI systems will be deployed, its access protocols and the capability to process data in a\nmanner that protects privacy.\nSurveillance or other AI-driven technologies should not be deployed to the extent\nof violating internationally and/or UAE\u2019s accepted standards of privacy and human d AI systems must guarantee privacy and data protection throughout a system\u2019s entire\ndignity and people rights. lifecycle. This includes the information initially provided by the user, as well as the\ninformation generated about the user over the course of their interaction with the system\nPrivacy by design should be embedded in AI systems and where possible AI\n(e.g. outputs that the AI systems generated for specific users or how users responded to\nalgorithm should have adequate privacy impact assessments.\nparticular recommendations). Digital records of human behavior may allow AI systems\nto infer not only individuals\u2019 preferences, but also their age, gender, religious or political\nAdequate data protection frameworks and governance mechanisms should be\nviews. To allow individuals to trust the data gathering process, it must be ensured that data\nestablished in a multi-stakeholder approach and ensured throughout the life cycle\ncollected about them will not be used to unlawfully or unfairly discriminate against them.\nof AI systems.\ne In any given organization that handles individuals\u2019 data (whether someone is a user of the\nAI developers and operators should strive to strike the balance between privacy\nsystem or not), data protocols governing data access should be put in place in line with\nrequirements, individual rights and innovation growth and society benefits.\nnational privacy legislation (be these universal or sector-specific). These protocols should\noutline who can access data and under which circumstances. Only duly qualified personnel\n3.8.2 Guidelines\nwith the competence and need to access individual\u2019s data should be allowed to do so.\n3.8.2.1\nEstablish mechanism for users to flag issues related to privacy or data protection\na AI Operators should review the system for proper consent logging, ability of users to revoke\npermission whenever applicable.\nb Consider training AI model without or with minimal user of potentially sensitive or personal\ndata.\nc Use measures to enhance privacy such as encryption, anonymization and aggregation.\n40 41f Algorithmic systems require adequate privacy impact assessments, which also include\nsocietal and ethical considerations of their use and an innovative use of the privacy by\ndesign approach. AI actors need to ensure that they are accountable for the design and\nimplementation of AI systems in such a way as to ensure that personal information is\nprotected throughout the life cycle of the AI system.\ng Establish data policies or equivalent frameworks, or reinforce existing ones, to ensure full\nsecurity for personal data and sensitive data, which, if disclosed, may cause exceptional\ndamage, injury or hardship to individuals. Examples include data relating to offences,\ncriminal proceedings and convictions, and related security measures; biometric, genetic\nand health data; and personal data such as that relating to race, descent, gender, age,\nlanguage, religion, political opinion, national origin, ethnic origin, social origin, economic or\nsocial condition of birth, or disability and any other characteristics.\nh Promote mechanisms, such as open repositories for publicly funded or publicly held data,\nsource code and data trusts, to support the safe, fair, legal and ethical sharing of data,\namong others.\ni Promote and facilitate the use of quality and robust datasets for training, development and\nuse of AI systems, and exercise vigilance in overseeing their collection and use.\nExample: with the help of AI & ML algorithms16 , Synthetic data can be created to enhance\nprivacy. The data created will have the same statistical characteristics for the testing\nenvironment and where a third party can have access.\n16 https://research.aimultiple.com/privacy-enhancing-technologies/\n42 43Microsoft. 2022. Microsoft Responsible AI Standard, v2. General Requirements. June 2022.\n04\nRetrieved from:\nhttps://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4ZPmV\nISO. 2022. Proposed ISO Standard on Risk Management of AI: What Businesses Should Know.\nBibliography\nMay 2022. Retrieved from:\nhttps://unesdoc.unesco.org/ark:/48223/pf0000381137/PDF/381137eng.pdf.multi\nGSMA 2022. The AI Ethics Playbook. Implementing ethical principles into everyday business.\nFebruary 2022. Retrieved from:\nhttps://www.gsma.com/betterfuture/wp-content/uploads/202201//The-Mobile-Industry-Ethics-\nPlaybook_Feb-2022.pdf\nOECD. 2022. AI Policy Observatory. Retrieved from:\nhttps://standards.ieee.org/wp-content/uploads/import/documents/other/ead_v2.pdf\nUNESCO. 2022. Recommendation on the Ethics of Artificial Intelligence. Adopted in November\n2021. Retrieved from:\nhttps://unesdoc.unesco.org/ark:/48223/pf0000381137/PDF/381137eng.pdf.multi\nEuropean Commission. 2021. Proposal for a Regulation of the European Parliament and of the\nCouncil Laying Down Harmonized Rules on Artificial Intelligence (Artificial Intelligence Act) and\nAmending Certain Union Legislative Acts. Retrieved from:\nhttps://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A52021PC0206\nEuropean Commission. High Level Expert Group on Artificial Intelligence ( AI HLEG ). Ethics\nGuidelines for Trustworthy AI . April 2019. Retrieved from:\nhttps://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai\nCentre for Data Ethics and Innovation. UK Government: The roadmap to an effective AI assurance\necosystem. December 2021. Retrieved from:\nhttps://www.gov.uk/government/publications/the-roadmap-to-an-effective-ai-assurance-\necosystem\nThe New York City Council. 2021. A Local Law to amend the administrative code of the city of New\nYork, in relation to automated employment decision tools. Retrieved from:\nhttps://legistar.council.nyc.gov/LegislationDetail.aspx?ID=4344524&GUID=%20B051915D-A9AC-\n451E-81F86596032-FA3F9&\nUNESCO, Recommendation on the Ethics of Artificial Intelligence. 2021. Retrieved from:\nhttps://unesdoc.unesco.org/ark:/48223/pf0000381137\n44 45AI NOW Institute. Algorithmic accountability for the public sector. August 2021. Retrieved from: CNIL. (2017). How can humans keep the upper hand? The ethical matters raised by algorithms\nhttps://www.opengovpartnership.org/documents/algorithmic-accountability-public-sector/ and artificial intelligence. Retrieved from:\nhttps://www.cnil.fr/sites/default/files/atoms/files/cnil_rapport_ai_gb_web.pdf\nIEEE 2019. Ethically Aligned Design. Version 2 \u2013 For Public Discussion. Retrieved from:\nhttps://standards.ieee.org/wp-content/uploads/import/documents/other/ead_v2.pdf Executive Office of the President of the United States. National Science and Technology Council.\nNetworking and Information Technology Research and Development Subcommittee. (2016). The\nnational artificial intelligence research and development strategic plan. Retrieved from:\nIEEE 2017. Ethically Aligned Designed : A vision for Prioritizing Human Well-being with\nhttps://www.nitrd.gov/PUBS/national_ai_rd_strategic_plan.pdf\nAutonomous and Intelligent Systems. Retrieved from:\nhttps://standards.ieee.org/wp-content/uploads/import/documents/other/ead_v2.pdf\nExecutive Office of the President of the United States National Science and Technology Council.\nCommittee on Technology. (2016). Preparing for the future of artificial intelligence. Retrieved from:\nGoogle. AI Principles 2021. Retrieved from:\nhttps://obamawhitehouse.archives.gov/sites/default/files/whitehouse_files/microsites/ostp\nhttps://ai.google/principles/\nNSTC/preparing_for_the_future_of_ai.pdf\nMicrosoft. Responsible AI Standard v2 (June 2022). Retrieved from:\nThe Headquarters for Japan\u2019s Economic Revitalization. (2015). New Robot Strategy. Japan\u2019s\nhttps://www.microsoft.com/en-us/ai/responsible-ai\nRobot Strategy. Vision, Strategy, Action Plan. Retrieved from:\nhttp://www.meti.go.jp/english/press/2015/pdf/0123_01b.pdf\nThe ECONOMIST Group 2022 . Pushing forward: the future of AI in the Middle East and North\nAfrica Report. Retrieved from:\nTreasury Board of Canada Secretariat. (2018). Responsible Artificial Intelligence in the\nhttps://impact.economist.com/perspectives/sites/default/files/google_ai_mena_report.pdf\nGovernment of Canada. Digital Disruption White Paper Series. Version 2.0.\nhttps://docs.google.com/document/d/1Sn-qBZUXEUG4dVk909eSg5qvfbpNlRhzIefWPtBwbxY/\nPDPC. (2018, June 5). Discussion paper on Artificial Intelligence (AI) and Personal Data.\nedit\nSingapore: Personal Data Protection Commission Singapore (PDPC). Retrieved from:\nhttps://www.pdpc.gov.sg/-/media/Files/PDPC/PDF-Files/Resource-for-Organisation/AI/\nThe Toronto Declaration: Protecting the right to equality and non-discrimination in machine\nDiscussion-Paper-on-AI-and-PD---050618.pdf\nlearning systems. Toronto, Canada: Amnesty International and Access Now. Retrieved from:\nhttps://www.accessnow.org/cms/assets/uploads/201808//The-Toronto-Declaration_ENG_08-\nITI. AI Policy Principles. Retrieved from:\n2018.pdf\nhttps://www.itic.org/public-policy/ITIAIPolicyPrinciplesFINAL.pdf\nhttp://data.parliament.uk/writtenevidence/committeeevidence.svc/evidencedocument/artificial-\nCabinet Office.(2016, May 19). Data Science Ethical Framework. Retrieved from: intelligence-committee/artificial-intelligence/oral/73546.html (house of lords select committee\nhttps://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/ oral evidence, Q61)oral evidence, Q61)\nfile/524298/Data_science_ethics_framework_v1.0_for_publication__1_.pdf\nHouse of Lords Select Committee on Artificial Intelligence. (2018). AI in the UK: ready, willing and\nEuropean Parliament. (2017, February 16). European Parliament resolution of 16 February 2017 able?\nwith recommendations to the Commission on Civil Law Rules on Robotics (20152103/(INL)). https://publications.parliament.uk/pa/ld201719/ldselect/ldai/100100/.pdf\nRetrieved from:\nhttp://www.europarl.europa.eu/sides/getDoc.do?pubRef=-//EP//TEXT+TA+P8-TA-2017- House of Commons Science and Technology Select Committee. Algorithms in Decision Making.\n0++0051DOC+XML+V0//EN https://publications.parliament.uk/pa/cm201719/cmselect/cmsctech/351351/.pdf\nVillani, C. (2018). For a meaningful Artificial Intelligence towards a French and European strategy.\nRetrieved from:\nhttps://www.aiforhumanity.fr/pdfs/MissionVillani_Report_ENG-VF.pdf\n46 47", "metadata": {"country": "UAE", "year": "2022", "legally_binding": "no", "binding_proof": "None", "date": "01/01/2022", "regulator": "UAE Artificial Intelligence Office", "type": "guidelines", "status": "enacted", "language": "English", "use_cases": "[1, 3, 5, 6]"}}
{"_id": "68695ba35520a4cc7989de7b", "title": "UAE Deepfake Content Regulation Guide", "source": "https://ai.gov.ae/", "text": "AI ETHICS\nARTIFICIAL INTELLIGENCE ETHICS\nPRINCIPLES & GUIDELINES\nPRINCIPLES & GUIDELINES\nDEC 2022Table of Content\nPage Page\n1. Overview 4 3.4 Explainable 29\n1.1 Introduction 5 3.4.1 Principle 29\nWe will make AI systems as explainable as technically possible\n1.2 Scope 6 3.4.2 Guidelines 29\n1.3 Strategic alignment 6 3.5 Robust, Safe and Secure AI 31\n1.4 Responsibility 8 3.5.1 Principles 31\nAI systems will be technically robust\n3.5.2 Guidelines 32\n1.5 Licensing 8\n3.6 Human Centred AI 34\n1.6 Toolkit 8\n3.6.1 Principles 34\nWe will give AI systems human values and make them beneficial to society\n3.6.2 Guidelines 35\n2. Definitions 10\n3.7 Sustainable and Environmentally Friendly AI 38\n3.7.1 Principle 38\n3. Principles and Guidelines 16\nWe will promote sustainabity and environemntly friendly AI 38\n3.7.2 Guidelines 38\n3.1 Fairness 17\n3.8 Privacy Preserving AI 38\n3.1.1 Principle 17\nWe will make AI systems fair\n3.8.1 Principle 40\n3.1.2 Guidelines 20\nWe will respect people\u2019s privacy\n3.8.2 Guidelines 40\n3.2 Accountable AI 20\n3.2.1 Principle 20\nWe will make AI systems accountable\n4. Bibliography 44\n3.2.2 Guidelines 20\n3.3 Transparent AI 26\n3.3.1 Principle 26\nWe will make AI systems transparent\n3.3.2 Guidelines 26\n2 301 1.1 Introduction\nAI\u2019s rapid advancement and innovation potential across a range of fields is incredibly exciting.\nYet a thorough and open discussion around AI ethics, and the principles organizations using this\nOverview\ntechnology must consider, is needed.\nThis document - AI Ethics: Principles and Guidelines \u2013 intends to meet this need to balance these\ntwo central considerations of AI. It is designed to offer detailed guidance to help AI actors adhere\nto eight principles of AI ethics.\nWe will make AI systems that are:\nexplainable transparent accountable fair\nprivacy preserving sustainable and human centered robust, safe and\nenvironmentally secure\nfriendly\nThe guidelines are non-binding, and are drafted as a collaborative multi-\nstakeholder effort, with full awareness of organizations\u2019 needs to innovate\nand protect their intellectual property. This is a collaborative process in\nwhich all stakeholders are invited to be part of an ongoing dialogue. We\nwould like to see the AI Ethics Guidelines evolve into a universal, practical\nand applicable framework informing ethical requirements for AI design and\nuse. The eventual goal is to reach widespread agreement and adoption of\ncommonly agreed policies to inform the ethical use of AI nationally and\naround the world.\n4 51.2 Scope Additionally, the AI Office is currently developing additional policies to be taken into consideration\nalong with this document:\nThis document gives non-mandatory guidelines for achieving the ethical design\nand deployment of AI systems in both the public and private sectors. AI already\nAI Seal AI Procurement Guidelines\nsurrounds us, but some applications are more visible and sensitive than others.\nThe UAE AI Seal brand (UAI) will be used The AI Procurement Guidelines will be\nThis document is applicable only to those AI systems which make or inform\nto attract talent and business from used to guide Federal Government entities\n\u2018significant decisions\u2019 \u2013 that is, those decisions which have the potential for\nacross the globe to come to the UAE to in the procurement of AI systems. They\nsignificant impact either on individuals or on society as a whole. They also apply to\ntest and develop AI. This includes a UAI will list the general principles, proposed\n\u2018critical decisions\u2019, which are a subset of significant decisions and are of especially\nmark recognizing high quality, ethical AI mechanisms for procurement and review\ncritical nature.\ncompanies. It would reward safe, efficient, of AI products between the federal\nverified AI technology with a \u2018UAI Seal government and vendors. The policy\nThis document guides the further developments of sector specific requirements\nof Approval\u2019. The UAE AI Seal brand is considers R&D for specific challenges\nand principles, hence every entity within its sector can further amend, use, or\ncurrently under development by the AI that do not currently have an AI solution\ncomplement its current frameworks to properly suit its context and the need of\nOffice. product that is market-ready. Based on\naffected stakeholders.\nglobal standards for AI Procurement, the\npolicy aims to incentivize AI adoption\nIt is a living document and will undergo further future reviews and enhancements.\nwithin government entities and promote\nAI R&D within the private sector for\nspecified government challenges. The\nprocurement policy is currently under\ndevelopment by the AI Office.\n1.3 Strategic alignment\nThis document comes as a fulfillment of the goals and aspirations set in the UAE National AI\nStrategy and the international Sustainable Development Goals (SDGs). The UAE has long been\nat the forefront of sustainable development. These shared global milestones have created global\nmovement and adoption in several disciplines and sectors. As an early adopter, the UAE has\nestablished a national committee on SDGs with a long-term realization of the UAE SDG agenda\n2030. These goals are further engrained and reflected in the AI Ethics Principles under the topics of\nfairness, inclusiveness, equality, human benefit, and rights.\n6 71.4 Responsibility\nThe AI Office will not be responsible for any misuse of the AI Ethics Principles and Guidelines. The\nuser bears all the consequences of their use.\n1.5 Licensing\nThis document is published under the terms of a Creative Commons Attribution 4.0 International\nLicense in order to facilitate its re-use by other governments and private sector organizations.\nIn summary this means you are free to share and adapt the material, including for commercial\npurposes, provided that you give appropriate credit to the UAE AI and Blockchain Council as its\nowner and do not suggest the Minister of State for Artificial Intelligence, Digital Economy and\nRemote Work Applications Office endorses your use.\n1.6 Toolkit\nThe document is part of a toolkit of self-governing nature that boosts awareness and enables\ngovernment and private institutions to pursue innovative use cases while maintaining human\nvalues and principles.\n8 92.1 AI developer organization\n02\nAn organization which does any of the following:\na determines the purpose of an AI system;\nDefinitions\nb designs an AI system;\nc builds an AI system, or:\nd performs technical maintenance or tuning on an AI system\nNote:\nThe definition applies regardless of whether the organization is the ultimate user of the system, or\nwhether they sell it on or give it away.\nExample: A company develops an artificially intelligent facial recognition system and\nsells it to a country\u2019s border force, who use it to identify suspicious personnel. The company\nis an AI developer organization and the border force is an AI operator organization.\n2.2 AI operator organization\nAn organization which does any of the following:\na uses AI systems in operations, backroom processes or decision-making;\nb uses an AI system to provide a service to an AI subject;\nc is a business owner of an AI system;\nd procures and treats data for use in an AI system; or\ne evaluates the use case for an AI system and decides whether to proceed.\nNotes:\n1. This definition applies regardless of whether the AI system was developed in-house or procured.\n2. It is possible for organizations to be both an AI developer organization and an AI operator organization.\n10 112.6 Critical decision\n2.3 Artificial Intelligence (also AI)\nThe capability of a functional unit to perform functions that are generally associated with human An individually significant decision which is deemed to either have a very large impact on an\nintelligence such as reasoning, learning and self-improvement1. individual or to have especially high stakes. These can be especially sensitive, have the potential\nto cause high loss or damage, be societally significant, or set an important precedent.\n2.4 Artificially Intelligent System (also AI system)\nNotes:\nThe types of decisions referred to here are the same as those in the definition of significant-at-\nA product, service, process or decision-making methodology whose operation or outcome is\nscale decisions, except in this case the effects are felt as a result of an individual decision rather\nmaterially influenced by artificially intelligent functional units.\nthan an aggregate of many decisions..\nNotes:\n1. It is not necessary for a system\u2019s outcome to be solely determined by artificially intelligent\nfunctional units in order for the system to be defined as an artificially intelligent system; and Example: A court determines whether a defendant is guilty of a criminal charge, with the\n2. A particular feature of AI systems is that they learn behavior and rules not explicitly punishment for guilt being a life sentence. This is a critical decision because it has a very\nprogrammed in. large impact on the life of the defendant and also sets precedent for similar cases in the\nfuture.\nExample 1: A small claims court uses Example 2: A government entity uses\n2.7 Functional Unit\nan artificially intelligent software package a chatbot which allows customers to ask\nto collect evidence pertaining to a case, routine questions, book appointments\nEntity of hardware or software, or both, capable of accomplishing a specified purpose3 .\ncompare it to similar cases in the past, and conduct minor financial transactions.\nand present a recommended decision The chatbot responds to customer\nto a judge. The judge determines the queries with pre-written responses and is\n2.8 Individually significant decision\nfinal outcome. This decision-making based on pre-programmed decision rules.\nA decision which has the potential for significant impact on at least one individual\u2019s\nmethodology is materially influenced Therefore, the chatbot is not an AI system.\ncircumstances, behavior or choices, or has legal or similarly significant effects on him or her.\nby an artificially intelligent functional If, however, the chatbot autonomously\nunit, and is therefore classified as an AI adjusted its treatment of customers\nsystem. based on the outcome of past cases, it\nwould be an AI system. Example: A company decides to make an employee redundant. This is an individually\nsignificant decision because of its potential impact on the employee\u2019s financial situation.\n2.5 Bias (of a system)\nInclination or prejudice for or against one person or group, especially in a way considered to be\nunfair2.\n2 Oxford Dictionary 2018, Oxford University Press, viewed online 4th October 2018, <https://en.oxforddictionaries.com>\n1 Consistent with ISO/IEC 2382:2015 3 From ISO/IEC 2382:2015\n12 132.9 Non-operational bias (of a system) 2.11 Significant decision\nBias that is either: A decision which is either individually significant or is part of a set of significant-at-scale\ndecisions.\na not a design feature; or\nb not important in achieving the stated purpose of the system. 2.12 Subject of an artificially intelligent system (also AI subject)\nA natural person who is any of the following:\n2.10 Set of significant-at-scale decisions\na an end-user of an AI system\nA set of decisions made by the same system or organization which, when taken in aggregate,\nb directly affected by the operation of or outcomes of an AI system, or:\nhave significant impact on society as a whole or groups within it.\nc a recipient of a service or recommendation provided by an AI system\nNotes:\n1. The decisions need not be individually significant in order to qualify, in aggregate, as a set of\nsignificant-at-scale decisions; and\n2. Examples of areas which have a large impact on society and which include but are not limited\nto: the large-scale allocation of resources or opportunities amongst groups; the structure of\ngovernment; the division of power between large entities or groups; the law, and its interpretation\nand enforcement; conflict and war; international relations, etc.\nExample: An AI system is used by a website to determine which content to show users.\nThis decision is not individually significant, since a user is not greatly affected by whether\na particular piece of media is shown to them. However, if the website is popular then the AI\nsystem may be making a set of significant-at-scale decisions, because any biases in the\nsystem will affect a large number of users.\n14 153.1 Fairness (Principle 1)\n03\n3.1.1 Principle\nPRINCIPLES AND We will make AI systems fair\nData ingested should, where possible, be accurate and representative of the affected\npopulation.\nGUIDELINES\nAlgorithms should avoid non-operational bias.\nSteps should be taken to mitigate and disclose the biases inherent in datasets.\nSignificant decisions should be provably fair.\nAll personnel involved in the development, deployment and use of AI systems have a role and\nresponsibility to operationalize AI fairness and should be educated accordingly.\n3.1.2 Guidelines\n3.1.2.1\nBenefits of AI systems should be available and accessible to all. AI systems should not discriminate\nagainst people or groups in a way that could have an adverse impact. Especially significant and\ncritical decisions made or assisted by AI should be provably fair.\na Considering that fairness can have many different definitions, an organization should\ndocument its own definition of fairness for the context that the AI system is going to be\nimplemented in. Organizations should document what the implemented fairness objective\nstands for and why this choice was considered the most suitable for the given scenario.\nb It is recommended to identify and document demographic groups that may be adversely\nimpacted and mitigate the risk where possible.\nc AI developers and AI operators could consider formal procedures such as Discrimination\nImpact Assessments as a means of ensuring fairness. This assessment should be\ndocumented as well as the mitigation measures that were implemented by the organization.\nThe impact assessment should be conducted pre-release and regularly after as an ongoing\nevaluation, results are advised to be documented.\n16 173.1.2.2 3.1.2.3\nConsideration should be given to whether the data ingested is accurate and representative Consideration should be given to whether decision-making processes introduce bias\nof the affected population\na When subjecting different groups to different decision-making processes, AI developers\nd Fairness has many different definitions in different cultures and for different contexts. should consider whether this will lead to non-operational bias.\nEncouraging a diverse and inclusive AI ecosystem is thus all the more crucial to ensure\nthat one definition of fairness does not contradict another, and that the process of defining b When evaluating the fairness of an AI system, AI developers and AI operators should\nfairness itself is fair, with under-represented groups represented in the discussion. consider whether AI subjects in the same circumstances receive equal treatment.\ne AI developers and AI operators should undertake reasonable data exploration and/or testing\nto identify potentially prejudicial decision-making tendencies in AI systems arising from\nExample: An organization uses an AI tool to automate the pre-screening of candidates\ndata inaccuracy and biases in the data.\nfor a job opening. It is trained on data from the company\u2019s existing employees, the majority\nof whom are from the same ethnic background. Therefore, the system learns to use name\nf AI developers and operators should evaluate all datasets to assess inclusiveness of and nationality as discriminating factors in filtering job applicants. This could have been\nidentified demographic groups and collect data to close any gaps.\nidentified through testing and rectified by, for example, balancing the training data or only\nusing relevant data fields for training.\ng AI developers and AI operators should refrain from training AI systems on data that is not\nlikely to be representative of the affected AI subjects, or is not likely to be accurate, whether\nthat be due to age, omission, method of collection, or other factors.\n3.1.2.4\nSignificant decisions informed by the use of AI should be fair\nExample: Following a natural disaster, a government relief agency uses an AI system to a AI developers and AI operators could consider formal procedures such as Discrimination\ndetect communities in greatest need by analyzing social media data from a range of websites. Impact Assessments as a means of ensuring fairness.\nHowever, those communities where smartphone penetration is lower having less presence on\n3.1.2.4\nsocial media, and so are at risk of receiving less attention. Therefore, the charity complements\nAI operators should consider whether their AI systems are accessible and usable in a fair\ntheir AI tool with traditional techniques to identify needy populations elsewhere.\nmanner across user groups\n3.1.2.5\nh AI developers should consider whether their AI systems can be expected to perform well Consideration should be given to the effect of diversity on the development and deployment\nwhen exposed to previously unseen data, especially when evaluating people who are not processes\nwell-represented in the training data.\nb Organizations should seek to include people from diverse demographic backgrounds across\nthe full lifecyle, to include design, development and deployment processes. Organizations\nshould seek to engage diverse internal and external groups also.\nc AI developers should consider whether the assumptions they make about AI subjects could\nbe wrong or are likely to lead to non-operational bias; if so, they should consider consulting\nthe AI subjects in a representative manner during the design, development and deployment\nto confirm these assumptions.\n18 193.2 Accountable AI (Principle 2)\n3.2.2.2\nPositive efforts should be made to identify and mitigate any significant risks inherent in the AI\nsystems designed\n3.2.1 Principle\na AI operators should only use AI systems that are backed by evidence-based academic or\nindustrial research, and AI developers should base their development on such research.\nWe will make AI systems accountable\nAccountability for the outcomes of an AI system lies not with the system itself but is\napportioned between those who design, develop and deploy it; Example: A foreign country has a government service which identifies parents who owe\nmoney in child maintenance. The data matching process is often incorrect due to misspelled\nDevelopers should make efforts to mitigate the risks inherent in the systems they design; names or missing data which results in some individuals being incorrectly targeted\nautomatically by the system with the result being a large bill, poor credit ratings and even\nAI systems should have built-in appeals procedures whereby users can challenge significant\nfreezing wages. The recourse for individuals who are incorrectly targeted is time-consuming\ndecisions;\nand not straightforward . If the potential impact of incorrect decisions had been assessed,\nmitigation measures such a user-friendly review procedure could have been set up.\nAI systems should be developed by diverse teams which include experts in the area in which\nthe system will be deployed; and\nAI systems should be subject to external audit and decision quality assurance. b AI operators should identify the likely impact of incorrect automated decisions on AI\nsubjects and, in the case where incorrect decisions are likely to cause significant cost or\ninconvenience, consider mitigating measures.\n3.2.2 Guidelines\nc AI operators should consider internal risk assessments or ethics frameworks as a means to\n3.2.2.1 facilitate the identification of risks and mitigating measures.\nAccountability for the outcomes of an AI system should not lie with the system itself\nd In designing AI systems to inform significant decisions, AI developers should consider\na Accountability for loss or damages resulting from the application of AI systems should not measures to maintain data accuracy over time, including:\nbe attributed to the system itself.\n\u2022 the completeness of the data.\nb AI operators and AI developers should consider designating individuals to be responsible for \u2022 timely update of the data, and.\ninvestigating and rectifying the cause of loss or damage arising from the deployment of AI\n\u2022 whether the context in which the data was collected affects its suitability for the\nsystems.\nintended use case.\nExample: A border camera scanning for predictors of risk may misinterpret a \u201ctic\u201d of an\nindividual with Tourette syndrome as suspicious. These can manifest in a diverse fashion, and\nshould not cause this person to undergo secondary inspection every time they pass through\nthe border . If the data is updated after the first case is encountered then it would avoid\ncausing inconvenience on subsequent visits.\n5 Government of Canada. (2018). Responsible AI in the Government of Canada. Digital Disruption White Paper Series. Version 2.0, p.26. Retrieved from:\nhttps://docs.google.com/document/d/1Sn-qBZUXEUG4dVk909eSg5qvfbpNlRhzIefWPtBwbxY/edit\n4 Cabinet Office (UK), Data Science Ethical Framework, Version 1.0, licensed under the Open Government License v3.0, p. 13 6 Stoica, I. et. al.,2017, A Berkeley View of Systems Challenges for AI, p. 2, https://arxiv.org/pdf/1712.05855.pdf\n20 21e AI developers and AI operators should tune AI models periodically to cater for changes to e Be mindful that there might be fundamental tensions between different principles and\ndata and/or models over time. requirements. Continuously identify, evaluate, document and communicate these trade-\noffs and their solutions.\nf AI operators should consider whether AI systems trained in a static environment will display\nmodel instability when deployed in dynamic environments. Adopt a Trustworthy AI assessment list when developing, deploying or using AI systems,\nf\nand adapt it to the specific use case in which the system is being applied.\nExample: AI systems will need to be able to adapt to the changes in the environment that g Keep in mind that such an assessment list will never be exhaustive. Ensuring Trustworthy\nthey are deployed in. For example, a self-driving car would need to quickly adapt to unexpected AI is not about ticking boxes, but about continuously identifying and implementing\nand dangerous road by learning in real time from other cars that have successfully dealt with requirements, evaluating solutions, ensuring improved outcomes throughout the AI\nthese conditions. In addition, such mission-critical applications must handle noisy inputs system\u2019s lifecycle, and involving stakeholders in this.\nand defend against malicious actors .\n3.2.2.4\nAI subjects should be able to challenge significant automated decisions concerning them and,\ng AI developers should collaborate with AI Operators to train models using historical data where appropriate, be able to opt out of such decisions\nfrom the Operator.\na AI operators using AI systems to inform significant decisions should provide procedures by\nwhich affected AI subjects can challenge a specific decision concerning them.\nh AI Operators should consider working with their vendors (AI developers) to continually\nmonitor performance.\nb AI operators should consider such procedures even for non-significant decisions.\ni AI Operators should subject AI systems informing significant decisions to quality checks\nat least as stringent as those that would be required of a human being taking the same c AI operators should make affected AI subjects aware of these procedures and should\ndesign them to be convenient and user-friendly.\ndecision.\nd AI operators should consider employing human case evaluators to review any such\n3.2.2.3\nchallenges and, when appropriate, overturn the challenged decision.\nAI systems informing critical decisions should be subject to appropriate external audit\na When AI systems are used for critical decisions, external auditing of the AI systems in\nquestion should be used as a means to ensure meaningful standards of transparency and Example: A bank allows customers to apply for a loan online by entering their data. The\naccountability are upheld. bank uses an AI system to automatically determine whether to give the loan and what the\ninterest rate should be. They provide users with an option to contest the decision and have\nb In the case that critical decisions are of civic interest, public release of the results of the it reviewed by a human . They also require that customers justify their challenge by filling in\naudit should be considered as a means of ensuring public processes remain accountable a form, which assists the case reviewer and deters customers from challenging a decision\nto those affected by them. without good reason.7\nIn the case that critical decision are life and death decisions, these decisions should be\nc\nsupported by further validation and verification via a human operator.\nd Facilitate traceability and auditability of AI systems, particularly in critical contexts or\nsituations.\n7 Adapted from EU Commission, Can I be subject to automated individual decision-making, including profiling? Retrieved from: https://ec.europa.eu/\ninfo/law/law-topic/data-protection/reform/rights-citizens/my-rights/can-i-be-subject-automated-individual-decision-making-including-profiling_\nen#example\n22 23e AI operators should consider instituting an opt-out mechanism for significant automated 3.2.2.7\ndecisions. AI operators should understand the AI systems they use sufficiently to assess their suitability\nfor the use case and to ensure accountability and transparency\nf AI operators could consider \u201ccrowd challenge\u201d mechanisms whereby a critical number of\ncomplaints triggers an investigation into the fairness and/or accuracy of a decision-making\na In the case of critical decisions, AI operators should avoid using AI systems that cannot be\nprocess as a whole.\nsubjected to meaningful standards of accountability and transparency.\n3.2.2.5 b AI developers should consider notifying customers and AI operators of the use cases for\nAI systems informing significant decisions should not attempt to make value judgements on which the system has been designed, and those for which it is not suitable.\npeople\u2019s behalf without prior consent\na When informing an AI subject about significant choices they will make, AI systems should\nnot unreasonably restrict the available options or otherwise attempt to influence their value\njudgements without the explicit consent of the AI subject in question.\n3.2.2.6\nAI systems informing significant decisions should be developed by diverse teams with\nappropriate backgrounds\na AI developers who develop AI systems which may be used to assist in making critical\ndecisions should involve in the process experts with a background in social science, policy,\nor another subject which prepares them to evaluate the broader societal impact of their\nwork.\nb Development of AI systems informing significant decisions should include consultation\nwith experts in the field in which the system will be deployed.\nExample: An app that uses AI to assess medical symptoms and has a large user base had\nto face regulatory scrutiny because of number of complaints from doctors. They warned\nthat the application can miss signs of serious illness. A number of different shortcomings\nwere identified by doctors, some of which the company could address and resolve.8\n8 Financial Times. 2018. High-profile health app under scrutiny after doctor\u2019s complaints. Retrieved from: https://www.ft.com/content/19dc6b7e-8529-\n11e896-dd-fa565ec55929\n24 253.3 Transparent AI (Principle 3)\nc To facilitate the above, AI developers and AI operators should consider documenting the\nfollowing information during the design, development and deployment phases, and retaining\n3.3.1 Principle this documentation for a length of time appropriate to the decision type or industry:\n\u2022 the provenance of the training data, the methods of collection and treatment, how the data was\nWe will make AI systems transparent\nmoved, and measures taken to maintain its accuracy over time;\n\u2022 the model design and algorithms employed; and\nDevelopers should build systems whose failures can be traced and diagnosed. \u2022 changes to the codebase, and authorship of those changes.\nPeople should be told when significant decisions about them are being made by AI. d Where possible given the model design, AI developers should consider building in a means\nby which the \u201cdecision journey\u201d of a specific outcome (i.e. the component decisions leading\nWithin the limits of privacy and the preservation of intellectual property, those who deploy to it) can be logged.\nAI systems should be transparent about the data and algorithms they use.\nResponsible disclosures should be provided in a timely manner, and provide reasonable\nExample: A technology company has a product which is designed to assist in medical\njustifications for AI systems outcomes. This includes information that helps people\ndiagnosis. It documents each stage of its reasoning and relates it back to the input data.9\nunderstand outcomes, like key factors used in decision making.\n3.3.2 Guidelines\n3.3.2.1\nTraceability should be considered for significant decisions, especially those that have the\npotential to result in loss, harm or damage.\na For AI systems which inform significant decisions, especially those with the potential to\ncause loss, harm or damage, AI developers should consider building in traceability (i.e. the\nability to trace the key factors leading to any specific decision).\nb Organizations should ensure that harms caused through AI systems are investigated and\nredressed, by enacting strong enforcement mechanisms and remedial actions, to make\ncertain that human rights and the rule of law are respected in the digital world and in the\nphysical world.\n9 See IBM WatsonPaths\n26 273.3.2.2 3.4 Explainable AI (Principle 4)\nPeople should be informed of the extent of their interaction with AI systems\n3.4.1 Principle\na AI operators should inform AI subjects when a significant decision affecting them has been\nmade by an AI system.\nWe will make AI systems as explainable as\ntechnically possible\nExample: A small claims court adjudicates minor civil matters such as debt collection and\nevictions. They introduce an AI system to suggest the outcome of a ruling. At the time of the\nDecisions and methodologies of AI systems which have a significant effect on individuals\nruling the plaintiff and defendant are notified that the decision was assisted by an AI system.\nshould be explainable to them, to the extent permitted by available technology.\nThe court also provides an explanation for the decision.\nIt should be possible to ascertain the key factors leading to any specific decision that\ncould have a significant effect on an individual.\nb If an AI system can convincingly impersonate a human being, it should do so only after\nIn the above situation we will provide channels through which people can request such\nnotifying the AI subject that it is an AI system.\nexplanations.\nExample: A technology company produces a conversational AI agent which can make 3.4.2 Guidelines\nsome phone calls on behalf of its users. Those who receive the calls may believe that they\n3.4.2.1\nare speaking to a human. Therefore, the company programs the agent to identify itself at\nAI operators could consider providing affected AI subjects with a high-level explanation of\nthe start of every conversation.\nhow their AI system works\na AI operators could consider informing the affected AI subjects in understandable, non-\ntechnical language of:\n\u2022 the data that is ingested by the system;\n\u2022 the types of algorithms employed;\n\u2022 the categories into which people can be placed; and\n\u2022 the most important features driving the outcomes of decisions\n\u2022 A comprehensive list of feature engineering and models that was considered during\nthe model building phase.\nExample: A person turned down for a credit card might be told that the algorithm took\ntheir credit history, age, and postcode into account, but not learn why their application was\nrejected10 .\n28 29b For non-sensitive public sector use cases designed for the common good, AI operators\ncould consider making source code, together with an explanation of the workings of the AI Example: The UK\u2019s NHS developed a tool called Predict, which allows women with breast\nsystem, available either publicly or upon request (this should be done only if there is low risk cancer to compare their case to other women who have had the same condition in the past,\nof people \u2018gaming the system\u2019). and visualize the expected survival rate under various treatment options. The website has an\nexplanation page which shows the weights behind various factors and contains a description\nc The AI operator should maintain necessary documentation that provides elaboration and of the underlying mathematics12 .\nclarification on how the algorithm works, for example documentation of processes, decision\nmaking flow charts of the system, and how the appeal process is embedded.\nd An individual should have the ability to contest and seek effective redress against decisions c In the case that such explanations are available, they should be easily and quickly accessible,\nmade by AI systems. These must be addressed by the group or team supporting these free of charge and user-friendly.\nmodels.\ne Develop appropriate impact indices for the evaluation of AI system technological 3.5 Robust, Safe and Secure AI (Principle 5)\ninterventions from multiple perspectives.\n3.5.1 Principles\n3.4.2.2\nAI operators should consider providing affected AI subjects with a means to request\nexplanations for specific significant decisions, to the extent possible given the state of present AI systems will be technically robust\nresearch and the choice of model\nAI systems should be technically robust with a preventative approach to risks which\na AI operators should consider providing a means by which people affected by a significant\noperates in a manner such that they reliably behave.\ndecision informed by AI can access the reasoning behind that decision.\nAI Developers should ensure AI systems will not cause any unintentional harm and\nadverse impacts.\nExample: The US Consumer Financial Protection Bureau requires that creditors in the US\nwho reject credit applications must explain to the applicant the principal reason(s) why their\nAI systems should be resilient to attacks and security such as data poisoning and\napplication was rejected (e.g. \u201clength of residence\u201d or \u201cage of automobile\u201d)11 . In particular,\nmodel leakage.\n\u201cstatements that the adverse action was based on the creditor`s internal standards or\npolicies or that the applicant, joint applicant, or similar party failed to achieve a qualifying AI systems should have safeguards that enable a fallback plan in case of problems.\nscore on the creditor`s credit scoring system are insufficient\u201d.\nAI system results should be reproducible.\nb Where such explainability is not possible given available technology, AI operators should\nconsider compromises such as counterfactual reasoning, or listing the most heavily\nweighted factors contributing to the decision.\n11 Consumer Financial Protection Bureau, 12 CFR Part 1002 - Equal Credit Opportunity Act (Regulation B), \u00a7 1002.9 Notifications, Retrieved from https://www.\n10 The Guardian. AI watchdog needed to regulate automated decision-making, say experts. Retrieved from: https://www.theguardian.com/technology/2017/ consumerfinance.gov/policy-compliance/rulemaking/regulations/1002/\njan/27/ai-artificial-intelligence-watchdog-needed-to-prevent-discriminatory-automated-decisions 12 Predict website, accessible at http://www.predict.nhs.uk/predict_v2.1/legal/algorithm\n30 313.5.2 Guidelines\n3.5.2.1 3.5.2.3\nAI Operators should continue conducting vulnerability assessments, verification of AI system AI Operators should assure end users of the system\u2019s reliability through documentation, and\ndifferent behaviors in unexpected situations and for any dual-use case, to include: operationalizing processes for testing and verification of desired outcomes , specifically:\na Putting measures to ensure integrity and resilience of IA system against potential attacks. a AI system should be designed with an approach to continue monitoring if the system meets\ngoals, purposes and intended application.\nb AI Operators should define an approach to ensure results are reproducible through clear\nExample: IBM researchers presented a novel approach to exploiting AI for malicious\nprocess and documentation.\npurposes. Their system, DeepLocker embeds AI capabilities within the malware itself in order\nto improve its evasion techniques. It uses an Artificial Intelligence model to identify its target\nc AI Operators should publish documentation to assure system robustness to end users.\nusing indicators like facial recognition, geolocation and voice recognition.\n3.5.2.4\nAI systems will be safe, secure and controllable by humans\nb Verifying how AI systems behave in unexpected situations and environments.\na Safety and security of people, be they operators, end-users or other parties, will be of\nc Taking preventative measures against any possible system dual-case use. paramount concern in the design of any AI system.\nb AI systems should be verifiably secure and controllable throughout their operational\n3.5.2.2\nlifetime, to the extent permitted by technology.\nAI operators should define a suitable fall back plan and test it to maintain readiness in\nunexpected situations and in high safety risks level, to include:\nc The continued security and privacy of users should be considered when decommissioning\na Developing a plan to measure and assess potential safety risks to you or any third party AI systems.\nfrom technology use accidental or malicious misuse.\nd AI systems that may directly impact people\u2019s lives in a significant way should receive\nb Define thresholds for system acceptable results and governance procure to fall back on commensurate care in their designs.\nalternative defined and tested plans.\ne Such systems should be able to be overridden or their decisions reversed by designated\nc Considering an insurance policy to mitigate risks arising from potential damages. individuals.\nExample: Adopting a risk-based approach to procurement and clearly communicating it to\nvendors can help address such issues by giving the procuring organization advance notice\nof the specific oversight capabilities it will need in future stages of the system lifecycle,\npreventing vendors from presenting intellectual property arguments against required testing,\nmonitoring and auditing of their AI systems going forward and rewarding vendors\u2014of all\nsizes\u2014that are more advanced and responsive in their responsible AI efforts.\n13 IBM. https://documents.trendmicro.com/assets/white_papers/wp-malicious-uses-and-abuses-of-artificial-intelligence.pdf\n32 333.6.2 Guidelines\n3.5.2.5\nAi systems should not be able to autonomously hurt, destroy or deceive humans\n3.6.2.1\na AI systems should be built to serve and inform, and not to deceive and manipulate.\nAI should be developed to align with human values, contribute to human flourishing and\nbenefit society.\nb Nations should collaborate to avoid an arms race in lethal autonomous weapons, and\nsuch weapons should be tightly controlled. a Society should be consulted in a representative fashion to inform the development of AI.\nc Active cooperation should be pursued to avoid corner-cutting on safety standards. b Stakeholders throughout society should be involved in the development of AI and its\ngovernance.\nSystems designed to inform significant decisions should do so impartially.\nd\nc Stakeholders should proactively engage in responsible stewardship of trustworthy AI in\npursuit of beneficial outcomes for people and the planet.\n3.6 Human Centered AI (Principle 6)\nd Organizations should prioritize having all their stakeholders learn about wellbeing metrics\nas a potential determinant of how they create, deploy, market and monitor their AI\n3.6.1 Principles technologies.\nWe will give AI systems human values and\nExample: IEEE P7010 recommended practice14 establishes wellbeing metrics relating to\nmake them beneficial to society human factors directly affected by intelligent and autonomous systems and establishes a\nbaseline for the types of objective and subjective data these systems should analyze and\nGovernment will support the research of the beneficial use of AI. include (in their programming and functioning) to proactively increase human wellbeing.\nStakeholders throughout society should be involved in the development of AI and its\ngovernance.\n3.6.2.2\nSystematic analyses that examine the ethics of designing affective systems to nudge human\nDesign AI systems to adopt, learn and follow the norms and values of the community\nbeings prior to deployment are needed.\nthey serve.\nSystematic analyses that examine the ethics of designing affective systems to nudge a Nudging in AI systems should have an opt-in system policy with explicit consent.\nhuman beings prior to deployment are needed.\nb We recommend that when appropriate, an affective system that nudges human beings\nDecisions related to lethal force, life and death should not be delegated to AI systems. should have the ability to accurately distinguish between users, including detecting\nRules and standards should be adopted to ensure effective human control over those characteristics such as whether the user is an adult or a child. Additional protections must\ndecisions. be put in place for vulnerable populations, such as children, when informed consent cannot\nbe obtained, or when it may not be a sufficient safeguard.\nc AI systems with nudging strategies must be carefully evaluated, monitored, and controlled.\n14 IEEE. https://sagroups.ieee.org/7010/\n34 353.6.2.3 f In situations where it is needed, human rights impact assessments and human rights due\nHumanity should retain the power to govern itself and make the final decision, with AI in an diligence, human determination codes of ethical conduct, or quality labels and certifications\nassisting role. intended to promote human centered values and fairness should be considered.\ng Mechanisms should be put into place to receive external feedback regarding AI systems\na Decisions related to lethal force, life and death should not be delegated to AI systems. Rules\nthat potentially infringe on fundamental rights.\nand standards should be adopted to ensure effective human control over those decisions.\nb Responsible parties (e.g., parents, nurse practitioners, social workers, and governments)\nshould be trained to detect the influence due to AI and ineffective mitigation techniques.\nIn the most extreme cases it should always be possible to shut down harmful AI system.\nc Those actions undertaken by an affective system that are most likely to generate an\nemotional response should be designed to be easily changed.\nd Users should be able to make informed autonomous decisions regarding AI systems. They\nshould be given appropriate knowledge and tools to comprehend and interact with AI\nsystems to satisfactory degree and, where possible to reasonably self-assess or challenge\nthe system.\n3.6.2.4\nAI systems should be designed in a way that respects the rule of law, human rights, society\nvalues, and should include appropriate safeguards to ensure a fair and just society.\na AI Developers should design AI systems to adopt, learn and follow the norms and values of\nthe community they serve.\nb Organizations could identify the norms of the specific community in which AI systems are\nto be deployed in. In particular, pay attention to the norms relevant to the kinds of tasks that\nthe AI systems are designed to perform. These could be documented as well as how these\nnorms are addressed by the AI system.\nc To respond to the dynamic change of norms in society the AI system could be able to adjust\nits existing norms and learn new ones, while being transparent about these changes.\nd Designers should consider forms and metrics for assessing an AI system\u2019s norm conformity\nover the lifetime of the system (e.g. human-machine agreement on moral decisions,\nverifiability of AI decisions, justified trust).\ne The norm identification process must document the similarities and differences between\nthe norms that humans apply to other humans and the norms they apply to AI systems.\nNorm implementations should be evaluated specifically against the norms that the\ncommunity expects the AI system to follow.\n36 373.7 Sustainable and Environmentally Friendly AI (Principle 7)\nd While there is no standardized means of reporting on the environmental impact of AI\nsystems, where possible, model development and commissioning should make carbon\n3.7.1 Principle impacts a core consideration alongside functional and business requirements.\ne Smaller models should be considered \u2013 shrinking down the model size and using fewer\nWe will promote sustainable and environmentally compute cycles (to balance financial and performance costs with the end performance of\nthe model).\nfriendly AI\nf Carbon awareness should be considered \u2013 adjusting the operational parameters of the AI\nAI systems should be used to benefit all human beings, including future generations. The\nsystem to dynamically select the best time and location for energy use from the grid can\nenvironment is fundamental to this.\nreduce its carbon footprint.\nThroughout the AI lifecycle, implementations should only be carried out on the basis of a full\nunderstanding and acknowledgement of implications for sustainability and environment.\nExample: In agriculture, AI can transform Example: Google`s DeepMind division\nAn AI system\u2019s development, deployment and use, should be assessed via critical examination production by better monitoring and has developed AI that teaches itself to\nof resource usage and energy consumption.\nmanaging environmental conditions minimize the use of energy to cool Google`s\nand crop yields. AI can help reduce both data centers. As a result, Google reduced\nMechanisms to measure environmental impact due to type of energy use and processing\nfertilizer and water, all while improving its data center energy requirements by\npower provided by data centers should be established.\ncrop yields. 40% 15.\n3.7.2 Guidelines\n3.7.2.1 Throughout the AI lifecycle, implementations should always be carried out after full\nunderstanding and acknowledgement of AI implications on sustainability and environment.\na The application of Artificial Intelligence to address sustainability challenges are well\nunderstood. Building and running green, sustainable AI systems is of increasing importance,\ngiven the large carbon footprint that they can generate and the wider context of addressing\nclimate change.\nb AI model development, and by extension commissioning, should therefore seek to balance\ntechnical performance with energy consumption and environmental impact.\nc Efforts should be made to estimate and understand sustainability and environmental\nimpact across the AI lifecycle (e.g. the energy consumption costs associated with model\ntraining, and CO2 emissions and cloud compute costs associated with the deployment\nand running of the system).\n15 https://www.deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-by-40\n38 393.8 Privacy Preserving AI (Principle 8)\n3.8.2.2\nEstablish an oversight mechanism for data collection, storage and processing and use across\nyour organization\n3.8.1 Principle\na Assess who can access data and under which conditions.\nWe will respect people\u2019s privacy\nb Assign specific responsibilities and role for Data Protection Officers.\nAI systems should respect privacy and use the minimum intrusion necessary.\nc Prevention of harm to privacy necessitates adequate data governance that covers the\nquality and integrity of the data used, its relevance in light of the domain in which the\nAI systems should uphold high standards of data governance and security,\nprotecting personal information. AI systems will be deployed, its access protocols and the capability to process data in a\nmanner that protects privacy.\nSurveillance or other AI-driven technologies should not be deployed to the extent\nof violating internationally and/or UAE\u2019s accepted standards of privacy and human d AI systems must guarantee privacy and data protection throughout a system\u2019s entire\ndignity and people rights. lifecycle. This includes the information initially provided by the user, as well as the\ninformation generated about the user over the course of their interaction with the system\nPrivacy by design should be embedded in AI systems and where possible AI\n(e.g. outputs that the AI systems generated for specific users or how users responded to\nalgorithm should have adequate privacy impact assessments.\nparticular recommendations). Digital records of human behavior may allow AI systems\nto infer not only individuals\u2019 preferences, but also their age, gender, religious or political\nAdequate data protection frameworks and governance mechanisms should be\nviews. To allow individuals to trust the data gathering process, it must be ensured that data\nestablished in a multi-stakeholder approach and ensured throughout the life cycle\ncollected about them will not be used to unlawfully or unfairly discriminate against them.\nof AI systems.\ne In any given organization that handles individuals\u2019 data (whether someone is a user of the\nAI developers and operators should strive to strike the balance between privacy\nsystem or not), data protocols governing data access should be put in place in line with\nrequirements, individual rights and innovation growth and society benefits.\nnational privacy legislation (be these universal or sector-specific). These protocols should\noutline who can access data and under which circumstances. Only duly qualified personnel\n3.8.2 Guidelines\nwith the competence and need to access individual\u2019s data should be allowed to do so.\n3.8.2.1\nEstablish mechanism for users to flag issues related to privacy or data protection\na AI Operators should review the system for proper consent logging, ability of users to revoke\npermission whenever applicable.\nb Consider training AI model without or with minimal user of potentially sensitive or personal\ndata.\nc Use measures to enhance privacy such as encryption, anonymization and aggregation.\n40 41f Algorithmic systems require adequate privacy impact assessments, which also include\nsocietal and ethical considerations of their use and an innovative use of the privacy by\ndesign approach. AI actors need to ensure that they are accountable for the design and\nimplementation of AI systems in such a way as to ensure that personal information is\nprotected throughout the life cycle of the AI system.\ng Establish data policies or equivalent frameworks, or reinforce existing ones, to ensure full\nsecurity for personal data and sensitive data, which, if disclosed, may cause exceptional\ndamage, injury or hardship to individuals. Examples include data relating to offences,\ncriminal proceedings and convictions, and related security measures; biometric, genetic\nand health data; and personal data such as that relating to race, descent, gender, age,\nlanguage, religion, political opinion, national origin, ethnic origin, social origin, economic or\nsocial condition of birth, or disability and any other characteristics.\nh Promote mechanisms, such as open repositories for publicly funded or publicly held data,\nsource code and data trusts, to support the safe, fair, legal and ethical sharing of data,\namong others.\ni Promote and facilitate the use of quality and robust datasets for training, development and\nuse of AI systems, and exercise vigilance in overseeing their collection and use.\nExample: with the help of AI & ML algorithms16 , Synthetic data can be created to enhance\nprivacy. The data created will have the same statistical characteristics for the testing\nenvironment and where a third party can have access.\n16 https://research.aimultiple.com/privacy-enhancing-technologies/\n42 43Microsoft. 2022. Microsoft Responsible AI Standard, v2. General Requirements. June 2022.\n04\nRetrieved from:\nhttps://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4ZPmV\nISO. 2022. Proposed ISO Standard on Risk Management of AI: What Businesses Should Know.\nBibliography\nMay 2022. Retrieved from:\nhttps://unesdoc.unesco.org/ark:/48223/pf0000381137/PDF/381137eng.pdf.multi\nGSMA 2022. The AI Ethics Playbook. Implementing ethical principles into everyday business.\nFebruary 2022. Retrieved from:\nhttps://www.gsma.com/betterfuture/wp-content/uploads/202201//The-Mobile-Industry-Ethics-\nPlaybook_Feb-2022.pdf\nOECD. 2022. AI Policy Observatory. Retrieved from:\nhttps://standards.ieee.org/wp-content/uploads/import/documents/other/ead_v2.pdf\nUNESCO. 2022. Recommendation on the Ethics of Artificial Intelligence. Adopted in November\n2021. Retrieved from:\nhttps://unesdoc.unesco.org/ark:/48223/pf0000381137/PDF/381137eng.pdf.multi\nEuropean Commission. 2021. Proposal for a Regulation of the European Parliament and of the\nCouncil Laying Down Harmonized Rules on Artificial Intelligence (Artificial Intelligence Act) and\nAmending Certain Union Legislative Acts. Retrieved from:\nhttps://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A52021PC0206\nEuropean Commission. High Level Expert Group on Artificial Intelligence ( AI HLEG ). Ethics\nGuidelines for Trustworthy AI . April 2019. Retrieved from:\nhttps://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai\nCentre for Data Ethics and Innovation. UK Government: The roadmap to an effective AI assurance\necosystem. December 2021. Retrieved from:\nhttps://www.gov.uk/government/publications/the-roadmap-to-an-effective-ai-assurance-\necosystem\nThe New York City Council. 2021. A Local Law to amend the administrative code of the city of New\nYork, in relation to automated employment decision tools. Retrieved from:\nhttps://legistar.council.nyc.gov/LegislationDetail.aspx?ID=4344524&GUID=%20B051915D-A9AC-\n451E-81F86596032-FA3F9&\nUNESCO, Recommendation on the Ethics of Artificial Intelligence. 2021. Retrieved from:\nhttps://unesdoc.unesco.org/ark:/48223/pf0000381137\n44 45AI NOW Institute. Algorithmic accountability for the public sector. August 2021. Retrieved from: CNIL. (2017). How can humans keep the upper hand? The ethical matters raised by algorithms\nhttps://www.opengovpartnership.org/documents/algorithmic-accountability-public-sector/ and artificial intelligence. Retrieved from:\nhttps://www.cnil.fr/sites/default/files/atoms/files/cnil_rapport_ai_gb_web.pdf\nIEEE 2019. Ethically Aligned Design. Version 2 \u2013 For Public Discussion. Retrieved from:\nhttps://standards.ieee.org/wp-content/uploads/import/documents/other/ead_v2.pdf Executive Office of the President of the United States. National Science and Technology Council.\nNetworking and Information Technology Research and Development Subcommittee. (2016). The\nnational artificial intelligence research and development strategic plan. Retrieved from:\nIEEE 2017. Ethically Aligned Designed : A vision for Prioritizing Human Well-being with\nhttps://www.nitrd.gov/PUBS/national_ai_rd_strategic_plan.pdf\nAutonomous and Intelligent Systems. Retrieved from:\nhttps://standards.ieee.org/wp-content/uploads/import/documents/other/ead_v2.pdf\nExecutive Office of the President of the United States National Science and Technology Council.\nCommittee on Technology. (2016). Preparing for the future of artificial intelligence. Retrieved from:\nGoogle. AI Principles 2021. Retrieved from:\nhttps://obamawhitehouse.archives.gov/sites/default/files/whitehouse_files/microsites/ostp\nhttps://ai.google/principles/\nNSTC/preparing_for_the_future_of_ai.pdf\nMicrosoft. Responsible AI Standard v2 (June 2022). Retrieved from:\nThe Headquarters for Japan\u2019s Economic Revitalization. (2015). New Robot Strategy. Japan\u2019s\nhttps://www.microsoft.com/en-us/ai/responsible-ai\nRobot Strategy. Vision, Strategy, Action Plan. Retrieved from:\nhttp://www.meti.go.jp/english/press/2015/pdf/0123_01b.pdf\nThe ECONOMIST Group 2022 . Pushing forward: the future of AI in the Middle East and North\nAfrica Report. Retrieved from:\nTreasury Board of Canada Secretariat. (2018). Responsible Artificial Intelligence in the\nhttps://impact.economist.com/perspectives/sites/default/files/google_ai_mena_report.pdf\nGovernment of Canada. Digital Disruption White Paper Series. Version 2.0.\nhttps://docs.google.com/document/d/1Sn-qBZUXEUG4dVk909eSg5qvfbpNlRhzIefWPtBwbxY/\nPDPC. (2018, June 5). Discussion paper on Artificial Intelligence (AI) and Personal Data.\nedit\nSingapore: Personal Data Protection Commission Singapore (PDPC). Retrieved from:\nhttps://www.pdpc.gov.sg/-/media/Files/PDPC/PDF-Files/Resource-for-Organisation/AI/\nThe Toronto Declaration: Protecting the right to equality and non-discrimination in machine\nDiscussion-Paper-on-AI-and-PD---050618.pdf\nlearning systems. Toronto, Canada: Amnesty International and Access Now. Retrieved from:\nhttps://www.accessnow.org/cms/assets/uploads/201808//The-Toronto-Declaration_ENG_08-\nITI. AI Policy Principles. Retrieved from:\n2018.pdf\nhttps://www.itic.org/public-policy/ITIAIPolicyPrinciplesFINAL.pdf\nhttp://data.parliament.uk/writtenevidence/committeeevidence.svc/evidencedocument/artificial-\nCabinet Office.(2016, May 19). Data Science Ethical Framework. Retrieved from: intelligence-committee/artificial-intelligence/oral/73546.html (house of lords select committee\nhttps://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/ oral evidence, Q61)oral evidence, Q61)\nfile/524298/Data_science_ethics_framework_v1.0_for_publication__1_.pdf\nHouse of Lords Select Committee on Artificial Intelligence. (2018). AI in the UK: ready, willing and\nEuropean Parliament. (2017, February 16). European Parliament resolution of 16 February 2017 able?\nwith recommendations to the Commission on Civil Law Rules on Robotics (20152103/(INL)). https://publications.parliament.uk/pa/ld201719/ldselect/ldai/100100/.pdf\nRetrieved from:\nhttp://www.europarl.europa.eu/sides/getDoc.do?pubRef=-//EP//TEXT+TA+P8-TA-2017- House of Commons Science and Technology Select Committee. Algorithms in Decision Making.\n0++0051DOC+XML+V0//EN https://publications.parliament.uk/pa/cm201719/cmselect/cmsctech/351351/.pdf\nVillani, C. (2018). For a meaningful Artificial Intelligence towards a French and European strategy.\nRetrieved from:\nhttps://www.aiforhumanity.fr/pdfs/MissionVillani_Report_ENG-VF.pdf\n46 47", "metadata": {"country": "UAE", "year": "2021", "legally_binding": "no", "binding_proof": "None", "date": "01/01/2021", "regulator": "UAE Government", "type": "regulation", "status": "enacted", "language": "English", "use_cases": "[2, 3, 6]"}}
{"_id": "68695bb35520a4cc7989de7c", "title": "UAE National Strategy for Artificial Intelligence", "source": "https://ai.gov.ae/", "text": "AI ETHICS\nARTIFICIAL INTELLIGENCE ETHICS\nPRINCIPLES & GUIDELINES\nPRINCIPLES & GUIDELINES\nDEC 2022Table of Content\nPage Page\n1. Overview 4 3.4 Explainable 29\n1.1 Introduction 5 3.4.1 Principle 29\nWe will make AI systems as explainable as technically possible\n1.2 Scope 6 3.4.2 Guidelines 29\n1.3 Strategic alignment 6 3.5 Robust, Safe and Secure AI 31\n1.4 Responsibility 8 3.5.1 Principles 31\nAI systems will be technically robust\n3.5.2 Guidelines 32\n1.5 Licensing 8\n3.6 Human Centred AI 34\n1.6 Toolkit 8\n3.6.1 Principles 34\nWe will give AI systems human values and make them beneficial to society\n3.6.2 Guidelines 35\n2. Definitions 10\n3.7 Sustainable and Environmentally Friendly AI 38\n3.7.1 Principle 38\n3. Principles and Guidelines 16\nWe will promote sustainabity and environemntly friendly AI 38\n3.7.2 Guidelines 38\n3.1 Fairness 17\n3.8 Privacy Preserving AI 38\n3.1.1 Principle 17\nWe will make AI systems fair\n3.8.1 Principle 40\n3.1.2 Guidelines 20\nWe will respect people\u2019s privacy\n3.8.2 Guidelines 40\n3.2 Accountable AI 20\n3.2.1 Principle 20\nWe will make AI systems accountable\n4. Bibliography 44\n3.2.2 Guidelines 20\n3.3 Transparent AI 26\n3.3.1 Principle 26\nWe will make AI systems transparent\n3.3.2 Guidelines 26\n2 301 1.1 Introduction\nAI\u2019s rapid advancement and innovation potential across a range of fields is incredibly exciting.\nYet a thorough and open discussion around AI ethics, and the principles organizations using this\nOverview\ntechnology must consider, is needed.\nThis document - AI Ethics: Principles and Guidelines \u2013 intends to meet this need to balance these\ntwo central considerations of AI. It is designed to offer detailed guidance to help AI actors adhere\nto eight principles of AI ethics.\nWe will make AI systems that are:\nexplainable transparent accountable fair\nprivacy preserving sustainable and human centered robust, safe and\nenvironmentally secure\nfriendly\nThe guidelines are non-binding, and are drafted as a collaborative multi-\nstakeholder effort, with full awareness of organizations\u2019 needs to innovate\nand protect their intellectual property. This is a collaborative process in\nwhich all stakeholders are invited to be part of an ongoing dialogue. We\nwould like to see the AI Ethics Guidelines evolve into a universal, practical\nand applicable framework informing ethical requirements for AI design and\nuse. The eventual goal is to reach widespread agreement and adoption of\ncommonly agreed policies to inform the ethical use of AI nationally and\naround the world.\n4 51.2 Scope Additionally, the AI Office is currently developing additional policies to be taken into consideration\nalong with this document:\nThis document gives non-mandatory guidelines for achieving the ethical design\nand deployment of AI systems in both the public and private sectors. AI already\nAI Seal AI Procurement Guidelines\nsurrounds us, but some applications are more visible and sensitive than others.\nThe UAE AI Seal brand (UAI) will be used The AI Procurement Guidelines will be\nThis document is applicable only to those AI systems which make or inform\nto attract talent and business from used to guide Federal Government entities\n\u2018significant decisions\u2019 \u2013 that is, those decisions which have the potential for\nacross the globe to come to the UAE to in the procurement of AI systems. They\nsignificant impact either on individuals or on society as a whole. They also apply to\ntest and develop AI. This includes a UAI will list the general principles, proposed\n\u2018critical decisions\u2019, which are a subset of significant decisions and are of especially\nmark recognizing high quality, ethical AI mechanisms for procurement and review\ncritical nature.\ncompanies. It would reward safe, efficient, of AI products between the federal\nverified AI technology with a \u2018UAI Seal government and vendors. The policy\nThis document guides the further developments of sector specific requirements\nof Approval\u2019. The UAE AI Seal brand is considers R&D for specific challenges\nand principles, hence every entity within its sector can further amend, use, or\ncurrently under development by the AI that do not currently have an AI solution\ncomplement its current frameworks to properly suit its context and the need of\nOffice. product that is market-ready. Based on\naffected stakeholders.\nglobal standards for AI Procurement, the\npolicy aims to incentivize AI adoption\nIt is a living document and will undergo further future reviews and enhancements.\nwithin government entities and promote\nAI R&D within the private sector for\nspecified government challenges. The\nprocurement policy is currently under\ndevelopment by the AI Office.\n1.3 Strategic alignment\nThis document comes as a fulfillment of the goals and aspirations set in the UAE National AI\nStrategy and the international Sustainable Development Goals (SDGs). The UAE has long been\nat the forefront of sustainable development. These shared global milestones have created global\nmovement and adoption in several disciplines and sectors. As an early adopter, the UAE has\nestablished a national committee on SDGs with a long-term realization of the UAE SDG agenda\n2030. These goals are further engrained and reflected in the AI Ethics Principles under the topics of\nfairness, inclusiveness, equality, human benefit, and rights.\n6 71.4 Responsibility\nThe AI Office will not be responsible for any misuse of the AI Ethics Principles and Guidelines. The\nuser bears all the consequences of their use.\n1.5 Licensing\nThis document is published under the terms of a Creative Commons Attribution 4.0 International\nLicense in order to facilitate its re-use by other governments and private sector organizations.\nIn summary this means you are free to share and adapt the material, including for commercial\npurposes, provided that you give appropriate credit to the UAE AI and Blockchain Council as its\nowner and do not suggest the Minister of State for Artificial Intelligence, Digital Economy and\nRemote Work Applications Office endorses your use.\n1.6 Toolkit\nThe document is part of a toolkit of self-governing nature that boosts awareness and enables\ngovernment and private institutions to pursue innovative use cases while maintaining human\nvalues and principles.\n8 92.1 AI developer organization\n02\nAn organization which does any of the following:\na determines the purpose of an AI system;\nDefinitions\nb designs an AI system;\nc builds an AI system, or:\nd performs technical maintenance or tuning on an AI system\nNote:\nThe definition applies regardless of whether the organization is the ultimate user of the system, or\nwhether they sell it on or give it away.\nExample: A company develops an artificially intelligent facial recognition system and\nsells it to a country\u2019s border force, who use it to identify suspicious personnel. The company\nis an AI developer organization and the border force is an AI operator organization.\n2.2 AI operator organization\nAn organization which does any of the following:\na uses AI systems in operations, backroom processes or decision-making;\nb uses an AI system to provide a service to an AI subject;\nc is a business owner of an AI system;\nd procures and treats data for use in an AI system; or\ne evaluates the use case for an AI system and decides whether to proceed.\nNotes:\n1. This definition applies regardless of whether the AI system was developed in-house or procured.\n2. It is possible for organizations to be both an AI developer organization and an AI operator organization.\n10 112.6 Critical decision\n2.3 Artificial Intelligence (also AI)\nThe capability of a functional unit to perform functions that are generally associated with human An individually significant decision which is deemed to either have a very large impact on an\nintelligence such as reasoning, learning and self-improvement1. individual or to have especially high stakes. These can be especially sensitive, have the potential\nto cause high loss or damage, be societally significant, or set an important precedent.\n2.4 Artificially Intelligent System (also AI system)\nNotes:\nThe types of decisions referred to here are the same as those in the definition of significant-at-\nA product, service, process or decision-making methodology whose operation or outcome is\nscale decisions, except in this case the effects are felt as a result of an individual decision rather\nmaterially influenced by artificially intelligent functional units.\nthan an aggregate of many decisions..\nNotes:\n1. It is not necessary for a system\u2019s outcome to be solely determined by artificially intelligent\nfunctional units in order for the system to be defined as an artificially intelligent system; and Example: A court determines whether a defendant is guilty of a criminal charge, with the\n2. A particular feature of AI systems is that they learn behavior and rules not explicitly punishment for guilt being a life sentence. This is a critical decision because it has a very\nprogrammed in. large impact on the life of the defendant and also sets precedent for similar cases in the\nfuture.\nExample 1: A small claims court uses Example 2: A government entity uses\n2.7 Functional Unit\nan artificially intelligent software package a chatbot which allows customers to ask\nto collect evidence pertaining to a case, routine questions, book appointments\nEntity of hardware or software, or both, capable of accomplishing a specified purpose3 .\ncompare it to similar cases in the past, and conduct minor financial transactions.\nand present a recommended decision The chatbot responds to customer\nto a judge. The judge determines the queries with pre-written responses and is\n2.8 Individually significant decision\nfinal outcome. This decision-making based on pre-programmed decision rules.\nA decision which has the potential for significant impact on at least one individual\u2019s\nmethodology is materially influenced Therefore, the chatbot is not an AI system.\ncircumstances, behavior or choices, or has legal or similarly significant effects on him or her.\nby an artificially intelligent functional If, however, the chatbot autonomously\nunit, and is therefore classified as an AI adjusted its treatment of customers\nsystem. based on the outcome of past cases, it\nwould be an AI system. Example: A company decides to make an employee redundant. This is an individually\nsignificant decision because of its potential impact on the employee\u2019s financial situation.\n2.5 Bias (of a system)\nInclination or prejudice for or against one person or group, especially in a way considered to be\nunfair2.\n2 Oxford Dictionary 2018, Oxford University Press, viewed online 4th October 2018, <https://en.oxforddictionaries.com>\n1 Consistent with ISO/IEC 2382:2015 3 From ISO/IEC 2382:2015\n12 132.9 Non-operational bias (of a system) 2.11 Significant decision\nBias that is either: A decision which is either individually significant or is part of a set of significant-at-scale\ndecisions.\na not a design feature; or\nb not important in achieving the stated purpose of the system. 2.12 Subject of an artificially intelligent system (also AI subject)\nA natural person who is any of the following:\n2.10 Set of significant-at-scale decisions\na an end-user of an AI system\nA set of decisions made by the same system or organization which, when taken in aggregate,\nb directly affected by the operation of or outcomes of an AI system, or:\nhave significant impact on society as a whole or groups within it.\nc a recipient of a service or recommendation provided by an AI system\nNotes:\n1. The decisions need not be individually significant in order to qualify, in aggregate, as a set of\nsignificant-at-scale decisions; and\n2. Examples of areas which have a large impact on society and which include but are not limited\nto: the large-scale allocation of resources or opportunities amongst groups; the structure of\ngovernment; the division of power between large entities or groups; the law, and its interpretation\nand enforcement; conflict and war; international relations, etc.\nExample: An AI system is used by a website to determine which content to show users.\nThis decision is not individually significant, since a user is not greatly affected by whether\na particular piece of media is shown to them. However, if the website is popular then the AI\nsystem may be making a set of significant-at-scale decisions, because any biases in the\nsystem will affect a large number of users.\n14 153.1 Fairness (Principle 1)\n03\n3.1.1 Principle\nPRINCIPLES AND We will make AI systems fair\nData ingested should, where possible, be accurate and representative of the affected\npopulation.\nGUIDELINES\nAlgorithms should avoid non-operational bias.\nSteps should be taken to mitigate and disclose the biases inherent in datasets.\nSignificant decisions should be provably fair.\nAll personnel involved in the development, deployment and use of AI systems have a role and\nresponsibility to operationalize AI fairness and should be educated accordingly.\n3.1.2 Guidelines\n3.1.2.1\nBenefits of AI systems should be available and accessible to all. AI systems should not discriminate\nagainst people or groups in a way that could have an adverse impact. Especially significant and\ncritical decisions made or assisted by AI should be provably fair.\na Considering that fairness can have many different definitions, an organization should\ndocument its own definition of fairness for the context that the AI system is going to be\nimplemented in. Organizations should document what the implemented fairness objective\nstands for and why this choice was considered the most suitable for the given scenario.\nb It is recommended to identify and document demographic groups that may be adversely\nimpacted and mitigate the risk where possible.\nc AI developers and AI operators could consider formal procedures such as Discrimination\nImpact Assessments as a means of ensuring fairness. This assessment should be\ndocumented as well as the mitigation measures that were implemented by the organization.\nThe impact assessment should be conducted pre-release and regularly after as an ongoing\nevaluation, results are advised to be documented.\n16 173.1.2.2 3.1.2.3\nConsideration should be given to whether the data ingested is accurate and representative Consideration should be given to whether decision-making processes introduce bias\nof the affected population\na When subjecting different groups to different decision-making processes, AI developers\nd Fairness has many different definitions in different cultures and for different contexts. should consider whether this will lead to non-operational bias.\nEncouraging a diverse and inclusive AI ecosystem is thus all the more crucial to ensure\nthat one definition of fairness does not contradict another, and that the process of defining b When evaluating the fairness of an AI system, AI developers and AI operators should\nfairness itself is fair, with under-represented groups represented in the discussion. consider whether AI subjects in the same circumstances receive equal treatment.\ne AI developers and AI operators should undertake reasonable data exploration and/or testing\nto identify potentially prejudicial decision-making tendencies in AI systems arising from\nExample: An organization uses an AI tool to automate the pre-screening of candidates\ndata inaccuracy and biases in the data.\nfor a job opening. It is trained on data from the company\u2019s existing employees, the majority\nof whom are from the same ethnic background. Therefore, the system learns to use name\nf AI developers and operators should evaluate all datasets to assess inclusiveness of and nationality as discriminating factors in filtering job applicants. This could have been\nidentified demographic groups and collect data to close any gaps.\nidentified through testing and rectified by, for example, balancing the training data or only\nusing relevant data fields for training.\ng AI developers and AI operators should refrain from training AI systems on data that is not\nlikely to be representative of the affected AI subjects, or is not likely to be accurate, whether\nthat be due to age, omission, method of collection, or other factors.\n3.1.2.4\nSignificant decisions informed by the use of AI should be fair\nExample: Following a natural disaster, a government relief agency uses an AI system to a AI developers and AI operators could consider formal procedures such as Discrimination\ndetect communities in greatest need by analyzing social media data from a range of websites. Impact Assessments as a means of ensuring fairness.\nHowever, those communities where smartphone penetration is lower having less presence on\n3.1.2.4\nsocial media, and so are at risk of receiving less attention. Therefore, the charity complements\nAI operators should consider whether their AI systems are accessible and usable in a fair\ntheir AI tool with traditional techniques to identify needy populations elsewhere.\nmanner across user groups\n3.1.2.5\nh AI developers should consider whether their AI systems can be expected to perform well Consideration should be given to the effect of diversity on the development and deployment\nwhen exposed to previously unseen data, especially when evaluating people who are not processes\nwell-represented in the training data.\nb Organizations should seek to include people from diverse demographic backgrounds across\nthe full lifecyle, to include design, development and deployment processes. Organizations\nshould seek to engage diverse internal and external groups also.\nc AI developers should consider whether the assumptions they make about AI subjects could\nbe wrong or are likely to lead to non-operational bias; if so, they should consider consulting\nthe AI subjects in a representative manner during the design, development and deployment\nto confirm these assumptions.\n18 193.2 Accountable AI (Principle 2)\n3.2.2.2\nPositive efforts should be made to identify and mitigate any significant risks inherent in the AI\nsystems designed\n3.2.1 Principle\na AI operators should only use AI systems that are backed by evidence-based academic or\nindustrial research, and AI developers should base their development on such research.\nWe will make AI systems accountable\nAccountability for the outcomes of an AI system lies not with the system itself but is\napportioned between those who design, develop and deploy it; Example: A foreign country has a government service which identifies parents who owe\nmoney in child maintenance. The data matching process is often incorrect due to misspelled\nDevelopers should make efforts to mitigate the risks inherent in the systems they design; names or missing data which results in some individuals being incorrectly targeted\nautomatically by the system with the result being a large bill, poor credit ratings and even\nAI systems should have built-in appeals procedures whereby users can challenge significant\nfreezing wages. The recourse for individuals who are incorrectly targeted is time-consuming\ndecisions;\nand not straightforward . If the potential impact of incorrect decisions had been assessed,\nmitigation measures such a user-friendly review procedure could have been set up.\nAI systems should be developed by diverse teams which include experts in the area in which\nthe system will be deployed; and\nAI systems should be subject to external audit and decision quality assurance. b AI operators should identify the likely impact of incorrect automated decisions on AI\nsubjects and, in the case where incorrect decisions are likely to cause significant cost or\ninconvenience, consider mitigating measures.\n3.2.2 Guidelines\nc AI operators should consider internal risk assessments or ethics frameworks as a means to\n3.2.2.1 facilitate the identification of risks and mitigating measures.\nAccountability for the outcomes of an AI system should not lie with the system itself\nd In designing AI systems to inform significant decisions, AI developers should consider\na Accountability for loss or damages resulting from the application of AI systems should not measures to maintain data accuracy over time, including:\nbe attributed to the system itself.\n\u2022 the completeness of the data.\nb AI operators and AI developers should consider designating individuals to be responsible for \u2022 timely update of the data, and.\ninvestigating and rectifying the cause of loss or damage arising from the deployment of AI\n\u2022 whether the context in which the data was collected affects its suitability for the\nsystems.\nintended use case.\nExample: A border camera scanning for predictors of risk may misinterpret a \u201ctic\u201d of an\nindividual with Tourette syndrome as suspicious. These can manifest in a diverse fashion, and\nshould not cause this person to undergo secondary inspection every time they pass through\nthe border . If the data is updated after the first case is encountered then it would avoid\ncausing inconvenience on subsequent visits.\n5 Government of Canada. (2018). Responsible AI in the Government of Canada. Digital Disruption White Paper Series. Version 2.0, p.26. Retrieved from:\nhttps://docs.google.com/document/d/1Sn-qBZUXEUG4dVk909eSg5qvfbpNlRhzIefWPtBwbxY/edit\n4 Cabinet Office (UK), Data Science Ethical Framework, Version 1.0, licensed under the Open Government License v3.0, p. 13 6 Stoica, I. et. al.,2017, A Berkeley View of Systems Challenges for AI, p. 2, https://arxiv.org/pdf/1712.05855.pdf\n20 21e AI developers and AI operators should tune AI models periodically to cater for changes to e Be mindful that there might be fundamental tensions between different principles and\ndata and/or models over time. requirements. Continuously identify, evaluate, document and communicate these trade-\noffs and their solutions.\nf AI operators should consider whether AI systems trained in a static environment will display\nmodel instability when deployed in dynamic environments. Adopt a Trustworthy AI assessment list when developing, deploying or using AI systems,\nf\nand adapt it to the specific use case in which the system is being applied.\nExample: AI systems will need to be able to adapt to the changes in the environment that g Keep in mind that such an assessment list will never be exhaustive. Ensuring Trustworthy\nthey are deployed in. For example, a self-driving car would need to quickly adapt to unexpected AI is not about ticking boxes, but about continuously identifying and implementing\nand dangerous road by learning in real time from other cars that have successfully dealt with requirements, evaluating solutions, ensuring improved outcomes throughout the AI\nthese conditions. In addition, such mission-critical applications must handle noisy inputs system\u2019s lifecycle, and involving stakeholders in this.\nand defend against malicious actors .\n3.2.2.4\nAI subjects should be able to challenge significant automated decisions concerning them and,\ng AI developers should collaborate with AI Operators to train models using historical data where appropriate, be able to opt out of such decisions\nfrom the Operator.\na AI operators using AI systems to inform significant decisions should provide procedures by\nwhich affected AI subjects can challenge a specific decision concerning them.\nh AI Operators should consider working with their vendors (AI developers) to continually\nmonitor performance.\nb AI operators should consider such procedures even for non-significant decisions.\ni AI Operators should subject AI systems informing significant decisions to quality checks\nat least as stringent as those that would be required of a human being taking the same c AI operators should make affected AI subjects aware of these procedures and should\ndesign them to be convenient and user-friendly.\ndecision.\nd AI operators should consider employing human case evaluators to review any such\n3.2.2.3\nchallenges and, when appropriate, overturn the challenged decision.\nAI systems informing critical decisions should be subject to appropriate external audit\na When AI systems are used for critical decisions, external auditing of the AI systems in\nquestion should be used as a means to ensure meaningful standards of transparency and Example: A bank allows customers to apply for a loan online by entering their data. The\naccountability are upheld. bank uses an AI system to automatically determine whether to give the loan and what the\ninterest rate should be. They provide users with an option to contest the decision and have\nb In the case that critical decisions are of civic interest, public release of the results of the it reviewed by a human . They also require that customers justify their challenge by filling in\naudit should be considered as a means of ensuring public processes remain accountable a form, which assists the case reviewer and deters customers from challenging a decision\nto those affected by them. without good reason.7\nIn the case that critical decision are life and death decisions, these decisions should be\nc\nsupported by further validation and verification via a human operator.\nd Facilitate traceability and auditability of AI systems, particularly in critical contexts or\nsituations.\n7 Adapted from EU Commission, Can I be subject to automated individual decision-making, including profiling? Retrieved from: https://ec.europa.eu/\ninfo/law/law-topic/data-protection/reform/rights-citizens/my-rights/can-i-be-subject-automated-individual-decision-making-including-profiling_\nen#example\n22 23e AI operators should consider instituting an opt-out mechanism for significant automated 3.2.2.7\ndecisions. AI operators should understand the AI systems they use sufficiently to assess their suitability\nfor the use case and to ensure accountability and transparency\nf AI operators could consider \u201ccrowd challenge\u201d mechanisms whereby a critical number of\ncomplaints triggers an investigation into the fairness and/or accuracy of a decision-making\na In the case of critical decisions, AI operators should avoid using AI systems that cannot be\nprocess as a whole.\nsubjected to meaningful standards of accountability and transparency.\n3.2.2.5 b AI developers should consider notifying customers and AI operators of the use cases for\nAI systems informing significant decisions should not attempt to make value judgements on which the system has been designed, and those for which it is not suitable.\npeople\u2019s behalf without prior consent\na When informing an AI subject about significant choices they will make, AI systems should\nnot unreasonably restrict the available options or otherwise attempt to influence their value\njudgements without the explicit consent of the AI subject in question.\n3.2.2.6\nAI systems informing significant decisions should be developed by diverse teams with\nappropriate backgrounds\na AI developers who develop AI systems which may be used to assist in making critical\ndecisions should involve in the process experts with a background in social science, policy,\nor another subject which prepares them to evaluate the broader societal impact of their\nwork.\nb Development of AI systems informing significant decisions should include consultation\nwith experts in the field in which the system will be deployed.\nExample: An app that uses AI to assess medical symptoms and has a large user base had\nto face regulatory scrutiny because of number of complaints from doctors. They warned\nthat the application can miss signs of serious illness. A number of different shortcomings\nwere identified by doctors, some of which the company could address and resolve.8\n8 Financial Times. 2018. High-profile health app under scrutiny after doctor\u2019s complaints. Retrieved from: https://www.ft.com/content/19dc6b7e-8529-\n11e896-dd-fa565ec55929\n24 253.3 Transparent AI (Principle 3)\nc To facilitate the above, AI developers and AI operators should consider documenting the\nfollowing information during the design, development and deployment phases, and retaining\n3.3.1 Principle this documentation for a length of time appropriate to the decision type or industry:\n\u2022 the provenance of the training data, the methods of collection and treatment, how the data was\nWe will make AI systems transparent\nmoved, and measures taken to maintain its accuracy over time;\n\u2022 the model design and algorithms employed; and\nDevelopers should build systems whose failures can be traced and diagnosed. \u2022 changes to the codebase, and authorship of those changes.\nPeople should be told when significant decisions about them are being made by AI. d Where possible given the model design, AI developers should consider building in a means\nby which the \u201cdecision journey\u201d of a specific outcome (i.e. the component decisions leading\nWithin the limits of privacy and the preservation of intellectual property, those who deploy to it) can be logged.\nAI systems should be transparent about the data and algorithms they use.\nResponsible disclosures should be provided in a timely manner, and provide reasonable\nExample: A technology company has a product which is designed to assist in medical\njustifications for AI systems outcomes. This includes information that helps people\ndiagnosis. It documents each stage of its reasoning and relates it back to the input data.9\nunderstand outcomes, like key factors used in decision making.\n3.3.2 Guidelines\n3.3.2.1\nTraceability should be considered for significant decisions, especially those that have the\npotential to result in loss, harm or damage.\na For AI systems which inform significant decisions, especially those with the potential to\ncause loss, harm or damage, AI developers should consider building in traceability (i.e. the\nability to trace the key factors leading to any specific decision).\nb Organizations should ensure that harms caused through AI systems are investigated and\nredressed, by enacting strong enforcement mechanisms and remedial actions, to make\ncertain that human rights and the rule of law are respected in the digital world and in the\nphysical world.\n9 See IBM WatsonPaths\n26 273.3.2.2 3.4 Explainable AI (Principle 4)\nPeople should be informed of the extent of their interaction with AI systems\n3.4.1 Principle\na AI operators should inform AI subjects when a significant decision affecting them has been\nmade by an AI system.\nWe will make AI systems as explainable as\ntechnically possible\nExample: A small claims court adjudicates minor civil matters such as debt collection and\nevictions. They introduce an AI system to suggest the outcome of a ruling. At the time of the\nDecisions and methodologies of AI systems which have a significant effect on individuals\nruling the plaintiff and defendant are notified that the decision was assisted by an AI system.\nshould be explainable to them, to the extent permitted by available technology.\nThe court also provides an explanation for the decision.\nIt should be possible to ascertain the key factors leading to any specific decision that\ncould have a significant effect on an individual.\nb If an AI system can convincingly impersonate a human being, it should do so only after\nIn the above situation we will provide channels through which people can request such\nnotifying the AI subject that it is an AI system.\nexplanations.\nExample: A technology company produces a conversational AI agent which can make 3.4.2 Guidelines\nsome phone calls on behalf of its users. Those who receive the calls may believe that they\n3.4.2.1\nare speaking to a human. Therefore, the company programs the agent to identify itself at\nAI operators could consider providing affected AI subjects with a high-level explanation of\nthe start of every conversation.\nhow their AI system works\na AI operators could consider informing the affected AI subjects in understandable, non-\ntechnical language of:\n\u2022 the data that is ingested by the system;\n\u2022 the types of algorithms employed;\n\u2022 the categories into which people can be placed; and\n\u2022 the most important features driving the outcomes of decisions\n\u2022 A comprehensive list of feature engineering and models that was considered during\nthe model building phase.\nExample: A person turned down for a credit card might be told that the algorithm took\ntheir credit history, age, and postcode into account, but not learn why their application was\nrejected10 .\n28 29b For non-sensitive public sector use cases designed for the common good, AI operators\ncould consider making source code, together with an explanation of the workings of the AI Example: The UK\u2019s NHS developed a tool called Predict, which allows women with breast\nsystem, available either publicly or upon request (this should be done only if there is low risk cancer to compare their case to other women who have had the same condition in the past,\nof people \u2018gaming the system\u2019). and visualize the expected survival rate under various treatment options. The website has an\nexplanation page which shows the weights behind various factors and contains a description\nc The AI operator should maintain necessary documentation that provides elaboration and of the underlying mathematics12 .\nclarification on how the algorithm works, for example documentation of processes, decision\nmaking flow charts of the system, and how the appeal process is embedded.\nd An individual should have the ability to contest and seek effective redress against decisions c In the case that such explanations are available, they should be easily and quickly accessible,\nmade by AI systems. These must be addressed by the group or team supporting these free of charge and user-friendly.\nmodels.\ne Develop appropriate impact indices for the evaluation of AI system technological 3.5 Robust, Safe and Secure AI (Principle 5)\ninterventions from multiple perspectives.\n3.5.1 Principles\n3.4.2.2\nAI operators should consider providing affected AI subjects with a means to request\nexplanations for specific significant decisions, to the extent possible given the state of present AI systems will be technically robust\nresearch and the choice of model\nAI systems should be technically robust with a preventative approach to risks which\na AI operators should consider providing a means by which people affected by a significant\noperates in a manner such that they reliably behave.\ndecision informed by AI can access the reasoning behind that decision.\nAI Developers should ensure AI systems will not cause any unintentional harm and\nadverse impacts.\nExample: The US Consumer Financial Protection Bureau requires that creditors in the US\nwho reject credit applications must explain to the applicant the principal reason(s) why their\nAI systems should be resilient to attacks and security such as data poisoning and\napplication was rejected (e.g. \u201clength of residence\u201d or \u201cage of automobile\u201d)11 . In particular,\nmodel leakage.\n\u201cstatements that the adverse action was based on the creditor`s internal standards or\npolicies or that the applicant, joint applicant, or similar party failed to achieve a qualifying AI systems should have safeguards that enable a fallback plan in case of problems.\nscore on the creditor`s credit scoring system are insufficient\u201d.\nAI system results should be reproducible.\nb Where such explainability is not possible given available technology, AI operators should\nconsider compromises such as counterfactual reasoning, or listing the most heavily\nweighted factors contributing to the decision.\n11 Consumer Financial Protection Bureau, 12 CFR Part 1002 - Equal Credit Opportunity Act (Regulation B), \u00a7 1002.9 Notifications, Retrieved from https://www.\n10 The Guardian. AI watchdog needed to regulate automated decision-making, say experts. Retrieved from: https://www.theguardian.com/technology/2017/ consumerfinance.gov/policy-compliance/rulemaking/regulations/1002/\njan/27/ai-artificial-intelligence-watchdog-needed-to-prevent-discriminatory-automated-decisions 12 Predict website, accessible at http://www.predict.nhs.uk/predict_v2.1/legal/algorithm\n30 313.5.2 Guidelines\n3.5.2.1 3.5.2.3\nAI Operators should continue conducting vulnerability assessments, verification of AI system AI Operators should assure end users of the system\u2019s reliability through documentation, and\ndifferent behaviors in unexpected situations and for any dual-use case, to include: operationalizing processes for testing and verification of desired outcomes , specifically:\na Putting measures to ensure integrity and resilience of IA system against potential attacks. a AI system should be designed with an approach to continue monitoring if the system meets\ngoals, purposes and intended application.\nb AI Operators should define an approach to ensure results are reproducible through clear\nExample: IBM researchers presented a novel approach to exploiting AI for malicious\nprocess and documentation.\npurposes. Their system, DeepLocker embeds AI capabilities within the malware itself in order\nto improve its evasion techniques. It uses an Artificial Intelligence model to identify its target\nc AI Operators should publish documentation to assure system robustness to end users.\nusing indicators like facial recognition, geolocation and voice recognition.\n3.5.2.4\nAI systems will be safe, secure and controllable by humans\nb Verifying how AI systems behave in unexpected situations and environments.\na Safety and security of people, be they operators, end-users or other parties, will be of\nc Taking preventative measures against any possible system dual-case use. paramount concern in the design of any AI system.\nb AI systems should be verifiably secure and controllable throughout their operational\n3.5.2.2\nlifetime, to the extent permitted by technology.\nAI operators should define a suitable fall back plan and test it to maintain readiness in\nunexpected situations and in high safety risks level, to include:\nc The continued security and privacy of users should be considered when decommissioning\na Developing a plan to measure and assess potential safety risks to you or any third party AI systems.\nfrom technology use accidental or malicious misuse.\nd AI systems that may directly impact people\u2019s lives in a significant way should receive\nb Define thresholds for system acceptable results and governance procure to fall back on commensurate care in their designs.\nalternative defined and tested plans.\ne Such systems should be able to be overridden or their decisions reversed by designated\nc Considering an insurance policy to mitigate risks arising from potential damages. individuals.\nExample: Adopting a risk-based approach to procurement and clearly communicating it to\nvendors can help address such issues by giving the procuring organization advance notice\nof the specific oversight capabilities it will need in future stages of the system lifecycle,\npreventing vendors from presenting intellectual property arguments against required testing,\nmonitoring and auditing of their AI systems going forward and rewarding vendors\u2014of all\nsizes\u2014that are more advanced and responsive in their responsible AI efforts.\n13 IBM. https://documents.trendmicro.com/assets/white_papers/wp-malicious-uses-and-abuses-of-artificial-intelligence.pdf\n32 333.6.2 Guidelines\n3.5.2.5\nAi systems should not be able to autonomously hurt, destroy or deceive humans\n3.6.2.1\na AI systems should be built to serve and inform, and not to deceive and manipulate.\nAI should be developed to align with human values, contribute to human flourishing and\nbenefit society.\nb Nations should collaborate to avoid an arms race in lethal autonomous weapons, and\nsuch weapons should be tightly controlled. a Society should be consulted in a representative fashion to inform the development of AI.\nc Active cooperation should be pursued to avoid corner-cutting on safety standards. b Stakeholders throughout society should be involved in the development of AI and its\ngovernance.\nSystems designed to inform significant decisions should do so impartially.\nd\nc Stakeholders should proactively engage in responsible stewardship of trustworthy AI in\npursuit of beneficial outcomes for people and the planet.\n3.6 Human Centered AI (Principle 6)\nd Organizations should prioritize having all their stakeholders learn about wellbeing metrics\nas a potential determinant of how they create, deploy, market and monitor their AI\n3.6.1 Principles technologies.\nWe will give AI systems human values and\nExample: IEEE P7010 recommended practice14 establishes wellbeing metrics relating to\nmake them beneficial to society human factors directly affected by intelligent and autonomous systems and establishes a\nbaseline for the types of objective and subjective data these systems should analyze and\nGovernment will support the research of the beneficial use of AI. include (in their programming and functioning) to proactively increase human wellbeing.\nStakeholders throughout society should be involved in the development of AI and its\ngovernance.\n3.6.2.2\nSystematic analyses that examine the ethics of designing affective systems to nudge human\nDesign AI systems to adopt, learn and follow the norms and values of the community\nbeings prior to deployment are needed.\nthey serve.\nSystematic analyses that examine the ethics of designing affective systems to nudge a Nudging in AI systems should have an opt-in system policy with explicit consent.\nhuman beings prior to deployment are needed.\nb We recommend that when appropriate, an affective system that nudges human beings\nDecisions related to lethal force, life and death should not be delegated to AI systems. should have the ability to accurately distinguish between users, including detecting\nRules and standards should be adopted to ensure effective human control over those characteristics such as whether the user is an adult or a child. Additional protections must\ndecisions. be put in place for vulnerable populations, such as children, when informed consent cannot\nbe obtained, or when it may not be a sufficient safeguard.\nc AI systems with nudging strategies must be carefully evaluated, monitored, and controlled.\n14 IEEE. https://sagroups.ieee.org/7010/\n34 353.6.2.3 f In situations where it is needed, human rights impact assessments and human rights due\nHumanity should retain the power to govern itself and make the final decision, with AI in an diligence, human determination codes of ethical conduct, or quality labels and certifications\nassisting role. intended to promote human centered values and fairness should be considered.\ng Mechanisms should be put into place to receive external feedback regarding AI systems\na Decisions related to lethal force, life and death should not be delegated to AI systems. Rules\nthat potentially infringe on fundamental rights.\nand standards should be adopted to ensure effective human control over those decisions.\nb Responsible parties (e.g., parents, nurse practitioners, social workers, and governments)\nshould be trained to detect the influence due to AI and ineffective mitigation techniques.\nIn the most extreme cases it should always be possible to shut down harmful AI system.\nc Those actions undertaken by an affective system that are most likely to generate an\nemotional response should be designed to be easily changed.\nd Users should be able to make informed autonomous decisions regarding AI systems. They\nshould be given appropriate knowledge and tools to comprehend and interact with AI\nsystems to satisfactory degree and, where possible to reasonably self-assess or challenge\nthe system.\n3.6.2.4\nAI systems should be designed in a way that respects the rule of law, human rights, society\nvalues, and should include appropriate safeguards to ensure a fair and just society.\na AI Developers should design AI systems to adopt, learn and follow the norms and values of\nthe community they serve.\nb Organizations could identify the norms of the specific community in which AI systems are\nto be deployed in. In particular, pay attention to the norms relevant to the kinds of tasks that\nthe AI systems are designed to perform. These could be documented as well as how these\nnorms are addressed by the AI system.\nc To respond to the dynamic change of norms in society the AI system could be able to adjust\nits existing norms and learn new ones, while being transparent about these changes.\nd Designers should consider forms and metrics for assessing an AI system\u2019s norm conformity\nover the lifetime of the system (e.g. human-machine agreement on moral decisions,\nverifiability of AI decisions, justified trust).\ne The norm identification process must document the similarities and differences between\nthe norms that humans apply to other humans and the norms they apply to AI systems.\nNorm implementations should be evaluated specifically against the norms that the\ncommunity expects the AI system to follow.\n36 373.7 Sustainable and Environmentally Friendly AI (Principle 7)\nd While there is no standardized means of reporting on the environmental impact of AI\nsystems, where possible, model development and commissioning should make carbon\n3.7.1 Principle impacts a core consideration alongside functional and business requirements.\ne Smaller models should be considered \u2013 shrinking down the model size and using fewer\nWe will promote sustainable and environmentally compute cycles (to balance financial and performance costs with the end performance of\nthe model).\nfriendly AI\nf Carbon awareness should be considered \u2013 adjusting the operational parameters of the AI\nAI systems should be used to benefit all human beings, including future generations. The\nsystem to dynamically select the best time and location for energy use from the grid can\nenvironment is fundamental to this.\nreduce its carbon footprint.\nThroughout the AI lifecycle, implementations should only be carried out on the basis of a full\nunderstanding and acknowledgement of implications for sustainability and environment.\nExample: In agriculture, AI can transform Example: Google`s DeepMind division\nAn AI system\u2019s development, deployment and use, should be assessed via critical examination production by better monitoring and has developed AI that teaches itself to\nof resource usage and energy consumption.\nmanaging environmental conditions minimize the use of energy to cool Google`s\nand crop yields. AI can help reduce both data centers. As a result, Google reduced\nMechanisms to measure environmental impact due to type of energy use and processing\nfertilizer and water, all while improving its data center energy requirements by\npower provided by data centers should be established.\ncrop yields. 40% 15.\n3.7.2 Guidelines\n3.7.2.1 Throughout the AI lifecycle, implementations should always be carried out after full\nunderstanding and acknowledgement of AI implications on sustainability and environment.\na The application of Artificial Intelligence to address sustainability challenges are well\nunderstood. Building and running green, sustainable AI systems is of increasing importance,\ngiven the large carbon footprint that they can generate and the wider context of addressing\nclimate change.\nb AI model development, and by extension commissioning, should therefore seek to balance\ntechnical performance with energy consumption and environmental impact.\nc Efforts should be made to estimate and understand sustainability and environmental\nimpact across the AI lifecycle (e.g. the energy consumption costs associated with model\ntraining, and CO2 emissions and cloud compute costs associated with the deployment\nand running of the system).\n15 https://www.deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-by-40\n38 393.8 Privacy Preserving AI (Principle 8)\n3.8.2.2\nEstablish an oversight mechanism for data collection, storage and processing and use across\nyour organization\n3.8.1 Principle\na Assess who can access data and under which conditions.\nWe will respect people\u2019s privacy\nb Assign specific responsibilities and role for Data Protection Officers.\nAI systems should respect privacy and use the minimum intrusion necessary.\nc Prevention of harm to privacy necessitates adequate data governance that covers the\nquality and integrity of the data used, its relevance in light of the domain in which the\nAI systems should uphold high standards of data governance and security,\nprotecting personal information. AI systems will be deployed, its access protocols and the capability to process data in a\nmanner that protects privacy.\nSurveillance or other AI-driven technologies should not be deployed to the extent\nof violating internationally and/or UAE\u2019s accepted standards of privacy and human d AI systems must guarantee privacy and data protection throughout a system\u2019s entire\ndignity and people rights. lifecycle. This includes the information initially provided by the user, as well as the\ninformation generated about the user over the course of their interaction with the system\nPrivacy by design should be embedded in AI systems and where possible AI\n(e.g. outputs that the AI systems generated for specific users or how users responded to\nalgorithm should have adequate privacy impact assessments.\nparticular recommendations). Digital records of human behavior may allow AI systems\nto infer not only individuals\u2019 preferences, but also their age, gender, religious or political\nAdequate data protection frameworks and governance mechanisms should be\nviews. To allow individuals to trust the data gathering process, it must be ensured that data\nestablished in a multi-stakeholder approach and ensured throughout the life cycle\ncollected about them will not be used to unlawfully or unfairly discriminate against them.\nof AI systems.\ne In any given organization that handles individuals\u2019 data (whether someone is a user of the\nAI developers and operators should strive to strike the balance between privacy\nsystem or not), data protocols governing data access should be put in place in line with\nrequirements, individual rights and innovation growth and society benefits.\nnational privacy legislation (be these universal or sector-specific). These protocols should\noutline who can access data and under which circumstances. Only duly qualified personnel\n3.8.2 Guidelines\nwith the competence and need to access individual\u2019s data should be allowed to do so.\n3.8.2.1\nEstablish mechanism for users to flag issues related to privacy or data protection\na AI Operators should review the system for proper consent logging, ability of users to revoke\npermission whenever applicable.\nb Consider training AI model without or with minimal user of potentially sensitive or personal\ndata.\nc Use measures to enhance privacy such as encryption, anonymization and aggregation.\n40 41f Algorithmic systems require adequate privacy impact assessments, which also include\nsocietal and ethical considerations of their use and an innovative use of the privacy by\ndesign approach. AI actors need to ensure that they are accountable for the design and\nimplementation of AI systems in such a way as to ensure that personal information is\nprotected throughout the life cycle of the AI system.\ng Establish data policies or equivalent frameworks, or reinforce existing ones, to ensure full\nsecurity for personal data and sensitive data, which, if disclosed, may cause exceptional\ndamage, injury or hardship to individuals. Examples include data relating to offences,\ncriminal proceedings and convictions, and related security measures; biometric, genetic\nand health data; and personal data such as that relating to race, descent, gender, age,\nlanguage, religion, political opinion, national origin, ethnic origin, social origin, economic or\nsocial condition of birth, or disability and any other characteristics.\nh Promote mechanisms, such as open repositories for publicly funded or publicly held data,\nsource code and data trusts, to support the safe, fair, legal and ethical sharing of data,\namong others.\ni Promote and facilitate the use of quality and robust datasets for training, development and\nuse of AI systems, and exercise vigilance in overseeing their collection and use.\nExample: with the help of AI & ML algorithms16 , Synthetic data can be created to enhance\nprivacy. The data created will have the same statistical characteristics for the testing\nenvironment and where a third party can have access.\n16 https://research.aimultiple.com/privacy-enhancing-technologies/\n42 43Microsoft. 2022. Microsoft Responsible AI Standard, v2. General Requirements. June 2022.\n04\nRetrieved from:\nhttps://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4ZPmV\nISO. 2022. Proposed ISO Standard on Risk Management of AI: What Businesses Should Know.\nBibliography\nMay 2022. Retrieved from:\nhttps://unesdoc.unesco.org/ark:/48223/pf0000381137/PDF/381137eng.pdf.multi\nGSMA 2022. The AI Ethics Playbook. Implementing ethical principles into everyday business.\nFebruary 2022. Retrieved from:\nhttps://www.gsma.com/betterfuture/wp-content/uploads/202201//The-Mobile-Industry-Ethics-\nPlaybook_Feb-2022.pdf\nOECD. 2022. AI Policy Observatory. Retrieved from:\nhttps://standards.ieee.org/wp-content/uploads/import/documents/other/ead_v2.pdf\nUNESCO. 2022. Recommendation on the Ethics of Artificial Intelligence. Adopted in November\n2021. Retrieved from:\nhttps://unesdoc.unesco.org/ark:/48223/pf0000381137/PDF/381137eng.pdf.multi\nEuropean Commission. 2021. Proposal for a Regulation of the European Parliament and of the\nCouncil Laying Down Harmonized Rules on Artificial Intelligence (Artificial Intelligence Act) and\nAmending Certain Union Legislative Acts. Retrieved from:\nhttps://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A52021PC0206\nEuropean Commission. High Level Expert Group on Artificial Intelligence ( AI HLEG ). Ethics\nGuidelines for Trustworthy AI . April 2019. Retrieved from:\nhttps://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai\nCentre for Data Ethics and Innovation. UK Government: The roadmap to an effective AI assurance\necosystem. December 2021. Retrieved from:\nhttps://www.gov.uk/government/publications/the-roadmap-to-an-effective-ai-assurance-\necosystem\nThe New York City Council. 2021. A Local Law to amend the administrative code of the city of New\nYork, in relation to automated employment decision tools. Retrieved from:\nhttps://legistar.council.nyc.gov/LegislationDetail.aspx?ID=4344524&GUID=%20B051915D-A9AC-\n451E-81F86596032-FA3F9&\nUNESCO, Recommendation on the Ethics of Artificial Intelligence. 2021. Retrieved from:\nhttps://unesdoc.unesco.org/ark:/48223/pf0000381137\n44 45AI NOW Institute. Algorithmic accountability for the public sector. August 2021. Retrieved from: CNIL. (2017). How can humans keep the upper hand? The ethical matters raised by algorithms\nhttps://www.opengovpartnership.org/documents/algorithmic-accountability-public-sector/ and artificial intelligence. Retrieved from:\nhttps://www.cnil.fr/sites/default/files/atoms/files/cnil_rapport_ai_gb_web.pdf\nIEEE 2019. Ethically Aligned Design. Version 2 \u2013 For Public Discussion. Retrieved from:\nhttps://standards.ieee.org/wp-content/uploads/import/documents/other/ead_v2.pdf Executive Office of the President of the United States. National Science and Technology Council.\nNetworking and Information Technology Research and Development Subcommittee. (2016). The\nnational artificial intelligence research and development strategic plan. Retrieved from:\nIEEE 2017. Ethically Aligned Designed : A vision for Prioritizing Human Well-being with\nhttps://www.nitrd.gov/PUBS/national_ai_rd_strategic_plan.pdf\nAutonomous and Intelligent Systems. Retrieved from:\nhttps://standards.ieee.org/wp-content/uploads/import/documents/other/ead_v2.pdf\nExecutive Office of the President of the United States National Science and Technology Council.\nCommittee on Technology. (2016). Preparing for the future of artificial intelligence. Retrieved from:\nGoogle. AI Principles 2021. Retrieved from:\nhttps://obamawhitehouse.archives.gov/sites/default/files/whitehouse_files/microsites/ostp\nhttps://ai.google/principles/\nNSTC/preparing_for_the_future_of_ai.pdf\nMicrosoft. Responsible AI Standard v2 (June 2022). Retrieved from:\nThe Headquarters for Japan\u2019s Economic Revitalization. (2015). New Robot Strategy. Japan\u2019s\nhttps://www.microsoft.com/en-us/ai/responsible-ai\nRobot Strategy. Vision, Strategy, Action Plan. Retrieved from:\nhttp://www.meti.go.jp/english/press/2015/pdf/0123_01b.pdf\nThe ECONOMIST Group 2022 . Pushing forward: the future of AI in the Middle East and North\nAfrica Report. Retrieved from:\nTreasury Board of Canada Secretariat. (2018). Responsible Artificial Intelligence in the\nhttps://impact.economist.com/perspectives/sites/default/files/google_ai_mena_report.pdf\nGovernment of Canada. Digital Disruption White Paper Series. Version 2.0.\nhttps://docs.google.com/document/d/1Sn-qBZUXEUG4dVk909eSg5qvfbpNlRhzIefWPtBwbxY/\nPDPC. (2018, June 5). Discussion paper on Artificial Intelligence (AI) and Personal Data.\nedit\nSingapore: Personal Data Protection Commission Singapore (PDPC). Retrieved from:\nhttps://www.pdpc.gov.sg/-/media/Files/PDPC/PDF-Files/Resource-for-Organisation/AI/\nThe Toronto Declaration: Protecting the right to equality and non-discrimination in machine\nDiscussion-Paper-on-AI-and-PD---050618.pdf\nlearning systems. Toronto, Canada: Amnesty International and Access Now. Retrieved from:\nhttps://www.accessnow.org/cms/assets/uploads/201808//The-Toronto-Declaration_ENG_08-\nITI. AI Policy Principles. Retrieved from:\n2018.pdf\nhttps://www.itic.org/public-policy/ITIAIPolicyPrinciplesFINAL.pdf\nhttp://data.parliament.uk/writtenevidence/committeeevidence.svc/evidencedocument/artificial-\nCabinet Office.(2016, May 19). Data Science Ethical Framework. Retrieved from: intelligence-committee/artificial-intelligence/oral/73546.html (house of lords select committee\nhttps://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/ oral evidence, Q61)oral evidence, Q61)\nfile/524298/Data_science_ethics_framework_v1.0_for_publication__1_.pdf\nHouse of Lords Select Committee on Artificial Intelligence. (2018). AI in the UK: ready, willing and\nEuropean Parliament. (2017, February 16). European Parliament resolution of 16 February 2017 able?\nwith recommendations to the Commission on Civil Law Rules on Robotics (20152103/(INL)). https://publications.parliament.uk/pa/ld201719/ldselect/ldai/100100/.pdf\nRetrieved from:\nhttp://www.europarl.europa.eu/sides/getDoc.do?pubRef=-//EP//TEXT+TA+P8-TA-2017- House of Commons Science and Technology Select Committee. Algorithms in Decision Making.\n0++0051DOC+XML+V0//EN https://publications.parliament.uk/pa/cm201719/cmselect/cmsctech/351351/.pdf\nVillani, C. (2018). For a meaningful Artificial Intelligence towards a French and European strategy.\nRetrieved from:\nhttps://www.aiforhumanity.fr/pdfs/MissionVillani_Report_ENG-VF.pdf\n46 47", "metadata": {"country": "UAE", "year": "2017", "legally_binding": "no", "binding_proof": "None", "date": "01/01/2017", "regulator": "UAE Cabinet / Smart Dubai", "type": "strategy", "status": "enacted", "language": "English", "use_cases": "[2, 5, 6]"}}
{"_id": "68695bc85520a4cc7989de7d", "title": "UAE Responsible Metaverse Self-Governance Charter", "source": "https://ai.gov.ae/", "text": "AI ETHICS\nARTIFICIAL INTELLIGENCE ETHICS\nPRINCIPLES & GUIDELINES\nPRINCIPLES & GUIDELINES\nDEC 2022Table of Content\nPage Page\n1. Overview 4 3.4 Explainable 29\n1.1 Introduction 5 3.4.1 Principle 29\nWe will make AI systems as explainable as technically possible\n1.2 Scope 6 3.4.2 Guidelines 29\n1.3 Strategic alignment 6 3.5 Robust, Safe and Secure AI 31\n1.4 Responsibility 8 3.5.1 Principles 31\nAI systems will be technically robust\n3.5.2 Guidelines 32\n1.5 Licensing 8\n3.6 Human Centred AI 34\n1.6 Toolkit 8\n3.6.1 Principles 34\nWe will give AI systems human values and make them beneficial to society\n3.6.2 Guidelines 35\n2. Definitions 10\n3.7 Sustainable and Environmentally Friendly AI 38\n3.7.1 Principle 38\n3. Principles and Guidelines 16\nWe will promote sustainabity and environemntly friendly AI 38\n3.7.2 Guidelines 38\n3.1 Fairness 17\n3.8 Privacy Preserving AI 38\n3.1.1 Principle 17\nWe will make AI systems fair\n3.8.1 Principle 40\n3.1.2 Guidelines 20\nWe will respect people\u2019s privacy\n3.8.2 Guidelines 40\n3.2 Accountable AI 20\n3.2.1 Principle 20\nWe will make AI systems accountable\n4. Bibliography 44\n3.2.2 Guidelines 20\n3.3 Transparent AI 26\n3.3.1 Principle 26\nWe will make AI systems transparent\n3.3.2 Guidelines 26\n2 301 1.1 Introduction\nAI\u2019s rapid advancement and innovation potential across a range of fields is incredibly exciting.\nYet a thorough and open discussion around AI ethics, and the principles organizations using this\nOverview\ntechnology must consider, is needed.\nThis document - AI Ethics: Principles and Guidelines \u2013 intends to meet this need to balance these\ntwo central considerations of AI. It is designed to offer detailed guidance to help AI actors adhere\nto eight principles of AI ethics.\nWe will make AI systems that are:\nexplainable transparent accountable fair\nprivacy preserving sustainable and human centered robust, safe and\nenvironmentally secure\nfriendly\nThe guidelines are non-binding, and are drafted as a collaborative multi-\nstakeholder effort, with full awareness of organizations\u2019 needs to innovate\nand protect their intellectual property. This is a collaborative process in\nwhich all stakeholders are invited to be part of an ongoing dialogue. We\nwould like to see the AI Ethics Guidelines evolve into a universal, practical\nand applicable framework informing ethical requirements for AI design and\nuse. The eventual goal is to reach widespread agreement and adoption of\ncommonly agreed policies to inform the ethical use of AI nationally and\naround the world.\n4 51.2 Scope Additionally, the AI Office is currently developing additional policies to be taken into consideration\nalong with this document:\nThis document gives non-mandatory guidelines for achieving the ethical design\nand deployment of AI systems in both the public and private sectors. AI already\nAI Seal AI Procurement Guidelines\nsurrounds us, but some applications are more visible and sensitive than others.\nThe UAE AI Seal brand (UAI) will be used The AI Procurement Guidelines will be\nThis document is applicable only to those AI systems which make or inform\nto attract talent and business from used to guide Federal Government entities\n\u2018significant decisions\u2019 \u2013 that is, those decisions which have the potential for\nacross the globe to come to the UAE to in the procurement of AI systems. They\nsignificant impact either on individuals or on society as a whole. They also apply to\ntest and develop AI. This includes a UAI will list the general principles, proposed\n\u2018critical decisions\u2019, which are a subset of significant decisions and are of especially\nmark recognizing high quality, ethical AI mechanisms for procurement and review\ncritical nature.\ncompanies. It would reward safe, efficient, of AI products between the federal\nverified AI technology with a \u2018UAI Seal government and vendors. The policy\nThis document guides the further developments of sector specific requirements\nof Approval\u2019. The UAE AI Seal brand is considers R&D for specific challenges\nand principles, hence every entity within its sector can further amend, use, or\ncurrently under development by the AI that do not currently have an AI solution\ncomplement its current frameworks to properly suit its context and the need of\nOffice. product that is market-ready. Based on\naffected stakeholders.\nglobal standards for AI Procurement, the\npolicy aims to incentivize AI adoption\nIt is a living document and will undergo further future reviews and enhancements.\nwithin government entities and promote\nAI R&D within the private sector for\nspecified government challenges. The\nprocurement policy is currently under\ndevelopment by the AI Office.\n1.3 Strategic alignment\nThis document comes as a fulfillment of the goals and aspirations set in the UAE National AI\nStrategy and the international Sustainable Development Goals (SDGs). The UAE has long been\nat the forefront of sustainable development. These shared global milestones have created global\nmovement and adoption in several disciplines and sectors. As an early adopter, the UAE has\nestablished a national committee on SDGs with a long-term realization of the UAE SDG agenda\n2030. These goals are further engrained and reflected in the AI Ethics Principles under the topics of\nfairness, inclusiveness, equality, human benefit, and rights.\n6 71.4 Responsibility\nThe AI Office will not be responsible for any misuse of the AI Ethics Principles and Guidelines. The\nuser bears all the consequences of their use.\n1.5 Licensing\nThis document is published under the terms of a Creative Commons Attribution 4.0 International\nLicense in order to facilitate its re-use by other governments and private sector organizations.\nIn summary this means you are free to share and adapt the material, including for commercial\npurposes, provided that you give appropriate credit to the UAE AI and Blockchain Council as its\nowner and do not suggest the Minister of State for Artificial Intelligence, Digital Economy and\nRemote Work Applications Office endorses your use.\n1.6 Toolkit\nThe document is part of a toolkit of self-governing nature that boosts awareness and enables\ngovernment and private institutions to pursue innovative use cases while maintaining human\nvalues and principles.\n8 92.1 AI developer organization\n02\nAn organization which does any of the following:\na determines the purpose of an AI system;\nDefinitions\nb designs an AI system;\nc builds an AI system, or:\nd performs technical maintenance or tuning on an AI system\nNote:\nThe definition applies regardless of whether the organization is the ultimate user of the system, or\nwhether they sell it on or give it away.\nExample: A company develops an artificially intelligent facial recognition system and\nsells it to a country\u2019s border force, who use it to identify suspicious personnel. The company\nis an AI developer organization and the border force is an AI operator organization.\n2.2 AI operator organization\nAn organization which does any of the following:\na uses AI systems in operations, backroom processes or decision-making;\nb uses an AI system to provide a service to an AI subject;\nc is a business owner of an AI system;\nd procures and treats data for use in an AI system; or\ne evaluates the use case for an AI system and decides whether to proceed.\nNotes:\n1. This definition applies regardless of whether the AI system was developed in-house or procured.\n2. It is possible for organizations to be both an AI developer organization and an AI operator organization.\n10 112.6 Critical decision\n2.3 Artificial Intelligence (also AI)\nThe capability of a functional unit to perform functions that are generally associated with human An individually significant decision which is deemed to either have a very large impact on an\nintelligence such as reasoning, learning and self-improvement1. individual or to have especially high stakes. These can be especially sensitive, have the potential\nto cause high loss or damage, be societally significant, or set an important precedent.\n2.4 Artificially Intelligent System (also AI system)\nNotes:\nThe types of decisions referred to here are the same as those in the definition of significant-at-\nA product, service, process or decision-making methodology whose operation or outcome is\nscale decisions, except in this case the effects are felt as a result of an individual decision rather\nmaterially influenced by artificially intelligent functional units.\nthan an aggregate of many decisions..\nNotes:\n1. It is not necessary for a system\u2019s outcome to be solely determined by artificially intelligent\nfunctional units in order for the system to be defined as an artificially intelligent system; and Example: A court determines whether a defendant is guilty of a criminal charge, with the\n2. A particular feature of AI systems is that they learn behavior and rules not explicitly punishment for guilt being a life sentence. This is a critical decision because it has a very\nprogrammed in. large impact on the life of the defendant and also sets precedent for similar cases in the\nfuture.\nExample 1: A small claims court uses Example 2: A government entity uses\n2.7 Functional Unit\nan artificially intelligent software package a chatbot which allows customers to ask\nto collect evidence pertaining to a case, routine questions, book appointments\nEntity of hardware or software, or both, capable of accomplishing a specified purpose3 .\ncompare it to similar cases in the past, and conduct minor financial transactions.\nand present a recommended decision The chatbot responds to customer\nto a judge. The judge determines the queries with pre-written responses and is\n2.8 Individually significant decision\nfinal outcome. This decision-making based on pre-programmed decision rules.\nA decision which has the potential for significant impact on at least one individual\u2019s\nmethodology is materially influenced Therefore, the chatbot is not an AI system.\ncircumstances, behavior or choices, or has legal or similarly significant effects on him or her.\nby an artificially intelligent functional If, however, the chatbot autonomously\nunit, and is therefore classified as an AI adjusted its treatment of customers\nsystem. based on the outcome of past cases, it\nwould be an AI system. Example: A company decides to make an employee redundant. This is an individually\nsignificant decision because of its potential impact on the employee\u2019s financial situation.\n2.5 Bias (of a system)\nInclination or prejudice for or against one person or group, especially in a way considered to be\nunfair2.\n2 Oxford Dictionary 2018, Oxford University Press, viewed online 4th October 2018, <https://en.oxforddictionaries.com>\n1 Consistent with ISO/IEC 2382:2015 3 From ISO/IEC 2382:2015\n12 132.9 Non-operational bias (of a system) 2.11 Significant decision\nBias that is either: A decision which is either individually significant or is part of a set of significant-at-scale\ndecisions.\na not a design feature; or\nb not important in achieving the stated purpose of the system. 2.12 Subject of an artificially intelligent system (also AI subject)\nA natural person who is any of the following:\n2.10 Set of significant-at-scale decisions\na an end-user of an AI system\nA set of decisions made by the same system or organization which, when taken in aggregate,\nb directly affected by the operation of or outcomes of an AI system, or:\nhave significant impact on society as a whole or groups within it.\nc a recipient of a service or recommendation provided by an AI system\nNotes:\n1. The decisions need not be individually significant in order to qualify, in aggregate, as a set of\nsignificant-at-scale decisions; and\n2. Examples of areas which have a large impact on society and which include but are not limited\nto: the large-scale allocation of resources or opportunities amongst groups; the structure of\ngovernment; the division of power between large entities or groups; the law, and its interpretation\nand enforcement; conflict and war; international relations, etc.\nExample: An AI system is used by a website to determine which content to show users.\nThis decision is not individually significant, since a user is not greatly affected by whether\na particular piece of media is shown to them. However, if the website is popular then the AI\nsystem may be making a set of significant-at-scale decisions, because any biases in the\nsystem will affect a large number of users.\n14 153.1 Fairness (Principle 1)\n03\n3.1.1 Principle\nPRINCIPLES AND We will make AI systems fair\nData ingested should, where possible, be accurate and representative of the affected\npopulation.\nGUIDELINES\nAlgorithms should avoid non-operational bias.\nSteps should be taken to mitigate and disclose the biases inherent in datasets.\nSignificant decisions should be provably fair.\nAll personnel involved in the development, deployment and use of AI systems have a role and\nresponsibility to operationalize AI fairness and should be educated accordingly.\n3.1.2 Guidelines\n3.1.2.1\nBenefits of AI systems should be available and accessible to all. AI systems should not discriminate\nagainst people or groups in a way that could have an adverse impact. Especially significant and\ncritical decisions made or assisted by AI should be provably fair.\na Considering that fairness can have many different definitions, an organization should\ndocument its own definition of fairness for the context that the AI system is going to be\nimplemented in. Organizations should document what the implemented fairness objective\nstands for and why this choice was considered the most suitable for the given scenario.\nb It is recommended to identify and document demographic groups that may be adversely\nimpacted and mitigate the risk where possible.\nc AI developers and AI operators could consider formal procedures such as Discrimination\nImpact Assessments as a means of ensuring fairness. This assessment should be\ndocumented as well as the mitigation measures that were implemented by the organization.\nThe impact assessment should be conducted pre-release and regularly after as an ongoing\nevaluation, results are advised to be documented.\n16 173.1.2.2 3.1.2.3\nConsideration should be given to whether the data ingested is accurate and representative Consideration should be given to whether decision-making processes introduce bias\nof the affected population\na When subjecting different groups to different decision-making processes, AI developers\nd Fairness has many different definitions in different cultures and for different contexts. should consider whether this will lead to non-operational bias.\nEncouraging a diverse and inclusive AI ecosystem is thus all the more crucial to ensure\nthat one definition of fairness does not contradict another, and that the process of defining b When evaluating the fairness of an AI system, AI developers and AI operators should\nfairness itself is fair, with under-represented groups represented in the discussion. consider whether AI subjects in the same circumstances receive equal treatment.\ne AI developers and AI operators should undertake reasonable data exploration and/or testing\nto identify potentially prejudicial decision-making tendencies in AI systems arising from\nExample: An organization uses an AI tool to automate the pre-screening of candidates\ndata inaccuracy and biases in the data.\nfor a job opening. It is trained on data from the company\u2019s existing employees, the majority\nof whom are from the same ethnic background. Therefore, the system learns to use name\nf AI developers and operators should evaluate all datasets to assess inclusiveness of and nationality as discriminating factors in filtering job applicants. This could have been\nidentified demographic groups and collect data to close any gaps.\nidentified through testing and rectified by, for example, balancing the training data or only\nusing relevant data fields for training.\ng AI developers and AI operators should refrain from training AI systems on data that is not\nlikely to be representative of the affected AI subjects, or is not likely to be accurate, whether\nthat be due to age, omission, method of collection, or other factors.\n3.1.2.4\nSignificant decisions informed by the use of AI should be fair\nExample: Following a natural disaster, a government relief agency uses an AI system to a AI developers and AI operators could consider formal procedures such as Discrimination\ndetect communities in greatest need by analyzing social media data from a range of websites. Impact Assessments as a means of ensuring fairness.\nHowever, those communities where smartphone penetration is lower having less presence on\n3.1.2.4\nsocial media, and so are at risk of receiving less attention. Therefore, the charity complements\nAI operators should consider whether their AI systems are accessible and usable in a fair\ntheir AI tool with traditional techniques to identify needy populations elsewhere.\nmanner across user groups\n3.1.2.5\nh AI developers should consider whether their AI systems can be expected to perform well Consideration should be given to the effect of diversity on the development and deployment\nwhen exposed to previously unseen data, especially when evaluating people who are not processes\nwell-represented in the training data.\nb Organizations should seek to include people from diverse demographic backgrounds across\nthe full lifecyle, to include design, development and deployment processes. Organizations\nshould seek to engage diverse internal and external groups also.\nc AI developers should consider whether the assumptions they make about AI subjects could\nbe wrong or are likely to lead to non-operational bias; if so, they should consider consulting\nthe AI subjects in a representative manner during the design, development and deployment\nto confirm these assumptions.\n18 193.2 Accountable AI (Principle 2)\n3.2.2.2\nPositive efforts should be made to identify and mitigate any significant risks inherent in the AI\nsystems designed\n3.2.1 Principle\na AI operators should only use AI systems that are backed by evidence-based academic or\nindustrial research, and AI developers should base their development on such research.\nWe will make AI systems accountable\nAccountability for the outcomes of an AI system lies not with the system itself but is\napportioned between those who design, develop and deploy it; Example: A foreign country has a government service which identifies parents who owe\nmoney in child maintenance. The data matching process is often incorrect due to misspelled\nDevelopers should make efforts to mitigate the risks inherent in the systems they design; names or missing data which results in some individuals being incorrectly targeted\nautomatically by the system with the result being a large bill, poor credit ratings and even\nAI systems should have built-in appeals procedures whereby users can challenge significant\nfreezing wages. The recourse for individuals who are incorrectly targeted is time-consuming\ndecisions;\nand not straightforward . If the potential impact of incorrect decisions had been assessed,\nmitigation measures such a user-friendly review procedure could have been set up.\nAI systems should be developed by diverse teams which include experts in the area in which\nthe system will be deployed; and\nAI systems should be subject to external audit and decision quality assurance. b AI operators should identify the likely impact of incorrect automated decisions on AI\nsubjects and, in the case where incorrect decisions are likely to cause significant cost or\ninconvenience, consider mitigating measures.\n3.2.2 Guidelines\nc AI operators should consider internal risk assessments or ethics frameworks as a means to\n3.2.2.1 facilitate the identification of risks and mitigating measures.\nAccountability for the outcomes of an AI system should not lie with the system itself\nd In designing AI systems to inform significant decisions, AI developers should consider\na Accountability for loss or damages resulting from the application of AI systems should not measures to maintain data accuracy over time, including:\nbe attributed to the system itself.\n\u2022 the completeness of the data.\nb AI operators and AI developers should consider designating individuals to be responsible for \u2022 timely update of the data, and.\ninvestigating and rectifying the cause of loss or damage arising from the deployment of AI\n\u2022 whether the context in which the data was collected affects its suitability for the\nsystems.\nintended use case.\nExample: A border camera scanning for predictors of risk may misinterpret a \u201ctic\u201d of an\nindividual with Tourette syndrome as suspicious. These can manifest in a diverse fashion, and\nshould not cause this person to undergo secondary inspection every time they pass through\nthe border . If the data is updated after the first case is encountered then it would avoid\ncausing inconvenience on subsequent visits.\n5 Government of Canada. (2018). Responsible AI in the Government of Canada. Digital Disruption White Paper Series. Version 2.0, p.26. Retrieved from:\nhttps://docs.google.com/document/d/1Sn-qBZUXEUG4dVk909eSg5qvfbpNlRhzIefWPtBwbxY/edit\n4 Cabinet Office (UK), Data Science Ethical Framework, Version 1.0, licensed under the Open Government License v3.0, p. 13 6 Stoica, I. et. al.,2017, A Berkeley View of Systems Challenges for AI, p. 2, https://arxiv.org/pdf/1712.05855.pdf\n20 21e AI developers and AI operators should tune AI models periodically to cater for changes to e Be mindful that there might be fundamental tensions between different principles and\ndata and/or models over time. requirements. Continuously identify, evaluate, document and communicate these trade-\noffs and their solutions.\nf AI operators should consider whether AI systems trained in a static environment will display\nmodel instability when deployed in dynamic environments. Adopt a Trustworthy AI assessment list when developing, deploying or using AI systems,\nf\nand adapt it to the specific use case in which the system is being applied.\nExample: AI systems will need to be able to adapt to the changes in the environment that g Keep in mind that such an assessment list will never be exhaustive. Ensuring Trustworthy\nthey are deployed in. For example, a self-driving car would need to quickly adapt to unexpected AI is not about ticking boxes, but about continuously identifying and implementing\nand dangerous road by learning in real time from other cars that have successfully dealt with requirements, evaluating solutions, ensuring improved outcomes throughout the AI\nthese conditions. In addition, such mission-critical applications must handle noisy inputs system\u2019s lifecycle, and involving stakeholders in this.\nand defend against malicious actors .\n3.2.2.4\nAI subjects should be able to challenge significant automated decisions concerning them and,\ng AI developers should collaborate with AI Operators to train models using historical data where appropriate, be able to opt out of such decisions\nfrom the Operator.\na AI operators using AI systems to inform significant decisions should provide procedures by\nwhich affected AI subjects can challenge a specific decision concerning them.\nh AI Operators should consider working with their vendors (AI developers) to continually\nmonitor performance.\nb AI operators should consider such procedures even for non-significant decisions.\ni AI Operators should subject AI systems informing significant decisions to quality checks\nat least as stringent as those that would be required of a human being taking the same c AI operators should make affected AI subjects aware of these procedures and should\ndesign them to be convenient and user-friendly.\ndecision.\nd AI operators should consider employing human case evaluators to review any such\n3.2.2.3\nchallenges and, when appropriate, overturn the challenged decision.\nAI systems informing critical decisions should be subject to appropriate external audit\na When AI systems are used for critical decisions, external auditing of the AI systems in\nquestion should be used as a means to ensure meaningful standards of transparency and Example: A bank allows customers to apply for a loan online by entering their data. The\naccountability are upheld. bank uses an AI system to automatically determine whether to give the loan and what the\ninterest rate should be. They provide users with an option to contest the decision and have\nb In the case that critical decisions are of civic interest, public release of the results of the it reviewed by a human . They also require that customers justify their challenge by filling in\naudit should be considered as a means of ensuring public processes remain accountable a form, which assists the case reviewer and deters customers from challenging a decision\nto those affected by them. without good reason.7\nIn the case that critical decision are life and death decisions, these decisions should be\nc\nsupported by further validation and verification via a human operator.\nd Facilitate traceability and auditability of AI systems, particularly in critical contexts or\nsituations.\n7 Adapted from EU Commission, Can I be subject to automated individual decision-making, including profiling? Retrieved from: https://ec.europa.eu/\ninfo/law/law-topic/data-protection/reform/rights-citizens/my-rights/can-i-be-subject-automated-individual-decision-making-including-profiling_\nen#example\n22 23e AI operators should consider instituting an opt-out mechanism for significant automated 3.2.2.7\ndecisions. AI operators should understand the AI systems they use sufficiently to assess their suitability\nfor the use case and to ensure accountability and transparency\nf AI operators could consider \u201ccrowd challenge\u201d mechanisms whereby a critical number of\ncomplaints triggers an investigation into the fairness and/or accuracy of a decision-making\na In the case of critical decisions, AI operators should avoid using AI systems that cannot be\nprocess as a whole.\nsubjected to meaningful standards of accountability and transparency.\n3.2.2.5 b AI developers should consider notifying customers and AI operators of the use cases for\nAI systems informing significant decisions should not attempt to make value judgements on which the system has been designed, and those for which it is not suitable.\npeople\u2019s behalf without prior consent\na When informing an AI subject about significant choices they will make, AI systems should\nnot unreasonably restrict the available options or otherwise attempt to influence their value\njudgements without the explicit consent of the AI subject in question.\n3.2.2.6\nAI systems informing significant decisions should be developed by diverse teams with\nappropriate backgrounds\na AI developers who develop AI systems which may be used to assist in making critical\ndecisions should involve in the process experts with a background in social science, policy,\nor another subject which prepares them to evaluate the broader societal impact of their\nwork.\nb Development of AI systems informing significant decisions should include consultation\nwith experts in the field in which the system will be deployed.\nExample: An app that uses AI to assess medical symptoms and has a large user base had\nto face regulatory scrutiny because of number of complaints from doctors. They warned\nthat the application can miss signs of serious illness. A number of different shortcomings\nwere identified by doctors, some of which the company could address and resolve.8\n8 Financial Times. 2018. High-profile health app under scrutiny after doctor\u2019s complaints. Retrieved from: https://www.ft.com/content/19dc6b7e-8529-\n11e896-dd-fa565ec55929\n24 253.3 Transparent AI (Principle 3)\nc To facilitate the above, AI developers and AI operators should consider documenting the\nfollowing information during the design, development and deployment phases, and retaining\n3.3.1 Principle this documentation for a length of time appropriate to the decision type or industry:\n\u2022 the provenance of the training data, the methods of collection and treatment, how the data was\nWe will make AI systems transparent\nmoved, and measures taken to maintain its accuracy over time;\n\u2022 the model design and algorithms employed; and\nDevelopers should build systems whose failures can be traced and diagnosed. \u2022 changes to the codebase, and authorship of those changes.\nPeople should be told when significant decisions about them are being made by AI. d Where possible given the model design, AI developers should consider building in a means\nby which the \u201cdecision journey\u201d of a specific outcome (i.e. the component decisions leading\nWithin the limits of privacy and the preservation of intellectual property, those who deploy to it) can be logged.\nAI systems should be transparent about the data and algorithms they use.\nResponsible disclosures should be provided in a timely manner, and provide reasonable\nExample: A technology company has a product which is designed to assist in medical\njustifications for AI systems outcomes. This includes information that helps people\ndiagnosis. It documents each stage of its reasoning and relates it back to the input data.9\nunderstand outcomes, like key factors used in decision making.\n3.3.2 Guidelines\n3.3.2.1\nTraceability should be considered for significant decisions, especially those that have the\npotential to result in loss, harm or damage.\na For AI systems which inform significant decisions, especially those with the potential to\ncause loss, harm or damage, AI developers should consider building in traceability (i.e. the\nability to trace the key factors leading to any specific decision).\nb Organizations should ensure that harms caused through AI systems are investigated and\nredressed, by enacting strong enforcement mechanisms and remedial actions, to make\ncertain that human rights and the rule of law are respected in the digital world and in the\nphysical world.\n9 See IBM WatsonPaths\n26 273.3.2.2 3.4 Explainable AI (Principle 4)\nPeople should be informed of the extent of their interaction with AI systems\n3.4.1 Principle\na AI operators should inform AI subjects when a significant decision affecting them has been\nmade by an AI system.\nWe will make AI systems as explainable as\ntechnically possible\nExample: A small claims court adjudicates minor civil matters such as debt collection and\nevictions. They introduce an AI system to suggest the outcome of a ruling. At the time of the\nDecisions and methodologies of AI systems which have a significant effect on individuals\nruling the plaintiff and defendant are notified that the decision was assisted by an AI system.\nshould be explainable to them, to the extent permitted by available technology.\nThe court also provides an explanation for the decision.\nIt should be possible to ascertain the key factors leading to any specific decision that\ncould have a significant effect on an individual.\nb If an AI system can convincingly impersonate a human being, it should do so only after\nIn the above situation we will provide channels through which people can request such\nnotifying the AI subject that it is an AI system.\nexplanations.\nExample: A technology company produces a conversational AI agent which can make 3.4.2 Guidelines\nsome phone calls on behalf of its users. Those who receive the calls may believe that they\n3.4.2.1\nare speaking to a human. Therefore, the company programs the agent to identify itself at\nAI operators could consider providing affected AI subjects with a high-level explanation of\nthe start of every conversation.\nhow their AI system works\na AI operators could consider informing the affected AI subjects in understandable, non-\ntechnical language of:\n\u2022 the data that is ingested by the system;\n\u2022 the types of algorithms employed;\n\u2022 the categories into which people can be placed; and\n\u2022 the most important features driving the outcomes of decisions\n\u2022 A comprehensive list of feature engineering and models that was considered during\nthe model building phase.\nExample: A person turned down for a credit card might be told that the algorithm took\ntheir credit history, age, and postcode into account, but not learn why their application was\nrejected10 .\n28 29b For non-sensitive public sector use cases designed for the common good, AI operators\ncould consider making source code, together with an explanation of the workings of the AI Example: The UK\u2019s NHS developed a tool called Predict, which allows women with breast\nsystem, available either publicly or upon request (this should be done only if there is low risk cancer to compare their case to other women who have had the same condition in the past,\nof people \u2018gaming the system\u2019). and visualize the expected survival rate under various treatment options. The website has an\nexplanation page which shows the weights behind various factors and contains a description\nc The AI operator should maintain necessary documentation that provides elaboration and of the underlying mathematics12 .\nclarification on how the algorithm works, for example documentation of processes, decision\nmaking flow charts of the system, and how the appeal process is embedded.\nd An individual should have the ability to contest and seek effective redress against decisions c In the case that such explanations are available, they should be easily and quickly accessible,\nmade by AI systems. These must be addressed by the group or team supporting these free of charge and user-friendly.\nmodels.\ne Develop appropriate impact indices for the evaluation of AI system technological 3.5 Robust, Safe and Secure AI (Principle 5)\ninterventions from multiple perspectives.\n3.5.1 Principles\n3.4.2.2\nAI operators should consider providing affected AI subjects with a means to request\nexplanations for specific significant decisions, to the extent possible given the state of present AI systems will be technically robust\nresearch and the choice of model\nAI systems should be technically robust with a preventative approach to risks which\na AI operators should consider providing a means by which people affected by a significant\noperates in a manner such that they reliably behave.\ndecision informed by AI can access the reasoning behind that decision.\nAI Developers should ensure AI systems will not cause any unintentional harm and\nadverse impacts.\nExample: The US Consumer Financial Protection Bureau requires that creditors in the US\nwho reject credit applications must explain to the applicant the principal reason(s) why their\nAI systems should be resilient to attacks and security such as data poisoning and\napplication was rejected (e.g. \u201clength of residence\u201d or \u201cage of automobile\u201d)11 . In particular,\nmodel leakage.\n\u201cstatements that the adverse action was based on the creditor`s internal standards or\npolicies or that the applicant, joint applicant, or similar party failed to achieve a qualifying AI systems should have safeguards that enable a fallback plan in case of problems.\nscore on the creditor`s credit scoring system are insufficient\u201d.\nAI system results should be reproducible.\nb Where such explainability is not possible given available technology, AI operators should\nconsider compromises such as counterfactual reasoning, or listing the most heavily\nweighted factors contributing to the decision.\n11 Consumer Financial Protection Bureau, 12 CFR Part 1002 - Equal Credit Opportunity Act (Regulation B), \u00a7 1002.9 Notifications, Retrieved from https://www.\n10 The Guardian. AI watchdog needed to regulate automated decision-making, say experts. Retrieved from: https://www.theguardian.com/technology/2017/ consumerfinance.gov/policy-compliance/rulemaking/regulations/1002/\njan/27/ai-artificial-intelligence-watchdog-needed-to-prevent-discriminatory-automated-decisions 12 Predict website, accessible at http://www.predict.nhs.uk/predict_v2.1/legal/algorithm\n30 313.5.2 Guidelines\n3.5.2.1 3.5.2.3\nAI Operators should continue conducting vulnerability assessments, verification of AI system AI Operators should assure end users of the system\u2019s reliability through documentation, and\ndifferent behaviors in unexpected situations and for any dual-use case, to include: operationalizing processes for testing and verification of desired outcomes , specifically:\na Putting measures to ensure integrity and resilience of IA system against potential attacks. a AI system should be designed with an approach to continue monitoring if the system meets\ngoals, purposes and intended application.\nb AI Operators should define an approach to ensure results are reproducible through clear\nExample: IBM researchers presented a novel approach to exploiting AI for malicious\nprocess and documentation.\npurposes. Their system, DeepLocker embeds AI capabilities within the malware itself in order\nto improve its evasion techniques. It uses an Artificial Intelligence model to identify its target\nc AI Operators should publish documentation to assure system robustness to end users.\nusing indicators like facial recognition, geolocation and voice recognition.\n3.5.2.4\nAI systems will be safe, secure and controllable by humans\nb Verifying how AI systems behave in unexpected situations and environments.\na Safety and security of people, be they operators, end-users or other parties, will be of\nc Taking preventative measures against any possible system dual-case use. paramount concern in the design of any AI system.\nb AI systems should be verifiably secure and controllable throughout their operational\n3.5.2.2\nlifetime, to the extent permitted by technology.\nAI operators should define a suitable fall back plan and test it to maintain readiness in\nunexpected situations and in high safety risks level, to include:\nc The continued security and privacy of users should be considered when decommissioning\na Developing a plan to measure and assess potential safety risks to you or any third party AI systems.\nfrom technology use accidental or malicious misuse.\nd AI systems that may directly impact people\u2019s lives in a significant way should receive\nb Define thresholds for system acceptable results and governance procure to fall back on commensurate care in their designs.\nalternative defined and tested plans.\ne Such systems should be able to be overridden or their decisions reversed by designated\nc Considering an insurance policy to mitigate risks arising from potential damages. individuals.\nExample: Adopting a risk-based approach to procurement and clearly communicating it to\nvendors can help address such issues by giving the procuring organization advance notice\nof the specific oversight capabilities it will need in future stages of the system lifecycle,\npreventing vendors from presenting intellectual property arguments against required testing,\nmonitoring and auditing of their AI systems going forward and rewarding vendors\u2014of all\nsizes\u2014that are more advanced and responsive in their responsible AI efforts.\n13 IBM. https://documents.trendmicro.com/assets/white_papers/wp-malicious-uses-and-abuses-of-artificial-intelligence.pdf\n32 333.6.2 Guidelines\n3.5.2.5\nAi systems should not be able to autonomously hurt, destroy or deceive humans\n3.6.2.1\na AI systems should be built to serve and inform, and not to deceive and manipulate.\nAI should be developed to align with human values, contribute to human flourishing and\nbenefit society.\nb Nations should collaborate to avoid an arms race in lethal autonomous weapons, and\nsuch weapons should be tightly controlled. a Society should be consulted in a representative fashion to inform the development of AI.\nc Active cooperation should be pursued to avoid corner-cutting on safety standards. b Stakeholders throughout society should be involved in the development of AI and its\ngovernance.\nSystems designed to inform significant decisions should do so impartially.\nd\nc Stakeholders should proactively engage in responsible stewardship of trustworthy AI in\npursuit of beneficial outcomes for people and the planet.\n3.6 Human Centered AI (Principle 6)\nd Organizations should prioritize having all their stakeholders learn about wellbeing metrics\nas a potential determinant of how they create, deploy, market and monitor their AI\n3.6.1 Principles technologies.\nWe will give AI systems human values and\nExample: IEEE P7010 recommended practice14 establishes wellbeing metrics relating to\nmake them beneficial to society human factors directly affected by intelligent and autonomous systems and establishes a\nbaseline for the types of objective and subjective data these systems should analyze and\nGovernment will support the research of the beneficial use of AI. include (in their programming and functioning) to proactively increase human wellbeing.\nStakeholders throughout society should be involved in the development of AI and its\ngovernance.\n3.6.2.2\nSystematic analyses that examine the ethics of designing affective systems to nudge human\nDesign AI systems to adopt, learn and follow the norms and values of the community\nbeings prior to deployment are needed.\nthey serve.\nSystematic analyses that examine the ethics of designing affective systems to nudge a Nudging in AI systems should have an opt-in system policy with explicit consent.\nhuman beings prior to deployment are needed.\nb We recommend that when appropriate, an affective system that nudges human beings\nDecisions related to lethal force, life and death should not be delegated to AI systems. should have the ability to accurately distinguish between users, including detecting\nRules and standards should be adopted to ensure effective human control over those characteristics such as whether the user is an adult or a child. Additional protections must\ndecisions. be put in place for vulnerable populations, such as children, when informed consent cannot\nbe obtained, or when it may not be a sufficient safeguard.\nc AI systems with nudging strategies must be carefully evaluated, monitored, and controlled.\n14 IEEE. https://sagroups.ieee.org/7010/\n34 353.6.2.3 f In situations where it is needed, human rights impact assessments and human rights due\nHumanity should retain the power to govern itself and make the final decision, with AI in an diligence, human determination codes of ethical conduct, or quality labels and certifications\nassisting role. intended to promote human centered values and fairness should be considered.\ng Mechanisms should be put into place to receive external feedback regarding AI systems\na Decisions related to lethal force, life and death should not be delegated to AI systems. Rules\nthat potentially infringe on fundamental rights.\nand standards should be adopted to ensure effective human control over those decisions.\nb Responsible parties (e.g., parents, nurse practitioners, social workers, and governments)\nshould be trained to detect the influence due to AI and ineffective mitigation techniques.\nIn the most extreme cases it should always be possible to shut down harmful AI system.\nc Those actions undertaken by an affective system that are most likely to generate an\nemotional response should be designed to be easily changed.\nd Users should be able to make informed autonomous decisions regarding AI systems. They\nshould be given appropriate knowledge and tools to comprehend and interact with AI\nsystems to satisfactory degree and, where possible to reasonably self-assess or challenge\nthe system.\n3.6.2.4\nAI systems should be designed in a way that respects the rule of law, human rights, society\nvalues, and should include appropriate safeguards to ensure a fair and just society.\na AI Developers should design AI systems to adopt, learn and follow the norms and values of\nthe community they serve.\nb Organizations could identify the norms of the specific community in which AI systems are\nto be deployed in. In particular, pay attention to the norms relevant to the kinds of tasks that\nthe AI systems are designed to perform. These could be documented as well as how these\nnorms are addressed by the AI system.\nc To respond to the dynamic change of norms in society the AI system could be able to adjust\nits existing norms and learn new ones, while being transparent about these changes.\nd Designers should consider forms and metrics for assessing an AI system\u2019s norm conformity\nover the lifetime of the system (e.g. human-machine agreement on moral decisions,\nverifiability of AI decisions, justified trust).\ne The norm identification process must document the similarities and differences between\nthe norms that humans apply to other humans and the norms they apply to AI systems.\nNorm implementations should be evaluated specifically against the norms that the\ncommunity expects the AI system to follow.\n36 373.7 Sustainable and Environmentally Friendly AI (Principle 7)\nd While there is no standardized means of reporting on the environmental impact of AI\nsystems, where possible, model development and commissioning should make carbon\n3.7.1 Principle impacts a core consideration alongside functional and business requirements.\ne Smaller models should be considered \u2013 shrinking down the model size and using fewer\nWe will promote sustainable and environmentally compute cycles (to balance financial and performance costs with the end performance of\nthe model).\nfriendly AI\nf Carbon awareness should be considered \u2013 adjusting the operational parameters of the AI\nAI systems should be used to benefit all human beings, including future generations. The\nsystem to dynamically select the best time and location for energy use from the grid can\nenvironment is fundamental to this.\nreduce its carbon footprint.\nThroughout the AI lifecycle, implementations should only be carried out on the basis of a full\nunderstanding and acknowledgement of implications for sustainability and environment.\nExample: In agriculture, AI can transform Example: Google`s DeepMind division\nAn AI system\u2019s development, deployment and use, should be assessed via critical examination production by better monitoring and has developed AI that teaches itself to\nof resource usage and energy consumption.\nmanaging environmental conditions minimize the use of energy to cool Google`s\nand crop yields. AI can help reduce both data centers. As a result, Google reduced\nMechanisms to measure environmental impact due to type of energy use and processing\nfertilizer and water, all while improving its data center energy requirements by\npower provided by data centers should be established.\ncrop yields. 40% 15.\n3.7.2 Guidelines\n3.7.2.1 Throughout the AI lifecycle, implementations should always be carried out after full\nunderstanding and acknowledgement of AI implications on sustainability and environment.\na The application of Artificial Intelligence to address sustainability challenges are well\nunderstood. Building and running green, sustainable AI systems is of increasing importance,\ngiven the large carbon footprint that they can generate and the wider context of addressing\nclimate change.\nb AI model development, and by extension commissioning, should therefore seek to balance\ntechnical performance with energy consumption and environmental impact.\nc Efforts should be made to estimate and understand sustainability and environmental\nimpact across the AI lifecycle (e.g. the energy consumption costs associated with model\ntraining, and CO2 emissions and cloud compute costs associated with the deployment\nand running of the system).\n15 https://www.deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-by-40\n38 393.8 Privacy Preserving AI (Principle 8)\n3.8.2.2\nEstablish an oversight mechanism for data collection, storage and processing and use across\nyour organization\n3.8.1 Principle\na Assess who can access data and under which conditions.\nWe will respect people\u2019s privacy\nb Assign specific responsibilities and role for Data Protection Officers.\nAI systems should respect privacy and use the minimum intrusion necessary.\nc Prevention of harm to privacy necessitates adequate data governance that covers the\nquality and integrity of the data used, its relevance in light of the domain in which the\nAI systems should uphold high standards of data governance and security,\nprotecting personal information. AI systems will be deployed, its access protocols and the capability to process data in a\nmanner that protects privacy.\nSurveillance or other AI-driven technologies should not be deployed to the extent\nof violating internationally and/or UAE\u2019s accepted standards of privacy and human d AI systems must guarantee privacy and data protection throughout a system\u2019s entire\ndignity and people rights. lifecycle. This includes the information initially provided by the user, as well as the\ninformation generated about the user over the course of their interaction with the system\nPrivacy by design should be embedded in AI systems and where possible AI\n(e.g. outputs that the AI systems generated for specific users or how users responded to\nalgorithm should have adequate privacy impact assessments.\nparticular recommendations). Digital records of human behavior may allow AI systems\nto infer not only individuals\u2019 preferences, but also their age, gender, religious or political\nAdequate data protection frameworks and governance mechanisms should be\nviews. To allow individuals to trust the data gathering process, it must be ensured that data\nestablished in a multi-stakeholder approach and ensured throughout the life cycle\ncollected about them will not be used to unlawfully or unfairly discriminate against them.\nof AI systems.\ne In any given organization that handles individuals\u2019 data (whether someone is a user of the\nAI developers and operators should strive to strike the balance between privacy\nsystem or not), data protocols governing data access should be put in place in line with\nrequirements, individual rights and innovation growth and society benefits.\nnational privacy legislation (be these universal or sector-specific). These protocols should\noutline who can access data and under which circumstances. Only duly qualified personnel\n3.8.2 Guidelines\nwith the competence and need to access individual\u2019s data should be allowed to do so.\n3.8.2.1\nEstablish mechanism for users to flag issues related to privacy or data protection\na AI Operators should review the system for proper consent logging, ability of users to revoke\npermission whenever applicable.\nb Consider training AI model without or with minimal user of potentially sensitive or personal\ndata.\nc Use measures to enhance privacy such as encryption, anonymization and aggregation.\n40 41f Algorithmic systems require adequate privacy impact assessments, which also include\nsocietal and ethical considerations of their use and an innovative use of the privacy by\ndesign approach. AI actors need to ensure that they are accountable for the design and\nimplementation of AI systems in such a way as to ensure that personal information is\nprotected throughout the life cycle of the AI system.\ng Establish data policies or equivalent frameworks, or reinforce existing ones, to ensure full\nsecurity for personal data and sensitive data, which, if disclosed, may cause exceptional\ndamage, injury or hardship to individuals. Examples include data relating to offences,\ncriminal proceedings and convictions, and related security measures; biometric, genetic\nand health data; and personal data such as that relating to race, descent, gender, age,\nlanguage, religion, political opinion, national origin, ethnic origin, social origin, economic or\nsocial condition of birth, or disability and any other characteristics.\nh Promote mechanisms, such as open repositories for publicly funded or publicly held data,\nsource code and data trusts, to support the safe, fair, legal and ethical sharing of data,\namong others.\ni Promote and facilitate the use of quality and robust datasets for training, development and\nuse of AI systems, and exercise vigilance in overseeing their collection and use.\nExample: with the help of AI & ML algorithms16 , Synthetic data can be created to enhance\nprivacy. The data created will have the same statistical characteristics for the testing\nenvironment and where a third party can have access.\n16 https://research.aimultiple.com/privacy-enhancing-technologies/\n42 43Microsoft. 2022. Microsoft Responsible AI Standard, v2. General Requirements. June 2022.\n04\nRetrieved from:\nhttps://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4ZPmV\nISO. 2022. Proposed ISO Standard on Risk Management of AI: What Businesses Should Know.\nBibliography\nMay 2022. Retrieved from:\nhttps://unesdoc.unesco.org/ark:/48223/pf0000381137/PDF/381137eng.pdf.multi\nGSMA 2022. The AI Ethics Playbook. Implementing ethical principles into everyday business.\nFebruary 2022. Retrieved from:\nhttps://www.gsma.com/betterfuture/wp-content/uploads/202201//The-Mobile-Industry-Ethics-\nPlaybook_Feb-2022.pdf\nOECD. 2022. AI Policy Observatory. Retrieved from:\nhttps://standards.ieee.org/wp-content/uploads/import/documents/other/ead_v2.pdf\nUNESCO. 2022. Recommendation on the Ethics of Artificial Intelligence. Adopted in November\n2021. Retrieved from:\nhttps://unesdoc.unesco.org/ark:/48223/pf0000381137/PDF/381137eng.pdf.multi\nEuropean Commission. 2021. Proposal for a Regulation of the European Parliament and of the\nCouncil Laying Down Harmonized Rules on Artificial Intelligence (Artificial Intelligence Act) and\nAmending Certain Union Legislative Acts. Retrieved from:\nhttps://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A52021PC0206\nEuropean Commission. High Level Expert Group on Artificial Intelligence ( AI HLEG ). Ethics\nGuidelines for Trustworthy AI . April 2019. Retrieved from:\nhttps://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai\nCentre for Data Ethics and Innovation. UK Government: The roadmap to an effective AI assurance\necosystem. December 2021. Retrieved from:\nhttps://www.gov.uk/government/publications/the-roadmap-to-an-effective-ai-assurance-\necosystem\nThe New York City Council. 2021. A Local Law to amend the administrative code of the city of New\nYork, in relation to automated employment decision tools. Retrieved from:\nhttps://legistar.council.nyc.gov/LegislationDetail.aspx?ID=4344524&GUID=%20B051915D-A9AC-\n451E-81F86596032-FA3F9&\nUNESCO, Recommendation on the Ethics of Artificial Intelligence. 2021. Retrieved from:\nhttps://unesdoc.unesco.org/ark:/48223/pf0000381137\n44 45AI NOW Institute. Algorithmic accountability for the public sector. August 2021. Retrieved from: CNIL. (2017). How can humans keep the upper hand? The ethical matters raised by algorithms\nhttps://www.opengovpartnership.org/documents/algorithmic-accountability-public-sector/ and artificial intelligence. Retrieved from:\nhttps://www.cnil.fr/sites/default/files/atoms/files/cnil_rapport_ai_gb_web.pdf\nIEEE 2019. Ethically Aligned Design. Version 2 \u2013 For Public Discussion. Retrieved from:\nhttps://standards.ieee.org/wp-content/uploads/import/documents/other/ead_v2.pdf Executive Office of the President of the United States. National Science and Technology Council.\nNetworking and Information Technology Research and Development Subcommittee. (2016). The\nnational artificial intelligence research and development strategic plan. Retrieved from:\nIEEE 2017. Ethically Aligned Designed : A vision for Prioritizing Human Well-being with\nhttps://www.nitrd.gov/PUBS/national_ai_rd_strategic_plan.pdf\nAutonomous and Intelligent Systems. Retrieved from:\nhttps://standards.ieee.org/wp-content/uploads/import/documents/other/ead_v2.pdf\nExecutive Office of the President of the United States National Science and Technology Council.\nCommittee on Technology. (2016). Preparing for the future of artificial intelligence. Retrieved from:\nGoogle. AI Principles 2021. Retrieved from:\nhttps://obamawhitehouse.archives.gov/sites/default/files/whitehouse_files/microsites/ostp\nhttps://ai.google/principles/\nNSTC/preparing_for_the_future_of_ai.pdf\nMicrosoft. Responsible AI Standard v2 (June 2022). Retrieved from:\nThe Headquarters for Japan\u2019s Economic Revitalization. (2015). New Robot Strategy. Japan\u2019s\nhttps://www.microsoft.com/en-us/ai/responsible-ai\nRobot Strategy. Vision, Strategy, Action Plan. Retrieved from:\nhttp://www.meti.go.jp/english/press/2015/pdf/0123_01b.pdf\nThe ECONOMIST Group 2022 . Pushing forward: the future of AI in the Middle East and North\nAfrica Report. Retrieved from:\nTreasury Board of Canada Secretariat. (2018). Responsible Artificial Intelligence in the\nhttps://impact.economist.com/perspectives/sites/default/files/google_ai_mena_report.pdf\nGovernment of Canada. Digital Disruption White Paper Series. Version 2.0.\nhttps://docs.google.com/document/d/1Sn-qBZUXEUG4dVk909eSg5qvfbpNlRhzIefWPtBwbxY/\nPDPC. (2018, June 5). Discussion paper on Artificial Intelligence (AI) and Personal Data.\nedit\nSingapore: Personal Data Protection Commission Singapore (PDPC). Retrieved from:\nhttps://www.pdpc.gov.sg/-/media/Files/PDPC/PDF-Files/Resource-for-Organisation/AI/\nThe Toronto Declaration: Protecting the right to equality and non-discrimination in machine\nDiscussion-Paper-on-AI-and-PD---050618.pdf\nlearning systems. Toronto, Canada: Amnesty International and Access Now. Retrieved from:\nhttps://www.accessnow.org/cms/assets/uploads/201808//The-Toronto-Declaration_ENG_08-\nITI. AI Policy Principles. Retrieved from:\n2018.pdf\nhttps://www.itic.org/public-policy/ITIAIPolicyPrinciplesFINAL.pdf\nhttp://data.parliament.uk/writtenevidence/committeeevidence.svc/evidencedocument/artificial-\nCabinet Office.(2016, May 19). Data Science Ethical Framework. Retrieved from: intelligence-committee/artificial-intelligence/oral/73546.html (house of lords select committee\nhttps://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/ oral evidence, Q61)oral evidence, Q61)\nfile/524298/Data_science_ethics_framework_v1.0_for_publication__1_.pdf\nHouse of Lords Select Committee on Artificial Intelligence. (2018). AI in the UK: ready, willing and\nEuropean Parliament. (2017, February 16). European Parliament resolution of 16 February 2017 able?\nwith recommendations to the Commission on Civil Law Rules on Robotics (20152103/(INL)). https://publications.parliament.uk/pa/ld201719/ldselect/ldai/100100/.pdf\nRetrieved from:\nhttp://www.europarl.europa.eu/sides/getDoc.do?pubRef=-//EP//TEXT+TA+P8-TA-2017- House of Commons Science and Technology Select Committee. Algorithms in Decision Making.\n0++0051DOC+XML+V0//EN https://publications.parliament.uk/pa/cm201719/cmselect/cmsctech/351351/.pdf\nVillani, C. (2018). For a meaningful Artificial Intelligence towards a French and European strategy.\nRetrieved from:\nhttps://www.aiforhumanity.fr/pdfs/MissionVillani_Report_ENG-VF.pdf\n46 47", "metadata": {"country": "UAE", "year": "2023", "legally_binding": "no", "binding_proof": "None", "date": "01/01/2023", "regulator": "UAE Government", "type": "charter", "status": "enacted", "language": "English", "use_cases": "[3, 6]"}}
{"_id": "68695bdf5520a4cc7989de7e", "title": "UAE Charter for the Development and Use of Artificial Intelligence", "source": "https://ai.gov.ae/", "text": "AI ETHICS\nARTIFICIAL INTELLIGENCE ETHICS\nPRINCIPLES & GUIDELINES\nPRINCIPLES & GUIDELINES\nDEC 2022Table of Content\nPage Page\n1. Overview 4 3.4 Explainable 29\n1.1 Introduction 5 3.4.1 Principle 29\nWe will make AI systems as explainable as technically possible\n1.2 Scope 6 3.4.2 Guidelines 29\n1.3 Strategic alignment 6 3.5 Robust, Safe and Secure AI 31\n1.4 Responsibility 8 3.5.1 Principles 31\nAI systems will be technically robust\n3.5.2 Guidelines 32\n1.5 Licensing 8\n3.6 Human Centred AI 34\n1.6 Toolkit 8\n3.6.1 Principles 34\nWe will give AI systems human values and make them beneficial to society\n3.6.2 Guidelines 35\n2. Definitions 10\n3.7 Sustainable and Environmentally Friendly AI 38\n3.7.1 Principle 38\n3. Principles and Guidelines 16\nWe will promote sustainabity and environemntly friendly AI 38\n3.7.2 Guidelines 38\n3.1 Fairness 17\n3.8 Privacy Preserving AI 38\n3.1.1 Principle 17\nWe will make AI systems fair\n3.8.1 Principle 40\n3.1.2 Guidelines 20\nWe will respect people\u2019s privacy\n3.8.2 Guidelines 40\n3.2 Accountable AI 20\n3.2.1 Principle 20\nWe will make AI systems accountable\n4. Bibliography 44\n3.2.2 Guidelines 20\n3.3 Transparent AI 26\n3.3.1 Principle 26\nWe will make AI systems transparent\n3.3.2 Guidelines 26\n2 301 1.1 Introduction\nAI\u2019s rapid advancement and innovation potential across a range of fields is incredibly exciting.\nYet a thorough and open discussion around AI ethics, and the principles organizations using this\nOverview\ntechnology must consider, is needed.\nThis document - AI Ethics: Principles and Guidelines \u2013 intends to meet this need to balance these\ntwo central considerations of AI. It is designed to offer detailed guidance to help AI actors adhere\nto eight principles of AI ethics.\nWe will make AI systems that are:\nexplainable transparent accountable fair\nprivacy preserving sustainable and human centered robust, safe and\nenvironmentally secure\nfriendly\nThe guidelines are non-binding, and are drafted as a collaborative multi-\nstakeholder effort, with full awareness of organizations\u2019 needs to innovate\nand protect their intellectual property. This is a collaborative process in\nwhich all stakeholders are invited to be part of an ongoing dialogue. We\nwould like to see the AI Ethics Guidelines evolve into a universal, practical\nand applicable framework informing ethical requirements for AI design and\nuse. The eventual goal is to reach widespread agreement and adoption of\ncommonly agreed policies to inform the ethical use of AI nationally and\naround the world.\n4 51.2 Scope Additionally, the AI Office is currently developing additional policies to be taken into consideration\nalong with this document:\nThis document gives non-mandatory guidelines for achieving the ethical design\nand deployment of AI systems in both the public and private sectors. AI already\nAI Seal AI Procurement Guidelines\nsurrounds us, but some applications are more visible and sensitive than others.\nThe UAE AI Seal brand (UAI) will be used The AI Procurement Guidelines will be\nThis document is applicable only to those AI systems which make or inform\nto attract talent and business from used to guide Federal Government entities\n\u2018significant decisions\u2019 \u2013 that is, those decisions which have the potential for\nacross the globe to come to the UAE to in the procurement of AI systems. They\nsignificant impact either on individuals or on society as a whole. They also apply to\ntest and develop AI. This includes a UAI will list the general principles, proposed\n\u2018critical decisions\u2019, which are a subset of significant decisions and are of especially\nmark recognizing high quality, ethical AI mechanisms for procurement and review\ncritical nature.\ncompanies. It would reward safe, efficient, of AI products between the federal\nverified AI technology with a \u2018UAI Seal government and vendors. The policy\nThis document guides the further developments of sector specific requirements\nof Approval\u2019. The UAE AI Seal brand is considers R&D for specific challenges\nand principles, hence every entity within its sector can further amend, use, or\ncurrently under development by the AI that do not currently have an AI solution\ncomplement its current frameworks to properly suit its context and the need of\nOffice. product that is market-ready. Based on\naffected stakeholders.\nglobal standards for AI Procurement, the\npolicy aims to incentivize AI adoption\nIt is a living document and will undergo further future reviews and enhancements.\nwithin government entities and promote\nAI R&D within the private sector for\nspecified government challenges. The\nprocurement policy is currently under\ndevelopment by the AI Office.\n1.3 Strategic alignment\nThis document comes as a fulfillment of the goals and aspirations set in the UAE National AI\nStrategy and the international Sustainable Development Goals (SDGs). The UAE has long been\nat the forefront of sustainable development. These shared global milestones have created global\nmovement and adoption in several disciplines and sectors. As an early adopter, the UAE has\nestablished a national committee on SDGs with a long-term realization of the UAE SDG agenda\n2030. These goals are further engrained and reflected in the AI Ethics Principles under the topics of\nfairness, inclusiveness, equality, human benefit, and rights.\n6 71.4 Responsibility\nThe AI Office will not be responsible for any misuse of the AI Ethics Principles and Guidelines. The\nuser bears all the consequences of their use.\n1.5 Licensing\nThis document is published under the terms of a Creative Commons Attribution 4.0 International\nLicense in order to facilitate its re-use by other governments and private sector organizations.\nIn summary this means you are free to share and adapt the material, including for commercial\npurposes, provided that you give appropriate credit to the UAE AI and Blockchain Council as its\nowner and do not suggest the Minister of State for Artificial Intelligence, Digital Economy and\nRemote Work Applications Office endorses your use.\n1.6 Toolkit\nThe document is part of a toolkit of self-governing nature that boosts awareness and enables\ngovernment and private institutions to pursue innovative use cases while maintaining human\nvalues and principles.\n8 92.1 AI developer organization\n02\nAn organization which does any of the following:\na determines the purpose of an AI system;\nDefinitions\nb designs an AI system;\nc builds an AI system, or:\nd performs technical maintenance or tuning on an AI system\nNote:\nThe definition applies regardless of whether the organization is the ultimate user of the system, or\nwhether they sell it on or give it away.\nExample: A company develops an artificially intelligent facial recognition system and\nsells it to a country\u2019s border force, who use it to identify suspicious personnel. The company\nis an AI developer organization and the border force is an AI operator organization.\n2.2 AI operator organization\nAn organization which does any of the following:\na uses AI systems in operations, backroom processes or decision-making;\nb uses an AI system to provide a service to an AI subject;\nc is a business owner of an AI system;\nd procures and treats data for use in an AI system; or\ne evaluates the use case for an AI system and decides whether to proceed.\nNotes:\n1. This definition applies regardless of whether the AI system was developed in-house or procured.\n2. It is possible for organizations to be both an AI developer organization and an AI operator organization.\n10 112.6 Critical decision\n2.3 Artificial Intelligence (also AI)\nThe capability of a functional unit to perform functions that are generally associated with human An individually significant decision which is deemed to either have a very large impact on an\nintelligence such as reasoning, learning and self-improvement1. individual or to have especially high stakes. These can be especially sensitive, have the potential\nto cause high loss or damage, be societally significant, or set an important precedent.\n2.4 Artificially Intelligent System (also AI system)\nNotes:\nThe types of decisions referred to here are the same as those in the definition of significant-at-\nA product, service, process or decision-making methodology whose operation or outcome is\nscale decisions, except in this case the effects are felt as a result of an individual decision rather\nmaterially influenced by artificially intelligent functional units.\nthan an aggregate of many decisions..\nNotes:\n1. It is not necessary for a system\u2019s outcome to be solely determined by artificially intelligent\nfunctional units in order for the system to be defined as an artificially intelligent system; and Example: A court determines whether a defendant is guilty of a criminal charge, with the\n2. A particular feature of AI systems is that they learn behavior and rules not explicitly punishment for guilt being a life sentence. This is a critical decision because it has a very\nprogrammed in. large impact on the life of the defendant and also sets precedent for similar cases in the\nfuture.\nExample 1: A small claims court uses Example 2: A government entity uses\n2.7 Functional Unit\nan artificially intelligent software package a chatbot which allows customers to ask\nto collect evidence pertaining to a case, routine questions, book appointments\nEntity of hardware or software, or both, capable of accomplishing a specified purpose3 .\ncompare it to similar cases in the past, and conduct minor financial transactions.\nand present a recommended decision The chatbot responds to customer\nto a judge. The judge determines the queries with pre-written responses and is\n2.8 Individually significant decision\nfinal outcome. This decision-making based on pre-programmed decision rules.\nA decision which has the potential for significant impact on at least one individual\u2019s\nmethodology is materially influenced Therefore, the chatbot is not an AI system.\ncircumstances, behavior or choices, or has legal or similarly significant effects on him or her.\nby an artificially intelligent functional If, however, the chatbot autonomously\nunit, and is therefore classified as an AI adjusted its treatment of customers\nsystem. based on the outcome of past cases, it\nwould be an AI system. Example: A company decides to make an employee redundant. This is an individually\nsignificant decision because of its potential impact on the employee\u2019s financial situation.\n2.5 Bias (of a system)\nInclination or prejudice for or against one person or group, especially in a way considered to be\nunfair2.\n2 Oxford Dictionary 2018, Oxford University Press, viewed online 4th October 2018, <https://en.oxforddictionaries.com>\n1 Consistent with ISO/IEC 2382:2015 3 From ISO/IEC 2382:2015\n12 132.9 Non-operational bias (of a system) 2.11 Significant decision\nBias that is either: A decision which is either individually significant or is part of a set of significant-at-scale\ndecisions.\na not a design feature; or\nb not important in achieving the stated purpose of the system. 2.12 Subject of an artificially intelligent system (also AI subject)\nA natural person who is any of the following:\n2.10 Set of significant-at-scale decisions\na an end-user of an AI system\nA set of decisions made by the same system or organization which, when taken in aggregate,\nb directly affected by the operation of or outcomes of an AI system, or:\nhave significant impact on society as a whole or groups within it.\nc a recipient of a service or recommendation provided by an AI system\nNotes:\n1. The decisions need not be individually significant in order to qualify, in aggregate, as a set of\nsignificant-at-scale decisions; and\n2. Examples of areas which have a large impact on society and which include but are not limited\nto: the large-scale allocation of resources or opportunities amongst groups; the structure of\ngovernment; the division of power between large entities or groups; the law, and its interpretation\nand enforcement; conflict and war; international relations, etc.\nExample: An AI system is used by a website to determine which content to show users.\nThis decision is not individually significant, since a user is not greatly affected by whether\na particular piece of media is shown to them. However, if the website is popular then the AI\nsystem may be making a set of significant-at-scale decisions, because any biases in the\nsystem will affect a large number of users.\n14 153.1 Fairness (Principle 1)\n03\n3.1.1 Principle\nPRINCIPLES AND We will make AI systems fair\nData ingested should, where possible, be accurate and representative of the affected\npopulation.\nGUIDELINES\nAlgorithms should avoid non-operational bias.\nSteps should be taken to mitigate and disclose the biases inherent in datasets.\nSignificant decisions should be provably fair.\nAll personnel involved in the development, deployment and use of AI systems have a role and\nresponsibility to operationalize AI fairness and should be educated accordingly.\n3.1.2 Guidelines\n3.1.2.1\nBenefits of AI systems should be available and accessible to all. AI systems should not discriminate\nagainst people or groups in a way that could have an adverse impact. Especially significant and\ncritical decisions made or assisted by AI should be provably fair.\na Considering that fairness can have many different definitions, an organization should\ndocument its own definition of fairness for the context that the AI system is going to be\nimplemented in. Organizations should document what the implemented fairness objective\nstands for and why this choice was considered the most suitable for the given scenario.\nb It is recommended to identify and document demographic groups that may be adversely\nimpacted and mitigate the risk where possible.\nc AI developers and AI operators could consider formal procedures such as Discrimination\nImpact Assessments as a means of ensuring fairness. This assessment should be\ndocumented as well as the mitigation measures that were implemented by the organization.\nThe impact assessment should be conducted pre-release and regularly after as an ongoing\nevaluation, results are advised to be documented.\n16 173.1.2.2 3.1.2.3\nConsideration should be given to whether the data ingested is accurate and representative Consideration should be given to whether decision-making processes introduce bias\nof the affected population\na When subjecting different groups to different decision-making processes, AI developers\nd Fairness has many different definitions in different cultures and for different contexts. should consider whether this will lead to non-operational bias.\nEncouraging a diverse and inclusive AI ecosystem is thus all the more crucial to ensure\nthat one definition of fairness does not contradict another, and that the process of defining b When evaluating the fairness of an AI system, AI developers and AI operators should\nfairness itself is fair, with under-represented groups represented in the discussion. consider whether AI subjects in the same circumstances receive equal treatment.\ne AI developers and AI operators should undertake reasonable data exploration and/or testing\nto identify potentially prejudicial decision-making tendencies in AI systems arising from\nExample: An organization uses an AI tool to automate the pre-screening of candidates\ndata inaccuracy and biases in the data.\nfor a job opening. It is trained on data from the company\u2019s existing employees, the majority\nof whom are from the same ethnic background. Therefore, the system learns to use name\nf AI developers and operators should evaluate all datasets to assess inclusiveness of and nationality as discriminating factors in filtering job applicants. This could have been\nidentified demographic groups and collect data to close any gaps.\nidentified through testing and rectified by, for example, balancing the training data or only\nusing relevant data fields for training.\ng AI developers and AI operators should refrain from training AI systems on data that is not\nlikely to be representative of the affected AI subjects, or is not likely to be accurate, whether\nthat be due to age, omission, method of collection, or other factors.\n3.1.2.4\nSignificant decisions informed by the use of AI should be fair\nExample: Following a natural disaster, a government relief agency uses an AI system to a AI developers and AI operators could consider formal procedures such as Discrimination\ndetect communities in greatest need by analyzing social media data from a range of websites. Impact Assessments as a means of ensuring fairness.\nHowever, those communities where smartphone penetration is lower having less presence on\n3.1.2.4\nsocial media, and so are at risk of receiving less attention. Therefore, the charity complements\nAI operators should consider whether their AI systems are accessible and usable in a fair\ntheir AI tool with traditional techniques to identify needy populations elsewhere.\nmanner across user groups\n3.1.2.5\nh AI developers should consider whether their AI systems can be expected to perform well Consideration should be given to the effect of diversity on the development and deployment\nwhen exposed to previously unseen data, especially when evaluating people who are not processes\nwell-represented in the training data.\nb Organizations should seek to include people from diverse demographic backgrounds across\nthe full lifecyle, to include design, development and deployment processes. Organizations\nshould seek to engage diverse internal and external groups also.\nc AI developers should consider whether the assumptions they make about AI subjects could\nbe wrong or are likely to lead to non-operational bias; if so, they should consider consulting\nthe AI subjects in a representative manner during the design, development and deployment\nto confirm these assumptions.\n18 193.2 Accountable AI (Principle 2)\n3.2.2.2\nPositive efforts should be made to identify and mitigate any significant risks inherent in the AI\nsystems designed\n3.2.1 Principle\na AI operators should only use AI systems that are backed by evidence-based academic or\nindustrial research, and AI developers should base their development on such research.\nWe will make AI systems accountable\nAccountability for the outcomes of an AI system lies not with the system itself but is\napportioned between those who design, develop and deploy it; Example: A foreign country has a government service which identifies parents who owe\nmoney in child maintenance. The data matching process is often incorrect due to misspelled\nDevelopers should make efforts to mitigate the risks inherent in the systems they design; names or missing data which results in some individuals being incorrectly targeted\nautomatically by the system with the result being a large bill, poor credit ratings and even\nAI systems should have built-in appeals procedures whereby users can challenge significant\nfreezing wages. The recourse for individuals who are incorrectly targeted is time-consuming\ndecisions;\nand not straightforward . If the potential impact of incorrect decisions had been assessed,\nmitigation measures such a user-friendly review procedure could have been set up.\nAI systems should be developed by diverse teams which include experts in the area in which\nthe system will be deployed; and\nAI systems should be subject to external audit and decision quality assurance. b AI operators should identify the likely impact of incorrect automated decisions on AI\nsubjects and, in the case where incorrect decisions are likely to cause significant cost or\ninconvenience, consider mitigating measures.\n3.2.2 Guidelines\nc AI operators should consider internal risk assessments or ethics frameworks as a means to\n3.2.2.1 facilitate the identification of risks and mitigating measures.\nAccountability for the outcomes of an AI system should not lie with the system itself\nd In designing AI systems to inform significant decisions, AI developers should consider\na Accountability for loss or damages resulting from the application of AI systems should not measures to maintain data accuracy over time, including:\nbe attributed to the system itself.\n\u2022 the completeness of the data.\nb AI operators and AI developers should consider designating individuals to be responsible for \u2022 timely update of the data, and.\ninvestigating and rectifying the cause of loss or damage arising from the deployment of AI\n\u2022 whether the context in which the data was collected affects its suitability for the\nsystems.\nintended use case.\nExample: A border camera scanning for predictors of risk may misinterpret a \u201ctic\u201d of an\nindividual with Tourette syndrome as suspicious. These can manifest in a diverse fashion, and\nshould not cause this person to undergo secondary inspection every time they pass through\nthe border . If the data is updated after the first case is encountered then it would avoid\ncausing inconvenience on subsequent visits.\n5 Government of Canada. (2018). Responsible AI in the Government of Canada. Digital Disruption White Paper Series. Version 2.0, p.26. Retrieved from:\nhttps://docs.google.com/document/d/1Sn-qBZUXEUG4dVk909eSg5qvfbpNlRhzIefWPtBwbxY/edit\n4 Cabinet Office (UK), Data Science Ethical Framework, Version 1.0, licensed under the Open Government License v3.0, p. 13 6 Stoica, I. et. al.,2017, A Berkeley View of Systems Challenges for AI, p. 2, https://arxiv.org/pdf/1712.05855.pdf\n20 21e AI developers and AI operators should tune AI models periodically to cater for changes to e Be mindful that there might be fundamental tensions between different principles and\ndata and/or models over time. requirements. Continuously identify, evaluate, document and communicate these trade-\noffs and their solutions.\nf AI operators should consider whether AI systems trained in a static environment will display\nmodel instability when deployed in dynamic environments. Adopt a Trustworthy AI assessment list when developing, deploying or using AI systems,\nf\nand adapt it to the specific use case in which the system is being applied.\nExample: AI systems will need to be able to adapt to the changes in the environment that g Keep in mind that such an assessment list will never be exhaustive. Ensuring Trustworthy\nthey are deployed in. For example, a self-driving car would need to quickly adapt to unexpected AI is not about ticking boxes, but about continuously identifying and implementing\nand dangerous road by learning in real time from other cars that have successfully dealt with requirements, evaluating solutions, ensuring improved outcomes throughout the AI\nthese conditions. In addition, such mission-critical applications must handle noisy inputs system\u2019s lifecycle, and involving stakeholders in this.\nand defend against malicious actors .\n3.2.2.4\nAI subjects should be able to challenge significant automated decisions concerning them and,\ng AI developers should collaborate with AI Operators to train models using historical data where appropriate, be able to opt out of such decisions\nfrom the Operator.\na AI operators using AI systems to inform significant decisions should provide procedures by\nwhich affected AI subjects can challenge a specific decision concerning them.\nh AI Operators should consider working with their vendors (AI developers) to continually\nmonitor performance.\nb AI operators should consider such procedures even for non-significant decisions.\ni AI Operators should subject AI systems informing significant decisions to quality checks\nat least as stringent as those that would be required of a human being taking the same c AI operators should make affected AI subjects aware of these procedures and should\ndesign them to be convenient and user-friendly.\ndecision.\nd AI operators should consider employing human case evaluators to review any such\n3.2.2.3\nchallenges and, when appropriate, overturn the challenged decision.\nAI systems informing critical decisions should be subject to appropriate external audit\na When AI systems are used for critical decisions, external auditing of the AI systems in\nquestion should be used as a means to ensure meaningful standards of transparency and Example: A bank allows customers to apply for a loan online by entering their data. The\naccountability are upheld. bank uses an AI system to automatically determine whether to give the loan and what the\ninterest rate should be. They provide users with an option to contest the decision and have\nb In the case that critical decisions are of civic interest, public release of the results of the it reviewed by a human . They also require that customers justify their challenge by filling in\naudit should be considered as a means of ensuring public processes remain accountable a form, which assists the case reviewer and deters customers from challenging a decision\nto those affected by them. without good reason.7\nIn the case that critical decision are life and death decisions, these decisions should be\nc\nsupported by further validation and verification via a human operator.\nd Facilitate traceability and auditability of AI systems, particularly in critical contexts or\nsituations.\n7 Adapted from EU Commission, Can I be subject to automated individual decision-making, including profiling? Retrieved from: https://ec.europa.eu/\ninfo/law/law-topic/data-protection/reform/rights-citizens/my-rights/can-i-be-subject-automated-individual-decision-making-including-profiling_\nen#example\n22 23e AI operators should consider instituting an opt-out mechanism for significant automated 3.2.2.7\ndecisions. AI operators should understand the AI systems they use sufficiently to assess their suitability\nfor the use case and to ensure accountability and transparency\nf AI operators could consider \u201ccrowd challenge\u201d mechanisms whereby a critical number of\ncomplaints triggers an investigation into the fairness and/or accuracy of a decision-making\na In the case of critical decisions, AI operators should avoid using AI systems that cannot be\nprocess as a whole.\nsubjected to meaningful standards of accountability and transparency.\n3.2.2.5 b AI developers should consider notifying customers and AI operators of the use cases for\nAI systems informing significant decisions should not attempt to make value judgements on which the system has been designed, and those for which it is not suitable.\npeople\u2019s behalf without prior consent\na When informing an AI subject about significant choices they will make, AI systems should\nnot unreasonably restrict the available options or otherwise attempt to influence their value\njudgements without the explicit consent of the AI subject in question.\n3.2.2.6\nAI systems informing significant decisions should be developed by diverse teams with\nappropriate backgrounds\na AI developers who develop AI systems which may be used to assist in making critical\ndecisions should involve in the process experts with a background in social science, policy,\nor another subject which prepares them to evaluate the broader societal impact of their\nwork.\nb Development of AI systems informing significant decisions should include consultation\nwith experts in the field in which the system will be deployed.\nExample: An app that uses AI to assess medical symptoms and has a large user base had\nto face regulatory scrutiny because of number of complaints from doctors. They warned\nthat the application can miss signs of serious illness. A number of different shortcomings\nwere identified by doctors, some of which the company could address and resolve.8\n8 Financial Times. 2018. High-profile health app under scrutiny after doctor\u2019s complaints. Retrieved from: https://www.ft.com/content/19dc6b7e-8529-\n11e896-dd-fa565ec55929\n24 253.3 Transparent AI (Principle 3)\nc To facilitate the above, AI developers and AI operators should consider documenting the\nfollowing information during the design, development and deployment phases, and retaining\n3.3.1 Principle this documentation for a length of time appropriate to the decision type or industry:\n\u2022 the provenance of the training data, the methods of collection and treatment, how the data was\nWe will make AI systems transparent\nmoved, and measures taken to maintain its accuracy over time;\n\u2022 the model design and algorithms employed; and\nDevelopers should build systems whose failures can be traced and diagnosed. \u2022 changes to the codebase, and authorship of those changes.\nPeople should be told when significant decisions about them are being made by AI. d Where possible given the model design, AI developers should consider building in a means\nby which the \u201cdecision journey\u201d of a specific outcome (i.e. the component decisions leading\nWithin the limits of privacy and the preservation of intellectual property, those who deploy to it) can be logged.\nAI systems should be transparent about the data and algorithms they use.\nResponsible disclosures should be provided in a timely manner, and provide reasonable\nExample: A technology company has a product which is designed to assist in medical\njustifications for AI systems outcomes. This includes information that helps people\ndiagnosis. It documents each stage of its reasoning and relates it back to the input data.9\nunderstand outcomes, like key factors used in decision making.\n3.3.2 Guidelines\n3.3.2.1\nTraceability should be considered for significant decisions, especially those that have the\npotential to result in loss, harm or damage.\na For AI systems which inform significant decisions, especially those with the potential to\ncause loss, harm or damage, AI developers should consider building in traceability (i.e. the\nability to trace the key factors leading to any specific decision).\nb Organizations should ensure that harms caused through AI systems are investigated and\nredressed, by enacting strong enforcement mechanisms and remedial actions, to make\ncertain that human rights and the rule of law are respected in the digital world and in the\nphysical world.\n9 See IBM WatsonPaths\n26 273.3.2.2 3.4 Explainable AI (Principle 4)\nPeople should be informed of the extent of their interaction with AI systems\n3.4.1 Principle\na AI operators should inform AI subjects when a significant decision affecting them has been\nmade by an AI system.\nWe will make AI systems as explainable as\ntechnically possible\nExample: A small claims court adjudicates minor civil matters such as debt collection and\nevictions. They introduce an AI system to suggest the outcome of a ruling. At the time of the\nDecisions and methodologies of AI systems which have a significant effect on individuals\nruling the plaintiff and defendant are notified that the decision was assisted by an AI system.\nshould be explainable to them, to the extent permitted by available technology.\nThe court also provides an explanation for the decision.\nIt should be possible to ascertain the key factors leading to any specific decision that\ncould have a significant effect on an individual.\nb If an AI system can convincingly impersonate a human being, it should do so only after\nIn the above situation we will provide channels through which people can request such\nnotifying the AI subject that it is an AI system.\nexplanations.\nExample: A technology company produces a conversational AI agent which can make 3.4.2 Guidelines\nsome phone calls on behalf of its users. Those who receive the calls may believe that they\n3.4.2.1\nare speaking to a human. Therefore, the company programs the agent to identify itself at\nAI operators could consider providing affected AI subjects with a high-level explanation of\nthe start of every conversation.\nhow their AI system works\na AI operators could consider informing the affected AI subjects in understandable, non-\ntechnical language of:\n\u2022 the data that is ingested by the system;\n\u2022 the types of algorithms employed;\n\u2022 the categories into which people can be placed; and\n\u2022 the most important features driving the outcomes of decisions\n\u2022 A comprehensive list of feature engineering and models that was considered during\nthe model building phase.\nExample: A person turned down for a credit card might be told that the algorithm took\ntheir credit history, age, and postcode into account, but not learn why their application was\nrejected10 .\n28 29b For non-sensitive public sector use cases designed for the common good, AI operators\ncould consider making source code, together with an explanation of the workings of the AI Example: The UK\u2019s NHS developed a tool called Predict, which allows women with breast\nsystem, available either publicly or upon request (this should be done only if there is low risk cancer to compare their case to other women who have had the same condition in the past,\nof people \u2018gaming the system\u2019). and visualize the expected survival rate under various treatment options. The website has an\nexplanation page which shows the weights behind various factors and contains a description\nc The AI operator should maintain necessary documentation that provides elaboration and of the underlying mathematics12 .\nclarification on how the algorithm works, for example documentation of processes, decision\nmaking flow charts of the system, and how the appeal process is embedded.\nd An individual should have the ability to contest and seek effective redress against decisions c In the case that such explanations are available, they should be easily and quickly accessible,\nmade by AI systems. These must be addressed by the group or team supporting these free of charge and user-friendly.\nmodels.\ne Develop appropriate impact indices for the evaluation of AI system technological 3.5 Robust, Safe and Secure AI (Principle 5)\ninterventions from multiple perspectives.\n3.5.1 Principles\n3.4.2.2\nAI operators should consider providing affected AI subjects with a means to request\nexplanations for specific significant decisions, to the extent possible given the state of present AI systems will be technically robust\nresearch and the choice of model\nAI systems should be technically robust with a preventative approach to risks which\na AI operators should consider providing a means by which people affected by a significant\noperates in a manner such that they reliably behave.\ndecision informed by AI can access the reasoning behind that decision.\nAI Developers should ensure AI systems will not cause any unintentional harm and\nadverse impacts.\nExample: The US Consumer Financial Protection Bureau requires that creditors in the US\nwho reject credit applications must explain to the applicant the principal reason(s) why their\nAI systems should be resilient to attacks and security such as data poisoning and\napplication was rejected (e.g. \u201clength of residence\u201d or \u201cage of automobile\u201d)11 . In particular,\nmodel leakage.\n\u201cstatements that the adverse action was based on the creditor`s internal standards or\npolicies or that the applicant, joint applicant, or similar party failed to achieve a qualifying AI systems should have safeguards that enable a fallback plan in case of problems.\nscore on the creditor`s credit scoring system are insufficient\u201d.\nAI system results should be reproducible.\nb Where such explainability is not possible given available technology, AI operators should\nconsider compromises such as counterfactual reasoning, or listing the most heavily\nweighted factors contributing to the decision.\n11 Consumer Financial Protection Bureau, 12 CFR Part 1002 - Equal Credit Opportunity Act (Regulation B), \u00a7 1002.9 Notifications, Retrieved from https://www.\n10 The Guardian. AI watchdog needed to regulate automated decision-making, say experts. Retrieved from: https://www.theguardian.com/technology/2017/ consumerfinance.gov/policy-compliance/rulemaking/regulations/1002/\njan/27/ai-artificial-intelligence-watchdog-needed-to-prevent-discriminatory-automated-decisions 12 Predict website, accessible at http://www.predict.nhs.uk/predict_v2.1/legal/algorithm\n30 313.5.2 Guidelines\n3.5.2.1 3.5.2.3\nAI Operators should continue conducting vulnerability assessments, verification of AI system AI Operators should assure end users of the system\u2019s reliability through documentation, and\ndifferent behaviors in unexpected situations and for any dual-use case, to include: operationalizing processes for testing and verification of desired outcomes , specifically:\na Putting measures to ensure integrity and resilience of IA system against potential attacks. a AI system should be designed with an approach to continue monitoring if the system meets\ngoals, purposes and intended application.\nb AI Operators should define an approach to ensure results are reproducible through clear\nExample: IBM researchers presented a novel approach to exploiting AI for malicious\nprocess and documentation.\npurposes. Their system, DeepLocker embeds AI capabilities within the malware itself in order\nto improve its evasion techniques. It uses an Artificial Intelligence model to identify its target\nc AI Operators should publish documentation to assure system robustness to end users.\nusing indicators like facial recognition, geolocation and voice recognition.\n3.5.2.4\nAI systems will be safe, secure and controllable by humans\nb Verifying how AI systems behave in unexpected situations and environments.\na Safety and security of people, be they operators, end-users or other parties, will be of\nc Taking preventative measures against any possible system dual-case use. paramount concern in the design of any AI system.\nb AI systems should be verifiably secure and controllable throughout their operational\n3.5.2.2\nlifetime, to the extent permitted by technology.\nAI operators should define a suitable fall back plan and test it to maintain readiness in\nunexpected situations and in high safety risks level, to include:\nc The continued security and privacy of users should be considered when decommissioning\na Developing a plan to measure and assess potential safety risks to you or any third party AI systems.\nfrom technology use accidental or malicious misuse.\nd AI systems that may directly impact people\u2019s lives in a significant way should receive\nb Define thresholds for system acceptable results and governance procure to fall back on commensurate care in their designs.\nalternative defined and tested plans.\ne Such systems should be able to be overridden or their decisions reversed by designated\nc Considering an insurance policy to mitigate risks arising from potential damages. individuals.\nExample: Adopting a risk-based approach to procurement and clearly communicating it to\nvendors can help address such issues by giving the procuring organization advance notice\nof the specific oversight capabilities it will need in future stages of the system lifecycle,\npreventing vendors from presenting intellectual property arguments against required testing,\nmonitoring and auditing of their AI systems going forward and rewarding vendors\u2014of all\nsizes\u2014that are more advanced and responsive in their responsible AI efforts.\n13 IBM. https://documents.trendmicro.com/assets/white_papers/wp-malicious-uses-and-abuses-of-artificial-intelligence.pdf\n32 333.6.2 Guidelines\n3.5.2.5\nAi systems should not be able to autonomously hurt, destroy or deceive humans\n3.6.2.1\na AI systems should be built to serve and inform, and not to deceive and manipulate.\nAI should be developed to align with human values, contribute to human flourishing and\nbenefit society.\nb Nations should collaborate to avoid an arms race in lethal autonomous weapons, and\nsuch weapons should be tightly controlled. a Society should be consulted in a representative fashion to inform the development of AI.\nc Active cooperation should be pursued to avoid corner-cutting on safety standards. b Stakeholders throughout society should be involved in the development of AI and its\ngovernance.\nSystems designed to inform significant decisions should do so impartially.\nd\nc Stakeholders should proactively engage in responsible stewardship of trustworthy AI in\npursuit of beneficial outcomes for people and the planet.\n3.6 Human Centered AI (Principle 6)\nd Organizations should prioritize having all their stakeholders learn about wellbeing metrics\nas a potential determinant of how they create, deploy, market and monitor their AI\n3.6.1 Principles technologies.\nWe will give AI systems human values and\nExample: IEEE P7010 recommended practice14 establishes wellbeing metrics relating to\nmake them beneficial to society human factors directly affected by intelligent and autonomous systems and establishes a\nbaseline for the types of objective and subjective data these systems should analyze and\nGovernment will support the research of the beneficial use of AI. include (in their programming and functioning) to proactively increase human wellbeing.\nStakeholders throughout society should be involved in the development of AI and its\ngovernance.\n3.6.2.2\nSystematic analyses that examine the ethics of designing affective systems to nudge human\nDesign AI systems to adopt, learn and follow the norms and values of the community\nbeings prior to deployment are needed.\nthey serve.\nSystematic analyses that examine the ethics of designing affective systems to nudge a Nudging in AI systems should have an opt-in system policy with explicit consent.\nhuman beings prior to deployment are needed.\nb We recommend that when appropriate, an affective system that nudges human beings\nDecisions related to lethal force, life and death should not be delegated to AI systems. should have the ability to accurately distinguish between users, including detecting\nRules and standards should be adopted to ensure effective human control over those characteristics such as whether the user is an adult or a child. Additional protections must\ndecisions. be put in place for vulnerable populations, such as children, when informed consent cannot\nbe obtained, or when it may not be a sufficient safeguard.\nc AI systems with nudging strategies must be carefully evaluated, monitored, and controlled.\n14 IEEE. https://sagroups.ieee.org/7010/\n34 353.6.2.3 f In situations where it is needed, human rights impact assessments and human rights due\nHumanity should retain the power to govern itself and make the final decision, with AI in an diligence, human determination codes of ethical conduct, or quality labels and certifications\nassisting role. intended to promote human centered values and fairness should be considered.\ng Mechanisms should be put into place to receive external feedback regarding AI systems\na Decisions related to lethal force, life and death should not be delegated to AI systems. Rules\nthat potentially infringe on fundamental rights.\nand standards should be adopted to ensure effective human control over those decisions.\nb Responsible parties (e.g., parents, nurse practitioners, social workers, and governments)\nshould be trained to detect the influence due to AI and ineffective mitigation techniques.\nIn the most extreme cases it should always be possible to shut down harmful AI system.\nc Those actions undertaken by an affective system that are most likely to generate an\nemotional response should be designed to be easily changed.\nd Users should be able to make informed autonomous decisions regarding AI systems. They\nshould be given appropriate knowledge and tools to comprehend and interact with AI\nsystems to satisfactory degree and, where possible to reasonably self-assess or challenge\nthe system.\n3.6.2.4\nAI systems should be designed in a way that respects the rule of law, human rights, society\nvalues, and should include appropriate safeguards to ensure a fair and just society.\na AI Developers should design AI systems to adopt, learn and follow the norms and values of\nthe community they serve.\nb Organizations could identify the norms of the specific community in which AI systems are\nto be deployed in. In particular, pay attention to the norms relevant to the kinds of tasks that\nthe AI systems are designed to perform. These could be documented as well as how these\nnorms are addressed by the AI system.\nc To respond to the dynamic change of norms in society the AI system could be able to adjust\nits existing norms and learn new ones, while being transparent about these changes.\nd Designers should consider forms and metrics for assessing an AI system\u2019s norm conformity\nover the lifetime of the system (e.g. human-machine agreement on moral decisions,\nverifiability of AI decisions, justified trust).\ne The norm identification process must document the similarities and differences between\nthe norms that humans apply to other humans and the norms they apply to AI systems.\nNorm implementations should be evaluated specifically against the norms that the\ncommunity expects the AI system to follow.\n36 373.7 Sustainable and Environmentally Friendly AI (Principle 7)\nd While there is no standardized means of reporting on the environmental impact of AI\nsystems, where possible, model development and commissioning should make carbon\n3.7.1 Principle impacts a core consideration alongside functional and business requirements.\ne Smaller models should be considered \u2013 shrinking down the model size and using fewer\nWe will promote sustainable and environmentally compute cycles (to balance financial and performance costs with the end performance of\nthe model).\nfriendly AI\nf Carbon awareness should be considered \u2013 adjusting the operational parameters of the AI\nAI systems should be used to benefit all human beings, including future generations. The\nsystem to dynamically select the best time and location for energy use from the grid can\nenvironment is fundamental to this.\nreduce its carbon footprint.\nThroughout the AI lifecycle, implementations should only be carried out on the basis of a full\nunderstanding and acknowledgement of implications for sustainability and environment.\nExample: In agriculture, AI can transform Example: Google`s DeepMind division\nAn AI system\u2019s development, deployment and use, should be assessed via critical examination production by better monitoring and has developed AI that teaches itself to\nof resource usage and energy consumption.\nmanaging environmental conditions minimize the use of energy to cool Google`s\nand crop yields. AI can help reduce both data centers. As a result, Google reduced\nMechanisms to measure environmental impact due to type of energy use and processing\nfertilizer and water, all while improving its data center energy requirements by\npower provided by data centers should be established.\ncrop yields. 40% 15.\n3.7.2 Guidelines\n3.7.2.1 Throughout the AI lifecycle, implementations should always be carried out after full\nunderstanding and acknowledgement of AI implications on sustainability and environment.\na The application of Artificial Intelligence to address sustainability challenges are well\nunderstood. Building and running green, sustainable AI systems is of increasing importance,\ngiven the large carbon footprint that they can generate and the wider context of addressing\nclimate change.\nb AI model development, and by extension commissioning, should therefore seek to balance\ntechnical performance with energy consumption and environmental impact.\nc Efforts should be made to estimate and understand sustainability and environmental\nimpact across the AI lifecycle (e.g. the energy consumption costs associated with model\ntraining, and CO2 emissions and cloud compute costs associated with the deployment\nand running of the system).\n15 https://www.deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-by-40\n38 393.8 Privacy Preserving AI (Principle 8)\n3.8.2.2\nEstablish an oversight mechanism for data collection, storage and processing and use across\nyour organization\n3.8.1 Principle\na Assess who can access data and under which conditions.\nWe will respect people\u2019s privacy\nb Assign specific responsibilities and role for Data Protection Officers.\nAI systems should respect privacy and use the minimum intrusion necessary.\nc Prevention of harm to privacy necessitates adequate data governance that covers the\nquality and integrity of the data used, its relevance in light of the domain in which the\nAI systems should uphold high standards of data governance and security,\nprotecting personal information. AI systems will be deployed, its access protocols and the capability to process data in a\nmanner that protects privacy.\nSurveillance or other AI-driven technologies should not be deployed to the extent\nof violating internationally and/or UAE\u2019s accepted standards of privacy and human d AI systems must guarantee privacy and data protection throughout a system\u2019s entire\ndignity and people rights. lifecycle. This includes the information initially provided by the user, as well as the\ninformation generated about the user over the course of their interaction with the system\nPrivacy by design should be embedded in AI systems and where possible AI\n(e.g. outputs that the AI systems generated for specific users or how users responded to\nalgorithm should have adequate privacy impact assessments.\nparticular recommendations). Digital records of human behavior may allow AI systems\nto infer not only individuals\u2019 preferences, but also their age, gender, religious or political\nAdequate data protection frameworks and governance mechanisms should be\nviews. To allow individuals to trust the data gathering process, it must be ensured that data\nestablished in a multi-stakeholder approach and ensured throughout the life cycle\ncollected about them will not be used to unlawfully or unfairly discriminate against them.\nof AI systems.\ne In any given organization that handles individuals\u2019 data (whether someone is a user of the\nAI developers and operators should strive to strike the balance between privacy\nsystem or not), data protocols governing data access should be put in place in line with\nrequirements, individual rights and innovation growth and society benefits.\nnational privacy legislation (be these universal or sector-specific). These protocols should\noutline who can access data and under which circumstances. Only duly qualified personnel\n3.8.2 Guidelines\nwith the competence and need to access individual\u2019s data should be allowed to do so.\n3.8.2.1\nEstablish mechanism for users to flag issues related to privacy or data protection\na AI Operators should review the system for proper consent logging, ability of users to revoke\npermission whenever applicable.\nb Consider training AI model without or with minimal user of potentially sensitive or personal\ndata.\nc Use measures to enhance privacy such as encryption, anonymization and aggregation.\n40 41f Algorithmic systems require adequate privacy impact assessments, which also include\nsocietal and ethical considerations of their use and an innovative use of the privacy by\ndesign approach. AI actors need to ensure that they are accountable for the design and\nimplementation of AI systems in such a way as to ensure that personal information is\nprotected throughout the life cycle of the AI system.\ng Establish data policies or equivalent frameworks, or reinforce existing ones, to ensure full\nsecurity for personal data and sensitive data, which, if disclosed, may cause exceptional\ndamage, injury or hardship to individuals. Examples include data relating to offences,\ncriminal proceedings and convictions, and related security measures; biometric, genetic\nand health data; and personal data such as that relating to race, descent, gender, age,\nlanguage, religion, political opinion, national origin, ethnic origin, social origin, economic or\nsocial condition of birth, or disability and any other characteristics.\nh Promote mechanisms, such as open repositories for publicly funded or publicly held data,\nsource code and data trusts, to support the safe, fair, legal and ethical sharing of data,\namong others.\ni Promote and facilitate the use of quality and robust datasets for training, development and\nuse of AI systems, and exercise vigilance in overseeing their collection and use.\nExample: with the help of AI & ML algorithms16 , Synthetic data can be created to enhance\nprivacy. The data created will have the same statistical characteristics for the testing\nenvironment and where a third party can have access.\n16 https://research.aimultiple.com/privacy-enhancing-technologies/\n42 43Microsoft. 2022. Microsoft Responsible AI Standard, v2. General Requirements. June 2022.\n04\nRetrieved from:\nhttps://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4ZPmV\nISO. 2022. Proposed ISO Standard on Risk Management of AI: What Businesses Should Know.\nBibliography\nMay 2022. Retrieved from:\nhttps://unesdoc.unesco.org/ark:/48223/pf0000381137/PDF/381137eng.pdf.multi\nGSMA 2022. The AI Ethics Playbook. Implementing ethical principles into everyday business.\nFebruary 2022. Retrieved from:\nhttps://www.gsma.com/betterfuture/wp-content/uploads/202201//The-Mobile-Industry-Ethics-\nPlaybook_Feb-2022.pdf\nOECD. 2022. AI Policy Observatory. Retrieved from:\nhttps://standards.ieee.org/wp-content/uploads/import/documents/other/ead_v2.pdf\nUNESCO. 2022. Recommendation on the Ethics of Artificial Intelligence. Adopted in November\n2021. Retrieved from:\nhttps://unesdoc.unesco.org/ark:/48223/pf0000381137/PDF/381137eng.pdf.multi\nEuropean Commission. 2021. Proposal for a Regulation of the European Parliament and of the\nCouncil Laying Down Harmonized Rules on Artificial Intelligence (Artificial Intelligence Act) and\nAmending Certain Union Legislative Acts. Retrieved from:\nhttps://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A52021PC0206\nEuropean Commission. High Level Expert Group on Artificial Intelligence ( AI HLEG ). Ethics\nGuidelines for Trustworthy AI . April 2019. Retrieved from:\nhttps://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai\nCentre for Data Ethics and Innovation. UK Government: The roadmap to an effective AI assurance\necosystem. December 2021. Retrieved from:\nhttps://www.gov.uk/government/publications/the-roadmap-to-an-effective-ai-assurance-\necosystem\nThe New York City Council. 2021. A Local Law to amend the administrative code of the city of New\nYork, in relation to automated employment decision tools. Retrieved from:\nhttps://legistar.council.nyc.gov/LegislationDetail.aspx?ID=4344524&GUID=%20B051915D-A9AC-\n451E-81F86596032-FA3F9&\nUNESCO, Recommendation on the Ethics of Artificial Intelligence. 2021. Retrieved from:\nhttps://unesdoc.unesco.org/ark:/48223/pf0000381137\n44 45AI NOW Institute. Algorithmic accountability for the public sector. August 2021. Retrieved from: CNIL. (2017). How can humans keep the upper hand? The ethical matters raised by algorithms\nhttps://www.opengovpartnership.org/documents/algorithmic-accountability-public-sector/ and artificial intelligence. Retrieved from:\nhttps://www.cnil.fr/sites/default/files/atoms/files/cnil_rapport_ai_gb_web.pdf\nIEEE 2019. Ethically Aligned Design. Version 2 \u2013 For Public Discussion. Retrieved from:\nhttps://standards.ieee.org/wp-content/uploads/import/documents/other/ead_v2.pdf Executive Office of the President of the United States. National Science and Technology Council.\nNetworking and Information Technology Research and Development Subcommittee. (2016). The\nnational artificial intelligence research and development strategic plan. Retrieved from:\nIEEE 2017. Ethically Aligned Designed : A vision for Prioritizing Human Well-being with\nhttps://www.nitrd.gov/PUBS/national_ai_rd_strategic_plan.pdf\nAutonomous and Intelligent Systems. Retrieved from:\nhttps://standards.ieee.org/wp-content/uploads/import/documents/other/ead_v2.pdf\nExecutive Office of the President of the United States National Science and Technology Council.\nCommittee on Technology. (2016). Preparing for the future of artificial intelligence. Retrieved from:\nGoogle. AI Principles 2021. Retrieved from:\nhttps://obamawhitehouse.archives.gov/sites/default/files/whitehouse_files/microsites/ostp\nhttps://ai.google/principles/\nNSTC/preparing_for_the_future_of_ai.pdf\nMicrosoft. Responsible AI Standard v2 (June 2022). Retrieved from:\nThe Headquarters for Japan\u2019s Economic Revitalization. (2015). New Robot Strategy. Japan\u2019s\nhttps://www.microsoft.com/en-us/ai/responsible-ai\nRobot Strategy. Vision, Strategy, Action Plan. Retrieved from:\nhttp://www.meti.go.jp/english/press/2015/pdf/0123_01b.pdf\nThe ECONOMIST Group 2022 . Pushing forward: the future of AI in the Middle East and North\nAfrica Report. Retrieved from:\nTreasury Board of Canada Secretariat. (2018). Responsible Artificial Intelligence in the\nhttps://impact.economist.com/perspectives/sites/default/files/google_ai_mena_report.pdf\nGovernment of Canada. Digital Disruption White Paper Series. Version 2.0.\nhttps://docs.google.com/document/d/1Sn-qBZUXEUG4dVk909eSg5qvfbpNlRhzIefWPtBwbxY/\nPDPC. (2018, June 5). Discussion paper on Artificial Intelligence (AI) and Personal Data.\nedit\nSingapore: Personal Data Protection Commission Singapore (PDPC). Retrieved from:\nhttps://www.pdpc.gov.sg/-/media/Files/PDPC/PDF-Files/Resource-for-Organisation/AI/\nThe Toronto Declaration: Protecting the right to equality and non-discrimination in machine\nDiscussion-Paper-on-AI-and-PD---050618.pdf\nlearning systems. Toronto, Canada: Amnesty International and Access Now. Retrieved from:\nhttps://www.accessnow.org/cms/assets/uploads/201808//The-Toronto-Declaration_ENG_08-\nITI. AI Policy Principles. Retrieved from:\n2018.pdf\nhttps://www.itic.org/public-policy/ITIAIPolicyPrinciplesFINAL.pdf\nhttp://data.parliament.uk/writtenevidence/committeeevidence.svc/evidencedocument/artificial-\nCabinet Office.(2016, May 19). Data Science Ethical Framework. Retrieved from: intelligence-committee/artificial-intelligence/oral/73546.html (house of lords select committee\nhttps://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/ oral evidence, Q61)oral evidence, Q61)\nfile/524298/Data_science_ethics_framework_v1.0_for_publication__1_.pdf\nHouse of Lords Select Committee on Artificial Intelligence. (2018). AI in the UK: ready, willing and\nEuropean Parliament. (2017, February 16). European Parliament resolution of 16 February 2017 able?\nwith recommendations to the Commission on Civil Law Rules on Robotics (20152103/(INL)). https://publications.parliament.uk/pa/ld201719/ldselect/ldai/100100/.pdf\nRetrieved from:\nhttp://www.europarl.europa.eu/sides/getDoc.do?pubRef=-//EP//TEXT+TA+P8-TA-2017- House of Commons Science and Technology Select Committee. Algorithms in Decision Making.\n0++0051DOC+XML+V0//EN https://publications.parliament.uk/pa/cm201719/cmselect/cmsctech/351351/.pdf\nVillani, C. (2018). For a meaningful Artificial Intelligence towards a French and European strategy.\nRetrieved from:\nhttps://www.aiforhumanity.fr/pdfs/MissionVillani_Report_ENG-VF.pdf\n46 47", "metadata": {"country": "UAE", "year": "2019", "legally_binding": "no", "binding_proof": "None", "date": "01/01/2019", "regulator": "UAE Cabinet", "type": "charter", "status": "enacted", "language": "English", "use_cases": "[1, 3, 5, 6]"}}
{"_id": "68695bf05520a4cc7989de7f", "title": "UAE Patent Law", "source": "https://ai.gov.ae/", "text": "AI ETHICS\nARTIFICIAL INTELLIGENCE ETHICS\nPRINCIPLES & GUIDELINES\nPRINCIPLES & GUIDELINES\nDEC 2022Table of Content\nPage Page\n1. Overview 4 3.4 Explainable 29\n1.1 Introduction 5 3.4.1 Principle 29\nWe will make AI systems as explainable as technically possible\n1.2 Scope 6 3.4.2 Guidelines 29\n1.3 Strategic alignment 6 3.5 Robust, Safe and Secure AI 31\n1.4 Responsibility 8 3.5.1 Principles 31\nAI systems will be technically robust\n3.5.2 Guidelines 32\n1.5 Licensing 8\n3.6 Human Centred AI 34\n1.6 Toolkit 8\n3.6.1 Principles 34\nWe will give AI systems human values and make them beneficial to society\n3.6.2 Guidelines 35\n2. Definitions 10\n3.7 Sustainable and Environmentally Friendly AI 38\n3.7.1 Principle 38\n3. Principles and Guidelines 16\nWe will promote sustainabity and environemntly friendly AI 38\n3.7.2 Guidelines 38\n3.1 Fairness 17\n3.8 Privacy Preserving AI 38\n3.1.1 Principle 17\nWe will make AI systems fair\n3.8.1 Principle 40\n3.1.2 Guidelines 20\nWe will respect people\u2019s privacy\n3.8.2 Guidelines 40\n3.2 Accountable AI 20\n3.2.1 Principle 20\nWe will make AI systems accountable\n4. Bibliography 44\n3.2.2 Guidelines 20\n3.3 Transparent AI 26\n3.3.1 Principle 26\nWe will make AI systems transparent\n3.3.2 Guidelines 26\n2 301 1.1 Introduction\nAI\u2019s rapid advancement and innovation potential across a range of fields is incredibly exciting.\nYet a thorough and open discussion around AI ethics, and the principles organizations using this\nOverview\ntechnology must consider, is needed.\nThis document - AI Ethics: Principles and Guidelines \u2013 intends to meet this need to balance these\ntwo central considerations of AI. It is designed to offer detailed guidance to help AI actors adhere\nto eight principles of AI ethics.\nWe will make AI systems that are:\nexplainable transparent accountable fair\nprivacy preserving sustainable and human centered robust, safe and\nenvironmentally secure\nfriendly\nThe guidelines are non-binding, and are drafted as a collaborative multi-\nstakeholder effort, with full awareness of organizations\u2019 needs to innovate\nand protect their intellectual property. This is a collaborative process in\nwhich all stakeholders are invited to be part of an ongoing dialogue. We\nwould like to see the AI Ethics Guidelines evolve into a universal, practical\nand applicable framework informing ethical requirements for AI design and\nuse. The eventual goal is to reach widespread agreement and adoption of\ncommonly agreed policies to inform the ethical use of AI nationally and\naround the world.\n4 51.2 Scope Additionally, the AI Office is currently developing additional policies to be taken into consideration\nalong with this document:\nThis document gives non-mandatory guidelines for achieving the ethical design\nand deployment of AI systems in both the public and private sectors. AI already\nAI Seal AI Procurement Guidelines\nsurrounds us, but some applications are more visible and sensitive than others.\nThe UAE AI Seal brand (UAI) will be used The AI Procurement Guidelines will be\nThis document is applicable only to those AI systems which make or inform\nto attract talent and business from used to guide Federal Government entities\n\u2018significant decisions\u2019 \u2013 that is, those decisions which have the potential for\nacross the globe to come to the UAE to in the procurement of AI systems. They\nsignificant impact either on individuals or on society as a whole. They also apply to\ntest and develop AI. This includes a UAI will list the general principles, proposed\n\u2018critical decisions\u2019, which are a subset of significant decisions and are of especially\nmark recognizing high quality, ethical AI mechanisms for procurement and review\ncritical nature.\ncompanies. It would reward safe, efficient, of AI products between the federal\nverified AI technology with a \u2018UAI Seal government and vendors. The policy\nThis document guides the further developments of sector specific requirements\nof Approval\u2019. The UAE AI Seal brand is considers R&D for specific challenges\nand principles, hence every entity within its sector can further amend, use, or\ncurrently under development by the AI that do not currently have an AI solution\ncomplement its current frameworks to properly suit its context and the need of\nOffice. product that is market-ready. Based on\naffected stakeholders.\nglobal standards for AI Procurement, the\npolicy aims to incentivize AI adoption\nIt is a living document and will undergo further future reviews and enhancements.\nwithin government entities and promote\nAI R&D within the private sector for\nspecified government challenges. The\nprocurement policy is currently under\ndevelopment by the AI Office.\n1.3 Strategic alignment\nThis document comes as a fulfillment of the goals and aspirations set in the UAE National AI\nStrategy and the international Sustainable Development Goals (SDGs). The UAE has long been\nat the forefront of sustainable development. These shared global milestones have created global\nmovement and adoption in several disciplines and sectors. As an early adopter, the UAE has\nestablished a national committee on SDGs with a long-term realization of the UAE SDG agenda\n2030. These goals are further engrained and reflected in the AI Ethics Principles under the topics of\nfairness, inclusiveness, equality, human benefit, and rights.\n6 71.4 Responsibility\nThe AI Office will not be responsible for any misuse of the AI Ethics Principles and Guidelines. The\nuser bears all the consequences of their use.\n1.5 Licensing\nThis document is published under the terms of a Creative Commons Attribution 4.0 International\nLicense in order to facilitate its re-use by other governments and private sector organizations.\nIn summary this means you are free to share and adapt the material, including for commercial\npurposes, provided that you give appropriate credit to the UAE AI and Blockchain Council as its\nowner and do not suggest the Minister of State for Artificial Intelligence, Digital Economy and\nRemote Work Applications Office endorses your use.\n1.6 Toolkit\nThe document is part of a toolkit of self-governing nature that boosts awareness and enables\ngovernment and private institutions to pursue innovative use cases while maintaining human\nvalues and principles.\n8 92.1 AI developer organization\n02\nAn organization which does any of the following:\na determines the purpose of an AI system;\nDefinitions\nb designs an AI system;\nc builds an AI system, or:\nd performs technical maintenance or tuning on an AI system\nNote:\nThe definition applies regardless of whether the organization is the ultimate user of the system, or\nwhether they sell it on or give it away.\nExample: A company develops an artificially intelligent facial recognition system and\nsells it to a country\u2019s border force, who use it to identify suspicious personnel. The company\nis an AI developer organization and the border force is an AI operator organization.\n2.2 AI operator organization\nAn organization which does any of the following:\na uses AI systems in operations, backroom processes or decision-making;\nb uses an AI system to provide a service to an AI subject;\nc is a business owner of an AI system;\nd procures and treats data for use in an AI system; or\ne evaluates the use case for an AI system and decides whether to proceed.\nNotes:\n1. This definition applies regardless of whether the AI system was developed in-house or procured.\n2. It is possible for organizations to be both an AI developer organization and an AI operator organization.\n10 112.6 Critical decision\n2.3 Artificial Intelligence (also AI)\nThe capability of a functional unit to perform functions that are generally associated with human An individually significant decision which is deemed to either have a very large impact on an\nintelligence such as reasoning, learning and self-improvement1. individual or to have especially high stakes. These can be especially sensitive, have the potential\nto cause high loss or damage, be societally significant, or set an important precedent.\n2.4 Artificially Intelligent System (also AI system)\nNotes:\nThe types of decisions referred to here are the same as those in the definition of significant-at-\nA product, service, process or decision-making methodology whose operation or outcome is\nscale decisions, except in this case the effects are felt as a result of an individual decision rather\nmaterially influenced by artificially intelligent functional units.\nthan an aggregate of many decisions..\nNotes:\n1. It is not necessary for a system\u2019s outcome to be solely determined by artificially intelligent\nfunctional units in order for the system to be defined as an artificially intelligent system; and Example: A court determines whether a defendant is guilty of a criminal charge, with the\n2. A particular feature of AI systems is that they learn behavior and rules not explicitly punishment for guilt being a life sentence. This is a critical decision because it has a very\nprogrammed in. large impact on the life of the defendant and also sets precedent for similar cases in the\nfuture.\nExample 1: A small claims court uses Example 2: A government entity uses\n2.7 Functional Unit\nan artificially intelligent software package a chatbot which allows customers to ask\nto collect evidence pertaining to a case, routine questions, book appointments\nEntity of hardware or software, or both, capable of accomplishing a specified purpose3 .\ncompare it to similar cases in the past, and conduct minor financial transactions.\nand present a recommended decision The chatbot responds to customer\nto a judge. The judge determines the queries with pre-written responses and is\n2.8 Individually significant decision\nfinal outcome. This decision-making based on pre-programmed decision rules.\nA decision which has the potential for significant impact on at least one individual\u2019s\nmethodology is materially influenced Therefore, the chatbot is not an AI system.\ncircumstances, behavior or choices, or has legal or similarly significant effects on him or her.\nby an artificially intelligent functional If, however, the chatbot autonomously\nunit, and is therefore classified as an AI adjusted its treatment of customers\nsystem. based on the outcome of past cases, it\nwould be an AI system. Example: A company decides to make an employee redundant. This is an individually\nsignificant decision because of its potential impact on the employee\u2019s financial situation.\n2.5 Bias (of a system)\nInclination or prejudice for or against one person or group, especially in a way considered to be\nunfair2.\n2 Oxford Dictionary 2018, Oxford University Press, viewed online 4th October 2018, <https://en.oxforddictionaries.com>\n1 Consistent with ISO/IEC 2382:2015 3 From ISO/IEC 2382:2015\n12 132.9 Non-operational bias (of a system) 2.11 Significant decision\nBias that is either: A decision which is either individually significant or is part of a set of significant-at-scale\ndecisions.\na not a design feature; or\nb not important in achieving the stated purpose of the system. 2.12 Subject of an artificially intelligent system (also AI subject)\nA natural person who is any of the following:\n2.10 Set of significant-at-scale decisions\na an end-user of an AI system\nA set of decisions made by the same system or organization which, when taken in aggregate,\nb directly affected by the operation of or outcomes of an AI system, or:\nhave significant impact on society as a whole or groups within it.\nc a recipient of a service or recommendation provided by an AI system\nNotes:\n1. The decisions need not be individually significant in order to qualify, in aggregate, as a set of\nsignificant-at-scale decisions; and\n2. Examples of areas which have a large impact on society and which include but are not limited\nto: the large-scale allocation of resources or opportunities amongst groups; the structure of\ngovernment; the division of power between large entities or groups; the law, and its interpretation\nand enforcement; conflict and war; international relations, etc.\nExample: An AI system is used by a website to determine which content to show users.\nThis decision is not individually significant, since a user is not greatly affected by whether\na particular piece of media is shown to them. However, if the website is popular then the AI\nsystem may be making a set of significant-at-scale decisions, because any biases in the\nsystem will affect a large number of users.\n14 153.1 Fairness (Principle 1)\n03\n3.1.1 Principle\nPRINCIPLES AND We will make AI systems fair\nData ingested should, where possible, be accurate and representative of the affected\npopulation.\nGUIDELINES\nAlgorithms should avoid non-operational bias.\nSteps should be taken to mitigate and disclose the biases inherent in datasets.\nSignificant decisions should be provably fair.\nAll personnel involved in the development, deployment and use of AI systems have a role and\nresponsibility to operationalize AI fairness and should be educated accordingly.\n3.1.2 Guidelines\n3.1.2.1\nBenefits of AI systems should be available and accessible to all. AI systems should not discriminate\nagainst people or groups in a way that could have an adverse impact. Especially significant and\ncritical decisions made or assisted by AI should be provably fair.\na Considering that fairness can have many different definitions, an organization should\ndocument its own definition of fairness for the context that the AI system is going to be\nimplemented in. Organizations should document what the implemented fairness objective\nstands for and why this choice was considered the most suitable for the given scenario.\nb It is recommended to identify and document demographic groups that may be adversely\nimpacted and mitigate the risk where possible.\nc AI developers and AI operators could consider formal procedures such as Discrimination\nImpact Assessments as a means of ensuring fairness. This assessment should be\ndocumented as well as the mitigation measures that were implemented by the organization.\nThe impact assessment should be conducted pre-release and regularly after as an ongoing\nevaluation, results are advised to be documented.\n16 173.1.2.2 3.1.2.3\nConsideration should be given to whether the data ingested is accurate and representative Consideration should be given to whether decision-making processes introduce bias\nof the affected population\na When subjecting different groups to different decision-making processes, AI developers\nd Fairness has many different definitions in different cultures and for different contexts. should consider whether this will lead to non-operational bias.\nEncouraging a diverse and inclusive AI ecosystem is thus all the more crucial to ensure\nthat one definition of fairness does not contradict another, and that the process of defining b When evaluating the fairness of an AI system, AI developers and AI operators should\nfairness itself is fair, with under-represented groups represented in the discussion. consider whether AI subjects in the same circumstances receive equal treatment.\ne AI developers and AI operators should undertake reasonable data exploration and/or testing\nto identify potentially prejudicial decision-making tendencies in AI systems arising from\nExample: An organization uses an AI tool to automate the pre-screening of candidates\ndata inaccuracy and biases in the data.\nfor a job opening. It is trained on data from the company\u2019s existing employees, the majority\nof whom are from the same ethnic background. Therefore, the system learns to use name\nf AI developers and operators should evaluate all datasets to assess inclusiveness of and nationality as discriminating factors in filtering job applicants. This could have been\nidentified demographic groups and collect data to close any gaps.\nidentified through testing and rectified by, for example, balancing the training data or only\nusing relevant data fields for training.\ng AI developers and AI operators should refrain from training AI systems on data that is not\nlikely to be representative of the affected AI subjects, or is not likely to be accurate, whether\nthat be due to age, omission, method of collection, or other factors.\n3.1.2.4\nSignificant decisions informed by the use of AI should be fair\nExample: Following a natural disaster, a government relief agency uses an AI system to a AI developers and AI operators could consider formal procedures such as Discrimination\ndetect communities in greatest need by analyzing social media data from a range of websites. Impact Assessments as a means of ensuring fairness.\nHowever, those communities where smartphone penetration is lower having less presence on\n3.1.2.4\nsocial media, and so are at risk of receiving less attention. Therefore, the charity complements\nAI operators should consider whether their AI systems are accessible and usable in a fair\ntheir AI tool with traditional techniques to identify needy populations elsewhere.\nmanner across user groups\n3.1.2.5\nh AI developers should consider whether their AI systems can be expected to perform well Consideration should be given to the effect of diversity on the development and deployment\nwhen exposed to previously unseen data, especially when evaluating people who are not processes\nwell-represented in the training data.\nb Organizations should seek to include people from diverse demographic backgrounds across\nthe full lifecyle, to include design, development and deployment processes. Organizations\nshould seek to engage diverse internal and external groups also.\nc AI developers should consider whether the assumptions they make about AI subjects could\nbe wrong or are likely to lead to non-operational bias; if so, they should consider consulting\nthe AI subjects in a representative manner during the design, development and deployment\nto confirm these assumptions.\n18 193.2 Accountable AI (Principle 2)\n3.2.2.2\nPositive efforts should be made to identify and mitigate any significant risks inherent in the AI\nsystems designed\n3.2.1 Principle\na AI operators should only use AI systems that are backed by evidence-based academic or\nindustrial research, and AI developers should base their development on such research.\nWe will make AI systems accountable\nAccountability for the outcomes of an AI system lies not with the system itself but is\napportioned between those who design, develop and deploy it; Example: A foreign country has a government service which identifies parents who owe\nmoney in child maintenance. The data matching process is often incorrect due to misspelled\nDevelopers should make efforts to mitigate the risks inherent in the systems they design; names or missing data which results in some individuals being incorrectly targeted\nautomatically by the system with the result being a large bill, poor credit ratings and even\nAI systems should have built-in appeals procedures whereby users can challenge significant\nfreezing wages. The recourse for individuals who are incorrectly targeted is time-consuming\ndecisions;\nand not straightforward . If the potential impact of incorrect decisions had been assessed,\nmitigation measures such a user-friendly review procedure could have been set up.\nAI systems should be developed by diverse teams which include experts in the area in which\nthe system will be deployed; and\nAI systems should be subject to external audit and decision quality assurance. b AI operators should identify the likely impact of incorrect automated decisions on AI\nsubjects and, in the case where incorrect decisions are likely to cause significant cost or\ninconvenience, consider mitigating measures.\n3.2.2 Guidelines\nc AI operators should consider internal risk assessments or ethics frameworks as a means to\n3.2.2.1 facilitate the identification of risks and mitigating measures.\nAccountability for the outcomes of an AI system should not lie with the system itself\nd In designing AI systems to inform significant decisions, AI developers should consider\na Accountability for loss or damages resulting from the application of AI systems should not measures to maintain data accuracy over time, including:\nbe attributed to the system itself.\n\u2022 the completeness of the data.\nb AI operators and AI developers should consider designating individuals to be responsible for \u2022 timely update of the data, and.\ninvestigating and rectifying the cause of loss or damage arising from the deployment of AI\n\u2022 whether the context in which the data was collected affects its suitability for the\nsystems.\nintended use case.\nExample: A border camera scanning for predictors of risk may misinterpret a \u201ctic\u201d of an\nindividual with Tourette syndrome as suspicious. These can manifest in a diverse fashion, and\nshould not cause this person to undergo secondary inspection every time they pass through\nthe border . If the data is updated after the first case is encountered then it would avoid\ncausing inconvenience on subsequent visits.\n5 Government of Canada. (2018). Responsible AI in the Government of Canada. Digital Disruption White Paper Series. Version 2.0, p.26. Retrieved from:\nhttps://docs.google.com/document/d/1Sn-qBZUXEUG4dVk909eSg5qvfbpNlRhzIefWPtBwbxY/edit\n4 Cabinet Office (UK), Data Science Ethical Framework, Version 1.0, licensed under the Open Government License v3.0, p. 13 6 Stoica, I. et. al.,2017, A Berkeley View of Systems Challenges for AI, p. 2, https://arxiv.org/pdf/1712.05855.pdf\n20 21e AI developers and AI operators should tune AI models periodically to cater for changes to e Be mindful that there might be fundamental tensions between different principles and\ndata and/or models over time. requirements. Continuously identify, evaluate, document and communicate these trade-\noffs and their solutions.\nf AI operators should consider whether AI systems trained in a static environment will display\nmodel instability when deployed in dynamic environments. Adopt a Trustworthy AI assessment list when developing, deploying or using AI systems,\nf\nand adapt it to the specific use case in which the system is being applied.\nExample: AI systems will need to be able to adapt to the changes in the environment that g Keep in mind that such an assessment list will never be exhaustive. Ensuring Trustworthy\nthey are deployed in. For example, a self-driving car would need to quickly adapt to unexpected AI is not about ticking boxes, but about continuously identifying and implementing\nand dangerous road by learning in real time from other cars that have successfully dealt with requirements, evaluating solutions, ensuring improved outcomes throughout the AI\nthese conditions. In addition, such mission-critical applications must handle noisy inputs system\u2019s lifecycle, and involving stakeholders in this.\nand defend against malicious actors .\n3.2.2.4\nAI subjects should be able to challenge significant automated decisions concerning them and,\ng AI developers should collaborate with AI Operators to train models using historical data where appropriate, be able to opt out of such decisions\nfrom the Operator.\na AI operators using AI systems to inform significant decisions should provide procedures by\nwhich affected AI subjects can challenge a specific decision concerning them.\nh AI Operators should consider working with their vendors (AI developers) to continually\nmonitor performance.\nb AI operators should consider such procedures even for non-significant decisions.\ni AI Operators should subject AI systems informing significant decisions to quality checks\nat least as stringent as those that would be required of a human being taking the same c AI operators should make affected AI subjects aware of these procedures and should\ndesign them to be convenient and user-friendly.\ndecision.\nd AI operators should consider employing human case evaluators to review any such\n3.2.2.3\nchallenges and, when appropriate, overturn the challenged decision.\nAI systems informing critical decisions should be subject to appropriate external audit\na When AI systems are used for critical decisions, external auditing of the AI systems in\nquestion should be used as a means to ensure meaningful standards of transparency and Example: A bank allows customers to apply for a loan online by entering their data. The\naccountability are upheld. bank uses an AI system to automatically determine whether to give the loan and what the\ninterest rate should be. They provide users with an option to contest the decision and have\nb In the case that critical decisions are of civic interest, public release of the results of the it reviewed by a human . They also require that customers justify their challenge by filling in\naudit should be considered as a means of ensuring public processes remain accountable a form, which assists the case reviewer and deters customers from challenging a decision\nto those affected by them. without good reason.7\nIn the case that critical decision are life and death decisions, these decisions should be\nc\nsupported by further validation and verification via a human operator.\nd Facilitate traceability and auditability of AI systems, particularly in critical contexts or\nsituations.\n7 Adapted from EU Commission, Can I be subject to automated individual decision-making, including profiling? Retrieved from: https://ec.europa.eu/\ninfo/law/law-topic/data-protection/reform/rights-citizens/my-rights/can-i-be-subject-automated-individual-decision-making-including-profiling_\nen#example\n22 23e AI operators should consider instituting an opt-out mechanism for significant automated 3.2.2.7\ndecisions. AI operators should understand the AI systems they use sufficiently to assess their suitability\nfor the use case and to ensure accountability and transparency\nf AI operators could consider \u201ccrowd challenge\u201d mechanisms whereby a critical number of\ncomplaints triggers an investigation into the fairness and/or accuracy of a decision-making\na In the case of critical decisions, AI operators should avoid using AI systems that cannot be\nprocess as a whole.\nsubjected to meaningful standards of accountability and transparency.\n3.2.2.5 b AI developers should consider notifying customers and AI operators of the use cases for\nAI systems informing significant decisions should not attempt to make value judgements on which the system has been designed, and those for which it is not suitable.\npeople\u2019s behalf without prior consent\na When informing an AI subject about significant choices they will make, AI systems should\nnot unreasonably restrict the available options or otherwise attempt to influence their value\njudgements without the explicit consent of the AI subject in question.\n3.2.2.6\nAI systems informing significant decisions should be developed by diverse teams with\nappropriate backgrounds\na AI developers who develop AI systems which may be used to assist in making critical\ndecisions should involve in the process experts with a background in social science, policy,\nor another subject which prepares them to evaluate the broader societal impact of their\nwork.\nb Development of AI systems informing significant decisions should include consultation\nwith experts in the field in which the system will be deployed.\nExample: An app that uses AI to assess medical symptoms and has a large user base had\nto face regulatory scrutiny because of number of complaints from doctors. They warned\nthat the application can miss signs of serious illness. A number of different shortcomings\nwere identified by doctors, some of which the company could address and resolve.8\n8 Financial Times. 2018. High-profile health app under scrutiny after doctor\u2019s complaints. Retrieved from: https://www.ft.com/content/19dc6b7e-8529-\n11e896-dd-fa565ec55929\n24 253.3 Transparent AI (Principle 3)\nc To facilitate the above, AI developers and AI operators should consider documenting the\nfollowing information during the design, development and deployment phases, and retaining\n3.3.1 Principle this documentation for a length of time appropriate to the decision type or industry:\n\u2022 the provenance of the training data, the methods of collection and treatment, how the data was\nWe will make AI systems transparent\nmoved, and measures taken to maintain its accuracy over time;\n\u2022 the model design and algorithms employed; and\nDevelopers should build systems whose failures can be traced and diagnosed. \u2022 changes to the codebase, and authorship of those changes.\nPeople should be told when significant decisions about them are being made by AI. d Where possible given the model design, AI developers should consider building in a means\nby which the \u201cdecision journey\u201d of a specific outcome (i.e. the component decisions leading\nWithin the limits of privacy and the preservation of intellectual property, those who deploy to it) can be logged.\nAI systems should be transparent about the data and algorithms they use.\nResponsible disclosures should be provided in a timely manner, and provide reasonable\nExample: A technology company has a product which is designed to assist in medical\njustifications for AI systems outcomes. This includes information that helps people\ndiagnosis. It documents each stage of its reasoning and relates it back to the input data.9\nunderstand outcomes, like key factors used in decision making.\n3.3.2 Guidelines\n3.3.2.1\nTraceability should be considered for significant decisions, especially those that have the\npotential to result in loss, harm or damage.\na For AI systems which inform significant decisions, especially those with the potential to\ncause loss, harm or damage, AI developers should consider building in traceability (i.e. the\nability to trace the key factors leading to any specific decision).\nb Organizations should ensure that harms caused through AI systems are investigated and\nredressed, by enacting strong enforcement mechanisms and remedial actions, to make\ncertain that human rights and the rule of law are respected in the digital world and in the\nphysical world.\n9 See IBM WatsonPaths\n26 273.3.2.2 3.4 Explainable AI (Principle 4)\nPeople should be informed of the extent of their interaction with AI systems\n3.4.1 Principle\na AI operators should inform AI subjects when a significant decision affecting them has been\nmade by an AI system.\nWe will make AI systems as explainable as\ntechnically possible\nExample: A small claims court adjudicates minor civil matters such as debt collection and\nevictions. They introduce an AI system to suggest the outcome of a ruling. At the time of the\nDecisions and methodologies of AI systems which have a significant effect on individuals\nruling the plaintiff and defendant are notified that the decision was assisted by an AI system.\nshould be explainable to them, to the extent permitted by available technology.\nThe court also provides an explanation for the decision.\nIt should be possible to ascertain the key factors leading to any specific decision that\ncould have a significant effect on an individual.\nb If an AI system can convincingly impersonate a human being, it should do so only after\nIn the above situation we will provide channels through which people can request such\nnotifying the AI subject that it is an AI system.\nexplanations.\nExample: A technology company produces a conversational AI agent which can make 3.4.2 Guidelines\nsome phone calls on behalf of its users. Those who receive the calls may believe that they\n3.4.2.1\nare speaking to a human. Therefore, the company programs the agent to identify itself at\nAI operators could consider providing affected AI subjects with a high-level explanation of\nthe start of every conversation.\nhow their AI system works\na AI operators could consider informing the affected AI subjects in understandable, non-\ntechnical language of:\n\u2022 the data that is ingested by the system;\n\u2022 the types of algorithms employed;\n\u2022 the categories into which people can be placed; and\n\u2022 the most important features driving the outcomes of decisions\n\u2022 A comprehensive list of feature engineering and models that was considered during\nthe model building phase.\nExample: A person turned down for a credit card might be told that the algorithm took\ntheir credit history, age, and postcode into account, but not learn why their application was\nrejected10 .\n28 29b For non-sensitive public sector use cases designed for the common good, AI operators\ncould consider making source code, together with an explanation of the workings of the AI Example: The UK\u2019s NHS developed a tool called Predict, which allows women with breast\nsystem, available either publicly or upon request (this should be done only if there is low risk cancer to compare their case to other women who have had the same condition in the past,\nof people \u2018gaming the system\u2019). and visualize the expected survival rate under various treatment options. The website has an\nexplanation page which shows the weights behind various factors and contains a description\nc The AI operator should maintain necessary documentation that provides elaboration and of the underlying mathematics12 .\nclarification on how the algorithm works, for example documentation of processes, decision\nmaking flow charts of the system, and how the appeal process is embedded.\nd An individual should have the ability to contest and seek effective redress against decisions c In the case that such explanations are available, they should be easily and quickly accessible,\nmade by AI systems. These must be addressed by the group or team supporting these free of charge and user-friendly.\nmodels.\ne Develop appropriate impact indices for the evaluation of AI system technological 3.5 Robust, Safe and Secure AI (Principle 5)\ninterventions from multiple perspectives.\n3.5.1 Principles\n3.4.2.2\nAI operators should consider providing affected AI subjects with a means to request\nexplanations for specific significant decisions, to the extent possible given the state of present AI systems will be technically robust\nresearch and the choice of model\nAI systems should be technically robust with a preventative approach to risks which\na AI operators should consider providing a means by which people affected by a significant\noperates in a manner such that they reliably behave.\ndecision informed by AI can access the reasoning behind that decision.\nAI Developers should ensure AI systems will not cause any unintentional harm and\nadverse impacts.\nExample: The US Consumer Financial Protection Bureau requires that creditors in the US\nwho reject credit applications must explain to the applicant the principal reason(s) why their\nAI systems should be resilient to attacks and security such as data poisoning and\napplication was rejected (e.g. \u201clength of residence\u201d or \u201cage of automobile\u201d)11 . In particular,\nmodel leakage.\n\u201cstatements that the adverse action was based on the creditor`s internal standards or\npolicies or that the applicant, joint applicant, or similar party failed to achieve a qualifying AI systems should have safeguards that enable a fallback plan in case of problems.\nscore on the creditor`s credit scoring system are insufficient\u201d.\nAI system results should be reproducible.\nb Where such explainability is not possible given available technology, AI operators should\nconsider compromises such as counterfactual reasoning, or listing the most heavily\nweighted factors contributing to the decision.\n11 Consumer Financial Protection Bureau, 12 CFR Part 1002 - Equal Credit Opportunity Act (Regulation B), \u00a7 1002.9 Notifications, Retrieved from https://www.\n10 The Guardian. AI watchdog needed to regulate automated decision-making, say experts. Retrieved from: https://www.theguardian.com/technology/2017/ consumerfinance.gov/policy-compliance/rulemaking/regulations/1002/\njan/27/ai-artificial-intelligence-watchdog-needed-to-prevent-discriminatory-automated-decisions 12 Predict website, accessible at http://www.predict.nhs.uk/predict_v2.1/legal/algorithm\n30 313.5.2 Guidelines\n3.5.2.1 3.5.2.3\nAI Operators should continue conducting vulnerability assessments, verification of AI system AI Operators should assure end users of the system\u2019s reliability through documentation, and\ndifferent behaviors in unexpected situations and for any dual-use case, to include: operationalizing processes for testing and verification of desired outcomes , specifically:\na Putting measures to ensure integrity and resilience of IA system against potential attacks. a AI system should be designed with an approach to continue monitoring if the system meets\ngoals, purposes and intended application.\nb AI Operators should define an approach to ensure results are reproducible through clear\nExample: IBM researchers presented a novel approach to exploiting AI for malicious\nprocess and documentation.\npurposes. Their system, DeepLocker embeds AI capabilities within the malware itself in order\nto improve its evasion techniques. It uses an Artificial Intelligence model to identify its target\nc AI Operators should publish documentation to assure system robustness to end users.\nusing indicators like facial recognition, geolocation and voice recognition.\n3.5.2.4\nAI systems will be safe, secure and controllable by humans\nb Verifying how AI systems behave in unexpected situations and environments.\na Safety and security of people, be they operators, end-users or other parties, will be of\nc Taking preventative measures against any possible system dual-case use. paramount concern in the design of any AI system.\nb AI systems should be verifiably secure and controllable throughout their operational\n3.5.2.2\nlifetime, to the extent permitted by technology.\nAI operators should define a suitable fall back plan and test it to maintain readiness in\nunexpected situations and in high safety risks level, to include:\nc The continued security and privacy of users should be considered when decommissioning\na Developing a plan to measure and assess potential safety risks to you or any third party AI systems.\nfrom technology use accidental or malicious misuse.\nd AI systems that may directly impact people\u2019s lives in a significant way should receive\nb Define thresholds for system acceptable results and governance procure to fall back on commensurate care in their designs.\nalternative defined and tested plans.\ne Such systems should be able to be overridden or their decisions reversed by designated\nc Considering an insurance policy to mitigate risks arising from potential damages. individuals.\nExample: Adopting a risk-based approach to procurement and clearly communicating it to\nvendors can help address such issues by giving the procuring organization advance notice\nof the specific oversight capabilities it will need in future stages of the system lifecycle,\npreventing vendors from presenting intellectual property arguments against required testing,\nmonitoring and auditing of their AI systems going forward and rewarding vendors\u2014of all\nsizes\u2014that are more advanced and responsive in their responsible AI efforts.\n13 IBM. https://documents.trendmicro.com/assets/white_papers/wp-malicious-uses-and-abuses-of-artificial-intelligence.pdf\n32 333.6.2 Guidelines\n3.5.2.5\nAi systems should not be able to autonomously hurt, destroy or deceive humans\n3.6.2.1\na AI systems should be built to serve and inform, and not to deceive and manipulate.\nAI should be developed to align with human values, contribute to human flourishing and\nbenefit society.\nb Nations should collaborate to avoid an arms race in lethal autonomous weapons, and\nsuch weapons should be tightly controlled. a Society should be consulted in a representative fashion to inform the development of AI.\nc Active cooperation should be pursued to avoid corner-cutting on safety standards. b Stakeholders throughout society should be involved in the development of AI and its\ngovernance.\nSystems designed to inform significant decisions should do so impartially.\nd\nc Stakeholders should proactively engage in responsible stewardship of trustworthy AI in\npursuit of beneficial outcomes for people and the planet.\n3.6 Human Centered AI (Principle 6)\nd Organizations should prioritize having all their stakeholders learn about wellbeing metrics\nas a potential determinant of how they create, deploy, market and monitor their AI\n3.6.1 Principles technologies.\nWe will give AI systems human values and\nExample: IEEE P7010 recommended practice14 establishes wellbeing metrics relating to\nmake them beneficial to society human factors directly affected by intelligent and autonomous systems and establishes a\nbaseline for the types of objective and subjective data these systems should analyze and\nGovernment will support the research of the beneficial use of AI. include (in their programming and functioning) to proactively increase human wellbeing.\nStakeholders throughout society should be involved in the development of AI and its\ngovernance.\n3.6.2.2\nSystematic analyses that examine the ethics of designing affective systems to nudge human\nDesign AI systems to adopt, learn and follow the norms and values of the community\nbeings prior to deployment are needed.\nthey serve.\nSystematic analyses that examine the ethics of designing affective systems to nudge a Nudging in AI systems should have an opt-in system policy with explicit consent.\nhuman beings prior to deployment are needed.\nb We recommend that when appropriate, an affective system that nudges human beings\nDecisions related to lethal force, life and death should not be delegated to AI systems. should have the ability to accurately distinguish between users, including detecting\nRules and standards should be adopted to ensure effective human control over those characteristics such as whether the user is an adult or a child. Additional protections must\ndecisions. be put in place for vulnerable populations, such as children, when informed consent cannot\nbe obtained, or when it may not be a sufficient safeguard.\nc AI systems with nudging strategies must be carefully evaluated, monitored, and controlled.\n14 IEEE. https://sagroups.ieee.org/7010/\n34 353.6.2.3 f In situations where it is needed, human rights impact assessments and human rights due\nHumanity should retain the power to govern itself and make the final decision, with AI in an diligence, human determination codes of ethical conduct, or quality labels and certifications\nassisting role. intended to promote human centered values and fairness should be considered.\ng Mechanisms should be put into place to receive external feedback regarding AI systems\na Decisions related to lethal force, life and death should not be delegated to AI systems. Rules\nthat potentially infringe on fundamental rights.\nand standards should be adopted to ensure effective human control over those decisions.\nb Responsible parties (e.g., parents, nurse practitioners, social workers, and governments)\nshould be trained to detect the influence due to AI and ineffective mitigation techniques.\nIn the most extreme cases it should always be possible to shut down harmful AI system.\nc Those actions undertaken by an affective system that are most likely to generate an\nemotional response should be designed to be easily changed.\nd Users should be able to make informed autonomous decisions regarding AI systems. They\nshould be given appropriate knowledge and tools to comprehend and interact with AI\nsystems to satisfactory degree and, where possible to reasonably self-assess or challenge\nthe system.\n3.6.2.4\nAI systems should be designed in a way that respects the rule of law, human rights, society\nvalues, and should include appropriate safeguards to ensure a fair and just society.\na AI Developers should design AI systems to adopt, learn and follow the norms and values of\nthe community they serve.\nb Organizations could identify the norms of the specific community in which AI systems are\nto be deployed in. In particular, pay attention to the norms relevant to the kinds of tasks that\nthe AI systems are designed to perform. These could be documented as well as how these\nnorms are addressed by the AI system.\nc To respond to the dynamic change of norms in society the AI system could be able to adjust\nits existing norms and learn new ones, while being transparent about these changes.\nd Designers should consider forms and metrics for assessing an AI system\u2019s norm conformity\nover the lifetime of the system (e.g. human-machine agreement on moral decisions,\nverifiability of AI decisions, justified trust).\ne The norm identification process must document the similarities and differences between\nthe norms that humans apply to other humans and the norms they apply to AI systems.\nNorm implementations should be evaluated specifically against the norms that the\ncommunity expects the AI system to follow.\n36 373.7 Sustainable and Environmentally Friendly AI (Principle 7)\nd While there is no standardized means of reporting on the environmental impact of AI\nsystems, where possible, model development and commissioning should make carbon\n3.7.1 Principle impacts a core consideration alongside functional and business requirements.\ne Smaller models should be considered \u2013 shrinking down the model size and using fewer\nWe will promote sustainable and environmentally compute cycles (to balance financial and performance costs with the end performance of\nthe model).\nfriendly AI\nf Carbon awareness should be considered \u2013 adjusting the operational parameters of the AI\nAI systems should be used to benefit all human beings, including future generations. The\nsystem to dynamically select the best time and location for energy use from the grid can\nenvironment is fundamental to this.\nreduce its carbon footprint.\nThroughout the AI lifecycle, implementations should only be carried out on the basis of a full\nunderstanding and acknowledgement of implications for sustainability and environment.\nExample: In agriculture, AI can transform Example: Google`s DeepMind division\nAn AI system\u2019s development, deployment and use, should be assessed via critical examination production by better monitoring and has developed AI that teaches itself to\nof resource usage and energy consumption.\nmanaging environmental conditions minimize the use of energy to cool Google`s\nand crop yields. AI can help reduce both data centers. As a result, Google reduced\nMechanisms to measure environmental impact due to type of energy use and processing\nfertilizer and water, all while improving its data center energy requirements by\npower provided by data centers should be established.\ncrop yields. 40% 15.\n3.7.2 Guidelines\n3.7.2.1 Throughout the AI lifecycle, implementations should always be carried out after full\nunderstanding and acknowledgement of AI implications on sustainability and environment.\na The application of Artificial Intelligence to address sustainability challenges are well\nunderstood. Building and running green, sustainable AI systems is of increasing importance,\ngiven the large carbon footprint that they can generate and the wider context of addressing\nclimate change.\nb AI model development, and by extension commissioning, should therefore seek to balance\ntechnical performance with energy consumption and environmental impact.\nc Efforts should be made to estimate and understand sustainability and environmental\nimpact across the AI lifecycle (e.g. the energy consumption costs associated with model\ntraining, and CO2 emissions and cloud compute costs associated with the deployment\nand running of the system).\n15 https://www.deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-by-40\n38 393.8 Privacy Preserving AI (Principle 8)\n3.8.2.2\nEstablish an oversight mechanism for data collection, storage and processing and use across\nyour organization\n3.8.1 Principle\na Assess who can access data and under which conditions.\nWe will respect people\u2019s privacy\nb Assign specific responsibilities and role for Data Protection Officers.\nAI systems should respect privacy and use the minimum intrusion necessary.\nc Prevention of harm to privacy necessitates adequate data governance that covers the\nquality and integrity of the data used, its relevance in light of the domain in which the\nAI systems should uphold high standards of data governance and security,\nprotecting personal information. AI systems will be deployed, its access protocols and the capability to process data in a\nmanner that protects privacy.\nSurveillance or other AI-driven technologies should not be deployed to the extent\nof violating internationally and/or UAE\u2019s accepted standards of privacy and human d AI systems must guarantee privacy and data protection throughout a system\u2019s entire\ndignity and people rights. lifecycle. This includes the information initially provided by the user, as well as the\ninformation generated about the user over the course of their interaction with the system\nPrivacy by design should be embedded in AI systems and where possible AI\n(e.g. outputs that the AI systems generated for specific users or how users responded to\nalgorithm should have adequate privacy impact assessments.\nparticular recommendations). Digital records of human behavior may allow AI systems\nto infer not only individuals\u2019 preferences, but also their age, gender, religious or political\nAdequate data protection frameworks and governance mechanisms should be\nviews. To allow individuals to trust the data gathering process, it must be ensured that data\nestablished in a multi-stakeholder approach and ensured throughout the life cycle\ncollected about them will not be used to unlawfully or unfairly discriminate against them.\nof AI systems.\ne In any given organization that handles individuals\u2019 data (whether someone is a user of the\nAI developers and operators should strive to strike the balance between privacy\nsystem or not), data protocols governing data access should be put in place in line with\nrequirements, individual rights and innovation growth and society benefits.\nnational privacy legislation (be these universal or sector-specific). These protocols should\noutline who can access data and under which circumstances. Only duly qualified personnel\n3.8.2 Guidelines\nwith the competence and need to access individual\u2019s data should be allowed to do so.\n3.8.2.1\nEstablish mechanism for users to flag issues related to privacy or data protection\na AI Operators should review the system for proper consent logging, ability of users to revoke\npermission whenever applicable.\nb Consider training AI model without or with minimal user of potentially sensitive or personal\ndata.\nc Use measures to enhance privacy such as encryption, anonymization and aggregation.\n40 41f Algorithmic systems require adequate privacy impact assessments, which also include\nsocietal and ethical considerations of their use and an innovative use of the privacy by\ndesign approach. AI actors need to ensure that they are accountable for the design and\nimplementation of AI systems in such a way as to ensure that personal information is\nprotected throughout the life cycle of the AI system.\ng Establish data policies or equivalent frameworks, or reinforce existing ones, to ensure full\nsecurity for personal data and sensitive data, which, if disclosed, may cause exceptional\ndamage, injury or hardship to individuals. Examples include data relating to offences,\ncriminal proceedings and convictions, and related security measures; biometric, genetic\nand health data; and personal data such as that relating to race, descent, gender, age,\nlanguage, religion, political opinion, national origin, ethnic origin, social origin, economic or\nsocial condition of birth, or disability and any other characteristics.\nh Promote mechanisms, such as open repositories for publicly funded or publicly held data,\nsource code and data trusts, to support the safe, fair, legal and ethical sharing of data,\namong others.\ni Promote and facilitate the use of quality and robust datasets for training, development and\nuse of AI systems, and exercise vigilance in overseeing their collection and use.\nExample: with the help of AI & ML algorithms16 , Synthetic data can be created to enhance\nprivacy. The data created will have the same statistical characteristics for the testing\nenvironment and where a third party can have access.\n16 https://research.aimultiple.com/privacy-enhancing-technologies/\n42 43Microsoft. 2022. Microsoft Responsible AI Standard, v2. General Requirements. June 2022.\n04\nRetrieved from:\nhttps://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4ZPmV\nISO. 2022. Proposed ISO Standard on Risk Management of AI: What Businesses Should Know.\nBibliography\nMay 2022. Retrieved from:\nhttps://unesdoc.unesco.org/ark:/48223/pf0000381137/PDF/381137eng.pdf.multi\nGSMA 2022. The AI Ethics Playbook. Implementing ethical principles into everyday business.\nFebruary 2022. Retrieved from:\nhttps://www.gsma.com/betterfuture/wp-content/uploads/202201//The-Mobile-Industry-Ethics-\nPlaybook_Feb-2022.pdf\nOECD. 2022. AI Policy Observatory. Retrieved from:\nhttps://standards.ieee.org/wp-content/uploads/import/documents/other/ead_v2.pdf\nUNESCO. 2022. Recommendation on the Ethics of Artificial Intelligence. Adopted in November\n2021. Retrieved from:\nhttps://unesdoc.unesco.org/ark:/48223/pf0000381137/PDF/381137eng.pdf.multi\nEuropean Commission. 2021. Proposal for a Regulation of the European Parliament and of the\nCouncil Laying Down Harmonized Rules on Artificial Intelligence (Artificial Intelligence Act) and\nAmending Certain Union Legislative Acts. Retrieved from:\nhttps://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A52021PC0206\nEuropean Commission. High Level Expert Group on Artificial Intelligence ( AI HLEG ). Ethics\nGuidelines for Trustworthy AI . April 2019. Retrieved from:\nhttps://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai\nCentre for Data Ethics and Innovation. UK Government: The roadmap to an effective AI assurance\necosystem. December 2021. Retrieved from:\nhttps://www.gov.uk/government/publications/the-roadmap-to-an-effective-ai-assurance-\necosystem\nThe New York City Council. 2021. A Local Law to amend the administrative code of the city of New\nYork, in relation to automated employment decision tools. Retrieved from:\nhttps://legistar.council.nyc.gov/LegislationDetail.aspx?ID=4344524&GUID=%20B051915D-A9AC-\n451E-81F86596032-FA3F9&\nUNESCO, Recommendation on the Ethics of Artificial Intelligence. 2021. Retrieved from:\nhttps://unesdoc.unesco.org/ark:/48223/pf0000381137\n44 45AI NOW Institute. Algorithmic accountability for the public sector. August 2021. Retrieved from: CNIL. (2017). How can humans keep the upper hand? The ethical matters raised by algorithms\nhttps://www.opengovpartnership.org/documents/algorithmic-accountability-public-sector/ and artificial intelligence. Retrieved from:\nhttps://www.cnil.fr/sites/default/files/atoms/files/cnil_rapport_ai_gb_web.pdf\nIEEE 2019. Ethically Aligned Design. Version 2 \u2013 For Public Discussion. Retrieved from:\nhttps://standards.ieee.org/wp-content/uploads/import/documents/other/ead_v2.pdf Executive Office of the President of the United States. National Science and Technology Council.\nNetworking and Information Technology Research and Development Subcommittee. (2016). The\nnational artificial intelligence research and development strategic plan. Retrieved from:\nIEEE 2017. Ethically Aligned Designed : A vision for Prioritizing Human Well-being with\nhttps://www.nitrd.gov/PUBS/national_ai_rd_strategic_plan.pdf\nAutonomous and Intelligent Systems. Retrieved from:\nhttps://standards.ieee.org/wp-content/uploads/import/documents/other/ead_v2.pdf\nExecutive Office of the President of the United States National Science and Technology Council.\nCommittee on Technology. (2016). Preparing for the future of artificial intelligence. Retrieved from:\nGoogle. AI Principles 2021. Retrieved from:\nhttps://obamawhitehouse.archives.gov/sites/default/files/whitehouse_files/microsites/ostp\nhttps://ai.google/principles/\nNSTC/preparing_for_the_future_of_ai.pdf\nMicrosoft. Responsible AI Standard v2 (June 2022). Retrieved from:\nThe Headquarters for Japan\u2019s Economic Revitalization. (2015). New Robot Strategy. Japan\u2019s\nhttps://www.microsoft.com/en-us/ai/responsible-ai\nRobot Strategy. Vision, Strategy, Action Plan. Retrieved from:\nhttp://www.meti.go.jp/english/press/2015/pdf/0123_01b.pdf\nThe ECONOMIST Group 2022 . Pushing forward: the future of AI in the Middle East and North\nAfrica Report. Retrieved from:\nTreasury Board of Canada Secretariat. (2018). Responsible Artificial Intelligence in the\nhttps://impact.economist.com/perspectives/sites/default/files/google_ai_mena_report.pdf\nGovernment of Canada. Digital Disruption White Paper Series. Version 2.0.\nhttps://docs.google.com/document/d/1Sn-qBZUXEUG4dVk909eSg5qvfbpNlRhzIefWPtBwbxY/\nPDPC. (2018, June 5). Discussion paper on Artificial Intelligence (AI) and Personal Data.\nedit\nSingapore: Personal Data Protection Commission Singapore (PDPC). Retrieved from:\nhttps://www.pdpc.gov.sg/-/media/Files/PDPC/PDF-Files/Resource-for-Organisation/AI/\nThe Toronto Declaration: Protecting the right to equality and non-discrimination in machine\nDiscussion-Paper-on-AI-and-PD---050618.pdf\nlearning systems. Toronto, Canada: Amnesty International and Access Now. Retrieved from:\nhttps://www.accessnow.org/cms/assets/uploads/201808//The-Toronto-Declaration_ENG_08-\nITI. AI Policy Principles. Retrieved from:\n2018.pdf\nhttps://www.itic.org/public-policy/ITIAIPolicyPrinciplesFINAL.pdf\nhttp://data.parliament.uk/writtenevidence/committeeevidence.svc/evidencedocument/artificial-\nCabinet Office.(2016, May 19). Data Science Ethical Framework. Retrieved from: intelligence-committee/artificial-intelligence/oral/73546.html (house of lords select committee\nhttps://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/ oral evidence, Q61)oral evidence, Q61)\nfile/524298/Data_science_ethics_framework_v1.0_for_publication__1_.pdf\nHouse of Lords Select Committee on Artificial Intelligence. (2018). AI in the UK: ready, willing and\nEuropean Parliament. (2017, February 16). European Parliament resolution of 16 February 2017 able?\nwith recommendations to the Commission on Civil Law Rules on Robotics (20152103/(INL)). https://publications.parliament.uk/pa/ld201719/ldselect/ldai/100100/.pdf\nRetrieved from:\nhttp://www.europarl.europa.eu/sides/getDoc.do?pubRef=-//EP//TEXT+TA+P8-TA-2017- House of Commons Science and Technology Select Committee. Algorithms in Decision Making.\n0++0051DOC+XML+V0//EN https://publications.parliament.uk/pa/cm201719/cmselect/cmsctech/351351/.pdf\nVillani, C. (2018). For a meaningful Artificial Intelligence towards a French and European strategy.\nRetrieved from:\nhttps://www.aiforhumanity.fr/pdfs/MissionVillani_Report_ENG-VF.pdf\n46 47", "metadata": {"country": "UAE", "year": "2021", "legally_binding": "yes", "binding_proof": "Law", "date": "01/01/2021", "regulator": "UAE Government", "type": "law", "status": "enacted", "language": "English", "use_cases": "[3, 6]"}}
{"_id": "686b61636e4e9653b2a68b1a", "title": "UAE Strategy for Artificial Intelligence 2031", "source": "https://ai.gov.ae/wp-content/uploads/2021/07/UAE-National-Strategy-for-Artificial-Intelligence-2031.pdf", "text": "\u0627\u0644\u0628\u0631\u0646\u0627\u0645\u062c \u0627\u0644\u0648\u0637\u0646\u064a \u0644\u0644\u0630\u0643\u0627\u0621 \u0627\u0644\u0627\u0635\u0637\u0646\u0627\u0639\u064a\nNATIONAL PROGRAM FOR ARTIFICIAL INTELLIGENCE\nUAE\nNATIONAL STRATEGY\nFOR ARTIFICIAL INTELLIGENCE\n2031\n\u039f\n\u039f\n\u0627\u0644\u0645\u0646\u062f \u0627\u0644\u0625\u0623\u0645\u0627\u0631\u0627\u062a \u0627\u0644\u0639\u0631\u0628\u0633\u0629\nVersion | 2018\nUAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 3\nWE WILL TRANSFORM THE UAE INTO A\nWORLD LEADER IN AI BY INVESTING IN\nPEOPLE AND INDUSTRIES THAT ARE KEY\nTO OUR SUCCESS.\nPage 6 MINISTERIAL FORWARD\nPage 8 EXECUTIVE SUMMARY\nPage 10 WHERE THE UAE HAS OPPORTUNITIES TO LEAD\n Page 10 Industry Assets & Emerging Sectors\n Page 13 Smart Government\n Page 13 Data Sharing and Governance\n Page 14 New Generation of Regional Talent\nPage 18 EIGHT STRATEGIC OBJECTIVES\nPage 20 OBJECTIVE 1: BUILD A REPUTATION AS AN AI DESTINATION\n Page 20 UAI Brand\nPage 22 OBJECTIVE 2: INCREASE THE UAE COMPETITIVE ASSETS IN PRIORITY\n SECTORS THROUGH DEPLOYMENT OF AI\n Page 22 Existing Assets\n Page 23 Emerging Sectors\n Page 25 Proof-of-Concept Support in Priority Sectors\nPage 26 OBJECTIVE 3: DEVELOP A FERTILE ECOSYSTEM FOR AI\n Page 26 AI Network\n Page 26 Applied AI Accelerator\n Page 28 AI Incentive Scheme for Overseas Companies\n Page 29 Business Support for UAE AI Firms\nPage 30 OBJECTIVE 4: ADOPT AI ACROSS CUSTOMER SERVICES TO IMPROVE\n LIVES AND GOVERNMENT\n Page 30 National AI Challenges\n Page 30 UAE Artificial Intelligence and Blockchain Council\nCONTENTS\nPage 32 OBJECTIVE 5: ATTRACT AND TRAIN TALENT FOR FUTURE JOBS\n ENABLED BY AI\n Page 33 Public AI Training\n Page 33 Upskilling Students\n Page 33 Government Training\n Page 34 Professional Upskilling\n Page 34 Employment Transition Support\nPage 36 OBJECTIVE 6: BRING WORLD-LEADING RESEARCH CAPABILITY TO\n WORK WITH TARGET INDUSTRIES\n Page 39 National Virtual AI Institute\n Page 39 Key Thinkers Program\n Page 39 AI Library\nPage 40 OBJECTIVE 7: PROVIDE THE DATA AND SUPPORTING INFRASTRUCTURE\n ESSENTIAL TO BECOME A TEST BED FOR AI\n Page 40 Data Sharing\n Page 40 Secure Data Infrastructure\nPage 42 OBJECTIVE 8: ENSURE STRONG GOVERNANCE AND\n EFFECTIVE REGULATION\n Page 42 National Governance Review\n Page 43 Second Global Governance of Artificial Intelligence Roundtable\n Page 43 Intergovernmental Panel on Artificial Intelligence\nPage 44 CONCLUSION\nPage 45 REFERENCES\nUAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 |\n6 UAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 |\n7\nOmar Sultan Al Olama\nMinister of State for Artificial Intelligence\nMINISTERIAL FORWARD\n\u201cWE WANT THE UAE TO BECOME THE\nWORLD\u2019S MOST PREPARED COUNTRY\nFOR ARTIFICIAL INTELLIGENCE.\u201d\nHis Highness Sheikh Mohammed bin Rashid Al Maktoum\nUAE Vice President and Prime Minister and Ruler of Dubai\nThe role of any new minister is to\nset a direction for their tenure and\norchestrate the vision set by the\nleadership.\nThe appointment as the UAE\u2019s Minister\nof State for Artificial Intelligence has\nbrought the opportunity of a new remit\nwithout pre-existing boundaries and\nconstraints.\nThe challenge in this role is balancing\na global context that is changing\nrapidly with a stable direction for our\nnation. Technological and economic\nopportunities come thick and fast,\nas do other nations vying for global\nleadership in different aspects of AI.\nDeveloping a roadmap for the UAE\u2019s\nrole required us to contextualize\nglobal debates to the challenges and\nopportunities of the Middle East,\nparticularly the unique situation of\nyoung and ambitious Arab countries.\nWe also benefited from conversations\nwith companies, politicians and leading\nexperts on AI from around the world:\nunderstanding what they look for when\ndeciding whom to work with and where\nto work.\nThe UAE government knows its\nstrength is in combining a strong vision\nwith active involvement \u2013 investment,\nlegislation and testbeds - for\ntechnological innovation. Therefore, for\nthis nation, being the most prepared\ncountry means a lot more than\ndeveloping legislation that responds to\nchanges in the world. Instead, it means\nproactively changing the world first. A\nrecent article summarizes this attitude:\n\u201cToday\u2019s biggest tech companies, led\nby Google and Amazon, want to put\nAI at the core of their businesses, and\nthe UAE hopes to do the same for an\nentire nation\n1.\u201d The UAE will build an AI\neconomy, not wait for one.\nThe National AI Strategy \u2013 UAI \u2013 from AI\nready to AI leader.\nThe UAE has a vision to become one\nof the leading nations in AI by 2031\nin alignment with the UAE Centennial\n2071, creating new economic,\neducational, and social opportunities\nfor citizens, governments and\nbusinesses and generating up to AED\n335 billion in extra growth. This report\nbreaks down our approach to this goal.\nAt the annual World Government\nSummit Meeting in February 2018, the\nUAE announced key elements of its\nstrategy \u2013 a welcoming destination for\ndeveloping AI products, new education\nprograms and championing good\ngovernance.\nWe have added more details on\nhow the UAE could become a fast\nadopter of emerging AI technologies\nacross Government, as well as attract\ntop AI talent to experiment with\nnew technologies and work in a\nsophisticated, secure ecosystem to\nsolve complex problems.\nWith this foundation of talent, as well\nas better governance of AI, we will\nhave the right conditions to develop\nnew AI solutions here in the UAE in\nthe coming decade and beyond. These\nnovel technologies have huge economic\npotential, including licensing and export\noverseas. \nUAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 |\n8 UAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 |\n9\nEXECUTIVE SUMMARY\nThe UAE sets a clear vision through its AI\nStrategy, to become the world leader in\nAI by 2031. Implementing this vision on\nthe ground requires rigorous dedication\nand clear steps that outline the path\nfor success. Hence, it is essential to set\nthe foundation, the AI Strategy, with\nclear strategic objectives that outline\nthe initiatives that are essential in\nachieving the milestones. It is notably\nworth mentioning that the AI Strategy\naligns with the UAE Centennial 2071,\nto make the UAE the best country in\nthe world by 2071. The AI Strategy will\ncontribute significantly in education,\neconomy, government development, and\ncommunity happiness through various\nAI technologies implementations in\ndifferent sectors to include energy,\ntourism, and education, to list few.\nThe UAE AI and Blockchain Council will\noverlook the implementation of the AI\nStrategy throughout all emirates but\nultimately, the implementation will be a\nmulti-stakeholders effort and cooperation\nfrom different local and federal entities in\nthe UAE.\nThere are eight strategic objectives\noutlined in the AI Strategy, namely:\n\u2022 Build a reputation as an AI destination. \u2022 Increase the UAE competitive assets in\npriority sectors through deployment of AI.\n\u2022 Develop a fertile ecosystem for AI.\n\u2022 Adopt AI across customer services to\nimprove lives and government.\n\u2022 Attract and train talent for future jobs\nenabled by AI.\n\u2022 Bring world-leading research capability to work with target industries.\n\u2022 Provide the data and supporting\ninfrastructure essential to become a\ntest bed for AI.\n\u2022 Ensure strong governance and effective\nregulation.\nThe UAE has a strong foundation\nconsisting of cohesive and diversified\nmultinational community that is a\nfast adapter to new and emerging\ntechnologies. Therefore, it acts as a\nmagnet that attracts the best talents\nfrom the globe to conduct their\nexperiments on AI solutions in the\nUAE and open the doors to practical\nimplementations in different sectors.\nUAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 10 UAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 11\nWHERE THE UAE HAS\nOPPORTUNITIES TO LEAD\nThe UAE\u2019s vision to become a world\nleader in AI does not mean aiming\nfor leadership across all technologies\nand sectors. The country will focus\non domains where it can have\nworld-leading assets and unique\nopportunities.\nTherefore, the mission for this first\nMinisterial term is to transform the UAE\ninto a world leader in AI by investing in\nthe people and industries that are key\nto the UAE\u2019s success.\nThe UAE will begin through its\nexisting strengths:\n1. Industry Assets & Emerging Sectors\n2. Smart Government\nAnd also focus on opportunities\nwhere it can lead:\n3. Data Sharing And Governance\n4. New Generation Of Regional Talent\nBy 2031, the very best version of the\nUAE would package these strengths\nand opportunities together. For\nexample, early Government adoption of\nAI will come with training for domestic\ntalent. Governance frameworks will\nbe evaluated by testing them in the\nUAE\u2019s industry pilots. The existence of\na strong government and governmentowned commercial sector in the UAE\nprovides novel opportunities for trialing\ngovernance, education and product\ninnovation in combination.\n1. Industry Assets &\n Emerging Sectors\nThe UAE has set priority sectors \u2013 these\nwill be the focus of initial activities.\nThis does not mean that the UAE will\nstop working on AI solutions in other\nsectors where AI can deliver other\nbenefits to society. It is also likely that\nthese priorities will change over time,\nas the UAE economy matures and new\nopportunities arise.\nBut in the first instance, the UAE will\nleverage physical and digital assets in\ntwo of its strongest existing sectors as\npart of adopting and trialing AI. Support\nwill also be given to developments\nin emerging sectors where the UAE\nhas the strong potential economic\ngains and where there are pockets of\nopportunity to lead globally.\nTherefore, the current priority sectors\nare:\nResources & Energy: from existing\ntechnology in the extraction industry\nto renewable energy and innovation in\nutilities.\nLogistics & Transport: longstanding\nair and sea hubs in the UAE make it\na valuable location for piloting new\nsystems in the sector.\nTourism & Hospitality: opportunity for\nglobally becoming first in customersupport AI, creating integrated and\npersonalized services for tourists in the\nUAE.\nHealthcare: a small sector with\nopportunity to be world leading in\nspecific treatments, particularly in rare\ndiseases.\nCybersecurity: a strategic imperative,\ngiven the rise of AI, the UAE will also\nconcentrate on building robust systems\nfor protection.\nEconomic Value of AI\nFurther details on these choices\nare in Objective 2. A core reason for\nchoosing these sectors came down to\nthe potential of AI deployment causing\ndisruption as well as pure economics,\nfor instance the potential of AED\n136 billion gain in services and trade\nsectors played a significant role in\nchoosing Tourism as a priority sector.\nAI in this growing consumer-facing\nsector could likely have spillovers into\nother service sectors. AED 91 billion\nin resources and utilities contributed\nto making energy a priority, as did the\nAED 19 billion in logistics.\nEstimates for global economic gains\nfrom automation technologies - 0.3%\nto 2.2 % growth in compound annual\nproductivity \u2013 are impressive\n2. Using\nthis kind of modeling of year on year\nproductivity gains, PwC estimated that\nAI will contribute AED 353 billion to\nGDP by 2030 (13.6% of GDP).\nGains from increased performance\noutweigh those that come from\nreplacing labor with machines in some\nsectors, which play a major role in the\nyoung and resource-rich economies\nof the Middle East. For example, 85%\nof gains in oil and gas are likely to\nbe in performance rather than labor\nsubstitution. This is similar in the\nredesign of the automotive industry\nor the large changes we are seeing to\nconsumer marketing techniques.\nSpending on AI is also a significant\neconomic factor. International Data\nCorporation estimates annual spending\non AI in the Middle East and Africa to\nreach AED 419.54 million by 2021,\nincreasing 32% a year.\nUsing the UAE\u2019s national statistics,\nthe absolute opportunity to increase\neconomic output based on current\ntechnology capabilities rather than\nfocusing on annual productivity growth\nwas calculated\n3. Assuming automation\nhappens to the full extent it can in\neach industry, there is a potential\ngain of AED 335 billion in increased\neconomic output for the UAE. This is\nthe equivalent to 26% increase. \nUAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 12 UAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 13\nThe UAE is already taking steps to apply AI in innovative ways across government\n\u2013 dynamically adjusting transport timetables to respond to incidents, using AI\nsensors for smart traffic, deploying facial recognition to monitor driver fatigue and\nintroducing chatbots to improve customer service.\nObjective 4 explains how the UAE will take steps to increase the amount of\ngovernment experimentation with AI to improve the lives of its citizens.\n3. Data Sharing and Governance\nIt is part of the UAE\u2019s ethos to turn ambitious visions into deliverable projects.\nThis connection between big ideas and practical implementation will, become\nan asset in AI policy discussions, that can fall easily into abstract or implausible\nscience fiction. Combining hands-on experience with new technologies and global\npolicy development is a strong way to develop a plausible, positive future for AI.\nHow will the UAE ensure AI is used for good?\nPublic debates about AI often focus on whether or not it could take over\nimportant human decisions: from whether we go to war, to who receives\nmedication.\nIt is almost a coincidence that the PwC estimate for 2030 gain in GDP is similar to the overall\nincrease in economic output that we estimated. The methods used to develop these estimates are\ndifferent: the GDP estimate is based on data so far on average annual growth from automation,\nwhereas we calculated the effects of specific technologies in different industries. Data from UAE\nFCSA National Accounts Estimates Tables 12, 13 and 15; Analyzed using the team\u2019s review of AI\napplications and desk research.\n2. Smart Government\nThe UAE public sector is already a leader in smart public service delivery:\nEstimated increase in output due to application of AI\nin industries (AED billion in current prices)3\n103\n19\n05\n39 39\n14\n02\n18\n03\n33\n13\n335 Output increase in current prices, by industry (Billion Dirhams)\nFinance,\nProfessional\nand Other\nServices\nConstruction\nResources\nMining Logistics Government\nand Social\nServices\nRetail Trade ICT Utilities Hospitality Agriculture Total\n86\nManufacturing\n\u201cWe initiated electronic services 16 years ago and\ntoday we are launching a fresh stage relying on\nArtificial Intelligence\u2026 we are seeking to adopt\nall tools and methodologies related to Artificial\nIntelligence to expedite and ensure more efficiency\nfor government services at all levels.\u201d\nHis Highness Sheikh Mohammed bin Rashid Al Maktoum\nUAE Vice President, Prime Minister and Ruler of Dubai\nUAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 14 UAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 15\nOut of the job functions reported\non LinkedIn profiles, operations,\ninformation technology and\nengineering are ranked 1, 4 and 6\nrespectively\n3.\nThere is some evidence that there is a\nsignificant subset of these individuals\nalready starting to combine technical\nskills and business operations. Data\nfrom jobseekers on the Middle East\nand South Asia jobs website, Bayt.\ncom, shows that business analysts\nwith technical skills often already\nhave sophisticated, AI-relevant skills.\n21% of the skills identified among this\ngroup were associated with business\nintelligence software like IBM Cognos,\nMicrosoft Power BI or Qlik Sense.\n20% were to do with data operations\nexperience, for example with Hadoop\nor Apache Pig. 8% were Machine\nLearning & Statistical Modeling tools\nlike Neural Networks\n5.\nThe UAE is also globally competitive\nwhen it comes to the proportion of\nuniversity graduates who study STEM\nsubjects (22% compared to 16% in\nthe US). These graduates already have\nthe base foundational skills relevant\nto AI (computer science, programming\nliteracy, and statistical analysis) and\nso can be rapidly upskilled to become\nAI-ready.\nThere continues to be a range of views\nabout the prospects of AI, and many\npotential future scenarios for AI in the\nUAE societies. There is still time to\nchange what this future will look like,\nmaking it one that more clearly reflects\nthe UAE\u2019s values. The actions we take\ntoday are still very much under human\ncontrol and can still reflect those\nvalues.\nResponding to this opportunity, several\nof the initiatives in this strategy aim to\ndevelop a values-driven approach to AI:\nThe UAE Government will play a direct\nrole in designing and enabling AI\nsystems that create the most value for\nsociety (objective 4). This will also give\nthe UAE practical experience of how\nthese systems operate and allow the\ncountry to identify, ahead of time, any\npotential unintentional consequences.\nThese schemes and the national pilots\nin objective 2 will guide the approach\nto the governance of AI (objective\n8). This approach to governance -\nembedded in worked examples - will\nhelp take the UAE beyond abstract\nstatements to useable guidelines for\nvalues-driven AI.\nThe schemes will advocate these\nguidelines on a global stage, working\nwith other countries and international\ntechnological groups (also objective 8).\nFinally, research that keeps to these\nethical principles will be rewarded\n(objective 6).\n4. New Generation of\n Regional Talent\nA young and growing regional\npopulation is often described only\nin terms of unemployment. Youth\nunemployment in the Arab countries\nand Middle East was 30.6 % in 2016.\nIt remains the highest of any region\nglobally.\nBut the Middle East and North\nAfrica has an unusual segment of\nthe professional workforce. There is\na high proportion of professionals\nalready involved in operations, IT and\nengineering.\nIn fact there are giants in the field of\nAI who have come from some of the\nmost fragile states in the Middle East.\nIyad Rahwan and Oussama Khatib were\nboth born in Aleppo, Syria. Iyad is now\nthe director and principal investigator\nof the Scalable Cooperation group at\nMIT Media Lab. Oussama is a professor\nof computer science at Stanford\nUniversity.\nThe UAE offers access to world-leading\nuniversities and a safe hub for highly\nskilled professionals to re-skill the\nmost in-demand AI roles. The country\nneeds to leverage on its geographic\nposition, and this existing cohort of\ntalent around it.\n14\nProduct Management\nPurchasing\nQuality Assurance\nReal Estate\nLegal\nConsulting\nSupport\nMarketing\nResearch\nMedia &\nCommunication\nProgram & Project\nManagement\nHuman Resources\nHealthcare Services\nCommunity & Social\nServices\nArts & Design\nAccounting\nAdministrative\nEntrepreneurship\nFinance\nEngineering\nEducation\nInformation\nTechnology\nSales\nBusiness Development\nOperations\nMilitary & Protective\nServices\n2016\n02468\n10\n12\nShare of Job Functions in MENA (%)\n4\nUAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 16 UAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 17\nShare of national graduates that are in\nAI-relevant specializations\n(statistics, mathematics, ICT technologies\nand engineering) for selected countries.6\nIndia\n31%\nAustralia\n18%\nUAE\n22%\nUK\n26%\nUSA\n16%\nAustralia 18\nEstonia 26\nFrance 25\nIndia 31\nOman 43\nRepublic of Korea 31\nSaudi Arabia 24\nTurkey 20\nUnited Arab Emirates 22\nUnited Kingdom of Great Britain and 26\nNorthern Ireland\nTunisia 44\nUnited States of America 16\nUAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 18 UAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 19\nThe UAE\u2019s first steps will build a strong brand through AI activities that\ndemonstrate the UAE as a testbed for AI technology. This will come through\nbrokering agreements with international firms to base pilots in the UAE (under\nobjective 2); it means coordinating access to domestic data systems applications\n(objective 3); and it will require government to take the lead in providing AIenhanced services (objective 4).\nThere are also some early steps to be made in starting to build stronger\nfoundations in talent, research, data and governance. Publicly accessible\nAI courses have already begun with large tech partners (objective 5); and\ninternational discussions on the positive use of AI provide an active platform\nfor better governance of these technologies (objective 8).\nAlthough priorities are set out above, there will need to be a complete system\nof support to move from a nation that adopts AI to one that is building and\nexporting it.\nOver time, AI activities will include more significant programs \u2013 from\nfunding for proof-of-concepts to a domestic AI Accelerator. The\nfoundation will also start to grow a new generation of AI-ready talent,\ncomplemented by regular presence from leading global AI researchers\nin the UAE and through playing a leading role\nin international governance initiatives.\nColleagues in the UAE Government, international bodies, educational\ninstitutions and global AI firms will play a significant role in achieving\nsome of these objectives. The Office of the Minister of State for\nArtificial Intelligence (henceforth \u2018AI Office\u2019) will help broker new\npartnerships, particularly in education and governance. In particular,\nthe AI Office aims to support other Ministries to make the most of\nworld-leading AI technologies in their projects and policies, as well as\ntrain a generation of AI-ready talent in the UAE.\nThe second half of this report provides more detail of the direction\nunder each objective. This includes detail of initiatives that are already\nrunning, ones that to start over the next 3 years, as well as examples\nof successful policies and projects from other nations that could\nprovide templates for UAE policymakers.\nEIGHT STRATEGIC\nOBJECTIVES\nLeadership\nVision\nAI Activity\nFoundation\n1.\nBuild a\nreputation as\nan AI destination.\n2.\nIncrease the UAE\ncompetitive assets\nin priority sectors\nthrough deployment\nof AI.\n5.\nAttract and train\ntalent for future\njobs enabled\nby AI.\n7.\nProvide the data\nand supporting\ninfracstructure\nessential to become a\ntest bed for AI.\n8.\nEnsure\nstrong\ngovernance\nand effective\nregulation.\n4.\nAdopt AI\nacross\ngovernment\nservices to\nimprove lives.\nEg. UAI brand.\nEg. Proof-of-concept in\npriority sectors.\nEg. Public AI basic\ntraining.\n6.\nBring world-leading\nresearch capability\nto work with\ntarget industries.\nEg. Key Thinkers\nprogram\nEg. Secure data\nintrastructure.\nEg. Intergovernmental panel\non artifical intelligence.\n3.\nDevelop\na fertile\necosystem\nfor AI.\nEg. Applied AI\naccelerator.\nEg. National AI\nchallenges.\nTo become one of the leading nations in AI by 2031\nUAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 20 UAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 21\nTo become a global AI leader, the UAE\nneeds to compete with destinations\naround the world that are also trying\nto attract scarce AI talent and grow AI\ninvestment. Boston, London, Beijing,\nShenzhen, Toronto and many other\nplaces are all vying to be the \u2018next\nSilicon Valley\u2019 for AI.\nAchieving this objective will require\na brand that is built on what\ndifferentiates the UAE: its established\nreputation as a bold innovator.\nThis reputation already brings\ncompanies to the country.\nSparkCognition, the world-leading AI\nfirm, recently announced their first\ninternational office outside the US to\nbe based in Dubai.\nThis objective relies heavily on\nachieving the other seven. Those\nobjectives are necessary but not\nsufficient to deliver the UAE as\na destination for AI talent and\ninvestment.\nThere will also need to be a brand\ncampaign that explains and illustrates\nthe UAE AI offering in a compelling and\nauthentic form. This brand will provide\na practical means to communicate this\nto the rest of the world. This has been\nannounced as \u2018UAI\u2019.\nUAI Brand\nThe UAE is developing a UAI brand\nand will use this to attract talent and\nbusiness from across the globe to come\nto the UAE to test and develop AI.\nThis includes a UAI mark recognizing\nhigh quality, ethical AI companies. It\nwould reward safe, efficient, verified\nAI technology with a \u2018UAI Seal of\nApproval\u2019.\nThe UAI will consist of four levels of\napproval to include Public Sector Level,\nPrivate Sector Level, Institutional Level,\nand Product Level.\nThe certification system is based\non the highest level of world-wide\nstandards that will establish the core\nrequirements in obtaining the UAE\nSeal of Approval. This robust, rigorous,\nand comprehensive certification\nmethodology will ensure verifying the\nentities with the best AI technology in\nthe region.\nThe UAE is aiming to host key\ninternational conferences and forums\non AI making it a hub for global experts\nand entrepreneurs. With this, the UAE\nwill become the center of AI startups in\nthe region.\nSingapore developed a successful\nbrand campaign called \u2018Smart Nation\u2019.\nIt demonstrated the connection\nbetween digital innovation and\nnational priorities, signaling that the\ndigital revolution is at the center of\nSingapore\u2019s national strategy. A brand\nidentity was backed up by regular\n5,\nsubstantive indicators of progress in\nthe field.\nOBJECTIVE 1:\nBUILD A REPUTATION AS\nAN AI DESTINATION \nUAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 22 UAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 23\nAI has the potential to generate up to\nAED 335 billion in the UAE economy\nboosting this by supporting industry\npilots in sectors where this kind of\nintervention will create the most\neconomic or social value.\nThis objective details our initial priority\nsectors. This effort is complemented\nby support for government services\nin objective 4. In the medium to long\nterm, these priority sectors could\nchange.\nExisting Assets\nThe largest economic gains from AI will\ncome in significant and mature sectors,\nwhere the AI potential is also high \u2013\nfinance, resources, construction and\nretail trade.\nGovernment has a role to play in\nsupporting industry to achieve these\ngains \u2013 helping industries to develop AI\nwhere a competitive advantage exists,\nincentivizing global AI firms to locate\nin the UAE and local firms to connect\nglobally, and finally by supporting\nbusiness more generally with advice\nto become successful in an AI enabled\nworld.\nIn three sectors, there are additional\nnational assets or need for innovation\nthat make them priorities:\na. Resources & Energy: The UAE is\nthe 5th largest exporter of oil in the\nworld. The existing extraction industry\nalready uses modelling software and\nalgorithms to support its operations.\nAs the UAE makes the transition\nto renewable energy supplies and\nmore efficient water desalination,\nthere is also an opportunity for AI\nsystems to play a fundamental role in\nenergy sector innovation. There is an\nopportunity to open up this sector to\nmore companies, and provide support\nfor proof-of-concept systems developed\nfirst in the UAE.\nThe UAE is planning a proof-of-concept\nto utilize AI systems in order to focus\nboth internally, to make energy saving\ndecisions, and globally to understand\nsupply and demand for oil.\nEnergy supply and utilities is also an\narea of innovation. From smart grids\nto water recycling, there also needs\nto be support for small companies and\nutilities supplies to test and improve\nthis infrastructure.\nb. Logistics & Transport: The UAE\nis a globally competitive transit hub.\n60 million people pass through Dubai\nAirport each year; 26 million pass\nthrough Abu Dhabi Airport. Jebel Ali\nport is the largest marine terminal in\nthe Middle East and provides market\naccess to over 2 billion people. Airport\nand port management companies\nfrom the UAE continue to expand their\nmanagement of overseas facilities.\nThe UAE\u2019s peninsular location between\nSouth Asia and East Africa provides\nan enduring advantage. There is an\nopportunity to make the most of these\nassets \u2013 facilitating test beds for new\ntechnologies in these locations by\ndeploying AI solutions for air traffic\nmanagement, baggage handling, and\nairplane boarding..\nDemonstrator projects that make the\nmost of these physical and digital\nassets in logistics and transport will be\nfunded.\nc. Tourism & Hospitality: Tourism\nis a highly visible, successful export\nsector for the UAE. There is a particular\nopportunity to integrate services\nattracting tourists to the UAE and the\npackages that are offered once they\nare here, including business travelers\nand those on short stopovers.\nThe greatest opportunity in Tourism\n& Hospitality comes from innovations\nthat have potential for spillover into\nother customer service sectors. AI can\nbe utilized to predict tourist\u2019s needs\nand provide customized services.\nEmerging Sectors\nThere are three other sectors,\nwhere the UAE has different kinds\nof advantage \u2013 these are not about\nexisting scale but pockets of\nopportunity that are already visible.\nThere have smaller, but valuable data\nassets; fast-growth and entrepreneurial\nactivities or areas where government\nare taking a lead in the sector.\nThe opportunity in this sector is to\ncreate partnerships that can boost\ntourist numbers in the UAE, where\nthose tourists have AI-driven schedules\nor use automated assistants during\ntheir stay.\na. Healthcare: Government does not\nown the healthcare industry in the\nUAE, but it plays a significant role\nin it. Dubai Health Authority\u2019s new\nDubai Genomics program hopes to\nbring population-scale whole-genome\nsequencing to the Emirate. The aim is\nto use the diverse genetic community\nin the UAE as a resource for new\nscientific studies, which make it\neasier to predict risks associated with\ngenetic-related illnesses. This kind\nof study, and similar uses of patient\ndata from UAE hospitals, could lead to\nnovel opportunities for digital health\ninnovation based in the UAE.\nThe opportunities will most likely\nbe low in number but some could\nbe world-leading or have significant\nimpact on the care of individuals with\nrare diseases. For example, testing\ndiagnostics in a clinic that has access\nto the latest monitoring technology,\ndetailed historical patient data and\na diverse population could provide a\nrare asset for healthcare companies.\nThere is also an opportunity to focus\non diseases that are prevalent in the\nregion, which receive relatively little\nattention from global pharmaceutical\ncompanies.\nOBJECTIVE 2:\nINCREASE THE UAE\nCOMPETITIVE ASSETS IN\nPRIORITY SECTORS\nTHROUGH DEPLOYMENT OF AI\nUAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 24 UAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 25\nThe AI Office is most interested in\nproviding access for companies and\nresearchers to hospital and national\ndatabases, where their work could\ndevelop specialist capability in\ndiagnostics that use AI, particularly for\ncommon diseases in the region.\nThe AI Office has funded research\ninto developing an AI algorithm for\ndetecting Tuberculosis in patients via\ndiagnosis of X-ray data and the pilot\nwas launched at the United Nations\nWorld Data Forum 2018.\nb. Cybersecurity: Historically, the UAE\nhas attracted the regional hubs for\nlarge technology companies like SAP\nor Microsoft, which often locate to free\nzones within the city. More recently,\nthe UAE has grown or attracted smaller\ncybersecurity firms.\nThere is significant potential benefit\nfor government in developing better\ncybersecurity for their own services\nand for making the UAE a secure\nenvironment for business. There is also\na strong entrepreneurial segment in\ncybersecurity, which the government\nwants to encourage.\nThere will be more than 7.5 billion\nInternet users by 2030 (90% of\nthe projected world population of\n8.5 billion). Like street crime, which\nhistorically grew in relation to\npopulation growth, similar evolution\nof cyber crime is being witnessed with\nprojected damage costs to hit USD\n6 trillion annually by 2021. Hence,\ncyber security is a big investment\nthat requires a priority considering\nthe global shift towards maintaining\nsafety. Over the next five years, the\nglobal spending on cyber security will\ncumulatively exceed USD 1 trillion7.\nSupporting pilots that demonstrate\nnew cybersecurity approaches in the\nUAE first. There is also interest in\ncommunity-building and skills-building\nprograms for SMEs and local talent\nthat focus specifically on AI as a risk or\nopportunity for cybersecurity.\nProof-of-Concept Support\nin Priority Sectors\nWithin these five priority sectors,\nthe UAE government will fund or\nbroker pilot projects. These proofs-ofconcepts could be designed by public\nsector, private sector or consortia.\nFunding will depend on how well\nproposed pilots map onto the reasoning\nfor each priority sector as detailed\nabove. For example, the AI Office is\nworking with various private sector\ncompanies to develop pilots that use\nquantum computing to support health\ndiagnostics and global energy supply\nmanagement.\nDeveloping AI technology in the UAE\nwill help the UAE diversify its economy,\nenhance productivity and find new\nsources of growth. It will also firmly\nestablish UAE\u2019s credentials as a world\nleader in AI and act as a catalyst to\nattract further talent and investment.\nFocusing efforts in industries with an\nobvious potential for AI development,\ncommercialization and export exists\nwill maximize the likelihood of success\nand the return on investment.\nUAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 26 UAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 27\nA combination of funding, knowledge,\nand strategic support will be needed to\ndevelop a domestic AI ecosystem. This\nstarts with better access to local data\ninfrastructure and funding for projects\nthat make the most of this which will\nlead to opportunities for building new\ncompanies. Once these elements are in\nplace, there needs to be incentives for\nworld-leading products and services to\nbe developed in the UAE.\nThere are difficulties and uncertainties\nin developing algorithmic services. AI\nsystems often require coordination\nacross several locations, firms or\nindustries. They also require trusted\npartners in order to automate products\nand services.\nGovernments can play an important\ncoordinating role, providing access to\nnetworks, data and finance that can\nhelp overcome these barriers.\nAI Network\nIn order to encourage more research,\ncollaboration and commercialization\nlocal expertise will be aggregated\nthrough the establishment of a network\nof researchers, industry experts and\npolicy experts from across the UAE.\nFunding for AI research and companies\ncould be provided according to priorities\nidentified by the group, backed by\nevidence from a survey of regional AI\nactivities.\nThe Mohammed bin Rashid Innovation\nFund has AED 2 billion to support local\ninnovators. Collaboration between the\nfund and the UAE Artificial Intelligence\nand Blockchain Council (see objective\n4) could support companies that\nneed access to government data or\npartnerships with Government.\nThe AI market is estimated to grow\nto USD 60 billion by 2021, with China\nalone aiming to create a USD 150\nbillion market by 2030\n8. The UAE\nwill need to accelerate domestic AI\ncommercialization in order to capture\nits share of this growing market. There\nare currently an estimated 2,600 AIfocused startups globally, but the vast\nmajority of them are in America and\nChina, along with economies like the UK\nand Japan\n9\n.\nThe UAE has an opportunity to become\na competitive regional hub for AI\nentrepreneurs through providing a\nsupportive ecosystem. A developed\necosystem of local startups will ensure\nthat AI solutions are catering to the\nmarket needs of the UAE economy,\nrather than being reliant on adapting\nimported ideas and products.\nOBJECTIVE 3:\nDEVELOP A FERTILE\nECOSYSTEM FOR AI\nApplied AI Accelerator\nThe AI Office will support the\ndevelopment of a domestic AI startup\nand product development ecosystem\nthrough incubator funds, mentoring,\nand publication of shared knowledge.\nZeroth.AI, a Hong Kong based AI\naccelerator, provides USD 120,000\nin seed capital to companies in the\nprogram for exchange for 10% equity\nstake. They also provide mentoring\nand support to get a long-term visa in\nHong Kong. Another example, this time\nof a government supporting industry\ndevelopment, is in AI Singapore\u2019s\n100 Experiments project. The project\nfunds researchers and academics\nwith up to USD 250,000 to work on\nindustry specific problems for which\nAI technologies may be quickly built,\nbut without the need for time and\nresource-consuming research. \nUAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 28 UAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 29\nAI Incentive Scheme for Overseas Companies\nGreater FDI by foreign firms will be a key enabler of industry development,\nbringing technology and skills to the UAE. The scope for greater FDI is real. 70%\nof global executives believe technological change will lead to an increase in global\nFDI10. While the UAE is seen as a promising source of FDI, it is not seen as a top\ndestination for FDI investment. Planned relaxation of foreign investment laws and\nimproving reputation for ease of doing business should facilitate greater FDI11.\nIncentives will be developed to encourage UAE firms to partner with global AI\ntechnology firms to foster greater links into global value chains and enable\ntechnology transfer from international firms. The incentives will also motivate\ninternational companies to set up regional offices in the UAE or relocate here.\nFor example, a new cyber research center in Stuttgart and Tubingen, Germany\n(the Max Planck Society\u2019s Institute for Intelligence Systems) attracted foreign\ninvestment from Amazon leading to an estimated 100 jobs over the next five years\nand providing EUR 420,000 per year to fund research students. Foreign investors\nwere driven by locating near this known center of talent, which previously had\nnot engaged with industry partners. Although the UAE does not have this kind of\nstrong AI talent hub, it is building attractors that will grow the technical community\nhere quickly.\nBusiness Support for UAE AI Firms\nOnce local systems and companies are in place, the next step is to form more\nambitious UAE presence in global markets.\nCreating global markets often requires investments that are speculative (e.g.\ninternational trips, marketing campaigns) or require coordination\n(e.g. trade missions, joint ventures). This can often be difficult for individual\nbusinesses to undertake. Similarly, investments in new products require largescale investments, which can carry too much risk for any one investor.\nGovernments can help business solve this problem by offering guidance,\nfinancial support, and by acting as a coordinator. Providing support and guidance\nwill overcome knowledge barriers to developing AI solutions and strengthen\nconnections into international markets, increasing exports and growth.\nUAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 30 UAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 31\nGovernment can play a strong role in\nmaking sure AI delivers the greatest\npublic value, by making citizens safer,\nhealthier and happier.\nThe UAE also faces significant social\nand economic challenges, where the\noutcomes for the population are poor\ncompared to other countries. For\nexample, high rates of obesity and\nheart disease, high rates of traffic\nfatalities, poor air quality and poor\neducation outcomes. Using AI to better\nrespond to these challenges has huge\npotential benefits. There is a role for\nGovernment in supercharging this \u2013\nproviding the focus, resources and drive\nto solve these challenges.\nNational AI Challenges\nA single program could be set up to\nsupport the best ideas from across\ngovernment, universities and the\nprivate sector, which solve the UAE\u2019s\nmost pressing challenges using AI. In\nAustralia, the government recently\nlaunched a National Missions Program,\nbeginning with making the nation the\nhealthiest in the world. This included\na step-change increase in investment\nin national genomics and personalized\nmedicine capability and its integration\ninto medical research and healthcare\nsystems.\nIn similar fashion, a nationwide\nprogram to tackle the UAE\u2019s distinctive\nchallenges will be launched. The best\nideas will receive funding, mentoring\nand access to data. Importantly, the\nprogram will have the potential to\ndemonstrate the benefit of AI to the\nUAE population and inspire the nation\nto embrace AI to make lives better.\nUAE Artificial Intelligence\nand Blockchain Council\nAI can also be used to improve the\nexperience and cost of government\ntransactions and services. There will be\nfewer time-consuming administrative\nprocesses, fewer errors, and more\nconvenient services.\nBuilding on a successful generation\nof digital government initiatives,\nthe UAE has an opportunity for\nglobal leadership. But government\nentities need support from political\nleaders to move key services \u2013 such\nas tax filings, applications, regulatory\ncompliance checks, payment of fines \u2013\nto interoperable digital platforms, with\nhigh-quality, complete and accessible\ndata.\nThe UAE Artificial Intelligence\nand Blockchain Council includes\nrepresentatives from all emirates\non both federal and local levels. The\nCouncil\u2019s main objective is to identify\nhow and where AI can be incorporated\nin government and what supporting\ninfrastructure it requires.\nOBJECTIVE 4:\nADOPT AI ACROSS CUSTOMER\nSERVICES TO IMPROVE\nLIVES AND GOVERNMENT\nUAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 32 UAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 33\nA recent study commissioned for the\n2018 World Government Summit\nin Dubai argued that for six Middle\nEast countries, 45% of the existing\nwork activities in the labor market\nare automatable today based on\ncurrent technologies. This average is\nslightly below the global average of\n50%12. The same study shows that\nthis risk is higher in sectors when\nemployees perform routine tasks like\nin manufacturing and transportation.\nIn the arts, education and healthcare,\nwhere human interaction or creativity is\nmore important, the risk is much lower.\nFor the UAE, around 43% of existing\nwork activities have the potential\nto be automated across key sectors\nsuch as administration, government,\nmanufacturing and construction. With\naround 70% of Emiratis employed in the\npublic sector, retraining of government\nworkers is particularly critical. It has\nbeen estimated that almost 300,000\njobs in the UAE in the Administrative\nand Support and Government sector\nmay be impacted by automation, with\naround 125,000 of these jobs held by\nUAE nationals13. This will have a major\nimpact on the public sector workforce\nand needs to be carefully managed,\nwith a 2016 survey of Emirati workers\nfinding the ideal future role for 54%\nwas one in administration i.e. a role\nthat may not exist in the future.\n\nThese predictions could prove\ninaccurate. Working practices in the\nUAE are often different to US job\ndescriptions, which are the ones\nused to estimate how work can be\nautomated. At the same time, the\ngrowing youth population in the region\nand dominance of job-related visas\nin the UAE could have the greatest\neffects on the underlying dynamics of\nthe workforce.\nGiven this, there is a significant lowskilled population whose job can easily\nbe changed by automation, but who\ncurrently have few skills to make the\nmost of these changes.\n40% of the UAE workforce has good\ndigital skills14. This is less than the\n56% of people with good digital skills\nin the UK, the top rated-nation in the\nAI-Readiness Index15.\nFor most of the population, developing\nbetter digital skills and basic\nunderstanding of AI will help them\nmake better decisions in an economy\nwhere automation technologies enter\nthe workplace.\nOBJECTIVE 5:\nATTRACT AND TRAIN\nTALENT FOR FUTURE\nJOBS ENABLED BY AI\nPublic AI Training\nFree courses are being run for UAE\nresidents to raise awareness and\nunderstanding of AI technologies. The\nUAE AI Summer Camp took place in the\nsummer of 2018 and it supports the\nefforts of future knowledge transfer\nand building a generation capable of\nadopting advanced technologies in\ndeveloping solutions for various future\nchallenges. Over 5000 UAE residents\nreceived specialized training on the\nfundamentals of AI with hands-on\nexperience .\nUpskilling Students\nThere is a similar opportunity in the\nstudent population. The UAE has a\nsmall student population, but a high\npercentage (22%) are in core STEM\nareas: ICT , engineering and natural\nsciences. Upskilling STEM graduates\nwith specialist courses will provide\nthe fastest short-term solution to\nincreasing the number of AI experts.\nThis upskilling will also provide a\nstronger pipeline of students able to\nundertake post-graduate training in AI\nto develop the pool of UAE talent able\nto build AI systems.\nThe United Kingdom has recently\nstated an aggressive target of having\nat least 1,000 government supported\nPhD places at any one time in AI and\nrelated disciplines, by 2025. In order\nto compete technically on a global\nscale the UAE must also be ambitious\nin its targets, to that effect, at the\nFebruary 2018 World Government\nSummit, the Minister of State for\nArtificial Intelligence announced that\nthe UAE has the intention to produce\nworld-class AI talent. This will be\ndone through upskilling 1/3 of the\nUAE\u2019s STEM graduates per year (2000\nstudents).\nGiven the public sector is a major\nemployer and potential user of AI in\nthe UAE, The AI Office has also started\nspecific training for government\nemployees.\nGovernment Training\nThe AI Office is offering more advanced\ncourses for Government employees\nstarting Q4 2018, focused on skills\nneeded to work with them being the\nAI Experts (ambassadors) in their\nentities. These require participants to\ncomplete a capstone project related\nto their current job. The aim is to\nensure that 100% of senior leadership\nin government - Director-General,\nMinisterial and Senior-Ministerial levels\n- are trained and versed in AI, with\nmore junior government employees\nbeing trained on a more ad hoc basis.\nDigital Skills in the UAE and UK\n05\n10\n35\n46\n45\n37\n15\n07\nUAE\nExpert\nExpert\nCompetent\nCompetent\nNovice\nNovice\nUnskilled\nUnskilled\nUK\nUAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 34 UAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 35\nProfessional Upskilling\nThere is also an opportunity to take professionals with expert digital and analytic skills and\nprovide them with the training needed to become specialists in AI. In the New Generation of\nRegional Talent section of this AI Strategy, the strong segment of professionals in the region\nwith operational and analytical skills was highlighted. It is the AI Office\u2019s aim to help upskill\nthese individuals. Upskilling existing professional workers in the UAE could include specialisttraining, secondments and study tours overseas.\nEmployment Transition Support\nSkills training for 60% of the workforce with low digital skills would benefit from more robust\ndata on current skills in the labor force and current job openings. The AI Office supports the\nMinister of State for Higher Education and Advanced Skills in their efforts to improve this data\ncollection, and champions efforts to develop a series of career advice tools and services to\nhelp current and future workers make more informed choices.\nThis illustration summarizes how skills training\ncould fit together across different segments of\nthe labor market.\nWorkers with low\ndigital skills\n(60% of the workforce)\nCareer advice and\ntraining services\nfor those at risk of\nautomation\nProfessionals\nwith expert\ndigital skills (5%)\nAdult Students\nProfessionals with digital\ncompetence (35%)\nFunding secondments\noverseas\nShort couses on\nspecific techniques\nAI experts (0.2%)\nCourses on basic digital\nand computational\nconcepts\nProject-based learning\nopportunities using\ncomputational concepts\nApprenticeship programmes\nCareer advice for students\nSummer short courses\nScholarships\nSchools\nUAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 36 UAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 37\nThis objective is concerned with\nbuilding the wider knowledge\nproduction in the UAE, including\nuniversity and commercial R&D.\nThis will need to include increasing\ninvestment in research and encouraging\nworld-class academics to work in the\nUAE.\nInvesting in AI R&D capability is a\nnecessary first step. The US, France,\nUK and China have embraced strategic\nnational plans to boost AI\u2019s share\nof its R&D investments. The UAE is\nranked 35th in the world for overall R&D\ninvestments.\nThere are researchers in UAE\ninstitutions developing or modifying\nalgorithmic or automated technologies.\nTo provide a targeted boost to R&D in\nAI, the focus will be on supporting and\nexpanding the research of this small\ncommunity.\nThe UK\u2019s national institute for data\nscience and artificial intelligence was\ncreated as a partnership between\ncenters of excellence at existing\nuniversities. Five founding universities\n\u2013 Cambridge, Edinburgh, Oxford, UCL\nand Warwick \u2013 and the UK Engineering\nand Physical Sciences Research Council\ncreated The Alan Turing Institute in\n2017. It provides coordination and\nsupport for the research community,\nwithout the overheads of establishing\na new university-scale institution. The\nAI Network in Objective 3 will help\nsupport similar coordination.\nEven in countries with a wellestablished research base, AI experts\nare in short supply and highly\nattractive to industry. For example,\n65% of Google DeepMind research hires\ncame from academia16. AI ideas are\nstill emerging and new technologies\nare still finding their way to industry.\nGovernments are investing heavily in\nAI, to supplement but by no means\nmatch significant investment by private\ncompanies.\nThe US and China are world-leaders in\ndeveloping domestic research capacity.\nThis dominance is visible in AI research\noutput, where the countries also\nproduce the most number of original\nresearch papers on AI17.\nCountries with fewer researchers are\nstill able to have research impact by\nbuilding capacity in strategic areas.\nCountries like Canada and Spain have\nalready developed hubs in AI-related\nresearch.\nOBJECTIVE 6:\nBRING WORLD-LEADING\nRESEARCH CAPABILITY\nTO WORK WITH TARGET\nINDUSTRIES\nUAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 38 UAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 39\nThe UAE is a young country that has\nnot yet established a strong academic\ntradition to provide a pipeline of world\nclass researchers. It will need to look\nfor other ways to access research\ntalent. With more than half the world\u2019s\npopulation just a five hour flight away,\nthe UAE is in a prime position to attract\nglobal research talent to visit the UAE\nto help build capacity and share their AI\nknowledge.\nShort-term opportunities for leading AI\nprofessors to work and experience the\npotential in the UAE may also support\nthe UAE to attract leading professors\nin the medium to longer term, and\ndevelop UAE university capacity.\nSaudi Arabia\u2019s Center for Complex\nEngineering Systems is a partnership\nbetween the Saudi Government\nand MIT, creating a flow of expert\nacademics to Saudi Arabia.\nNational Virtual AI\nInstitute\nTo ensure this increase in investment\nis well targeted, the AI Office will\nsurvey current local R&D capacity. This\nwill help identify options for what is\nrequired and how best to boost R&D\nthat can be directly applied to industry,\nproviding a medium-longer term\nsolution to addressing the UAE\u2019s R&D\ngap. Following the survey, the UAE will\nlaunch a National Virtual AI Institute\nwith stakeholder partners to aggregate\nthe best local and global expertise\nin the region, and to encourage\nmore R&D activity, collaboration and\ncommercialisation. The AI Network in\nObjective 3 will provide the platform for\nthis R&D network.\nKey Thinkers Program\nA program to attract key AI thinkers\nto visit the UAE will be initiated.\nThese key AI thinkers will participate\nin workshops and lectures with local\nuniversities and businesses.\nKey Thinkers will also be provided with\nincentives to run research projects in\npartnership with these local bodies.\nIn line with objective 4, improving\nlives, and objective 8, good governance\nof AI, The AI Office will also want to\nrecognize and reward AI research with\nthe greatest value to society. An\naward for programs with outstanding\ngovernance frameworks or the greatest\nsocial impact would be a helpful\nincentive.\nAI Library\nThe research gap can be closed if the\nbenefit of research can be shared\nwith those who have an interest in AI.\nIn order to boost further innovation,\nthe AI Office will work on creating an\nopen-access digital library of research\nand papers in both English and Arabic.\nThis will be a first-of-a-kind initiative\nto boost the research sector in the\nregion. The UAE will also endeavor to\ncreate accessible summaries of UAE\ngovernment funded AI research and\nprograms in order to help encourage\nthe development of AI solutions. The\nAI Library will be a joint collaboration\nbetween academia, industry and\ngovernment.\nPercentage of population working as\nfull time researchers in science,\ntechnology, and innovation18\n.\n0.69%\n0.66%\n0.59%\n0.45%\n0.44%\n0.43%\n0.43%\n0.42%\n0.41%\n0.40%\n0.38%\n0.32%\n0.31%\n0.26%\n0.20%\n0.19%\n0.12%\n0.07%\n0.04%\n0.02%\n0.01%\nKorea\nSingapore\nNorway\nNetherlands\nUK\nGermany\nCanada\nUSA\nAustralia\nFrance\nNew Zealand\nGreece\nRussia\nSpain\nItaly\nUAE\nChina\nBrazil\nSouth Africa\nIndia\nKuwait\nUAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 40 UAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 41\nGovernments around the world are\nincreasingly recognizing the value of\nthe vast data sets they collate. Machine\nlearning models need access to training\ndata sets, and open data can also be\nused to test and improve AI systems\u2019\nperformance.\nThe UAE has taken steps towards\nopening data to improve transparency,\nbut still significantly lags behind other\ncountries in the number of open data\nsets it releases. 537 datasets are\ncurrently available, whereas Turkey\nhas shared 1,280 and Canada has over\n10,000.\nData Sharing\nThe UAE has an opportunity to become\na leader in available open data for\ntraining and developing AI systems.\nThe greatest advantage that the\nUAE has is in its diversified culture,\nwith more than 200 nationalities\nresiding in the UAE. Given the unique\nmixture of cultures in the UAE, the\ndata sets that the country holds is\nimpeccable. This data in combination\nwith machine learning can aid in\naccurately diagnosing diseases such\nas Tuberculosis (TB) using Artificial\nIntelligence. The UAE realizes that\nthe oil of the future is data and will\ninvest into creating a robust data\ninfrastructure.\nThe UAE\u2019s ambition is to create a\ndata-sharing program, providing shared\nopen and standardized AI-ready data,\ncollected through a consistent data\nstandard.\nThe X-Road platform in Estonia\nsupports access and combination of\ngovernment and private databases,\nsetting the stage for the application\nof machine learning tools. The data\nsolution saves citizens over 800 years\nof working time per annum.\nSecure Data\nInfrastructure\nA secure data infrastructure will be\nnecessary to facilitate data sharing,\nand manage privacy concerns. Investing\nin a single AI data infrastructure makes\nit easier to do this efficiently, and\nmakes it simpler to access data relevant\nto research or developing new products\nand services.\nSome countries have already\nexperimented with virtual data\nlibraries. Australia\u2019s SURE (Secure\nUser Research Environment) allows\nresearchers to access data in hospitals,\nOBJECTIVE 7:\nPROVIDE THE DATA\nAND SUPPORTING\nINFRASTRUCTURE\nESSENTIAL TO BECOME\nA TEST BED FOR AI\ngeneral practice and cancer registries.\nWhile designed to handle health data,\nit is now being used by other agencies\nwith sensitive data, e.g. the Australian\nTaxation Office and the Australian\nDepartment of Social Services. SURE\noffers a data repository service, where\na user can purchase secure, hosted\nspace for multiple datasets and projects\nand set their own data governance\nframework, if approved by SURE. SURE\nalso offers single project workspaces\nwhere SURE manages both the data\ngovernance and technical aspects\nof hosting. In these cases, a user\nmust seek research ethics committee\napproval for the research before a\nworkspace will be granted.\nBeyond national datasets, there\nis also a need for data protection\nand authentication as part of good\ncorporate practice in the UAE. As the\nconsultative group under objective 8\ndevelops, they will begin to address\nthese issues. Europe\u2019s General Data\nProtection Regulation includes new\nrights for consumers; it provides\nan opportunity to re-consider how\nconsumer data is handled, even for\ncustomers who are not European\ncitizens. \nUAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 42 UAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 43\nThe speed of developments in AI is a\nchallenge for governance. With vast\nresearch efforts around the globe, it\nis difficult to ensure this technology\nis developed in a safe and ethical\nenvironment.\nAs Governments and leading AI\nthinkers around the world grapple\nwith this challenge, there is an\nopportunity for the UAE to learn from\nthe best and collaborate with others\nto ensure effective governance and\nregulation of AI, both domestically and\ninternationally.\nThe UAE has the ambition to take a\nleading role in the development of\nresponsible AI and advancing the\nregulation of AI. For example, the UAE\nhas an important contribution to make\nto this global discussion by connecting\nabstract discussions to pilots run by, or\nin partnership with government.\nThis will also mean working to make\nsure the UAE has the legal environment\nto support innovation in general\nand the adoption of AI in particular.\nInnovations in AI technology often\nrequire rapid changes in regulatory\nsettings and can create risks to society.\nThe adoption of interconnected data\nsystems and the growing dependence\nof major industries on software also\nmakes an economy more vulnerable to\ndigital disruption.\nCyberwarfare capability will continue\nto grow, meaning that cybersecurity\nwill become increasingly important.\nIn the absence of a coherent national\nstrategy, cybersecurity would be\ndeveloped on an ad hoc basis. This\nis inefficient and risks leaving gaps.\nThis will be addressed through the\ngovernance review.\nNational Governance\nReview\nThe UAE Artificial Intelligence and\nBlockchain Council will add to its remit\nto review national approaches to issues\nsuch as data management, ethics and\ncybersecurity. They will also review the\nlatest international best practices in\nlegislation and global risks from AI.\nFurthermore, the Council will ultimately\noversee the implementation of the AI\nStrategy in the UAE.\nOther countries have developed\nadvisory structures that combine\nexpertise in technical fields and\nregulation. The 2016 White House\nArtificial Intelligence Strategy formed\na standing committee consisted of\nregulators and industry experts.\nCalifornia\u2019s Little Hoover Commission\nis currently studying the impact of\nAI on regulatory settings through a\ncommittee of experts. France has\ncreated a national AI ethics committee,\nOBJECTIVE 8:\nENSURE STRONG\nGOVERNANCE AND\nEFFECTIVE REGULATION\nas well as ethics-by-design training\nfor tech developers to build ethical\nconsiderations into their projects.\nGlobally, the UAE has begun work on a\nnumber of initiatives to help develop\nresponsible AI.\nDuring the World Government Summit\nin February 2018, over 100 leading\nexperts at the inaugural Global\nGovernance of Artificial Intelligence\nRoundtable were hosted. This\ncollection of AI experts debated how\ngovernments could best navigate the\nchallenges posed by the rapid rise of AI.\nSecond Global\nGovernance of Artificial\nIntelligence Roundtable\nFor the 2019 World Government\nSummit, The AI Office is working with\nUNESCO, OECD, IEEE and the Council\non Extended Intelligence in identifying\nthe foremost experts and themes to\nexplore. All of these working groups\nwill then present their outcomes at a\nHigh Level Ministerial Panel composed\nof the world\u2019s foremost Minister\u2019s of\nDigital, Technology and ICT who are\nresponsible for the development and\nuse of AI in their countries.\nIntergovernmental Panel\non Artificial Intelligence\nA natural evolution of the Roundtable\nis in the formulation of an\nintergovernmental body, dedicated\nto providing a mechanism for\ngovernments and private companies to\nbetter understand AI and its impact on\nsocieties in order to help give a solid\nframework for future regulation, in a\nmore tangible and enforceable manner.\nIn March 2018, President Macron, at\nthe launch of the French Artificial\nIntelligence Strategy, announced\na desire to establish an \u201cIPCC for\nArtificial Intelligence\u201d \u2013 referring to the\nIntergovernmental Panel on Climate\nChange. The UAE has expressed\na desire to work with France and\nother governments in creating the\nfoundations for such a body. The AI\nOffice is actively working toward\nmaking this happen. \nUAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 44 UAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 45\nCONCLUSION REFERENCES\nThe UAE is unlike any other country in its diversified population, comprising of\nunique talents \u2013 we aim to give this human potential the best opportunities to\nnourish and flourish. Given this human potential, the UAE has always aimed at not\njust being better, but to become the best.\nThe AI National Strategy is a cornerstone of the UAE Centennial 2071 and is a\nmajor variable in the overall equation. It will bring transformation to a new level\nby 2031 and set the foundations for future generations in the UAE to become the\nbest.\nAs one of the first movers in paving the path for AI nationally, a plethora of\nchallenges is certain to arise, but we are true believers in that nothing is\nimpossible in the UAE. We are a country that is known for tackling challenges\nhead on, creating new opportunities, and deploying innovative solutions.\nAs Minister of State for Artificial Intelligence, I aim to catalyze the responsible\ndevelopment of AI within our country, in order to help us reach the UAE Centennial\n2071 \u2013 and to act as an inspiration for other nations to harness this technology\nfor the betterment of humankind.\n1: Futurism.com, \u201cAn Inside Look at the First Nation With a State Minister for Artificial\nIntelligence\u201d (2017).\n2: McKinsey & Company, \u201cThe Future of Jobs in the Middle East\u201d (2018). SOURCES:\nNicholas Crafts, \u201cSteam as a general purpose technology: A growth accounting\nperspective,\u201d Economic Journal, volume 114, issue 495, April 2004; Mary O\u2019Mahony\nand Marcel P. Timmer, \u201cOutput, input, and productivity measures at the industry level:\nThe EU KLEMS database,\u201d Economic Journal, volume 119, issue 538, June 2009; Georg\nGraetz and Guy Michaels, Robots at work, Centre for Economic Performance discussion\npaper 1335, March 2015; McKinsey Global Institute analysis.\n3: Data from UAE FCSA National Accounts Estimates Tables 12, 13 and 15.\n4: LinkedIn data, analyzed for: World Economic Forum, \u201cThe Future of Jobs and Skills in\nthe Middle East and North Africa Preparing the Region for the Fourth Industrial\nRevolution\u201d (2017).\n5: Analysis by Bayt.com data librarians.\n6: UNESCO Institute for Statistics, latest data for each nation (2015 onwards).\n7: Cybersecurity Ventures Official Annual Cybercrime Report.\n8: Forrester, \u201cPredictions 2017: Artificial Intelligence Will Drive The Insights Revolution\u201d\n(2016) and Government of China, \u201cA Next Generation Artificial Intelligence\nDevelopment Plan\u201d (2017).\n9: Tencent, \u201cGlobal AI Talent White Paper\u201d (2017).\n10: UNCTAD, IPA survey (2017).\n11: World Bank, Ease of Doing Business Index (2018).\n12: McKinsey & Company, \u201cThe Future of Jobs in the Middle East\u201d (2018).\n13: Ibid\n14: Digital skills classifications based on our team\u2019s analysis of occupations and\nclassifications with data from UAE Labor Force Survey. Additional data from the UK\nONS Labor Force Survey and Digital Skills Taskforce and McKinsey & Company, \u201cThe\nFuture of Jobs in the Middle East\u201d (2018). 2030 projection based on estimated\nhistorical rate of change.\n15: Oxford Insights, AI readiness Index (2017).\n16: Nature, \u201cAI talent grab sparks excitement and concern\u201d (2016).\n17: Based on analysis of Elsevier/ Scopus data. Times Higher Education, \u201cCountries and\nUniversities leading on AI research\u201d (2017).\n18: Based on latest UNESCO data. 2015 data for majority of countries.\n\u201cWe, in the UAE, have no such word as \u201cimpossible\u201d;\nit does not exist in our lexicon. Such a word is used\nby the lazy and the weak, who fear challenges\nand progress. When one doubts his potential and\ncapabilities as well as his confidence, he will lose the\ncompass that leads him to success and excellence,\nthus failing to achieve his goal.\u201d\nHis Highness Sheikh Mohammed bin Rashid Al Maktoum\nUAE Vice President, Prime Minister and Ruler of Dubai\nUAE NATIONAL STRATEGY FOR ARTIFICIAL INTELLIGENCE 2031 | 46\nwww.ai.gov.ae", "metadata": {"country": "UAE", "year": "2017", "legally_binding": "no", "binding_proof": "None", "date": "10-15", "regulator": "UAE Council for Artificial Intelligence and Blockchain, Ministry of Artificial Intelligence, Digital Economy and Remote Work Applications", "type": "strategy", "status": "active", "language": "English, Arabic", "use_cases": "[2, 5, 6]"}}
{"_id": "686b620f6e4e9653b2a68b1b", "title": "UAE Charter for the Development and Use of Artificial Intelligence", "source": "https://uaelegislation.gov.ae/en/policy/details/the-uae-charter-for-the-development-and-use-of-artificial-intelligence", "text": "The UAE Charter for the Development and Use of Artificial Intelligence\nIssued Date\n\n10 Jun 2024\nThe Lead Entity\n\nMinister of State for Artificial Intelligence and Digital Economy and Remote Work Applications Office\nSector\n\nTelecommunication, Technology and Space\n\nIndex\nSearch within the policy details\n\nIndex\nIntroduction\nPolicy Objectives\nPriorities & Key Components\nExpected Outcomes\nTarget Audience\n\nThe UAE Charter for the Development and Use of Artificial Intelligence\nIntroduction\nThe Charter for the Development and Use of Artificial Intelligence aims to achieve the strategic goals of the UAE Strategy for Artificial Intelligence, reflecting the leadership's vision of transforming the country into a global hub for developing and adopting AI solutions and applications across various fields. The charter also covers principles to enhance awareness in the AI field for an inclusive future and for leveraging AI advancements to guarantee equitable technological access for all segments of society. Additionally, it includes principles for compliance with existing legislation and agreements related to the development and use of AI in the country. \nThe UAE seeks to establish itself as a leading global player in the field of AI by developing future-ready infrastructure and an integrated system that utilizes AI in all vital sectors, providing a thriving environment that adheres to the highest standards of safety and privacy, and enhancing public trust in these applications.\n \n\nPolicy Objectives\nEnsure the use of artificial intelligence in an ethical and responsible manner.\nProtect privacy and data security.\nBalance technological advancement with social values.\nEncourage innovation and support economic growth.\nIncrease awareness and education about artificial intelligence.\nEnhance transparency and accountability in its use.\nImprove the UAE\u2019s global standing in technology and innovation.\n \nPriorities & Key Components\nStrengthening Human-Machine Ties: \nThe UAE aims to enhance the harmonious and beneficial relationship between AI and humans, ensuring that all AI developments prioritize human well-being and progress.\nSafety: \nThe UAE places great importance on safety, ensuring that all AI systems comply with the highest safety standards. The country encourages modifying or removing systems that pose risks. \nAlgorithmic Bias:\nThe UAE aims to address the challenges posed by AI algorithms regarding algorithmic bias, contributing to a fair and equitable environment for all community members. This promotes responsible development of AI technologies, making them inclusive and accessible to everyone, supporting diversity, and respecting individual differences. It ensures equal technological benefits and improves quality of life without exclusion or discrimination.\nData Privacy:\n\u200b\u200b\u200b\u200b\u200b\u200b\u200bIn line with the UAE\u2019s stance on privacy rights, while data is essential for AI development, supporting and promoting innovation in AI, the privacy of community members remains a top priority.\nTransparency: \n\u200b\u200b\u200b\u200b\u200b\u200b\u200bThe UAE seeks to create a clear understanding of AI and how systems operate and make decisions, which helps build trust, enhance responsibility, and accountability in the use of these technologies.\u200b\u200b\u200b\u200b\u200b\u200b\u200b\nHuman Oversight: \n\u200b\u200b\u200b\u200b\u200b\u200b\u200bThe Charter emphasizes the irreplaceable value of human judgment and human oversight over AI, aligning with ethical values and social standards to correct any errors or biases that may arise.\nGovernance and Accountability: \n\u200b\u200b\u200b\u200b\u200b\u200b\u200bThe UAE adopts a responsible and proactive stance, emphasizing the importance of governance and accountability in AI to ensure the technology is used ethically and transparently.\nTechnological Excellence:\n\u200b\u200b\u200b\u200b\u200b\u200b\u200bAI should be a beacon of innovation, reflecting the UAE\u2019s vision of digital, technological, and scientific excellence. The UAE seeks global leadership by adopting technological excellence in AI to drive innovation, enhance competitiveness, and improve quality of life through innovative and effective solutions to complex challenges, contributing to sustainable progress benefiting society as a whole. \nHuman Commitment:\n\u200b\u200b\u200b\u200b\u200b\u200b\u200bHuman commitment in AI reflects the spirit of the UAE, essential for ensuring that the development of this technology serves the public good. It focuses on enhancing human well-being and protecting fundamental rights, emphasizing the importance of placing human values at the heart of technological innovation to ensure a positive and lasting impact on society.\nPeaceful Coexistence with AI: \n\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200bPeaceful coexistence with AI is crucial to ensure technology enhances the well-being and progress of our communities without compromising human security or fundamental rights.\nPromoting AI Awareness for an Inclusive Future:\n\u200b\u200b\u200b\u200b\u200b\u200b\u200bIt is essential to create an inclusive future that ensures everyone can benefit from AI advancements, guaranteeing equitable access to this technology and its advantages for all segments of society.\u200b\u200b\u200b\u200b\u200b\u200b\u200b\nCommitment to Treaties and Applicable Laws: \n\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200bThe UAE emphasizes the importance of complying with international treaties and local laws in the development and use of AI. Top of FormBottom of Form\nExpected Outcomes\nEnhance innovation and attract investments, thereby increasing efficiency and productivity.\nImprove quality of life by enhancing healthcare and educational services, and by increasing awareness and education about artificial intelligence.\nUtilize artificial intelligence to improve sustainability and protect the environment.\nEnhance government services and improve their efficiency.\nBoost the UAE's global standing in innovation and technology, contributing to an improved ranking in global indices.\n \nTarget Audience\nThe public sector, private sector companies, consumers, AI developers, researchers, educational institutions, research centers, and the international community. ", "metadata": {"country": "UAE", "year": "2022", "legally_binding": "no", "binding_proof": "None", "date": "06-10", "regulator": "UAE Council for Artificial Intelligence and Blockchain", "type": "policy framework", "status": "active", "language": "English, Arabic", "use_cases": "[1, 3, 5, 6]"}}
{"_id": "686b62746e4e9653b2a68b1c", "title": "AI Ethics Principles and Guidelines", "source": "https://ai.gov.ae/wp-content/uploads/2023/03/MOCAI-AI-Ethics-EN-1.pdf", "text": "PRINCIPLES & GUIDELINES\nARTIFICIAL INTELLIGENCE ETHICS AI ETHICS\nPRINCIPLES & GUIDELINES\nDEC 2022\nPage Page\n1. Overview 4\n1.1 Introduction 5\n1.2 Scope 6\n1.3 Strategic alignment 6\n1.4 Responsibility 8\n1.5 Licensing 8\n1.6 Toolkit 8\n2. Definitions 10\n3. Principles and Guidelines 16\n3.1 Fairness 17\n3.1.1 Principle 17\nWe will make AI systems fair\n3.1.2 Guidelines 20\n3.2 Accountable AI 20\n3.2.1 Principle 20\nWe will make AI systems accountable\n3.2.2 Guidelines 20\n3.3 Transparent AI 26\n3.3.1 Principle 26\nWe will make AI systems transparent\n3.3.2 Guidelines 26\n3.4 Explainable 29\n3.4.1 Principle 29\nWe will make AI systems as explainable as technically possible\n3.4.2 Guidelines 29\n3.5 Robust, Safe and Secure AI 31\n3.5.1 Principles 31\nAI systems will be technically robust\n3.5.2 Guidelines 32\n3.6 Human Centred AI 34\n3.6.1 Principles 34\nWe will give AI systems human values and make them beneficial to society\n3.6.2 Guidelines 35\n3.7 Sustainable and Environmentally Friendly AI 38\n3.7.1 Principle 38\nWe will promote sustainabity and environemntly friendly AI 38\n3.7.2 Guidelines 38\n3.8 Privacy Preserving AI 38\n3.8.1 Principle 40\nWe will respect people\u2019s privacy\n3.8.2 Guidelines 40\n4. Bibliography 44\nTable of Content\n2 3\nOverview\n01 1.1 Introduction\nAI\u2019s rapid advancement and innovation potential across a range of fields is incredibly exciting.\nYet a thorough and open discussion around AI ethics, and the principles organizations using this\ntechnology must consider, is needed.\nThis document - AI Ethics: Principles and Guidelines \u2013 intends to meet this need to balance these\ntwo central considerations of AI. It is designed to offer detailed guidance to help AI actors adhere\nto eight principles of AI ethics.\nThe guidelines are non-binding, and are drafted as a collaborative multistakeholder effort, with full awareness of organizations\u2019 needs to innovate\nand protect their intellectual property. This is a collaborative process in\nwhich all stakeholders are invited to be part of an ongoing dialogue. We\nwould like to see the AI Ethics Guidelines evolve into a universal, practical\nand applicable framework informing ethical requirements for AI design and\nuse. The eventual goal is to reach widespread agreement and adoption of\ncommonly agreed policies to inform the ethical use of AI nationally and\naround the world.\nWe will make AI systems that are:\nfair\nrobust, safe and\nsecure\naccountable\nhuman centered\ntransparent\nsustainable and\nenvironmentally\nfriendly\nexplainable\nprivacy preserving\n4 5\n1.2 Scope\n1.3 Strategic alignment\nThis document gives non-mandatory guidelines for achieving the ethical design\nand deployment of AI systems in both the public and private sectors. AI already\nsurrounds us, but some applications are more visible and sensitive than others.\nThis document is applicable only to those AI systems which make or inform\n\u2018significant decisions\u2019 \u2013 that is, those decisions which have the potential for\nsignificant impact either on individuals or on society as a whole. They also apply to\n\u2018critical decisions\u2019, which are a subset of significant decisions and are of especially\ncritical nature.\nThis document guides the further developments of sector specific requirements\nand principles, hence every entity within its sector can further amend, use, or\ncomplement its current frameworks to properly suit its context and the need of\naffected stakeholders.\nIt is a living document and will undergo further future reviews and enhancements.\nAdditionally, the AI Office is currently developing additional policies to be taken into consideration\nalong with this document:\nThis document comes as a fulfillment of the goals and aspirations set in the UAE National AI\nStrategy and the international Sustainable Development Goals (SDGs). The UAE has long been\nat the forefront of sustainable development. These shared global milestones have created global\nmovement and adoption in several disciplines and sectors. As an early adopter, the UAE has\nestablished a national committee on SDGs with a long-term realization of the UAE SDG agenda\n2030. These goals are further engrained and reflected in the AI Ethics Principles under the topics of\nfairness, inclusiveness, equality, human benefit, and rights.\nAI Seal\nThe UAE AI Seal brand (UAI) will be used\nto attract talent and business from\nacross the globe to come to the UAE to\ntest and develop AI. This includes a UAI\nmark recognizing high quality, ethical AI\ncompanies. It would reward safe, efficient,\nverified AI technology with a \u2018UAI Seal\nof Approval\u2019. The UAE AI Seal brand is\ncurrently under development by the AI\nOffice.\nAI Procurement Guidelines\nThe AI Procurement Guidelines will be\nused to guide Federal Government entities\nin the procurement of AI systems. They\nwill list the general principles, proposed\nmechanisms for procurement and review\nof AI products between the federal\ngovernment and vendors. The policy\nconsiders R&D for specific challenges\nthat do not currently have an AI solution\nproduct that is market-ready. Based on\nglobal standards for AI Procurement, the\npolicy aims to incentivize AI adoption\nwithin government entities and promote\nAI R&D within the private sector for\nspecified government challenges. The\nprocurement policy is currently under\ndevelopment by the AI Office.\n6 7\nThe AI Office will not be responsible for any misuse of the AI Ethics Principles and Guidelines. The\nuser bears all the consequences of their use.\nThis document is published under the terms of a Creative Commons Attribution 4.0 International\nLicense in order to facilitate its re-use by other governments and private sector organizations.\nIn summary this means you are free to share and adapt the material, including for commercial\npurposes, provided that you give appropriate credit to the UAE AI and Blockchain Council as its\nowner and do not suggest the Minister of State for Artificial Intelligence, Digital Economy and\nRemote Work Applications Office endorses your use.\nThe document is part of a toolkit of self-governing nature that boosts awareness and enables\ngovernment and private institutions to pursue innovative use cases while maintaining human\nvalues and principles.\n1.4 Responsibility\n1.5 Licensing\n1.6 Toolkit\n8 9\n2.1 AI developer organization\n2.2 AI operator organization\ndetermines the purpose of an AI system;\ndesigns an AI system;\nbuilds an AI system, or:\nperforms technical maintenance or tuning on an AI system\nuses AI systems in operations, backroom processes or decision-making;\nuses an AI system to provide a service to an AI subject;\nis a business owner of an AI system;\nprocures and treats data for use in an AI system; or\nevaluates the use case for an AI system and decides whether to proceed.\nAn organization which does any of the following:\nAn organization which does any of the following:\na\nd\nc\nb\nNote:\nThe definition applies regardless of whether the organization is the ultimate user of the system, or\nwhether they sell it on or give it away.\nNotes:\n1. This definition applies regardless of whether the AI system was developed in-house or procured.\n2. It is possible for organizations to be both an AI developer organization and an AI operator organization.\nDefinitions\n02\na\nd\ne\nc\nb\nExample: A company develops an artificially intelligent facial recognition system and\nsells it to a country\u2019s border force, who use it to identify suspicious personnel. The company\nis an AI developer organization and the border force is an AI operator organization.\n10 11\nThe capability of a functional unit to perform functions that are generally associated with human\nintelligence such as reasoning, learning and self-improvement1\n.\nInclination or prejudice for or against one person or group, especially in a way considered to be\nunfair2.\nAn individually significant decision which is deemed to either have a very large impact on an\nindividual or to have especially high stakes. These can be especially sensitive, have the potential\nto cause high loss or damage, be societally significant, or set an important precedent.\nEntity of hardware or software, or both, capable of accomplishing a specified purpose3 .\nA decision which has the potential for significant impact on at least one individual\u2019s\ncircumstances, behavior or choices, or has legal or similarly significant effects on him or her.\nA product, service, process or decision-making methodology whose operation or outcome is\nmaterially influenced by artificially intelligent functional units.\n1 Consistent with ISO/IEC 2382:2015\n2.3 Artificial Intelligence (also AI)\n2.5 Bias (of a system)\n2.6 Critical decision\n2.7 Functional Unit\n2.8 Individually significant decision\n2.4 Artificially Intelligent System (also AI system)\n2 Oxford Dictionary 2018, Oxford University Press, viewed online 4th October 2018, <https://en.oxforddictionaries.com>\n3 From ISO/IEC 2382:2015\nExample: A company decides to make an employee redundant. This is an individually\nsignificant decision because of its potential impact on the employee\u2019s financial situation.\nExample: A court determines whether a defendant is guilty of a criminal charge, with the\npunishment for guilt being a life sentence. This is a critical decision because it has a very\nlarge impact on the life of the defendant and also sets precedent for similar cases in the\nfuture.\nExample 1: A small claims court uses\nan artificially intelligent software package\nto collect evidence pertaining to a case,\ncompare it to similar cases in the past,\nand present a recommended decision\nto a judge. The judge determines the\nfinal outcome. This decision-making\nmethodology is materially influenced\nby an artificially intelligent functional\nunit, and is therefore classified as an AI\nsystem.\nExample 2: A government entity uses\na chatbot which allows customers to ask\nroutine questions, book appointments\nand conduct minor financial transactions.\nThe chatbot responds to customer\nqueries with pre-written responses and is\nbased on pre-programmed decision rules.\nTherefore, the chatbot is not an AI system.\nIf, however, the chatbot autonomously\nadjusted its treatment of customers\nbased on the outcome of past cases, it\nwould be an AI system.\nNotes:\n1. It is not necessary for a system\u2019s outcome to be solely determined by artificially intelligent\nfunctional units in order for the system to be defined as an artificially intelligent system; and\n2. A particular feature of AI systems is that they learn behavior and rules not explicitly\nprogrammed in.\nNotes:\nThe types of decisions referred to here are the same as those in the definition of significant-atscale decisions, except in this case the effects are felt as a result of an individual decision rather\nthan an aggregate of many decisions..\n12 13\n2.9 Non-operational bias (of a system)\n2.12 Subject of an artificially intelligent system (also AI subject)\n2.10 Set of significant-at-scale decisions\n2.11 Significant decision\nBias that is either:\nA natural person who is any of the following:\nA set of decisions made by the same system or organization which, when taken in aggregate,\nhave significant impact on society as a whole or groups within it.\nA decision which is either individually significant or is part of a set of significant-at-scale\ndecisions.\nnot a design feature; or\nnot important in achieving the stated purpose of the system.\nExample: An AI system is used by a website to determine which content to show users.\nThis decision is not individually significant, since a user is not greatly affected by whether\na particular piece of media is shown to them. However, if the website is popular then the AI\nsystem may be making a set of significant-at-scale decisions, because any biases in the\nsystem will affect a large number of users.\nan end-user of an AI system\ndirectly affected by the operation of or outcomes of an AI system, or:\na recipient of a service or recommendation provided by an AI system\na\nb\na\nb\nc\nNotes:\n1. The decisions need not be individually significant in order to qualify, in aggregate, as a set of\nsignificant-at-scale decisions; and\n2. Examples of areas which have a large impact on society and which include but are not limited\nto: the large-scale allocation of resources or opportunities amongst groups; the structure of\ngovernment; the division of power between large entities or groups; the law, and its interpretation\nand enforcement; conflict and war; international relations, etc.\n14 15\nPRINCIPLES AND\nGUIDELINES\n03 3.1 Fairness (Principle 1)\n3.1.2 Guidelines\nWe will make AI systems fair\n3.1.2.1\nBenefits of AI systems should be available and accessible to all. AI systems should not discriminate\nagainst people or groups in a way that could have an adverse impact. Especially significant and\ncritical decisions made or assisted by AI should be provably fair.\nData ingested should, where possible, be accurate and representative of the affected\npopulation.\nAlgorithms should avoid non-operational bias.\nSteps should be taken to mitigate and disclose the biases inherent in datasets.\nSignificant decisions should be provably fair.\nAll personnel involved in the development, deployment and use of AI systems have a role and\nresponsibility to operationalize AI fairness and should be educated accordingly.\nConsidering that fairness can have many different definitions, an organization should\ndocument its own definition of fairness for the context that the AI system is going to be\nimplemented in. Organizations should document what the implemented fairness objective\nstands for and why this choice was considered the most suitable for the given scenario.\nIt is recommended to identify and document demographic groups that may be adversely\nimpacted and mitigate the risk where possible.\nAI developers and AI operators could consider formal procedures such as Discrimination\nImpact Assessments as a means of ensuring fairness. This assessment should be\ndocumented as well as the mitigation measures that were implemented by the organization.\nThe impact assessment should be conducted pre-release and regularly after as an ongoing\nevaluation, results are advised to be documented.\n3.1.1 Principle\na\nb\nc\n16 17\n3.1.2.2\nConsideration should be given to whether the data ingested is accurate and representative\nof the affected population\n3.1.2.3\nConsideration should be given to whether decision-making processes introduce bias\n3.1.2.4\nSignificant decisions informed by the use of AI should be fair\n3.1.2.4\nAI operators should consider whether their AI systems are accessible and usable in a fair\nmanner across user groups\n3.1.2.5\nConsideration should be given to the effect of diversity on the development and deployment\nprocesses\nFairness has many different definitions in different cultures and for different contexts.\nEncouraging a diverse and inclusive AI ecosystem is thus all the more crucial to ensure\nthat one definition of fairness does not contradict another, and that the process of defining\nfairness itself is fair, with under-represented groups represented in the discussion.\nAI developers and AI operators should undertake reasonable data exploration and/or testing\nto identify potentially prejudicial decision-making tendencies in AI systems arising from\ndata inaccuracy and biases in the data.\nAI developers and operators should evaluate all datasets to assess inclusiveness of\nidentified demographic groups and collect data to close any gaps.\nAI developers and AI operators should refrain from training AI systems on data that is not\nlikely to be representative of the affected AI subjects, or is not likely to be accurate, whether\nthat be due to age, omission, method of collection, or other factors.\nWhen subjecting different groups to different decision-making processes, AI developers\nshould consider whether this will lead to non-operational bias.\nWhen evaluating the fairness of an AI system, AI developers and AI operators should\nconsider whether AI subjects in the same circumstances receive equal treatment.\nOrganizations should seek to include people from diverse demographic backgrounds across\nthe full lifecyle, to include design, development and deployment processes. Organizations\nshould seek to engage diverse internal and external groups also.\nAI developers should consider whether the assumptions they make about AI subjects could\nbe wrong or are likely to lead to non-operational bias; if so, they should consider consulting\nthe AI subjects in a representative manner during the design, development and deployment\nto confirm these assumptions.\nAI developers and AI operators could consider formal procedures such as Discrimination\nImpact Assessments as a means of ensuring fairness.\nAI developers should consider whether their AI systems can be expected to perform well\nwhen exposed to previously unseen data, especially when evaluating people who are not\nwell-represented in the training data.\nd\ne\nf\ng\nh\na\nb\na\nb\nc\nExample: Following a natural disaster, a government relief agency uses an AI system to\ndetect communities in greatest need by analyzing social media data from a range of websites.\nHowever, those communities where smartphone penetration is lower having less presence on\nsocial media, and so are at risk of receiving less attention. Therefore, the charity complements\ntheir AI tool with traditional techniques to identify needy populations elsewhere.\nExample: An organization uses an AI tool to automate the pre-screening of candidates\nfor a job opening. It is trained on data from the company\u2019s existing employees, the majority\nof whom are from the same ethnic background. Therefore, the system learns to use name\nand nationality as discriminating factors in filtering job applicants. This could have been\nidentified through testing and rectified by, for example, balancing the training data or only\nusing relevant data fields for training.\n18 19\n3.2 Accountable AI (Principle 2)\n3.2.1 Principle\n3.2.2 Guidelines\n3.2.2.1\nAccountability for the outcomes of an AI system should not lie with the system itself\n3.2.2.2\nPositive efforts should be made to identify and mitigate any significant risks inherent in the AI\nsystems designed\nAccountability for loss or damages resulting from the application of AI systems should not\nbe attributed to the system itself.\nAI operators and AI developers should consider designating individuals to be responsible for\ninvestigating and rectifying the cause of loss or damage arising from the deployment of AI\nsystems.\n5 Government of Canada. (2018). Responsible AI in the Government of Canada. Digital Disruption White Paper Series. Version 2.0, p.26. Retrieved from:\nhttps://docs.google.com/document/d/1Sn-qBZUXEUG4dVk909eSg5qvfbpNlRhzIefWPtBwbxY/edit\n6 Stoica, I. et. al.,2017, A Berkeley View of Systems Challenges for AI, p. 2, https://arxiv.org/pdf/1712.05855.pdf 4 Cabinet Office (UK), Data Science Ethical Framework, Version 1.0, licensed under the Open Government License v3.0, p. 13\nAI operators should identify the likely impact of incorrect automated decisions on AI\nsubjects and, in the case where incorrect decisions are likely to cause significant cost or\ninconvenience, consider mitigating measures.\nAI operators should consider internal risk assessments or ethics frameworks as a means to\nfacilitate the identification of risks and mitigating measures.\nIn designing AI systems to inform significant decisions, AI developers should consider\nmeasures to maintain data accuracy over time, including:\n\u2022 the completeness of the data.\n\u2022 timely update of the data, and.\n\u2022 whether the context in which the data was collected affects its suitability for the\nintended use case.\nAI operators should only use AI systems that are backed by evidence-based academic or\nindustrial research, and AI developers should base their development on such research.\na\nb\na\nb\nc\nd\nExample: A foreign country has a government service which identifies parents who owe\nmoney in child maintenance. The data matching process is often incorrect due to misspelled\nnames or missing data which results in some individuals being incorrectly targeted\nautomatically by the system with the result being a large bill, poor credit ratings and even\nfreezing wages. The recourse for individuals who are incorrectly targeted is time-consuming\nand not straightforward . If the potential impact of incorrect decisions had been assessed,\nmitigation measures such a user-friendly review procedure could have been set up.\nExample: A border camera scanning for predictors of risk may misinterpret a \u201ctic\u201d of an\nindividual with Tourette syndrome as suspicious. These can manifest in a diverse fashion, and\nshould not cause this person to undergo secondary inspection every time they pass through\nthe border . If the data is updated after the first case is encountered then it would avoid\ncausing inconvenience on subsequent visits.\nWe will make AI systems accountable\nAccountability for the outcomes of an AI system lies not with the system itself but is\napportioned between those who design, develop and deploy it;\nDevelopers should make efforts to mitigate the risks inherent in the systems they design;\nAI systems should have built-in appeals procedures whereby users can challenge significant\ndecisions;\nAI systems should be developed by diverse teams which include experts in the area in which\nthe system will be deployed; and\nAI systems should be subject to external audit and decision quality assurance.\n20 21\nAI developers and AI operators should tune AI models periodically to cater for changes to\ndata and/or models over time.\nAI operators should consider whether AI systems trained in a static environment will display\nmodel instability when deployed in dynamic environments.\nBe mindful that there might be fundamental tensions between different principles and\nrequirements. Continuously identify, evaluate, document and communicate these tradeoffs and their solutions.\nAdopt a Trustworthy AI assessment list when developing, deploying or using AI systems,\nand adapt it to the specific use case in which the system is being applied.\nKeep in mind that such an assessment list will never be exhaustive. Ensuring Trustworthy\nAI is not about ticking boxes, but about continuously identifying and implementing\nrequirements, evaluating solutions, ensuring improved outcomes throughout the AI\nsystem\u2019s lifecycle, and involving stakeholders in this.\nAI operators using AI systems to inform significant decisions should provide procedures by\nwhich affected AI subjects can challenge a specific decision concerning them.\nAI operators should consider such procedures even for non-significant decisions.\nAI operators should make affected AI subjects aware of these procedures and should\ndesign them to be convenient and user-friendly.\nAI operators should consider employing human case evaluators to review any such\nchallenges and, when appropriate, overturn the challenged decision.\nAI developers should collaborate with AI Operators to train models using historical data\nfrom the Operator.\nAI Operators should consider working with their vendors (AI developers) to continually\nmonitor performance.\nAI Operators should subject AI systems informing significant decisions to quality checks\nat least as stringent as those that would be required of a human being taking the same\ndecision.\nWhen AI systems are used for critical decisions, external auditing of the AI systems in\nquestion should be used as a means to ensure meaningful standards of transparency and\naccountability are upheld.\nIn the case that critical decisions are of civic interest, public release of the results of the\naudit should be considered as a means of ensuring public processes remain accountable\nto those affected by them.\nIn the case that critical decision are life and death decisions, these decisions should be\nsupported by further validation and verification via a human operator.\nFacilitate traceability and auditability of AI systems, particularly in critical contexts or\nsituations.\n3.2.2.3\nAI systems informing critical decisions should be subject to appropriate external audit\n3.2.2.4\nAI subjects should be able to challenge significant automated decisions concerning them and,\nwhere appropriate, be able to opt out of such decisions\n7\n Adapted from EU Commission, Can I be subject to automated individual decision-making, including profiling? Retrieved from: https://ec.europa.eu/\ninfo/law/law-topic/data-protection/reform/rights-citizens/my-rights/can-i-be-subject-automated-individual-decision-making-including-profiling_\nen#example\ne\nf\ng\nh\ni\nd\ne\nf\ng\na\nb\nc\nd\nExample: AI systems will need to be able to adapt to the changes in the environment that\nthey are deployed in. For example, a self-driving car would need to quickly adapt to unexpected\nand dangerous road by learning in real time from other cars that have successfully dealt with\nthese conditions. In addition, such mission-critical applications must handle noisy inputs\nand defend against malicious actors .\nExample: A bank allows customers to apply for a loan online by entering their data. The\nbank uses an AI system to automatically determine whether to give the loan and what the\ninterest rate should be. They provide users with an option to contest the decision and have\nit reviewed by a human . They also require that customers justify their challenge by filling in\na form, which assists the case reviewer and deters customers from challenging a decision\nwithout good reason.7\nc\nb\na\n22 23\nAI operators should consider instituting an opt-out mechanism for significant automated\ndecisions.\nAI operators could consider \u201ccrowd challenge\u201d mechanisms whereby a critical number of\ncomplaints triggers an investigation into the fairness and/or accuracy of a decision-making\nprocess as a whole.\nWhen informing an AI subject about significant choices they will make, AI systems should\nnot unreasonably restrict the available options or otherwise attempt to influence their value\njudgements without the explicit consent of the AI subject in question.\n3.2.2.5\nAI systems informing significant decisions should not attempt to make value judgements on\npeople\u2019s behalf without prior consent\n3.2.2.6\nAI systems informing significant decisions should be developed by diverse teams with\nappropriate backgrounds\n3.2.2.7\nAI operators should understand the AI systems they use sufficiently to assess their suitability\nfor the use case and to ensure accountability and transparency\nAI developers who develop AI systems which may be used to assist in making critical\ndecisions should involve in the process experts with a background in social science, policy,\nor another subject which prepares them to evaluate the broader societal impact of their\nwork.\nDevelopment of AI systems informing significant decisions should include consultation\nwith experts in the field in which the system will be deployed.\nIn the case of critical decisions, AI operators should avoid using AI systems that cannot be\nsubjected to meaningful standards of accountability and transparency.\nAI developers should consider notifying customers and AI operators of the use cases for\nwhich the system has been designed, and those for which it is not suitable.\n8 Financial Times. 2018. High-profile health app under scrutiny after doctor\u2019s complaints. Retrieved from: https://www.ft.com/content/19dc6b7e-8529-\n11e896-dd-fa565ec55929\ne\nf\na\na\nb\na\nb\nExample: An app that uses AI to assess medical symptoms and has a large user base had\nto face regulatory scrutiny because of number of complaints from doctors. They warned\nthat the application can miss signs of serious illness. A number of different shortcomings\nwere identified by doctors, some of which the company could address and resolve.8\n24 25\n3.3 Transparent AI (Principle 3)\nWe will make AI systems transparent\n3.3.1 Principle\n3.3.2 Guidelines\n3.3.2.1\nTraceability should be considered for significant decisions, especially those that have the\npotential to result in loss, harm or damage.\nFor AI systems which inform significant decisions, especially those with the potential to\ncause loss, harm or damage, AI developers should consider building in traceability (i.e. the\nability to trace the key factors leading to any specific decision).\nOrganizations should ensure that harms caused through AI systems are investigated and\nredressed, by enacting strong enforcement mechanisms and remedial actions, to make\ncertain that human rights and the rule of law are respected in the digital world and in the\nphysical world.\nTo facilitate the above, AI developers and AI operators should consider documenting the\nfollowing information during the design, development and deployment phases, and retaining\nthis documentation for a length of time appropriate to the decision type or industry:\n\u2022 the provenance of the training data, the methods of collection and treatment, how the data was\nmoved, and measures taken to maintain its accuracy over time;\n\u2022 the model design and algorithms employed; and\n\u2022 changes to the codebase, and authorship of those changes.\nWhere possible given the model design, AI developers should consider building in a means\nby which the \u201cdecision journey\u201d of a specific outcome (i.e. the component decisions leading\nto it) can be logged.\n9 See IBM WatsonPaths\na\nb\nc\nd\nExample: A technology company has a product which is designed to assist in medical\ndiagnosis. It documents each stage of its reasoning and relates it back to the input data.9\nDevelopers should build systems whose failures can be traced and diagnosed.\nPeople should be told when significant decisions about them are being made by AI.\nWithin the limits of privacy and the preservation of intellectual property, those who deploy\nAI systems should be transparent about the data and algorithms they use.\nResponsible disclosures should be provided in a timely manner, and provide reasonable\njustifications for AI systems outcomes. This includes information that helps people\nunderstand outcomes, like key factors used in decision making.\n26 27\n3.3.2.2\nPeople should be informed of the extent of their interaction with AI systems\nAI operators should inform AI subjects when a significant decision affecting them has been\nmade by an AI system.\nIf an AI system can convincingly impersonate a human being, it should do so only after\nnotifying the AI subject that it is an AI system.\n3.4 Explainable AI (Principle 4)\nWe will make AI systems as explainable as\ntechnically possible\n3.4.1 Principle\nDecisions and methodologies of AI systems which have a significant effect on individuals\nshould be explainable to them, to the extent permitted by available technology.\nIt should be possible to ascertain the key factors leading to any specific decision that\ncould have a significant effect on an individual.\nIn the above situation we will provide channels through which people can request such\nexplanations.\n3.4.2 Guidelines\n3.4.2.1\nAI operators could consider providing affected AI subjects with a high-level explanation of\nhow their AI system works\nAI operators could consider informing the affected AI subjects in understandable, nontechnical language of:\n\u2022 the data that is ingested by the system;\n\u2022 the types of algorithms employed;\n\u2022 the categories into which people can be placed; and\n\u2022 the most important features driving the outcomes of decisions\n\u2022 A comprehensive list of feature engineering and models that was considered during\nthe model building phase.\na\nb\na\nExample: A small claims court adjudicates minor civil matters such as debt collection and\nevictions. They introduce an AI system to suggest the outcome of a ruling. At the time of the\nruling the plaintiff and defendant are notified that the decision was assisted by an AI system.\nThe court also provides an explanation for the decision.\nExample: A technology company produces a conversational AI agent which can make\nsome phone calls on behalf of its users. Those who receive the calls may believe that they\nare speaking to a human. Therefore, the company programs the agent to identify itself at\nthe start of every conversation.\nExample: A person turned down for a credit card might be told that the algorithm took\ntheir credit history, age, and postcode into account, but not learn why their application was\nrejected10 .\n28 29\nFor non-sensitive public sector use cases designed for the common good, AI operators\ncould consider making source code, together with an explanation of the workings of the AI\nsystem, available either publicly or upon request (this should be done only if there is low risk\nof people \u2018gaming the system\u2019).\nThe AI operator should maintain necessary documentation that provides elaboration and\nclarification on how the algorithm works, for example documentation of processes, decision\nmaking flow charts of the system, and how the appeal process is embedded.\nAn individual should have the ability to contest and seek effective redress against decisions\nmade by AI systems. These must be addressed by the group or team supporting these\nmodels.\nDevelop appropriate impact indices for the evaluation of AI system technological\ninterventions from multiple perspectives.\nIn the case that such explanations are available, they should be easily and quickly accessible,\nfree of charge and user-friendly.\n3.4.2.2\nAI operators should consider providing affected AI subjects with a means to request\nexplanations for specific significant decisions, to the extent possible given the state of present\nresearch and the choice of model\nAI operators should consider providing a means by which people affected by a significant\ndecision informed by AI can access the reasoning behind that decision.\nWhere such explainability is not possible given available technology, AI operators should\nconsider compromises such as counterfactual reasoning, or listing the most heavily\nweighted factors contributing to the decision.\n3.5 Robust, Safe and Secure AI (Principle 5)\nAI systems will be technically robust\n3.5.1 Principles\nAI systems should be technically robust with a preventative approach to risks which\noperates in a manner such that they reliably behave.\nAI Developers should ensure AI systems will not cause any unintentional harm and\nadverse impacts.\nAI systems should be resilient to attacks and security such as data poisoning and\nmodel leakage.\nAI systems should have safeguards that enable a fallback plan in case of problems.\nAI system results should be reproducible.\n11 Consumer Financial Protection Bureau, 12 CFR Part 1002 - Equal Credit Opportunity Act (Regulation B), \u00a7 1002.9 Notifications, Retrieved from https://www.\nconsumerfinance.gov/policy-compliance/rulemaking/regulations/1002/\n12 Predict website, accessible at http://www.predict.nhs.uk/predict_v2.1/legal/algorithm\n10 The Guardian. AI watchdog needed to regulate automated decision-making, say experts. Retrieved from: https://www.theguardian.com/technology/2017/\njan/27/ai-artificial-intelligence-watchdog-needed-to-prevent-discriminatory-automated-decisions\nb\nc\nd\ne\na\nb\nc\nExample: The US Consumer Financial Protection Bureau requires that creditors in the US\nwho reject credit applications must explain to the applicant the principal reason(s) why their\napplication was rejected (e.g. \u201clength of residence\u201d or \u201cage of automobile\u201d)11 . In particular,\n\u201cstatements that the adverse action was based on the creditor`s internal standards or\npolicies or that the applicant, joint applicant, or similar party failed to achieve a qualifying\nscore on the creditor`s credit scoring system are insufficient\u201d.\nExample: The UK\u2019s NHS developed a tool called Predict, which allows women with breast\ncancer to compare their case to other women who have had the same condition in the past,\nand visualize the expected survival rate under various treatment options. The website has an\nexplanation page which shows the weights behind various factors and contains a description\nof the underlying mathematics12 .\n30 31\n3.5.2 Guidelines\n3.5.2.1\nAI Operators should continue conducting vulnerability assessments, verification of AI system\ndifferent behaviors in unexpected situations and for any dual-use case, to include:\n3.5.2.2\nAI operators should define a suitable fall back plan and test it to maintain readiness in\nunexpected situations and in high safety risks level, to include:\n3.5.2.3\nAI Operators should assure end users of the system\u2019s reliability through documentation, and\noperationalizing processes for testing and verification of desired outcomes , specifically:\n3.5.2.4\nAI systems will be safe, secure and controllable by humans\nPutting measures to ensure integrity and resilience of IA system against potential attacks.\nDeveloping a plan to measure and assess potential safety risks to you or any third party\nfrom technology use accidental or malicious misuse.\nDefine thresholds for system acceptable results and governance procure to fall back on\nalternative defined and tested plans.\nConsidering an insurance policy to mitigate risks arising from potential damages.\nAI system should be designed with an approach to continue monitoring if the system meets\ngoals, purposes and intended application.\nAI Operators should define an approach to ensure results are reproducible through clear\nprocess and documentation.\nAI Operators should publish documentation to assure system robustness to end users.\nSafety and security of people, be they operators, end-users or other parties, will be of\nparamount concern in the design of any AI system.\nAI systems should be verifiably secure and controllable throughout their operational\nlifetime, to the extent permitted by technology.\nThe continued security and privacy of users should be considered when decommissioning\nAI systems.\nAI systems that may directly impact people\u2019s lives in a significant way should receive\ncommensurate care in their designs.\nSuch systems should be able to be overridden or their decisions reversed by designated\nindividuals.\nVerifying how AI systems behave in unexpected situations and environments.\nTaking preventative measures against any possible system dual-case use.\n13 IBM. https://documents.trendmicro.com/assets/white_papers/wp-malicious-uses-and-abuses-of-artificial-intelligence.pdf\na\nb\nc\na\nb\nc\na\na\nb\nb\nc\nc\nd\ne\nExample: IBM researchers presented a novel approach to exploiting AI for malicious\npurposes. Their system, DeepLocker embeds AI capabilities within the malware itself in order\nto improve its evasion techniques. It uses an Artificial Intelligence model to identify its target\nusing indicators like facial recognition, geolocation and voice recognition.\nExample: Adopting a risk-based approach to procurement and clearly communicating it to\nvendors can help address such issues by giving the procuring organization advance notice\nof the specific oversight capabilities it will need in future stages of the system lifecycle,\npreventing vendors from presenting intellectual property arguments against required testing,\nmonitoring and auditing of their AI systems going forward and rewarding vendors\u2014of all\nsizes\u2014that are more advanced and responsive in their responsible AI efforts.\n32 33\n3.5.2.5\nAi systems should not be able to autonomously hurt, destroy or deceive humans\nAI systems should be built to serve and inform, and not to deceive and manipulate.\nNations should collaborate to avoid an arms race in lethal autonomous weapons, and\nsuch weapons should be tightly controlled.\nActive cooperation should be pursued to avoid corner-cutting on safety standards.\nSystems designed to inform significant decisions should do so impartially.\n3.6 Human Centered AI (Principle 6)\nWe will give AI systems human values and\nmake them beneficial to society\n3.6.1 Principles\nGovernment will support the research of the beneficial use of AI.\nStakeholders throughout society should be involved in the development of AI and its\ngovernance.\nDesign AI systems to adopt, learn and follow the norms and values of the community\nthey serve.\nSystematic analyses that examine the ethics of designing affective systems to nudge\nhuman beings prior to deployment are needed.\nDecisions related to lethal force, life and death should not be delegated to AI systems.\nRules and standards should be adopted to ensure effective human control over those\ndecisions.\n3.6.2 Guidelines\n3.6.2.1\nAI should be developed to align with human values, contribute to human flourishing and\nbenefit society.\n3.6.2.2\nSystematic analyses that examine the ethics of designing affective systems to nudge human\nbeings prior to deployment are needed.\nSociety should be consulted in a representative fashion to inform the development of AI.\nStakeholders throughout society should be involved in the development of AI and its\ngovernance.\nStakeholders should proactively engage in responsible stewardship of trustworthy AI in\npursuit of beneficial outcomes for people and the planet.\nOrganizations should prioritize having all their stakeholders learn about wellbeing metrics\nas a potential determinant of how they create, deploy, market and monitor their AI\ntechnologies.\nNudging in AI systems should have an opt-in system policy with explicit consent.\nWe recommend that when appropriate, an affective system that nudges human beings\nshould have the ability to accurately distinguish between users, including detecting\ncharacteristics such as whether the user is an adult or a child. Additional protections must\nbe put in place for vulnerable populations, such as children, when informed consent cannot\nbe obtained, or when it may not be a sufficient safeguard.\nAI systems with nudging strategies must be carefully evaluated, monitored, and controlled.\n14 IEEE. https://sagroups.ieee.org/7010/\na\nb\nc\nd\na\nb\nd\nc\na\nb\nc\nExample: IEEE P7010 recommended practice14 establishes wellbeing metrics relating to\nhuman factors directly affected by intelligent and autonomous systems and establishes a\nbaseline for the types of objective and subjective data these systems should analyze and\ninclude (in their programming and functioning) to proactively increase human wellbeing.\n34 35\n3.6.2.3\nHumanity should retain the power to govern itself and make the final decision, with AI in an\nassisting role.\n3.6.2.4\nAI systems should be designed in a way that respects the rule of law, human rights, society\nvalues, and should include appropriate safeguards to ensure a fair and just society.\nDecisions related to lethal force, life and death should not be delegated to AI systems. Rules\nand standards should be adopted to ensure effective human control over those decisions.\nResponsible parties (e.g., parents, nurse practitioners, social workers, and governments)\nshould be trained to detect the influence due to AI and ineffective mitigation techniques.\nIn the most extreme cases it should always be possible to shut down harmful AI system.\nThose actions undertaken by an affective system that are most likely to generate an\nemotional response should be designed to be easily changed.\nUsers should be able to make informed autonomous decisions regarding AI systems. They\nshould be given appropriate knowledge and tools to comprehend and interact with AI\nsystems to satisfactory degree and, where possible to reasonably self-assess or challenge\nthe system.\nAI Developers should design AI systems to adopt, learn and follow the norms and values of\nthe community they serve.\nOrganizations could identify the norms of the specific community in which AI systems are\nto be deployed in. In particular, pay attention to the norms relevant to the kinds of tasks that\nthe AI systems are designed to perform. These could be documented as well as how these\nnorms are addressed by the AI system.\nTo respond to the dynamic change of norms in society the AI system could be able to adjust\nits existing norms and learn new ones, while being transparent about these changes.\nDesigners should consider forms and metrics for assessing an AI system\u2019s norm conformity\nover the lifetime of the system (e.g. human-machine agreement on moral decisions,\nverifiability of AI decisions, justified trust).\nThe norm identification process must document the similarities and differences between\nthe norms that humans apply to other humans and the norms they apply to AI systems.\nNorm implementations should be evaluated specifically against the norms that the\ncommunity expects the AI system to follow.\nIn situations where it is needed, human rights impact assessments and human rights due\ndiligence, human determination codes of ethical conduct, or quality labels and certifications\nintended to promote human centered values and fairness should be considered.\nMechanisms should be put into place to receive external feedback regarding AI systems\nthat potentially infringe on fundamental rights.\na\nb\nc\nd\na\nb\nc\nd\ne\nf\ng\n36 37\n3.7 Sustainable and Environmentally Friendly AI (Principle 7)\nWe will promote sustainable and environmentally\nfriendly AI\n3.7.1 Principle\n3.7.2 Guidelines\n3.7.2.1 Throughout the AI lifecycle, implementations should always be carried out after full\nunderstanding and acknowledgement of AI implications on sustainability and environment.\nThe application of Artificial Intelligence to address sustainability challenges are well\nunderstood. Building and running green, sustainable AI systems is of increasing importance,\ngiven the large carbon footprint that they can generate and the wider context of addressing\nclimate change.\nAI model development, and by extension commissioning, should therefore seek to balance\ntechnical performance with energy consumption and environmental impact.\nEfforts should be made to estimate and understand sustainability and environmental\nimpact across the AI lifecycle (e.g. the energy consumption costs associated with model\ntraining, and CO2 emissions and cloud compute costs associated with the deployment\nand running of the system).\nWhile there is no standardized means of reporting on the environmental impact of AI\nsystems, where possible, model development and commissioning should make carbon\nimpacts a core consideration alongside functional and business requirements.\nSmaller models should be considered \u2013 shrinking down the model size and using fewer\ncompute cycles (to balance financial and performance costs with the end performance of\nthe model).\nCarbon awareness should be considered \u2013 adjusting the operational parameters of the AI\nsystem to dynamically select the best time and location for energy use from the grid can\nreduce its carbon footprint.\nAI systems should be used to benefit all human beings, including future generations. The\nenvironment is fundamental to this.\nThroughout the AI lifecycle, implementations should only be carried out on the basis of a full\nunderstanding and acknowledgement of implications for sustainability and environment.\nAn AI system\u2019s development, deployment and use, should be assessed via critical examination\nof resource usage and energy consumption.\nMechanisms to measure environmental impact due to type of energy use and processing\npower provided by data centers should be established.\n15 https://www.deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-by-40\na\nb\nc\nd\ne\nf\nExample: In agriculture, AI can transform\nproduction by better monitoring and\nmanaging environmental conditions\nand crop yields. AI can help reduce both\nfertilizer and water, all while improving\ncrop yields.\nExample: Google`s DeepMind division\nhas developed AI that teaches itself to\nminimize the use of energy to cool Google`s\ndata centers. As a result, Google reduced\nits data center energy requirements by\n40% 15.\n38 39\n3.8 Privacy Preserving AI (Principle 8)\nWe will respect people\u2019s privacy\n3.8.1 Principle\nAI systems should respect privacy and use the minimum intrusion necessary.\nAI systems should uphold high standards of data governance and security,\nprotecting personal information.\nSurveillance or other AI-driven technologies should not be deployed to the extent\nof violating internationally and/or UAE\u2019s accepted standards of privacy and human\ndignity and people rights.\nPrivacy by design should be embedded in AI systems and where possible AI\nalgorithm should have adequate privacy impact assessments.\nAdequate data protection frameworks and governance mechanisms should be\nestablished in a multi-stakeholder approach and ensured throughout the life cycle\nof AI systems.\nAI developers and operators should strive to strike the balance between privacy\nrequirements, individual rights and innovation growth and society benefits.\n3.8.2 Guidelines\n3.8.2.1\nEstablish mechanism for users to flag issues related to privacy or data protection\nAI Operators should review the system for proper consent logging, ability of users to revoke\npermission whenever applicable.\nConsider training AI model without or with minimal user of potentially sensitive or personal\ndata.\nUse measures to enhance privacy such as encryption, anonymization and aggregation.\n3.8.2.2\nEstablish an oversight mechanism for data collection, storage and processing and use across\nyour organization\nAssess who can access data and under which conditions.\nAssign specific responsibilities and role for Data Protection Officers.\nPrevention of harm to privacy necessitates adequate data governance that covers the\nquality and integrity of the data used, its relevance in light of the domain in which the\nAI systems will be deployed, its access protocols and the capability to process data in a\nmanner that protects privacy.\nAI systems must guarantee privacy and data protection throughout a system\u2019s entire\nlifecycle. This includes the information initially provided by the user, as well as the\ninformation generated about the user over the course of their interaction with the system\n(e.g. outputs that the AI systems generated for specific users or how users responded to\nparticular recommendations). Digital records of human behavior may allow AI systems\nto infer not only individuals\u2019 preferences, but also their age, gender, religious or political\nviews. To allow individuals to trust the data gathering process, it must be ensured that data\ncollected about them will not be used to unlawfully or unfairly discriminate against them.\nIn any given organization that handles individuals\u2019 data (whether someone is a user of the\nsystem or not), data protocols governing data access should be put in place in line with\nnational privacy legislation (be these universal or sector-specific). These protocols should\noutline who can access data and under which circumstances. Only duly qualified personnel\nwith the competence and need to access individual\u2019s data should be allowed to do so.\na\nb\nc\na\nb\nc\nd\ne\n40 41\nAlgorithmic systems require adequate privacy impact assessments, which also include\nsocietal and ethical considerations of their use and an innovative use of the privacy by\ndesign approach. AI actors need to ensure that they are accountable for the design and\nimplementation of AI systems in such a way as to ensure that personal information is\nprotected throughout the life cycle of the AI system.\nEstablish data policies or equivalent frameworks, or reinforce existing ones, to ensure full\nsecurity for personal data and sensitive data, which, if disclosed, may cause exceptional\ndamage, injury or hardship to individuals. Examples include data relating to offences,\ncriminal proceedings and convictions, and related security measures; biometric, genetic\nand health data; and personal data such as that relating to race, descent, gender, age,\nlanguage, religion, political opinion, national origin, ethnic origin, social origin, economic or\nsocial condition of birth, or disability and any other characteristics.\nPromote mechanisms, such as open repositories for publicly funded or publicly held data,\nsource code and data trusts, to support the safe, fair, legal and ethical sharing of data,\namong others.\nPromote and facilitate the use of quality and robust datasets for training, development and\nuse of AI systems, and exercise vigilance in overseeing their collection and use.\n16 https://research.aimultiple.com/privacy-enhancing-technologies/\nf\ng\nh\ni\nExample: with the help of AI & ML algorithms16 , Synthetic data can be created to enhance\nprivacy. The data created will have the same statistical characteristics for the testing\nenvironment and where a third party can have access.\n42 43\nBibliography\n04 Microsoft. 2022. Microsoft Responsible AI Standard, v2. General Requirements. June 2022.\nRetrieved from:\nhttps://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4ZPmV\nISO. 2022. Proposed ISO Standard on Risk Management of AI: What Businesses Should Know.\nMay 2022. Retrieved from:\nhttps://unesdoc.unesco.org/ark:/48223/pf0000381137/PDF/381137eng.pdf.multi\nGSMA 2022. The AI Ethics Playbook. Implementing ethical principles into everyday business.\nFebruary 2022. Retrieved from:\nhttps://www.gsma.com/betterfuture/wp-content/uploads/202201//The-Mobile-Industry-EthicsPlaybook_Feb-2022.pdf\nOECD. 2022. AI Policy Observatory. Retrieved from:\nhttps://standards.ieee.org/wp-content/uploads/import/documents/other/ead_v2.pdf\nUNESCO. 2022. Recommendation on the Ethics of Artificial Intelligence. Adopted in November\n2021. Retrieved from:\nhttps://unesdoc.unesco.org/ark:/48223/pf0000381137/PDF/381137eng.pdf.multi\nEuropean Commission. 2021. Proposal for a Regulation of the European Parliament and of the\nCouncil Laying Down Harmonized Rules on Artificial Intelligence (Artificial Intelligence Act) and\nAmending Certain Union Legislative Acts. Retrieved from:\nhttps://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A52021PC0206\nEuropean Commission. High Level Expert Group on Artificial Intelligence ( AI HLEG ). Ethics\nGuidelines for Trustworthy AI . April 2019. Retrieved from:\nhttps://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai\nCentre for Data Ethics and Innovation. UK Government: The roadmap to an effective AI assurance\necosystem. December 2021. Retrieved from:\nhttps://www.gov.uk/government/publications/the-roadmap-to-an-effective-ai-assuranceecosystem\nThe New York City Council. 2021. A Local Law to amend the administrative code of the city of New\nYork, in relation to automated employment decision tools. Retrieved from:\nhttps://legistar.council.nyc.gov/LegislationDetail.aspx?ID=4344524&GUID=%20B051915D-A9AC451E-81F86596032-FA3F9&\nUNESCO, Recommendation on the Ethics of Artificial Intelligence. 2021. Retrieved from:\nhttps://unesdoc.unesco.org/ark:/48223/pf0000381137\n44 45\nAI NOW Institute. Algorithmic accountability for the public sector. August 2021. Retrieved from:\nhttps://www.opengovpartnership.org/documents/algorithmic-accountability-public-sector/\nIEEE 2019. Ethically Aligned Design. Version 2 \u2013 For Public Discussion. Retrieved from:\nhttps://standards.ieee.org/wp-content/uploads/import/documents/other/ead_v2.pdf\nIEEE 2017. Ethically Aligned Designed : A vision for Prioritizing Human Well-being with\nAutonomous and Intelligent Systems. Retrieved from:\nhttps://standards.ieee.org/wp-content/uploads/import/documents/other/ead_v2.pdf\nGoogle. AI Principles 2021. Retrieved from:\nhttps://ai.google/principles/\nMicrosoft. Responsible AI Standard v2 (June 2022). Retrieved from:\nhttps://www.microsoft.com/en-us/ai/responsible-ai\nThe ECONOMIST Group 2022 . Pushing forward: the future of AI in the Middle East and North\nAfrica Report. Retrieved from:\nhttps://impact.economist.com/perspectives/sites/default/files/google_ai_mena_report.pdf\nPDPC. (2018, June 5). Discussion paper on Artificial Intelligence (AI) and Personal Data.\nSingapore: Personal Data Protection Commission Singapore (PDPC). Retrieved from:\nhttps://www.pdpc.gov.sg/-/media/Files/PDPC/PDF-Files/Resource-for-Organisation/AI/\nDiscussion-Paper-on-AI-and-PD---050618.pdf\nITI. AI Policy Principles. Retrieved from:\nhttps://www.itic.org/public-policy/ITIAIPolicyPrinciplesFINAL.pdf\nCabinet Office.(2016, May 19). Data Science Ethical Framework. Retrieved from:\nhttps://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/\nfile/524298/Data_science_ethics_framework_v1.0_for_publication__1_.pdf\nEuropean Parliament. (2017, February 16). European Parliament resolution of 16 February 2017\nwith recommendations to the Commission on Civil Law Rules on Robotics (20152103/(INL)).\nRetrieved from:\nhttp://www.europarl.europa.eu/sides/getDoc.do?pubRef=-//EP//TEXT+TA+P8-TA-2017-\n0++0051DOC+XML+V0//EN\nVillani, C. (2018). For a meaningful Artificial Intelligence towards a French and European strategy.\nRetrieved from:\nhttps://www.aiforhumanity.fr/pdfs/MissionVillani_Report_ENG-VF.pdf\nCNIL. (2017). How can humans keep the upper hand? The ethical matters raised by algorithms\nand artificial intelligence. Retrieved from:\nhttps://www.cnil.fr/sites/default/files/atoms/files/cnil_rapport_ai_gb_web.pdf\nExecutive Office of the President of the United States. National Science and Technology Council.\nNetworking and Information Technology Research and Development Subcommittee. (2016). The\nnational artificial intelligence research and development strategic plan. Retrieved from:\nhttps://www.nitrd.gov/PUBS/national_ai_rd_strategic_plan.pdf\nExecutive Office of the President of the United States National Science and Technology Council.\nCommittee on Technology. (2016). Preparing for the future of artificial intelligence. Retrieved from:\nhttps://obamawhitehouse.archives.gov/sites/default/files/whitehouse_files/microsites/ostp\nNSTC/preparing_for_the_future_of_ai.pdf\nThe Headquarters for Japan\u2019s Economic Revitalization. (2015). New Robot Strategy. Japan\u2019s\nRobot Strategy. Vision, Strategy, Action Plan. Retrieved from:\nhttp://www.meti.go.jp/english/press/2015/pdf/0123_01b.pdf\nTreasury Board of Canada Secretariat. (2018). Responsible Artificial Intelligence in the\nGovernment of Canada. Digital Disruption White Paper Series. Version 2.0.\nhttps://docs.google.com/document/d/1Sn-qBZUXEUG4dVk909eSg5qvfbpNlRhzIefWPtBwbxY/\nedit\nThe Toronto Declaration: Protecting the right to equality and non-discrimination in machine\nlearning systems. Toronto, Canada: Amnesty International and Access Now. Retrieved from:\nhttps://www.accessnow.org/cms/assets/uploads/201808//The-Toronto-Declaration_ENG_08-\n2018.pdf\nhttp://data.parliament.uk/writtenevidence/committeeevidence.svc/evidencedocument/artificialintelligence-committee/artificial-intelligence/oral/73546.html (house of lords select committee\noral evidence, Q61)oral evidence, Q61)\nHouse of Lords Select Committee on Artificial Intelligence. (2018). AI in the UK: ready, willing and\nable?\nhttps://publications.parliament.uk/pa/ld201719/ldselect/ldai/100100/.pdf\nHouse of Commons Science and Technology Select Committee. Algorithms in Decision Making.\nhttps://publications.parliament.uk/pa/cm201719/cmselect/cmsctech/351351/.pdf\n46 47 ", "metadata": {"country": "UAE", "year": "2022", "legally_binding": "no", "binding_proof": "None", "date": "12-15", "regulator": "Ministry of Artificial Intelligence, Digital Economy and Remote Work Applications", "type": "non-binding guidelines", "status": "active", "language": "English, Arabic", "use_cases": "[1, 3, 5, 6]"}}
