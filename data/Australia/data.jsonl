{"_id": "686a91dd287dec28178eb1f4", "title": "Australia AI Ethics Principles", "source": "https://www.industry.gov.au/publications/australias-artificial-intelligence-ethics-principles/australias-ai-ethics-principles", "text": "Australia's 8 Artificial Intelligence (AI) Ethics Principles are designed to ensure AI is safe, secure and reliable.\n\nThey will help:\n\nachieve safer, more reliable and fairer outcomes for all Australians\n\nreduce the risk of negative impact on those affected by AI applications\n\nbusinesses and governments to practice the highest ethical standards when designing, developing and implementing AI.\n\nWhen to apply these principles\n\nThe principles are entirely voluntary. They are designed to prompt organisations to consider the impact of using AI enabled systems. We intend them to be aspirational and complement, not substitute, existing AI regulations and practices.\n\nNot every principle will be relevant to your use of AI.\n\nNot every business uses AI, and not every use of AI requires comprehensive analysis against the principles. For example, many businesses use systems that may incorporate AI such as email or accounting software. This use is unlikely to be of sufficient impact to require the use of the principles.\n\nIf your AI use doesn't involve or affect human beings, you may not need to consider all of the principles.\n\nConsider these questions\n\nTo help decide whether you should apply these ethical principles, consider the following threshold questions. If you answer 'yes' then applying the principles could help you plan for better outcomes.\n\nWill the AI system you are developing or implementing be used to make decisions or in other ways have a significant impact (positive or negative) on people (including marginalised groups), the environment or society?\n\nAre you unsure about how the AI system may impact your organisation or your customers/clients?\n\nPrinciples at a glance\n\nHuman, societal and environmental wellbeing: AI systems should benefit individuals, society and the environment.\n\nAI systems should benefit individuals, society and the environment. Human-centred values: AI systems should respect human rights, diversity, and the autonomy of individuals.\n\nAI systems should respect human rights, diversity, and the autonomy of individuals. Fairness: AI systems should be inclusive and accessible, and should not involve or result in unfair discrimination against individuals, communities or groups.\n\nAI systems should be inclusive and accessible, and should not involve or result in unfair discrimination against individuals, communities or groups. Privacy protection and security: AI systems should respect and uphold privacy rights and data protection, and ensure the security of data.\n\nAI systems should respect and uphold privacy rights and data protection, and ensure the security of data. Reliability and safety: AI systems should reliably operate in accordance with their intended purpose.\n\nAI systems should reliably operate in accordance with their intended purpose. Transparency and explainability: There should be transparency and responsible disclosure so people can understand when they are being significantly impacted by AI, and can find out when an AI system is engaging with them.\n\nThere should be transparency and responsible disclosure so people can understand when they are being significantly impacted by AI, and can find out when an AI system is engaging with them. Contestability: When an AI system significantly impacts a person, community, group or environment, there should be a timely process to allow people to challenge the use or outcomes of the AI system.\n\nWhen an AI system significantly impacts a person, community, group or environment, there should be a timely process to allow people to challenge the use or outcomes of the AI system. Accountability: People responsible for the different phases of the AI system lifecycle should be identifiable and accountable for the outcomes of the AI systems, and human oversight of AI systems should be enabled.\n\nPrinciples in detail\n\nHuman, social and environmental wellbeing\n\nThroughout their lifecycle, AI systems should benefit individuals, society and the environment.\n\nThis principle aims to clearly indicate from the outset that AI systems should be used for beneficial outcomes for individuals, society and the environment. AI system objectives should be clearly identified and justified. AI systems that help address areas of global concern should be encouraged, like the United Nation's Sustainable Development Goals. Ideally, AI systems should be used to benefit all human beings, including future generations.\n\nAI systems designed for legitimate internal business purposes, like increasing efficiency, can have broader impacts on individual, social and environmental wellbeing. Those impacts, both positive and negative, should be accounted for throughout the AI system's lifecycle, including impacts outside the organisation.\n\nHuman-centred values\n\nThroughout their lifecycle, AI systems should respect human rights, diversity, and the autonomy of individuals.\n\nThis principle aims to ensure that AI systems are aligned with human values. Machines should serve humans, and not the other way around. AI systems should enable an equitable and democratic society by respecting, protecting and promoting human rights, enabling diversity, respecting human freedom and the autonomy of individuals, and protecting the environment.\n\nHuman rights risks need to be carefully considered, as AI systems can equally enable and hamper such fundamental rights. It's permissible to interfere with certain human rights where it's reasonable, necessary and proportionate.\n\nAll people interacting with AI systems should be able to keep full and effective control over themselves. AI systems should not undermine the democratic process, and should not undertake actions that threaten individual autonomy, like deception, unfair manipulation, unjustified surveillance, and failing to maintain alignment between a disclosed purpose and true action.\n\nAI systems should be designed to augment, complement and empower human cognitive, social and cultural skills. Organisations designing, developing, deploying or operating AI systems should ideally hire staff from diverse backgrounds, cultures and disciplines to ensure a wide range of perspectives, and to minimise the risk of missing important considerations only noticeable by some stakeholders.\n\nFairness\n\nThroughout their lifecycle, AI systems should be inclusive and accessible, and should not involve or result in unfair discrimination against individuals, communities or groups.\n\nThis principle aims to ensure that AI systems are fair and that they enable inclusion throughout their entire lifecycle. AI systems should be user-centric and designed in a way that allows all people interacting with it to access the related products or services. This includes both appropriate consultation with stakeholders, who may be affected by the AI system throughout its lifecycle, and ensuring people receive equitable access and treatment.\n\nThis is particularly important given concerns about the potential for AI to perpetuate societal injustices and have a disparate impact on vulnerable and underrepresented groups including, but not limited to, groups relating to age, disability, race, sex, intersex status, gender identity and sexual orientation. Measures should be taken to ensure the AI produced decisions are compliant with anti-discrimination laws.\n\nPrivacy protection and security\n\nThroughout their lifecycle, AI systems should respect and uphold privacy rights and data protection, and ensure the security of data.\n\nThis principle aims to ensure respect for privacy and data protection when using AI systems. This includes ensuring proper data governance, and management, for all data used and generated by the AI system throughout its lifecycle. For example, maintaining privacy through appropriate data anonymisation where used by AI systems. Further, the connection between data, and inferences drawn from that data by AI systems, should be sound and assessed in an ongoing manner.\n\nThis principle also aims to ensure appropriate data and AI system security measures are in place. This includes the identification of potential security vulnerabilities, and assurance of resilience to adversarial attacks. Security measures should account for unintended applications of AI systems, and potential abuse risks, with appropriate mitigation measures.\n\nReliability and safety\n\nThroughout their lifecycle, AI systems should reliably operate in accordance with their intended purpose.\n\nThis principle aims to ensure that AI systems reliably operate in accordance with their intended purpose throughout their lifecycle. This includes ensuring AI systems are reliable, accurate and reproducible as appropriate.\n\nAI systems should not pose unreasonable safety risks, and should adopt safety measures that are proportionate to the magnitude of potential risks. AI systems should be monitored and tested to ensure they continue to meet their intended purpose, and any identified problems should be addressed with ongoing risk management as appropriate. Responsibility should be clearly and appropriately identified, for ensuring that an AI system is robust and safe.\n\nTransparency and explainability\n\nThere should be transparency and responsible disclosure so people can understand when they are being significantly impacted by AI, and can find out when an AI system is engaging with them.\n\nAchieving transparency in AI systems through responsible disclosure is important to each stakeholder group for the following reasons :\n\nfor users, what the system is doing and why\n\nfor creators, including those undertaking the validation and certification of AI, the systems' processes and input data\n\nfor those deploying and operating the system, to understand processes and input data\n\nfor an accident investigator, if accidents occur\n\nfor regulators in the context of investigations\n\nfor those in the legal process, to inform evidence and decision-making\n\nfor the public, to build confidence in the technology\n\nResponsible disclosures should be provided in a timely manner, and provide reasonable justifications for AI systems outcomes. This includes information that helps people understand outcomes, like key factors used in decision making.\n\nThis principle also aims to ensure people have the ability to find out when an AI system is engaging with them (regardless of the level of impact), and are able to obtain a reasonable disclosure regarding the AI system.\n\nContestability\n\nWhen an AI system significantly impacts a person, community, group or environment, there should be a timely process to allow people to challenge the use or outcomes of the AI system.\n\nThis principle aims to ensure the provision of efficient, accessible mechanisms that allow people to challenge the use or output of an AI system, when that AI system significantly impacts a person, community, group or environment. The definition of the threshold for 'significant impact' will depend on the context, impact and application of the AI system in question.\n\nKnowing that redress for harm is possible, when things go wrong, is key to ensuring public trust in AI. Particular attention should be paid to vulnerable persons or groups.\n\nThere should be sufficient access to the information available to the algorithm, and inferences drawn, to make contestability effective. In the case of decisions significantly affecting rights, there should be an effective system of oversight, which makes appropriate use of human judgment.\n\nAccountability\n\nThose responsible for the different phases of the AI system lifecycle should be identifiable and accountable for the outcomes of the AI systems, and human oversight of AI systems should be enabled.\n\nThis principle aims to acknowledge the relevant organisations' and individuals' responsibility for the outcomes of the AI systems that they design, develop, deploy and operate. The application of legal principles regarding accountability for AI systems is still developing.\n\nMechanisms should be put in place to ensure responsibility and accountability for AI systems and their outcomes. This includes both before and after their design, development, deployment and operation. The organisation and individual accountable for the decision should be identifiable as necessary. They must consider the appropriate level of human control or oversight for the particular AI system or use case.\n\nAI systems that have a significant impact on an individual's rights should be accountable to external review, this includes providing timely, accurate, and complete information for the purposes of independent oversight bodies.\n", "metadata": {"country": "Australia", "year": "2019", "legally_binding": "no", "binding_proof": "principles", "date": "-", "regulator": "Department of Industry, Science & Resources", "type": "principles", "status": "not sure", "language": "EN", "use_cases": "[1, 3, 6]"}}
{"_id": "686ac1bc65a67285275e3c41", "title": "Australia AI Ethics Principles", "source": "https://www.whitecase.com/insight-our-thinking/ai-watch-global-regulatory-tracker-australia", "text": "Laws/Regulations directly regulating AI (the \u201cAI Regulations\u201d) Australia has not yet enacted any specific statutes or regulations that directly regulate AI. To date, Australia's response to AI has been voluntary and includes: The AI Ethics Principles published in 2019 1 (see \"Key compliance requirements\" section below) which comprise eight voluntary principles for the responsible design, development and implementation of AI, which are consistent with the OECD's Principles on AI.\n\n(see \"Key compliance requirements\" section below) which comprise eight voluntary principles for the responsible design, development and implementation of AI, which are consistent with the OECD's Principles on AI. The Voluntary AI Safety Standard2 (see \"Key compliance requirements\" section below) which comprise ten voluntary guardrails that cover aspects such as transparency with other organizations, accountability processes and risk management of AI. It offers practical guidance for Australian organizations to mitigate risks while leveraging the benefits of AI. In June 2023, the Commonwealth Department of Industry, Science and Resources (the \"Department\") commenced a consultation into \"Safe and Responsible AI in Australia\" (the \"Consultation\")3 which focused on developing governance mechanisms to ensure the safe and responsible development and use of AI and identifying potential gaps in Australia's current regulatory frameworks. On January 17, 2024, the Australian Government published its interim response to the Consultation (the \"Interim Response\"),4 which identified that current regulatory frameworks may not sufficiently prevent harms arising from the use of AI systems in legitimate but high-risk contexts. Accordingly, it appears that major reform could be expected in the medium term, where a risk-based framework will likely be adopted with an initial focus on appropriate mandatory safeguards and how best to implement them. Following the Interim Response, the Australian Government announced the establishment of a new Artificial Intelligence Expert Group to assist the Department in developing regulations on transparency, testing and accountability, including options for mandatory AI guardrails in high-risk settings. In August 2024, the Australian Government introduced the Voluntary AI Safety Standard and in September 2024, the Australian Government published a proposals paper introducing mandatory guardrails for AI in high-risk settings (the \"Proposals Paper\")5 (see \"Key compliance requirements\" section below) which largely mirror the Voluntary AI Safety Standard. The Proposals Paper outlines potential AI regulatory options and seeks feedback on proposed mandatory AI guardrails, as well as principles used to categorize an AI system as 'high-risk'. Proposed regulatory options to mandate guardrails include adapting existing regulatory frameworks, creating new framework legislation and amending associated existing legislation, or introducing a new AI Act.\n\nStatus of the AI Regulations\n\nAs noted above, as yet there are no specific statutes or regulations in Australia that directly regulate AI. Neither the Consultation, the Interim Response, nor the Proposals Paper provide any indicative timeline for when specific AI regulation might be expected.\n\nOther laws affecting AI\n\nThere are various laws that do not seek to regulate AI, but that may affect the development or use of AI in Australia. A non-exhaustive list of these laws include:\n\nThe Online Safety Act 2021, 6 which includes mechanisms to address online safety issues, extending to AI generated material\n\nwhich includes mechanisms to address online safety issues, extending to AI generated material The Australian Consumer Law, which was applied to algorithmic decision making in a Federal Court case which ordered Trivago to pay $44.7 million in penalties for misleading consumers about room rates in the recommendations made by its algorithm 7\n\nThe Privacy Act 1988 8\n\nThe Corporations Act 2001 9\n\nIntellectual property laws may affect several aspects of AI development and use\n\nAnti-discrimination laws, for example, where an individual is a victim of a discriminatory outcome resulting from an AI-driven process\n\nIn the Interim Response, the Australian Government acknowledged that existing laws will likely need to be strengthened to address harms posed by AI. To that end, on September 12, 2024, the Australian Government introduced the 'Communications Legislation Amendment (Combatting Misinformation and Disinformation) Bill 2024' to provide the Australian Communications and Media Authority (ACMA) with regulatory powers to combat online misinformation and disinformation, extending to content on digital platforms that are generated by AI.\n\nDefinition of \u201cAI\u201d\n\nNo definition of AI has been formally adopted by any statutes or regulations in Australia. In the Consultation, the Commonwealth Department adopted the following definitions:10\n\n\" AI \" means \"an engineered system that generates predictive outputs such as content, forecasts, recommendations or decisions for a given set of human-defined objectives or parameters without explicit programming. AI systems are designed to operate with varying levels of automation\"\n\n\" means \"an engineered system that generates predictive outputs such as content, forecasts, recommendations or decisions for a given set of human-defined objectives or parameters without explicit programming. AI systems are designed to operate with varying levels of automation\" \" Machine learning \" means \"the patterns derived from training data using machine learning algorithms, which can be applied to new data for prediction or decision-making purposes\"\n\n\" means \"the patterns derived from training data using machine learning algorithms, which can be applied to new data for prediction or decision-making purposes\" \"Generative AI\" means \"models [that] generate novel content such as text, images, audio and code in response to prompts\"\n\nThe definitions of AI, machine learning and algorithm are stated to be based on the International Organization for Standardization's definitions.11\n\nThe Voluntary AI Safety Standard provides a framework definition for safe and responsible AI:\n\n\"Safe and responsible AI\" means \"AI should be designed, developed, deployed and used in a way that is safe. Its use should be human-centered, trustworthy and responsible. AI systems should be developed and used in a way that provides benefits while minimizing the risk of negative impact to people, groups, and wider society.\"\n\nIn the Proposals Paper, the Australian Government is seeking public consultation on whether to implement a principles-based approach or a list-based approach (as adopted in the EU and Canada) to define \"high-risk AI\".\n\nTerritorial scope\n\nAs noted above, there are no specific statutes or regulations in Australia that directly regulate AI. Accordingly, there is little to no guidance on any specific territorial scope at this stage.\n\nSectoral scope\n\nAs noted above, there are no specific statutes or regulations in Australia that directly regulate AI. Accordingly, there is little to no guidance on any specific sectoral scope at this stage. To date, the focus of the Consultation, Interim Response and Proposals Paper has not been sector-specific, and it is expected that any AI-specific regulations will apply across all sectors of the Australian economy.\n\nCompliance roles\n\nAs noted above, there are no specific statutes or regulations in Australia that directly regulate AI.\n\nHowever, AI developers and users should note that the proposed mandatory AI guardrails impose compliance requirements on Australian organizations. These requirements include: (i) establishing internal governance protocols to ensure compliance with the guardrails; (ii) maintaining records to enable third parties to assess compliance; and (iii) undertaking conformity assessments in order to certify compliance with the guardrails.\n\nCore issues that the AI Regulations seek to address\n\nWhile voluntary, the AI Ethics Principles are designed to ensure AI is \"safe, secure and reliable\" by: (i) achieving safer, more reliable and fairer outcomes for all Australians; (ii) reducing the risk of negative impact on those affected by AI applications; and (iii) assisting businesses and governments to practice the highest ethical standards when designing, developing and implementing AI. By implementing the AI Ethics Principles, as well as the Voluntary AI Safety Standard which seeks to address similar issues, businesses can begin to develop the practices needed for future AI regulatory environments.\n\nThe proposed mandatory guardrails set clear regulatory expectations from the Australian Government on the safe and responsible use of AI. They focus on providing Australian businesses with: (i) greater regulatory certainty over the AI landscape; (ii) mechanisms to mitigate risks and harms associated with AI; and (iii) public trust in the development and deployment of AI in Australia in high-risk settings.\n\nRisk categorization\n\nAs noted above, there are no specific statutes or regulations in Australia that directly regulate AI.\n\nIn the Interim Response, the Australian Government indicated that it would adopt a risk-based framework to AI regulation, meaning the regulatory requirements will be commensurate to the level of risk posed by the specific use, deployment or development of AI where, for example, higher risk AI applications will likely include uses that may result in negative impacts for people that are difficult or impossible to reverse.\n\nThis risk-based framework is further reflected in the Proposals Paper, which outlines a risk-based approach for new mandatory AI guardrails enforced in Australia. In implementing mandatory AI guardrails, the Australian Government will consider: (i) the levels of risk and key attributes of identified risks; and (ii) the balance of ex ante (preventative) and ex post (remedial) regulatory measures to successfully mitigate against identified risks.\n\nIn addition, specific risk categories were identified in the Proposals Paper:\n\nCategory 1: This category relates to known or foreseeable uses of an AI system and risk is measured based on the context in which the AI system will be applied.\n\nCategory 2: This category applies to advanced AI systems with unforeseeable applications and emergent risks. Risk is assessed based on the AI system's potential use \u2013 or misuse \u2013 for purposes that may cause widescale harm. By the time these risks become foreseeable, it may be too late to apply effective preventative measures.\n\nKey compliance requirements\n\nAs noted above, there are no specific statutes or regulations in Australia that directly regulate AI. The voluntary AI Ethics Principles identify the following broad principles for ensuring safe, secure and reliable AI:\n\nHuman, societal and environmental wellbeing: AI systems should benefit individuals, society and the environment\n\nAI systems should benefit individuals, society and the environment Human-centered values: AI systems should respect human rights, diversity, and the autonomy of individuals\n\nAI systems should respect human rights, diversity, and the autonomy of individuals Fairness: AI systems should be inclusive and accessible, and should not involve or result in unfair discrimination against individuals, communities or groups\n\nAI systems should be inclusive and accessible, and should not involve or result in unfair discrimination against individuals, communities or groups Privacy protection and security: AI systems should respect and uphold privacy rights and data protection, and ensure the security of data\n\nAI systems should respect and uphold privacy rights and data protection, and ensure the security of data Reliability and safety: AI systems should reliably operate in accordance with their intended purpose\n\nAI systems should reliably operate in accordance with their intended purpose Transparency and explainability: There should be transparency and responsible disclosure so people can understand when they are being significantly impacted by AI, and can find out when an AI system is engaging with them\n\nThere should be transparency and responsible disclosure so people can understand when they are being significantly impacted by AI, and can find out when an AI system is engaging with them Contestability: When an AI system significantly impacts a person, community, group or environment, there should be a timely process to allow people to challenge the use or outcomes of the AI system\n\nWhen an AI system significantly impacts a person, community, group or environment, there should be a timely process to allow people to challenge the use or outcomes of the AI system Accountability: People responsible for the different phases of the AI system lifecycle should be identifiable and accountable for the outcomes of the AI systems, and human oversight of AI systems should be enabled\n\nThe Voluntary AI Safety Standard identifies the following principles for ensuring safe, secure and reliable AI:\n\nAccountability: Establish, implement and publish an accountability process that outlines governance polices and regulatory compliance strategies\n\nEstablish, implement and publish an accountability process that outlines governance polices and regulatory compliance strategies Risk Management: Establish and implement risk management processes to identify and mitigate risks that are known or foreseeable\n\nEstablish and implement risk management processes to identify and mitigate risks that are known or foreseeable Data Governance: Protect AI systems by implementing data governance, privacy and cybersecurity measures to manage security vulnerabilities such as data quality and data access\n\nProtect AI systems by implementing data governance, privacy and cybersecurity measures to manage security vulnerabilities such as data quality and data access Model Testing: Test AI models and evaluate performance before placing high-risk AI systems on the market, as well as continuously monitor the system once deployed\n\nTest AI models and evaluate performance before placing high-risk AI systems on the market, as well as continuously monitor the system once deployed Human Oversight: Enable human control and intervention across AI systems to achieve meaningful human oversight\n\nEnable human control and intervention across AI systems to achieve meaningful human oversight User Information: Inform end-users on how AI is being used, particularly around AI-enabled decisions, AI interactions and AI-generated content\n\nInform end-users on how AI is being used, particularly around AI-enabled decisions, AI interactions and AI-generated content Complaints Mechanism: Establish processes for people negatively impacted by high-risk AI systems to contest AI-enabled decisions or make complaints about their experience\n\nEstablish processes for people negatively impacted by high-risk AI systems to contest AI-enabled decisions or make complaints about their experience Transparency: Be transparent with other organizations across the AI supply chain by sharing information about data, models, and systems to effectively mitigate risks\n\nBe transparent with other organizations across the AI supply chain by sharing information about data, models, and systems to effectively mitigate risks Record Keeping: Keep and maintain records, including technical documentation, to allow third parties to assess compliance with guardrails\n\nKeep and maintain records, including technical documentation, to allow third parties to assess compliance with guardrails Engage stakeholders: Engage stakeholders and evaluate their needs and circumstances, with a focus on safety, diversity, inclusion and fairness\n\nThe Proposals Paper suggests transitioning from the current voluntary AI guardrails, as outlined in the Voluntary AI Safety Standard, to mandatory guardrails. Consequently, organizations developing or deploying high-risk AI systems would be required to adhere to the first nine voluntary guardrails listed above.\n\nThe tenth proposed mandatory guardrail (Conformity Assessments) differs from the tenth voluntary guiderail, which emphasizes ongoing stakeholder engagement to evaluate stakeholders' needs and circumstances based on safety, diversity, inclusion and fairness.\n\nThe Proposal Paper and its mandatory guardrails have been developed based on AI regulations enforced in Canada and Europe, specifically the Artificial Intelligence and Data Act in Canada and the EU AI Act in Europe.\n\nRegulators\n\nThere is currently no AI specific regulator in Australia.\n\nHowever, it is expected that sector-specific regulators such as the Australian Competition and Consumer Commission, the ACMA, the Office of the Australian Information Commissioner and the e-Safety Commissioner will be involved in the Australian Government's approach to the regulation of AI in Australia. As noted above, we can expect that the ACMA will be given certain regulatory powers to combat online misinformation on digital platforms that are generated by AI.\n\nEnforcement powers and penalties\n\nAs noted above, there are no specific statutes or regulations in Australia that directly regulate AI. The use, deployment or development of AI may be subject to enforcement and penalties if it breaches other, non-AI specific statutes and regulations.\n\n1 The AI Ethics Principles (2019) is available here .\n\n2 The Voluntary AI Safety Standard is available here .\n\n3 The Consultation paper is available here .\n\n4 The Interim Response is available here .\n\n5 The Proposals paper is available here .\n\n6 The Online Safety Act 2021 (Cth) is available here .\n\n7 See Australian Competition and Consumer Commission v Trivago N.V. [2020] FCA 16 here .\n\n8 The Privacy Act 1988 (Cth) is available here .\n\n9 The Corporations Act 2001 (Cth) is available here .\n\n10 See the Consultation at page 5.\n\n11 See ISO/IEC 22989: Artificial intelligence concepts and terminology here .\n\nWhite & Case means the international legal practice comprising White & Case LLP, a New York State registered limited liability partnership, White & Case LLP, a limited liability partnership incorporated under English law and all other affiliated partnerships, companies and entities.\n\nThis article is prepared for the general information of interested persons. It is not, and does not attempt to be, comprehensive in nature. Due to the general nature of its content, it should not be regarded as legal advice.", "metadata": {"country": "Australia", "year": "2024", "legally_binding": "no", "binding_proof": "Bill", "date": "09/-/2024", "regulator": "Australian Communications & Media Authority (ACMA)", "type": "", "status": "proposed", "language": "EN", "use_cases": "[1, 3, 6]"}}
{"_id": "686ac5ac65a67285275e3c42", "title": "Australia National Framework Government", "source": "https://www.whitecase.com/insight-our-thinking/ai-watch-global-regulatory-tracker-australia", "text": "National framework for the assurance of artificial intelligence in government A joint approach to safe and responsible AI by the Australian, state and territory governments. 21 June 2024 --- Page 2 --- The Australian, state and territory governments acknowledge the Traditional Custodians of Country throughout Australia and recognise the continuing connection to lands, waters and communities. We pay our respects to Aboriginal and Torres Strait Islander cultures and to Elders past and present. Version 1.0, published 21 June 2024 \u00a9 2024 Commonwealth of Australia Authored by the Australian, state and territory governments. Published by the Australian Government. With the exception of the Commonwealth Coat of Arms, the logos of the Australian, state and territory governments, and where otherwise noted, this work is licensed under the Creative Commons BY 4.0 licence. This means this licence only applies to material as set out in this document. The details of the relevant licence conditions, as well as the full legal code, are available on the Creative Commons website (https://creativecommons.org/licenses/by/4.0/) To reference this document, use the author-date system as demonstrated in this example: Australian Government et al. (2024) National framework for the assurance of artificial intelligence in government, Australian Government, accessed DD Month YYYY. National framework for the assurance of artificial intelligence in government ii --- Page 3 --- Contents Statement from Data and Digital Ministers 1 Introduction 2 Cornerstones of assurance 6 Implementing Australia\u2019s AI Ethics Principles in government 12 1. Human, societal and environmental wellbeing 13 2. Human-centred values 14 3. Fairness 16 4. Privacy protection and security 18 5. Reliability and safety 20 6. Transparency and explainability 21 7. Contestability 23 8. Accountability 25 Resources 26 National framework for the assurance of artificial intelligence in government iii --- Page 4 --- Statement from Data and Digital Ministers Artificial intelligence (AI), while not new, is a transformative technology undergoing accelerated development and adoption. It presents great opportunities for all levels of government to transform public service delivery and enhance societal, economic and environmental wellbeing. However, we know there are risks with governments\u2019 use of AI that require careful oversight, including legal, privacy, security and ethical risks such as bias and fairness. The importance of managing these risks has been outlined in the Australian Government\u2019s interim response to the safe and responsible use of AI consultation. We recognise that public confidence and trust is essential to governments embracing the opportunities and realising the full potential of AI. To gain public confidence and trust, we commit to being exemplars in the safe and responsible use of AI. This requires a lawful, ethical approach that places the rights, wellbeing and interests of people first. This national framework for the assurance of AI in government is a key step towards gaining public confidence and trust in the safe and responsible use of AI by Australia\u2019s governments. Based on Australia\u2019s AI Ethics Principles, it sets foundations for a nationally consistent approach to AI assurance, providing clear expectations, as well as consistency and certainty for our partners. It will assist governments to develop, procure and deploy AI in a safe and responsible way. We recognise the scale and nature of AI developments can be uncertain. However, by embedding a principles-based approach in this national framework, we commit to flexibility, responsiveness, continuing collaboration and improvement of our AI assurance processes. Our commitment to putting the rights, wellbeing and interests of people first remains steadfast and unchanged. National framework for the assurance of artificial intelligence in government 1 --- Page 5 --- Introduction The national framework for the assurance of AI in government provides for a nationally consistent approach for the assurance of artificial intelligence use in government. National Australian, state Australia\u2019s AI Ethics informs framework for the aligns and territory Principles assurance of AI in government government frameworks consistent with whole-of-economy safe and responsible use of AI Based on Australia\u2019s AI Ethics Principles (DISR 2019), and consistent with broader work on safe and responsible AI, the framework establishes cornerstones and practices of AI assurance. Instead of focusing on technical detail, the framework sets foundations across all aspects of government, with jurisdictions to develop specific policies and guidance considerate of their own legislative, policy and operational context. Assurance is an essential part of the broader governance of how governments use AI, including its development, procurement and deployment. This enables governments to: \u2022 understand the expected benefits of AI \u2022 identify risks and apply mitigations \u2022 ensure lawful use \u2022 understand if AI is operating as expected \u2022 demonstrate, through evidence, that the use of AI is safe and responsible. As the Australian, state and territory governments continue to use AI, they will likely develop new or improved assurance practices based on their unique successes, vulnerabilities and impacts. These learnings will be shared and incorporated into future iterations of this framework. National framework for the assurance of artificial intelligence in government 2 --- Page 6 --- Introduction Complementary initiatives The national framework for the assurance of AI in government complements local and global initiatives on the safe and responsible use of AI, both by governments and in wider economies. The Australian, state and territory governments will consider these and other initiatives as they develop their unique assurance approaches. Australia\u2019s AI Ethics Framework First published in 2019 and developed by the CSIRO\u2019s Data61 and the Department of Industry, Science and Resources (DISR). Australia\u2019 AI Ethics Framework (DISR 2019) defines the ethics principles which inform the practices found in this national assurance framework. Explore the ethics framework on the DISR website. Safe and responsible AI in Australia Following consultation initiated DISR in 2023, the Australian Government committed to ensuring the use of AI systems in high-risk settings is safe and reliable while use in low-risk settings can continue largely unimpeded. As set out in the government\u2019s interim response to the consultation, this work will ensure AI is used safely and responsibly across the wider economy. A crucial element of this agenda is the role of government as an exemplar in the safe and responsible use of AI. Read the Australian Government\u2019s interim response on the DISR website. NSW Artificial Intelligence Assurance Framework When published in 2022, the NSW Government became the world\u2019s first to mandate an assurance framework for the use of AI systems. The NSW Artificial Intelligence Assurance Framework (Digital NSW 2022) assists project teams using AI to comprehensively analyse and document their projects\u2019 AI specific risks. It also assists teams to implement risk mitigation strategies and establish clear governance and accountability measures. Access the NSW Artificial Intelligence Assurance Framework on the digital.nsw website. National framework for the assurance of artificial intelligence in government 3 --- Page 7 --- Introduction OECD principles for responsible stewardship of trustworthy AI Committed to by the Australian Government when first published by the Organisation for Economic Cooperation and Development in 2019. These principles aim to foster innovation and trust in AI by promoting the responsible stewardship of trustworthy AI while ensuring respect for human rights and democratic values. Read the recommendation containing the principles on the OECD website. Bletchley Declaration on AI safety Agreed to by Australia, alongside another 27 countries and the European Union, at the UK Government\u2019s 2023 AI Safety Summit. The declaration affirms that AI should be designed, developed, deployed, and used in a manner that is safe, human-centric, trustworthy and responsible. Read the text of the Bletchley Declaration as hosted on the DISR website. Seoul Declaration for safe, innovative and inclusive AI On 21 May 2024, the Australian Government agreed to 3 outcomes at the AI Seoul Summit, South Korea: \u2022 Declaration for safe, innovative and inclusive AI \u2022 Statement of Intent toward International Cooperation on AI Safety Science \u2022 Ministerial Statement for advancing AI safety, innovation and inclusivity. The agreements build on the Bletchley Declaration, confirming a shared understanding of opportunities and risks, and committing nations to deeper international cooperation and dialogue. Read the text of the Seoul Declaration as hosted on the DISR website. National framework for the assurance of artificial intelligence in government 4 --- Page 8 --- Introduction What is an AI system? In November 2023, OECD member countries approved this revised definition of an AI system: \u2018A machine-based system that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations or decisions that can influence physical or virtual environments. Different AI systems vary in their levels of autonomy and adaptiveness after deployment.\u2019 To avoid definitional complexities the Australian, state and territory governments should consider practical guidance for staff to identify when AI assurance processes apply, such as when: \u2022 the team identifies that the project, product or service uses AI \u2022 a vendor describes its product or service as using AI \u2022 users, the public or other stakeholders believe the project, product or service uses AI. National framework for the assurance of artificial intelligence in government 5 --- Page 9 --- Cornerstones of assurance In October 2023, the Department of Prime Minister and Cabinet (PM&C) published How artificial intelligence might affect the trustworthiness of public service delivery? (PM&C 2023). The report identified that current trust in AI is low, and developing community trust would be a key enabler of government adoption of AI technology. Alignment to Australia\u2019s AI Ethics Principles, developed by the CSIRO\u2019s Data61 and DISR, will ensure the trustworthy use of AI by governments in Australia. Each of its 8 ethics principles inform the assurance practices found in this framework and are also consistent with the Australian government\u2019s broader work on safe and responsible AI. They will help governments demonstrate and achieve: \u2022 safer, more reliable and fairer outcomes for all \u2022 reduced risk of negative impact on those affected by AI \u2022 the highest ethical standards when designing, developing and implementing AI. To effectively apply the AI ethics principles, governments should also consider the following cornerstones for their assurance practices. Governance AI governance comprises the organisational structure, policies, processes, regulation, roles, responsibilities and risk management frameworks that ensures the safe and responsible use of AI in a way that is fit for the future. The use of AI presents challenges that requires a combination of technical, social and legal capabilities and expertise. These cut across core government functions such as data and technology governance, privacy, human rights, diversity and inclusion, ethics, cyber security, audit, intellectual property, risk management, digital investment and procurement. Implementation of AI should therefore be driven by business or policy areas and be supported by technologists. National framework for the assurance of artificial intelligence in government 6 --- Page 10 --- Cornerstones of assurance Existing decision-making and accountability structures should be adapted and updated to govern the use of AI. This reflects the likely impacts upon a range of government functions, allows for diverse perspectives, designates lines of responsibility and provides clear sight to agency leaders of the AI uses they are accountable for. Governance structures should be proportionate and adaptable to encourage innovation while maintaining ethical standards and protecting public interests. At the agency level, leaders should commit to the safe and responsible use of AI and develop a positive AI risk culture to make open, proactive AI risk management an intrinsic part of everyday work. They should provide the necessary information, training and resources for staff to have the knowledge and means to: \u2022 align with the government\u2019s objectives \u2022 use AI ethically and lawfully \u2022 exercise discretion and judgement in using AI outputs \u2022 identify, report and mitigate risks \u2022 consider testing, transparency and accountability requirements \u2022 support the community through changes to public service delivery \u2022 clearly explain AI-influenced outcomes. Data governance The quality of an AI model\u2019s output is driven by the quality of its data. It\u2019s therefore important to create, collect, manage, use and maintain datasets that are authenticated, reliable, accurate and representative, and maintain robust data governance practices that complies with relevant legislation. Data governance comprises the policies, processes, structures, roles and responsibilities to achieve this and is as important as any other governance process. It ensures responsible parties understand their legislative and administrative obligations, see the value it adds to their work and their government\u2019s objectives. Data governance is also an exercise in risk management because it allows governments to minimise risks around the data it holds, while gaining maximum value from it. National framework for the assurance of artificial intelligence in government 7 --- Page 11 --- Cornerstones of assurance A risk-based approach The use of AI should be assessed and managed on a case-by-case basis. This ensures safe and responsible development, procurement and deployment in high- risk settings, with minimal administrative burden in lower-risk settings. The level of risk depends on the specifics of each case, including factors such as the business domain context and data characteristics. Self-assessment models, such as the NSW Artificial Intelligence Assurance Framework, help to identify, assess, document and manage these risks. Risks should be managed throughout the AI system lifecycle, including reviews at transitions between lifecycle phases. The OECD defines the phases of an AI system as: 1. design, data and models - a context-dependent sequence encompassing planning and design, data collection, processing and model building. 2. verification and validation 3. deployment 4. operation and monitoring. This AI system lifecycle may be embedded within the broader project management and procurement lifecycles, and risks may need re- evaluation where a significant change occurs at any phase. During system development governments should exercise discretion, prioritising traceability for datasets, processes, and decisions based on the potential for harm. Monitoring and feedback loops should be established to address emerging risks, unintended consequences or performance issues. Plans should be made for risks presented by obsolete and legacy AI systems. Governments should also consider oversight mechanisms for high-risk settings, including but not limited to external or internal review bodies, advisory bodies or AI risk committees, to provide consistent, expert advice and recommendations. National framework for the assurance of artificial intelligence in government 8 --- Page 12 --- Cornerstones of assurance In focus: risk-based regulation The Australian Government\u2019s 2023 \u2018Safe and Responsible AI in Australia\u2019 consultation found strong public support for Australia to follow a risk-based approach to regulating AI. As set out in the government\u2019s interim response, the government is now considering options for mandatory guardrails for organisations designing, developing and deploying AI systems in high-risk settings. This work focuses on testing, transparency and accountability measures and is being informed by a temporary AI expert group. Standards Where practical, governments should align their approaches to relevant AI standards. Standards outline specifications, procedures, and guidelines to enable the safe, responsible, consistent, and effective implementation AI in a consistent and interoperable manner. Some current AI governance and management standards include: \u2022 AS ISO/IEC 42001:2023 Information technology - Artificial intelligence - Management system \u2022 AS ISO/IEC 23894:2023 Information technology - Artificial intelligence - Guidance on risk management \u2022 AS ISO/IEC 38507:2022 Information technology - Governance of IT - Governance implications of the use of artificial intelligence by organizations Governments should regularly check the Standards Australia website for new AI related standards. National framework for the assurance of artificial intelligence in government 9 --- Page 13 --- Cornerstones of assurance Procurement Careful consideration must be applied to procurement documentation and contractual agreements when procuring AI systems or products. This may require consideration of: \u2022 AI ethics principles \u2022 clearly established accountabilities \u2022 transparency of data \u2022 access to relevant information assets \u2022 proof of performance testing throughout an AI system\u2019s life cycle. It is essential to remain mindful of the rapid pace of AI advancements and ensure contracts are adaptable to changes in technology. Governments should also consider internal skills development and knowledge transfer between vendors and staff to ensure sufficient understanding of a system\u2019s operation and outputs, avoid vendor lock-in and ensure that vendors and staff fulfill their responsibilities. Due diligence in procurement plays a critical role in managing new risks, such as transparency and explainability of \u2018black box\u2019 AI systems like foundation models. AI can also amplify existing risks, such as privacy and security. Governments must evaluate whether existing standard contractual clauses adequately cover these new and amplified risks. Consideration should be made to a vendor\u2019s capability to support the review, ongoing monitoring or evaluation of a system\u2019s outputs in the event of an incident or a stakeholder raising concerns. This should include providing evidence and support for review mechanisms. Governments may face trade-offs between a procured component\u2019s benefits and inherent assurance challenges, and resolutions will vary according to use case and tolerance threshold. Ultimately, procurement should prioritise alignment with ethics principles alongside delivering on a government\u2019s desired outcomes. National framework for the assurance of artificial intelligence in government 10 --- Page 14 --- Cornerstones of assurance In focus: responsible use of generative AI Generative AI (also known as foundational models, large language models or LLMs) has garnered wide attention since the public release of ChatGPT in November 2022. Whereas traditional AI has focused primarily on analysing data and subsequently making predictions, generative AI is able to create content across a wide range of mediums, including text, images, music and programming code, based on instructions or prompts provided by a user and informed by large datasets. Recognising the potential and risk of generative AI, governments across Australia have released guidance for its use in the public service, including: \u2022 Interim guidance on government use of public generative AI tools (DTA 2023) \u2022 Use of generative AI in Queensland Government (Department of Transport and Main Roads, Queensland Government 2023) \u2022 Artificial Intelligence and public records (Queensland State Archives 2024) \u2022 Public statement: Use of Microsoft Copilot for 365 in the Victorian public sector (OVIC 2023) \u2022 Public Statement: Use of personal information with ChatGPT (OVIC 2024) \u2022 Generative AI: basic guidance (Department of Customer Service, NSW Government n.d.) and accompanying strategy, policy and practical resources \u2022 Guideline for the use of Large Language Model AI Tools and Utilities (Department of Premier and Cabinet, Government of South Australia 2023). Common across government guidance is focus on human oversight and human accountability for the use of content produced using generative AI to ensure compliance with policies, legal obligations and ethical principles. This includes instructions on the use and protection of classified or sensitive information including personal information. National framework for the assurance of artificial intelligence in government 11 --- Page 15 --- Implementing Australia\u2019s AI Ethics Principles in government The following practices are mapped to Australia\u2019s 8 AI Ethics Principles, demonstrating how governments can practically apply them to their assurance of AI. Their application may differ according to jurisdictional specific governance and assurance protocols. Similarly, different use cases present different risks with some requiring a higher standard of assurance than others. Therefore, not all AI use cases will require the detailed application of all available practices to be considered safe and responsible. These practices were developed by drawing extensively from the existing practices of the Australian, state and territory governments, as well as these publications: \u2022 NSW Artificial Intelligence Assurance Framework (Digital NSW 2022) \u2022 Adoption of Artificial Intelligence in the Public Sector (DTA 2023) \u2022 Safe and responsible AI in Australia consultation: Australian Government\u2019s interim response (DISR 2024) \u2022 Implementing Australia\u2019s AI Ethics Principles (Gradient Institute and CSIRO 2023) \u2022 Responsible AI Pattern Catalogue (CSIRO 2023) \u2022 How might artificial intelligence affect the trustworthiness of public service delivery? (PM&C 2023) National framework for the assurance of artificial intelligence in government 12 --- Page 16 --- Implementing Australia\u2019s AI Ethics Principles in government 1. Human, societal and environmental wellbeing Throughout their lifecycle, AI systems should benefit individuals, society and the environment 1.1. Document intentions Governments should define and document the purpose and objectives of a use case and the outcomes expected for people, society and the environment. Document risks, consider whether the use of AI is preferable, whether there is a clear public benefit and what non-AI alternatives are available. Existing frameworks or policies for benefits realisation may assist. 1.2. Consult with stakeholders Governments should identify and consult with stakeholders, including subject matter and legal experts, and impacted groups and their representatives. Seek input from stakeholders early to allow for the early identification and mitigation of risks. 1.3. Assess impact Governments should assess the likely impacts of an AI use case on people, communities, societal and environmental wellbeing to determine if benefits outweigh risks and manage said impacts appropriately. Methods such as algorithmic and stakeholder impact assessments may assist. National framework for the assurance of artificial intelligence in government 13 --- Page 17 --- Implementing Australia\u2019s AI Ethics Principles in government 2. Human-centred values AI systems should respect human rights, diversity and the autonomy of individuals. 2.1. Comply with rights protections Governments will ensure their use of AI complies with legal protections for human rights. This may include those protected under: \u2022 legislation at all levels of government \u2022 Australia\u2019s international human rights obligations \u2022 the Australian and state constitutions \u2022 interpretation of common law. Any use will also align with related obligations, policies and guidelines for the public sector, workplace health and safety, human rights, and diversity and inclusion. Human rights impact assessments may assist to identify, assess and mitigate human rights risks. Where necessary seek advice from subject matter experts. 2.2. Incorporate diverse perspectives Governments should involve people with different lived experiences, including marginalisation, throughout the lifecycles of a use case to gather informed perspectives, remove preconceptions and avoid overlooking important considerations. This may include representation of: \u2022 people living with disability \u2022 multi-cultural communities \u2022 religious communities \u2022 people from different socio-economic backgrounds \u2022 diverse genders and sexualities \u2022 Aboriginal and Torres Strait Islander people. National framework for the assurance of artificial intelligence in government 14 --- Page 18 --- Implementing Australia\u2019s AI Ethics Principles in government 2.3. Ensure digital inclusion Governments should align to digital service and inclusion standards, and account for the needs, context and experience of individual users across an AI use case\u2019s lifecycle. Consider assistive technologies to support people who live with disability. In focus: The CSIRO\u2019s Guidelines for Diversity and Inclusion in Artificial Intelligence The CSIRO\u2019s Guidelines for Diversity and Inclusion in Artificial Intelligence (Zowghi D and da Rimini F 2023) address the evolving and holistic nature of AI technologies, the importance of diversity and inclusion consideration in the development and deployment of AI, and the potential consequences of neglecting it. The guidelines emphasise the importance of a socio-technical perspective on diversity and inclusion in AI, highlighting the necessity of involving relevant stakeholders with diverse attributes, examining cultural dynamics and norms, and evaluating societal impacts. Explore the guidelines on the CSIRO website. National framework for the assurance of artificial intelligence in government 15 --- Page 19 --- Implementing Australia\u2019s AI Ethics Principles in government 3. Fairness AI systems should be inclusive and accessible, and should not involve or result in unfair discrimination against individuals, communities or groups. 3.1. Define fairness in context Governments should consider the expected benefits and potential impacts of using AI, as well as vulnerabilities of impacted groups, to determine \u2018fairness\u2019 in a use case\u2019s context. 3.2. Comply with anti-discrimination obligations Governments will ensure their use of AI complies with relevant anti-discrimination legislation, policies and guidelines for protected attributes. These may include: \u2022 age \u2022 disability \u2022 race \u2022 religion \u2022 sex \u2022 intersex status \u2022 gender identity \u2022 sexual orientation. Well trained and supported staff should be able to identify, report and resolve biased AI outputs. Where necessary, seek advice from subject matter experts. 3.3. Ensure quality of data and design Governments should ensure high-quality data and algorithmic design. Audits of AI inputs and outputs for unfair biases, data quality statements and other data governance and management practices may assist to understand and mitigate bias in AI systems. National framework for the assurance of artificial intelligence in government 16 --- Page 20 --- Implementing Australia\u2019s AI Ethics Principles in government In focus: the Australian Human Rights Commission\u2019s Using artificial intelligence to make decisions: Addressing the problem of algorithmic bias \u2022 Technical Paper This technical paper is a collaborative partnership between the Australian Human Rights Commission, Gradient Institute, Consumer Policy Research Centre, CHOICE and CSIRO\u2019s Data61. It explores how the problem of algorithmic bias can arise in decision making that uses artificial intelligence and how this problem can produce unfair, and potentially unlawful, decisions as it may lead to a person being unfairly treated or even suffering unlawful discrimination based on characteristics such as race, age, sex or disability. It demonstrates how the risk of algorithmic bias can be identified and steps that can be taken to address or mitigate this problem. This paper forms part of a AHRC\u2019s Human Rights and Technology Project. You can read the technical paper on the AHRC website. National framework for the assurance of artificial intelligence in government 17 --- Page 21 --- Implementing Australia\u2019s AI Ethics Principles in government 4. Privacy protection and security AI systems should respect and uphold privacy rights of individuals and ensure the protection of data. 4.1. Comply with privacy obligations Governments will ensure their use of AI complies with legislation, policy and guidelines that govern consent, collection, storage, use, disclosure and retention of personal information. This may include informing people when their personal information is being collected for an AI system or when personal information is used for a secondary purpose such as AI system training. \u2018Privacy by design\u2019 principles and privacy impact assessments may assist to identify, assess and mitigate privacy risks. Where necessary, seek advice from subject matter experts. 4.2. Minimise and protect personal information Governments should assess whether the collection, use and disclosure of personal information is necessary, reasonable and proportionate for each AI use case. Consider if similar outcomes can be achieved with privacy enhancing technologies. Synthetic data, data anonymisation and deidentification, encryption, secure aggregation and other measures may assist to reduce privacy risks. Sensitive information should always be managed with caution. National framework for the assurance of artificial intelligence in government 18 --- Page 22 --- Implementing Australia\u2019s AI Ethics Principles in government 4.3. Secure systems and data Governments should ensure each use case complies with security and data protection legislation, policies and guidelines, including through an AI system\u2019s supply chains. Security considerations should be consistent with the cyber security strategies and polices of impacted jurisdictions. Access to systems, applications and data repositories should be limited to authorised staff as required by their duties. Where necessary, seek advice from subject matter experts. Governments should consider relevant security guidance and strategies including: \u2022 2023-2030 Australian Cyber Security Strategy (Home Affairs 2023) \u2022 Hosting Certification Framework (Home Affairs n.d.) \u2022 Engaging with Artificial Intelligence (ASD 2024) \u2022 Deploying AI Systems Securely (ASD 2024) \u2022 Countering the Insider Threat: A guide for Australian Government (Attorney- General\u2019s Department 2023) In focus: Office of the Victorian Information Commissioner\u2019s Artificial Intelligence \u2013 Understanding Privacy Obligations Published in April 2021, the Office of the Victorian Information Commissioner\u2019s Artificial Intelligence \u2013 Understanding Privacy Obligations (OVIC 2021) provides guidance to assist Victorian Public Service organisations consider their privacy obligations when using or considering the use of personal information in AI systems or applications. It covers the collection, use, handling and governance of personal information within this context. Organisations should conduct a privacy impact assessment when designing or implementing AI systems to help identify potential privacy risks associated with the collection and use of personal information in the AI system. National framework for the assurance of artificial intelligence in government 19 --- Page 23 --- Implementing Australia\u2019s AI Ethics Principles in government 5. Reliability and safety Throughout their lifecycle, AI systems should reliably operate in accordance with their intended purpose. 5.1. Use appropriate datasets Governments should ensure that, wherever practical, AI systems are trained and validated on accurate, representative, authenticated and reliable datasets that are suitable for the specific use case. 5.2. Conduct pilot studies Governments should evaluate AI systems in small-scale pilot environments to identify and mitigate problems and iterate and scale the solution. Consider the trade-offs between governance and effectiveness: a highly controlled environment may not accurately reflect the full risk and opportunity landscape, while a less controlled environment may pose governance challenges. 5.3. Test and verify Governments should test and verify the performance of AI systems. Red teaming, conformity assessments, reinforcement from human feedback, metrics and performance testing, and other methods may assist. 5.4. Monitor and evaluate Governments should ensure their use of AI is continuously monitored and evaluated to ensure its operation is safe, reliable and aligned to ethics principles. This should encompass an AI system\u2019s performance, its use by people, and impacts on people, society and the environment, including feedback from those impacted by AI-influenced outcomes. 5.5. Be prepared to disengage Governments should be prepared to quickly and safely disengage an AI system when an unresolvable problem is identified. This could include a data breach, unauthorised access or system compromise. Consider such scenarios in business continuity, data breach and security response plans. National framework for the assurance of artificial intelligence in government 20 --- Page 24 --- Implementing Australia\u2019s AI Ethics Principles in government 6. Transparency and explainability There should be transparency and responsible disclosure so people can understand when they are being significantly impacted by AI, and can find out when an AI system is engaging with them. 6.1. Disclose the use of AI Governments should ensure their use of AI is disclosed to users or people who may be impacted by it. Governments should maintain a register of when it uses AI, its purpose, intended uses, and limitations. 6.2. Maintain reliable data and information assets Governments should comply with legislation, policies and standards for maintaining reliable records of decisions, testing, and the information and data assets used in an AI system. This will enable internal and external scrutiny, continuity of knowledge and accountability. 6.3. Provide clear explanations Governments should provide clear, simple explanations for how an AI system reaches an outcome. This includes: \u2022 inputs and variables and how these have influenced the reliability of the system \u2022 the results of testing including technical and human validation \u2022 the implementation of human oversight. When explainability is limited, governments should weigh the benefits of AI use against explainability limitations. Where a decision is made to proceed with AI use, document reasons and apply heightened levels of oversight and control. When an AI system influences or is used as part of administrative decision making, decisions should be explainable, and humans accountable. National framework for the assurance of artificial intelligence in government 21 --- Page 25 --- Implementing Australia\u2019s AI Ethics Principles in government 6.4. Support and enable frontline staff Governments should ensure staff at frontline agencies are well-trained and supported to clearly explain AI-influenced outcomes to users and people. Consider the importance of human-to-human relationships for a range of people, including vulnerable people or groups, people facing complex needs and those uncomfortable with government\u2019s use of AI. In focus: Public Record Office Victoria\u2019s AI Technologies and Recordkeeping Policy Released in March 2024, Victoria\u2019s Artificial Intelligence (AI) Technologies and Recordkeeping Policy (PROV 2024) was designed to address transparency and accountability concerns in relation to AI implementation and use and to enable explainable AI use. This includes the production of full and accurate records/data, as well as the appropriate management of those records/data in accordance with the PROV Recordkeeping Standards framework. National framework for the assurance of artificial intelligence in government 22 --- Page 26 --- Implementing Australia\u2019s AI Ethics Principles in government 7. Contestability When an AI system significantly impacts a person, community, group or environment, there should be a timely process to allow people to challenge the use or outcomes of the AI system. 7.1. Understand legal obligations Governments will ensure their use of AI in administrative decision-making complies with law, policy and guidelines that regulate such processes. This includes principles of legality, fairness, rationality and transparency, and access to reviews, dispute resolutions and investigations. Where necessary, governments should seek legal advice as to their legal obligations and proposed use of AI. 7.2. Communicate rights and protections clearly Governments should clearly communicate the rights and protections of those impacted by each AI use case and create an avenue to voice concerns and objections and seek recourse and redress. This includes clearly communicating the channels and processes to challenge the use or outcomes of an AI system. Feedback and response mechanisms should be clear and transparent, ensure timely human review and exist across the use case\u2019s lifecycles. National framework for the assurance of artificial intelligence in government 23 --- Page 27 --- Implementing Australia\u2019s AI Ethics Principles in government In focus: the Commonwealth Ombudsman\u2019s Automated Decision-making Better Practice Guide Released in March 2020, the Automated Decision-making Better Practice Guide [PDF 571KB] (Commonwealth Ombudsman 2020) recognises the significant role automation plays in administrative decision-making. The key message of the guide is that people must be at the center of service delivery. It provides specific guidance on administrative law, privacy, governance and design, transparency and accountability, and monitoring and evaluation of automated decision-making systems including those that contain AI. It also provides practical tools for agencies, including a checklist designed to assist managers and project officers during the design and implementation of new automated systems, and ongoing assurance processes for once a system is operational. Similarly, the NSW Ombudsman has released guidance on automated decision making in the public sector. National framework for the assurance of artificial intelligence in government 24 --- Page 28 --- Implementing Australia\u2019s AI Ethics Principles in government 8. Accountability Those responsible for the different phases of the AI system lifecycle should be identifiable and accountable for the outcomes of the AI systems, and human oversight of AI systems should be enabled. 8.1. Establish clear roles and responsibilities Governments should ensure their use of AI is overseen by clearly identified roles and lines of accountability. Governments should consider: \u2022 the role of senior leadership and area-specific responsibilities \u2022 security, data governance, privacy and other obligations \u2022 integration with existing governance and risk management frameworks. 8.2. Train staff and embed capability Governments should establish policies, procedures, and training to ensure all staff understand their duties and responsibilities, understand system limitations and implement AI assurance practices. 8.3. Embed a positive risk culture Governments should ensure a positive risk culture, promoting open, proactive AI risk management as an intrinsic part of everyday practice. This fosters open discussion of uncertainties and opportunities, encourages staff to express their concerns and maintains processes to escalate to the appropriate accountable parties. 8.4. Avoid overreliance Governments remain responsible for all outputs generated by AI systems and must ensure incorrect outputs are flagged and addressed. Governments should therefore consider the level of reliance on their use of AI and its potential risk and accountability challenges. Overreliance can lead to the acceptance of incorrect or biased outputs, and risks to business continuity. National framework for the assurance of artificial intelligence in government 25 --- Page 29 --- Resources Resources ASD (Australian Signals Directorate) (n.d.) Information Security Manual (ISM), ASD, Australian Government, accessed 25 March 2024. https://www.cyber.gov. au/resources-business-and-government/essential-cyber-security/ism \u2014\u2014(n.d.) Legacy ICT management, ASD website, accessed 22 April 2024. https:// www.cyber.gov.au/resources-business-and-government/maintaining-devices- and-systems/system-hardening-and-administration/legacy-ict-management \u2014\u2014(2024) Deploying AI Systems Securely, ASD, Australian Government, accessed 25 March 2024. https://www.cyber.gov.au/resources-business-and-government/ governance-and-user-education/artificial-intelligence/deploying-ai-systems-securely \u2014\u2014(2024) Engaging with Artificial Intelligence, ASD, Australian Government, accessed 25 March 2024. https://www.cyber.gov.au/resources-business-and-government/ governance-and-user-education/governance/engaging-with-artificial-intelligence \u2014\u2014(2023) Guidelines for secure AI system development, ASD, Australian Government, accessed 25 March 2024. https://www.cyber.gov.au/about-us/view- all-content/advice-and-guidance/guidelines-secure-ai-system-development \u2014\u2014(2021) Identifying Cyber Supply Chain Risks, ASD, Australian Government, accessed 25 March 2024. https://www.cyber.gov.au/resources-business- and-government/maintaining-devices-and-systems/outsourcing-and- procurement/cyber-supply-chains/identifying-cyber-supply-chain-risks Attorney-General\u2019s Department (n.d.) Australia\u2019s anti-discrimination law, Attorney-General\u2019s Department website, accessed 25 March 2024. https://www.ag.gov.au/rights-and- protections/human-rights-and-anti-discrimination/australias-anti-discrimination-law \u2014\u2014(n.d.) Human rights protections, Attorney-General\u2019s Department website, accessed 25 March 2024. https://www.ag.gov.au/rights-and-protections/ human-rights-and-anti-discrimination/human-rights-protections \u2014\u2014(n.d.) Public sector guidance sheets, Attorney-General\u2019s Department website, accessed 25 March 2024. https://www.ag.gov.au/rights-and-protections/human-rights- and-anti-discrimination/human-rights-scrutiny/public-sector-guidance-sheets National framework for the assurance of artificial intelligence in government 26 --- Page 30 --- Resources \u2014\u2014(2023) Countering the Insider Threat: A guide for Australian Government, Attorney- General\u2019s Department, Australian Government, accessed 25 March 2024. https://www. ag.gov.au/integrity/publications/countering-insider-threat-guide-australian-government Australian Government (2024) Framework for Governance of Indigenous Data, National Indigenous Australians Agency, Australian Government, accessed 30 May 2024. https:// www.niaa.gov.au/resource-centre/framework-governance-indigenous-data Australian Human Rights Commission (n.d.) Human Rights and Technology Project, AHRC website, accessed 25 March 2024. https://humanrights.gov.au/our-work/technology-and-human-rights \u2014\u2014(2021) Using artificial intelligence to make decisions: Addressing the problem of algorithmic bias \u2022 Technical Paper, AHRC, Australian Government, accessed 25 March 2024. https://humanrights.gov.au/our-work/technology-and- human-rights/publications/technical-paper-addressing-algorithmic-bias Commonwealth Ombudsman (2020) Automated Decision-making Better Practice Guide [PDF 571KB], Commonwealth Ombudsman, Australian Government, accessed 25 March 2024. https://www.ombudsman.gov.au/__data/assets/pdf_file/0029/288236/ OMB1188-Automated-Decision-Making-Report_Final-A1898885.pdf CSIRO (n.d.) National Artificial Intelligence Centre, CSIRO website, accessed 25 March 2024. https://www.csiro.au/en/work-with-us/industries/technology/National-AI-Centre \u2014\u2014(n.d.) Responsible AI Network resources, CSIRO website, accessed 25 March 2024. https://www.csiro.au/en/work-with-us/industries/technology/National-AI- Centre/Responsible-AI-Network/Responsible-AI-Network-resources-archive \u2014\u2014(2023) Diversity and Inclusion in Artificial Intelligence, CSIRO website, accessed 25 March 2024. https://research.csiro.au/ss/team/diai/ \u2014\u2014(2023) Responsible AI Pattern Catalogue, CSIRO, Australian Government, accessed 25 March 2024. https://research.csiro.au/ss/science/projects/responsible-ai-pattern-catalogue/ \u2014\u2014(2019) Artificial Intelligence: Australia\u2019s Ethics Framework, CSIRO, Australian Government, accessed 25 March 2024. https://www.csiro. au/en/research/technology-space/ai/ai-ethics-framework Department of Customer Service (n.d.) Generative AI: basic guidance, Department of Customer Service, NSW Government, accessed 25 March 2024. https://www. digital.nsw.gov.au/policy/artificial-intelligence/generative-ai-basic-guidance \u2014\u2014(2022) Artificial Intelligence (AI), Department of Customer Service website, accessed 25 March 2024. https://www.digital.nsw.gov.au/policy/artificial-intelligence National framework for the assurance of artificial intelligence in government 27 --- Page 31 --- Resources \u2014\u2014(2022) NSW Artificial Intelligence Assurance Framework, Department of Customer Service, NSW Government, accessed 22 March 2024. https://www.digital.nsw.gov.au/ policy/artificial-intelligence/nsw-artificial-intelligence-assurance-framework Department of Finance (2023) Risk Management Toolkit, Finance website, accessed 25 March 2024. https://www.finance.gov.au/government/ comcover/risk-services/management/risk-management-toolkit DISR (Department of Industry, Science and Resources) (2024) Safe and responsible AI in Australia consultation: Australian Government\u2019s interim response, DISR, Australian Government, accessed 22 March 2024. https://consult.industry.gov.au/supporting-responsible-ai \u2014\u2014(2024) The Seoul Declaration by countries attending the AI Seoul Summit, 21-22 May 2024, DISR, Australian Government, accessed 27 May 2024. https://www.industry.gov.au/ publications/seoul-declaration-countries-attending-ai-seoul-summit-21-22-may-2024 \u2014\u2014(2023) The Bletchley Declaration by Countries Attending the AI Safety Summit, 1\u20132 November 2023, DISR, Australian Government, accessed 22 March 2024. https://www.industry.gov.au/publications/bletchley-declaration- countries-attending-ai-safety-summit-1-2-november-2023 \u2014\u2014(2019) Australia\u2019s AI Ethics Principles, DISR website, accessed 22 March 2024. https://www.industry.gov.au/publications/australias-artificial- intelligence-ethics-framework/australias-ai-ethics-principles \u2014\u2014(2019) Australia\u2019s Artificial Intelligence Ethics Framework, DISR website, accessed 22 March 2024. https://www.industry.gov.au/ publications/australias-artificial-intelligence-ethics-framework DTA (Digital Transformation Agency) (2023) Adoption of Artificial Intelligence in the Public Sector, Australian Government Architecture website, accessed 25 March 2024. https://architecture.digital.gov.au/adoption-artificial-intelligence-public-sector-0 \u2014\u2014(2019) Interim guidance on government use of public generative AI tools, Australian Government Architecture website, accessed 25 March 2024. https://architecture.digital.gov.au/generative-ai Government of South Australia (2023) Guideline for the use of Large Language Model AI Tools and Utilities, Office of the Chief Information Officer, Department of the Premier and Cabinet, Government of South Australia, accessed 25 March 2024. https://www.dpc.sa.gov. au/responsibilities/ict-digital-cyber-security/policies-and-guidelines/artificial-intelligence \u2014\u2014(n.d.) Online Accessibility Toolkit [website], accessibility.sa.gov.au, accessed 25 March 2024. National framework for the assurance of artificial intelligence in government 28 --- Page 32 --- Resources Hiroshima AI Process (2023) Hiroshima Process International Code of Conduct for Organizations Developing Advanced AI Systems, Ministry of Internal Affairs and Communications, Government of Japan, accessed 25 March 2024. https://www.soumu.go.jp/hiroshimaaiprocess/en/documents.html Home Affairs (Department of Home Affairs) (n.d.) Hosting Certification Framework [website], hostingcertification.gov.au, accessed 22 April 2024. \u2014\u2014(n.d.) Protective Security Policy Framework (PSPF) [website], protectivesecurity.gov.au, accessed 25 March 2024. \u2014\u2014(2023) 2023-2030 Australian Cyber Security Strategy, Home Affairs, Australian Government, accessed 22 April 2024. https://www.homeaffairs.gov.au/about-us/our- portfolios/cyber-security/strategy/2023-2030-australian-cyber-security-strategy ISO (International Organization for Standardization) (2024) ISO/IEC DIS 42005 - Information technology \u2014 Artificial intelligence \u2014 AI system impact assessment, ISO, accessed 25 March 2024. https://www.iso.org/standard/44545.html \u2014\u2014(2022) Artificial intelligence, ISO website, accessed 25 March 2024. https://www.iso.org/sectors/it-technologies/ai NSW Ombudsman (2021) Automated decision-making in the public sector, NSW Ombudsman website, accessed 25 March 2024. https://www.ombo.nsw.gov.au/ guidance-for-agencies/automated-decision-making-in-the-public-sector OAIC (Office of the Australian Information Commissioner) (n.d.) Privacy impact assessments, OAIC website, accessed 25 March 2024. https://www.oaic.gov.au/privacy/privacy- guidance-for-organisations-and-government-agencies/privacy-impact-assessments \u2014\u2014(n.d.) State and territory privacy legislation, OAIC website, accessed 25 March 2024. https://www.oaic.gov.au/privacy/privacy-legislation/state-and- territory-privacy-legislation/state-and-territory-privacy-legislation \u2014\u2014(2018) Guide to data analytics and the Australian Privacy Principles, OAIC, Australian Government, accessed 25 March 2024. https://www.oaic.gov.au/ privacy/privacy-guidance-for-organisations-and-government-agencies/more- guidance/guide-to-data-analytics-and-the-australian-privacy-principles National framework for the assurance of artificial intelligence in government 29 --- Page 33 --- Resources OECD (Organization for Economic Cooperation and Development) (2024) Explanatory memorandum on the updated OECD definition of an AI system, OECD, accessed 25 March 2024. https://www.oecd.org/publications/explanatory-memorandum- on-the-updated-oecd-definition-of-an-ai-system-623da898-en.htm \u2014\u2014(2023) OECD AI Principles overview, OECD website, accessed 22 March 2024. https://oecd.ai/en/ai-principles \u2014\u2014(2023) Recommendation of the Council on Artificial Intelligence, OECD/LEGAL/0449, OECD, accessed 22 March 2024. https://legalinstruments.oecd.org/en/instruments/OECD-LEGAL-0449 OVIC (Office of the Victorian Information Commissioner) (2024) Public Statement: Use of personal information with ChatGPT, OVIC, State Government of Victoria, accessed 27 May 2024. https://ovic.vic.gov.au/privacy/resources-for-organisations/ public-statement-use-of-personal-information-with-chatgpt/ \u2014\u2014(2023) Public Statement: Use of Microsoft 365 Copilot in the Victorian public sector, OVIC, State Government of Victoria, accessed 25 March 2024. https://ovic.vic. gov.au/privacy/resources-for-organisations/vps-use-of-microsoft-365-copilot/ \u2014\u2014(2018) Artificial Intelligence \u2013 Understanding Privacy Obligations, OVIC, State Government of Victoria, accessed 25 March 2024. https://ovic.vic.gov.au/privacy/ resources-for-organisations/artificial-intelligence-understanding-privacy-obligations/ \u2014\u2014(2018) Artificial Intelligence and Privacy \u2013 Issues and Challenges, OVIC, State Government of Victoria, accessed 25 March 2024. https://ovic.vic.gov.au/privacy/ resources-for-organisations/artificial-intelligence-and-privacy-issues-and-challenges/ PM&C (Department of Prime Minister and Cabinet) (2023) How might artificial intelligence affect the trustworthiness of public service delivery?, PM&C, Australian Government, accessed 22 March 2024. https://www.pmc.gov.au/resources/long- term-insights-briefings/how-might-ai-affect-trust-public-service-delivery Public Record Office Victoria (2024) Artificial Intelligence, PROV website, accessed 25 March 2024. https://prov.vic.gov.au/recordkeeping-government/a-z-topics/AI \u2014\u2014(2024) Artificial Intelligence (AI) Technologies and Recordkeeping Policy, PROV, State Government of Victoria, accessed 25 March 2024. https://prov.vic.gov.au/recordkeeping- government/document-library/ai-technologies-policy-ai-technologies-and-recordkeeping National framework for the assurance of artificial intelligence in government 30 --- Page 34 --- Resources Queensland Government Customer and Digital Group (2023) Use of generative AI in Queensland Government, Department of Transport and Main Roads, Queensland Government, accessed 25 March 2024. https://www.forgov.qld.gov.au/information-and-communication-technology/ qgea-policies-standards-and-guidelines/use-of-generative-ai-in-queensland-government Queensland State Archives (2024) Artificial Intelligence and public records, Queensland State Archives website, accessed 25 March 2024. https://www.forgov.qld.gov.au/information-and- communication-technology/recordkeeping-and-information-management/recordkeeping/ resources-and-tools-for-records-management/artificial-intelligence-and-public-records Reid A, O\u2019Callaghan S and Lu Y (2023) Implementing Australia\u2019s AI Ethics Principles: A selection of Responsible AI practices and resources, Gradient Institute and CSIRO, accessed 25 March 2024. https://www.csiro.au/en/work-with-us/industries/ technology/national-ai-centre/implementing-australias-ai-ethics-principles-report Zowghi D and da Rimini F (2023) \u2018Diversity and Inclusion in Artificial Intelligence\u2019, in Lu Q et al. (eds) Responsible AI: Best Practices for Creating Trustworthy AI Systems, Addison-Wesley, Sydney. https://research.csiro.au/ss/guidelines-for-diversity-and-inclusion-in-artificial-intelligence/ National framework for the assurance of artificial intelligence in government 31", "metadata": {"country": "Australia", "year": "2024", "legally_binding": "yes", "binding_proof": "for the government", "date": "09/-/2024", "regulator": "Data & Digital Ministers", "type": "", "status": "In development", "language": "EN", "use_cases": "[1, 3, 6]"}}
{"_id": "686ac72e65a67285275e3c43", "title": "Australia Policy for the responsible use of AI in government", "source": "https://www.digital.gov.au/policy/ai/policy", "text": "Policy introduction\n\nThe increasing adoption of artificial intelligence (AI) is reshaping the economy, society and government. While the technology is moving fast, the lasting impacts of AI on the activities of government are likely to be transformational.\n\nThis policy provides a framework to position the Australian Government as an exemplar under its broader safe and responsible AI agenda.\n\nAI has an immense potential to improve social and economic wellbeing. Development and deployment of AI is accelerating. It already permeates institutions, infrastructure, products and services, with this transformation occurring across the economy and in government.\n\nFor government, the benefits of adopting AI include more efficient and accurate agency operations, better data analysis and evidence-based decisions, and improved service delivery for people and business. Many areas of the Australian Public Service (APS) already use AI to improve their work and engagement with the public.\n\nTo unlock innovative use of AI, Australia needs a modern and effective regulatory system. Internationally, governments are introducing new regulations to address AI\u2019s distinct risks, focused on preventative, risk-based guardrails that apply across the supply chain and throughout the AI lifecycle.\n\nThe Australian Government\u2019s consultations on safe and responsible AI show our current regulatory system is not fit for purpose to respond to the distinct risks that AI poses.\n\nThe consultation also found that the public expects government to be an exemplar of safe and responsible adoption and use of AI technologies. Public trust in AI and government\u2019s use of it is low, which acts as a handbrake on adoption. The preparedness and maturity for managing AI varies across the APS. AI technologies change at speed and scale, presenting further risks if not acted upon quickly to mitigate them.\n\nThis means government has an elevated level of responsibility for its use of AI and should be held to a higher standard of ethical behaviour.\n\nThe Australian Government\u2019s interim response to the consultations included a commitment to creating a regulatory environment that builds community trust and promotes innovation and adoption. It outlines pathways to ensure the design, development and deployment of AI in legitimate, but high-risk settings is safe and can be relied upon, while ensuring AI in low-risk settings can continue largely unimpeded.\n\nThis policy is a first step in the journey to position government as an exemplar in its safe and responsible use of AI, in line with the Australian community\u2019s expectations. It sits alongside whole-of-economy measures such as mandatory guardrails and voluntary industry safety measures.\n\nThe policy aims to create a coordinated approach to government\u2019s use of AI and has been designed to complement and strengthen \u2013 not duplicate \u2013 existing frameworks in use by the APS.\n\nIn recognition of the speed and scale of change in this area, the policy is designed to evolve over time as the technology changes, leading practices develop, and the broader regulatory environment matures.", "metadata": {"country": "Australia", "year": "2024", "legally_binding": "yes", "binding_proof": "for the government", "date": "-/-/2024", "regulator": "Digital Transformation Agency (DTA)", "type": "", "status": "Mandated policy", "language": "EN", "use_cases": "[1, 2, 3, 4, 6]"}}
{"_id": "686aca8165a67285275e3c44", "title": "Australia Voluntary AI Safety Standard", "source": "https://www.industry.gov.au/sites/default/files/2024-09/voluntary-ai-safety-standard.pdf", "text": "Voluntary AI Safety Standard August 2024 | industry.gov.au/NAIC --- Page 2 --- Copyright \u00a9 Commonwealth of Australia 2024 Ownership of intellectual property rights Unless otherwise noted, copyright (and any other intellectual property rights, if any) in this publication is owned by the Commonwealth of Australia. Creative Commons Attribution 4.0 International Licence CC BY 4.0 All material in this publication is licensed under a Creative Commons Attribution 4.0 International Licence, with the exception of: \u2022 the Commonwealth Coat of Arms \u2022 content supplied by third parties \u2022 logos \u2022 any material protected by trademark or otherwise noted in this publication. Creative Commons Attribution 4.0 International Licence is a standard form licence agreement that allows you to copy, distribute, transmit and adapt this publication provided you attribute the work. A summary of the licence terms is available from https://creativecommons.org/licenses/by/4.0/. The full licence terms are available from https://creativecommons.org/licenses/by/4.0/legalcode. Content contained herein should be attributed as Voluntary AI Safety Standard, Australian Government Department of Industry, Science and Resources. This notice excludes the Commonwealth Coat of Arms, any logos and any material protected by trademark or otherwise noted in the publication, from the application of the Creative Commons licence. These are all forms of property which the Commonwealth cannot or usually would not licence others to use. Disclaimer The purpose of this publication is to provide best practice guidance on implementing safe and responsible AI practices for Australian organisations. The Commonwealth as represented by the Department of Industry, Science and Resources has exercised due care and skill in the preparation and compilation of the information in this publication. The Commonwealth does not guarantee the accuracy, reliability or completeness of the information contained in this publication. Interested parties should make their own independent inquires and obtain their own independent professional advice prior to relying on, or making any decisions in relation to, the information provided in this publication. The Commonwealth accepts no responsibility or liability for any damage, loss or expense incurred as a result of the reliance on information contained in this publication. This publication does not indicate commitment by the Commonwealth to a particular course of action. Voluntary AI Safety Standard | industry.gov.au/NAIC ii --- Page 3 --- Acknowledgement of Country Our department recognises the First Peoples of this Nation and their ongoing cultural and spiritual connections to the lands, waters, seas, skies, and communities. We Acknowledge First Nations Peoples as the Traditional Custodians and Lore Keepers of the oldest living culture and pay respects to their Elders past and present. We extend that respect to all First Nations Peoples. Voluntary AI Safety Standard | industry.gov.au/NAIC iii --- Page 4 --- Introduction The Voluntary AI Safety Standard gives practical guidance to all Australian organisations on how to safely and responsibly use and innovate with artificial intelligence (AI). Through the Safe and Responsible AI agenda, the Australian Government is acting to ensure that the development and deployment of AI systems in Australia in legitimate but high-risk settings is safe and can be relied on, while ensuring the use of AI in low-risk settings can continue to flourish largely unimpeded. In 2023, the government underwent consultation through its discussion paper on \u2018Safe and Responsible AI in Australia\u2019. In the Interim Response areas of government action were outlined, including: \u2022 delivering regulatory clarity and certainty \u2022 supporting and promoting best practice for safety \u2022 ensuring government is an exemplar in the use of AI \u2022 engaging internationally on how to govern AI. The response also recognised the need to consider building AI capability in Australia. To support and promote best practice, an immediate action was to work in close consultation with industry to develop a Voluntary AI Safety Standard. This standard complements the broader Safe and Responsible AI agenda, including developing options on mandatory guardrails for those developing and deploying AI in Australia in high-risk settings. While there are examples of good practice through Australia, approaches are inconsistent. This is causing confusion for organisations and making it difficult for them to understand what they need to do to develop and use AI in a safe and responsible way. The standard establishes a consistent practice for organisations. It also sets expectations for what future legislation may look like as the government considers its options on mandatory guardrails. The standard consists of 10 voluntary guardrails that apply to all organisations throughout the AI supply chain. They include testing, transparency and accountability requirements across the supply chain. They also explain what developers and deployers of AI systems must do to comply with the guardrails. The guardrails help organisations to benefit from AI while mitigating and managing the risks that AI may pose to organisations, people and groups. 1. Establish, implement, and publish an accountability process including governance, internal capability and a strategy for regulatory compliance. 2. Establish and implement a risk management process to identify and mitigate risks. 3. Protect AI systems, and implement data governance measures to manage data quality and provenance. 4. Test AI models and systems to evaluate model performance and monitor the system once deployed. 5. Enable human control or intervention in an AI system to achieve meaningful human oversight. 6. Inform end-users regarding AI-enabled decisions, interactions with AI and AI-generated content. 7. Establish processes for people impacted by AI systems to challenge use or outcomes. 8. Be transparent with other organisations across the AI supply chain about data, models and systems to help them effectively address risks. 9. Keep and maintain records to allow third parties to assess compliance with guardrails. 10. Engage your stakeholders and evaluate their needs and circumstances, with a focus on safety, diversity, inclusion and fairness. The first 9 voluntary guardrails have been aligned closely with proposed mandatory guardrails, with the exception of the 10th voluntary guardrail, which emphasises the importance of ongoing engagement with stakeholders to evaluate their needs and circumstances. Conformity assessments, proposed in the Voluntary AI Safety Standard | industry.gov.au/NAIC iv --- Page 5 --- 10th mandatory guardrail, are being prepared for in the Voluntary AI Safety Standard through several voluntary steps organisations can be taking now to improve their record keeping, transparency and testing approaches. Figure 1: Application of the guardrails An AI deployer is an individual or organisation that supplies or uses an AI system to provide a product or service. Deployment can be internal to the business or external. When deployment is external it can impact others, such as customers or other people, who are not deployers of the system. While the first version of the standard applies to both AI deployers and AI developers, it focuses on providing guidance at the organisational and system level for AI deployers. This reflects feedback received while we developed the standard. We heard that deployers, which are the majority of businesses in the Australian ecosystem who are using AI, had the greatest need for guidance on how to adopt best practice. Focusing on deployers also supports them to work with developers on the practices needed to support the safe and responsible use of AI across the supply chain. We will include the additional, more complex guidance for AI developers in the next version of the standard. To aid deployers of AI systems, the 10 guardrails include procurement guidance. This will ensure AI suppliers and developers are aligning to the guardrails through contractual agreements. Voluntary AI Safety Standard | industry.gov.au/NAIC v --- Page 6 --- The guardrails on a page Guardrails 1. Establish, implement and publish an Guardrail one creates the foundation for your accountability process including organisation\u2019s use of AI. Set up the required accountability governance, internal capability and a processes to guide your organisation\u2019s safe and strategy for regulatory compliance. responsible use of AI, including: \u2022 an overall owner for AI use \u2022 an AI strategy \u2022 any training your organisation will need. 2. Establish and implement a risk Set up a risk management process that assesses the AI management process to identify and impact and risk based on how you use the AI system. mitigate risks. Begin with the full range of potential harms with information from a stakeholder impact assessment (guardrail 10). You must complete risk assessments on an ongoing basis to ensure the risk mitigations are effective 3. Protect AI systems, and implement You must have appropriate data governance, privacy and data governance measures to cybersecurity measures in place to appropriately manage manage data quality and provenance. and protect AI systems. These will differ depending on use case and risk profile, but organisations must account for the unique characteristics of AI systems such as: \u2022 data quality \u2022 data provenance \u2022 cyber vulnerabilities. 4. Test AI models and systems to Thoroughly test AI systems and AI models before evaluate model performance and deployment, and then monitor for potential behaviour monitor the system once deployed. changes or unintended consequences. You should perform these tests according to your clearly defined acceptance criteria that consider your risk and impact assessment. 5. Enable human control or intervention It is critical to enable human control or intervention in an AI system to achieve meaningful mechanisms as needed across the AI system lifecycle. AI human oversight. systems are generally made up of multiple components supplied by different parties in the supply chain. Meaningful human oversight will let you intervene if you need to and reduce the potential for unintended consequences and harms. 6. Inform end-users regarding AI- Create trust with users. Give people, society and other enabled decisions, interactions with organisations confidence that you are using AI safely and AI and AI-generated content. responsibly. Disclose when you use AI, its role and when you are generating content using AI. Disclosure can occur in many ways. It is up to the organisation to identify the most appropriate mechanism based on the use case, stakeholders and technology used. Voluntary AI Safety Standard | industry.gov.au/NAIC vi --- Page 7 --- Guardrails 7. Establish processes for people Organisations must provide processes for users, impacted by AI systems to challenge organisations, people and society impacted by AI systems use or outcomes to challenge how they are using AI and contest decisions, outcomes or interactions that involve AI. 8. Be transparent with other Organisations must provide information to other organisations across the AI supply organisations across the AI supply chain so they can: chain about data, models and systems to help them effectively \u2022 understand the components used including data, address risks models and systems \u2022 understand how it was built \u2022 understand and manage the risk of the use of the AI system. 9. Keep and maintain records to allow Organisations must maintain records to show that they third parties to assess compliance have adopted and are complying with the guardrails. This with guardrails. includes maintaining an AI inventory and consistent AI system documentation. 10. Engage your stakeholders and It is critical for organisations to identify and engage with evaluate their needs and stakeholders over the life of the AI system. This helps circumstances, with a focus on organisations to identify potential harms and understand safety, diversity, inclusion and fairness. if there are any potential or real unintended consequences from the use of AI. Deployers must identify potential bias, minimise negative effects of unwanted bias, ensure accessibility and remove ethical prejudices from the AI solution or component. Voluntary AI Safety Standard | industry.gov.au/NAIC vii --- Page 8 --- Contents The guardrails on a page .......................................................................................................... vi Part 1. Guide to this standard ................................................................................................... 2 Introducing the first version of Australia\u2019s AI Safety Standard ................................................................ 2 A human-centred standard ................................................................................................................. 3 An internationally consistent standard................................................................................................. 5 Part 2. Foundational concepts for the standard: risks, harms and legal context ............................. 6 AI systems have specific characteristics that amplify risks.................................................................... 6 The standard supports a risk-based approach to AI harm prevention ..................................................... 7 System factors and attributes that amplify risks and harms .................................................................. 8 The legal landscape for AI in Australia ................................................................................................ 11 Part 3. The guardrails .............................................................................................................. 13 Using the guardrails .......................................................................................................................... 14 The guardrails Guardrail 1: Establish, implement and publish an accountability process including governance, internal capability and a strategy for regulatory compliance. ........................................................... 16 Guardrail 2: Establish and implement a risk management process to identify and mitigate risks ....... 19 Guardrail 3: Protect AI systems and implement data governance measures to manage data quality and provenance............................................................................................................................ 22 Guardrail 4. Test AI models and systems to evaluate model performance and monitor the system once deployed. ............................................................................................................................. 25 Guardrail 5: Enable human control or intervention in an AI system to achieve meaningful human oversight across the lifecycle. ....................................................................................................... 29 Guardrail 6: Inform end-users regarding AI-enabled decisions, interactions with AI and AI-generated content. ....................................................................................................................................... 31 Guardrail 7: Establish processes for people impacted by AI systems to challenge use or outcomes. . 34 Guardrail 8: Be transparent with other organisations in the lifecycle of an AI system or model to effectively address risks. ............................................................................................................... 36 Guardrail 9: Keep and maintain records to allow third parties to assess compliance with guardrails. . 39 Guardrail 10: Engage your stakeholders and evaluate their needs and circumstances, with a focus on safety, diversity, inclusion and fairness. .................................................................................... 42 Appendices ........................................................................................................................... 45 Part 4: Applying and adopting the standard through examples. ............................................................ 45 Example 1: General-purpose AI Chatbot ............................................................................................ 45 Example 2: Facial recognition technology .......................................................................................... 50 Example 3: Recommender engine ..................................................................................................... 52 Example 4: Warehouse accident detection ........................................................................................ 54 Acknowledgements .......................................................................................................................... 58 References ...................................................................................................................................... 59 Voluntary AI Safety Standard | industry.gov.au/NAIC 1 --- Page 9 --- Guide to this standard The Voluntary AI Safety Standard has 4 parts. 1. Part 1 explains the purpose of the standard and its value to organisations. It gives context and grounding for the standard through a human-centred lens. 2. Part 2 describes the foundational concepts of risks and harms and the legal context for AI systems. 3. Part 3 presents the core content of the standard as a set of 10 guardrails. 4. Part 4 gives examples to help organisations understand why and how they might adopt the standard. Part 1: Introducing the first version of Australia\u2019s AI Safety Standard We designed Australia\u2019s first voluntary AI safety standard to help organisations develop and deploy AI systems in Australia safely and reliably. Adopting AI and automation is projected to contribute $170 billion to $600 billion of GDP. Australian organisations and the Australian economy can gain significant benefits if they can capture this.1 The standard offers a set of voluntary guardrails to establish consistent practices for organisations to adopt AI in a safe and responsible way. This is in line with current and evolving legal and regulatory obligations and public expectations. While this standard applies to all organisations across the AI supply chain, this first version of the standard focuses more closely on organisations that deploy AI systems. The next version will expand on technical practices and guidance for AI developers. Definitions Safe and responsible AI: AI should be designed, developed, deployed and used in a way that is safe. Its use should be human-centred, trustworthy and responsible. AI systems should be developed and used in a way that provides benefits while minimising the risk of negative impact to people, groups, and wider society. AI deployer: An individual or organisation that supplies or uses an AI system to provide a product or service. Deployment can be internal to an organisation, or external and impacting others, such as customers or other people who are not deployers of the system. AI developer: An organisation or entity that designs, develops, tests and provides AI technologies such as AI models and components. AI user: An entity that uses or relies on an AI system. This entity can range from an organisation (such as business, government or not-for-profit), an individual or other system. Affected stakeholder: An entity impacted by the decisions or behaviours of an AI system, such as an organisation, individual, community or other system. A complete list of terms and definitions is available in the terms and definitions. While there are already examples of good AI practice in Australia, organisations need clearer guidance. By adopting this standard, organisations will be able to use AI safely and responsibly. The standard consists of 10 voluntary guardrails that apply to all organisations across the AI supply chain. The voluntary guardrails establish consistent practice to adopt AI in a safe and responsible way. This will give certainty to all organisations about what developers and deployers of AI systems must do to comply with the guardrails. Voluntary AI Safety Standard | industry.gov.au/NAIC 2 --- Page 10 --- In the government\u2019s January Interim Response to the Safe and Responsible AI discussion paper, the government identified actions to take. These included working with industry to develop this Voluntary AI Safety Standard. This standard sits alongside a broader suite of government actions enabling safe and responsible AI under 5 pillars, outlined in Figure 2. Actions included in the 5 pillars include the Proposals Paper for Introducing Mandatory Guardrails for AI in High-Risk Settings, the National Framework for the Assurance of Artificial Intelligence in Government and the Policy for Responsible Use of AI in Government. The standard will continue to evolve alongside the broader activities underway by government to ensure alignment and consistency for safe and responsible AI. Figure 2: Actions the government is taking to support safe and responsible AI in Australia Why implement a voluntary standard? The standard establishes a consistent practice for organisations. It sets expectations for what future legislation may look like as the government considers its options on mandatory guardrails. It also gives organisations the best practice AI governance and ethical practices, which offers them a competitive advantage. The standard is designed to guide organisations to: \u2022 raise the levels of safe and responsible capability across Australia \u2022 protect people and communities from harms \u2022 avoid reputational and financial risks to their organisations \u2022 increase organisational and community trust and confidence in AI systems, services and products \u2022 align with legal obligations and expectations of the Australian population \u2022 operate more seamlessly in an international economy. This will lead to the longer-term benefits of improved safety, quality and reliability of AI in Australia. It will support broader use of AI products and services, increased market competition and opportunities for technological innovation. Voluntary AI Safety Standard | industry.gov.au/NAIC 3 --- Page 11 --- A human-centred standard This standard adopts a human-centred approach to AI development and deployment. This is in line with Australia\u2019s AI Ethics Principles2 and Australia\u2019s commitment to international declarations such as the Bletchley Declaration.3 A human-centred approach helps make sure technologies are fit-for -purpose while serving humans, respecting individual rights and protecting marginalised groups. In the context of safe and responsible AI system usage, a human-centred approach means: \u2022 Protecting people. The standard is designed to help leaders and business owners identify, prevent, minimise and remedy a wide range of harms and AI-related risks relevant to their organisation. This is in line with the government\u2019s Interim Response. However, its main purpose is to protect the safety of people and their rights. A human-centred approach to AI upholds Australia\u2019s responsibility to human rights protections. These protections are enshrined in a range of federal and state and territory instruments, the Australian Constitution and the common law.4 See Part 2 for the specific characteristics of AI systems that can amplify existing risks and create new harms for people, organisations, groups or society. \u2022 Upholding diversity, inclusion and fairness. The standard is designed to help organisations ensure AI systems serve all people in Australia, regardless of racial background, gender, age, disability status or other attribute. \u2022 Prioritising people through human-centred design. Human-centred design is an approach to technology design, development and deployment that recognises and balances human goals, relationships and social contexts with the capabilities and limitations of technical systems.5 The standard offers practical ways to prioritise the needs of humans in the use of AI systems. \u2022 Deploying trustworthy AI systems to support social licence. To unlock the greatest possible value from AI, an organisation deploying it must have social licence for its use. This social licence is based on stakeholders believing in the trustworthiness of the AI system. It is only by earning and maintaining the trust of stakeholders that an organisation can be confident it possesses the social licence needed to deploy AI systems. Bias This standard defines bias as the \u2018systematic difference in the treatment of certain objects, people or groups in comparison to others\u2019. It can be the basis for unfairness, defined as \u2018unjustified differential treatment that preferentially benefits certain groups more than others\u2019. For some use cases, such as healthcare, accounting for gender differences can be essential to understand the risk factors or treatment appropriate for an individual or group. This justifies a differential treatment.6 Bias becomes problematic or \u2018unwanted\u2019 when it results in unfavourable treatment for people or groups. This unfair disadvantage then becomes unlawful discrimination if that treatment is a result of a \u2018protected attribute\u2019:7 \u2022 age \u2022 disability \u2022 race, including colour, national or ethnic origin or immigrant status \u2022 sex, pregnancy, marital or relationship status, family responsibilities or breastfeeding \u2022 sexual orientation, gender identity or intersex status. Voluntary AI Safety Standard | industry.gov.au/NAIC 4 --- Page 12 --- An internationally consistent standard Recognising that Australia is an open, trading economy, the standard\u2019s recommended processes and practices are consistent with current international standards and best practice. This supports Australian organisations who operate internationally by aligning Australian practices with other jurisdictions\u2019 expectations. It also aims to avoid creating barriers to international organisations operating in Australia compared to other markets. The standard draws on and is aligned with a range of international standards. Most important is the leading international standard on AI management systems, AS ISO/IEC 42001:2023, and the US standard on AI risk management, NIST AI RMF 1.0.8 Each requirement in the standard guardrails gives references as to how it is aligned with relevant international and local standards or practices. Future versions will reflect changes in the international landscape. Voluntary AI Safety Standard | industry.gov.au/NAIC 5 --- Page 13 --- Part 2: Foundational concepts for the standard: risks, harms and legal context AI systems have specific characteristics that amplify risks AI systems span a wide range of technical approaches. Organisations can use them for many tasks, such as helping with prediction, classification, optimisation or content generation. At their core, AI systems are software-based tools. AI systems fall broadly into 2 types, each with different strengths and risks: \u2022 Narrow AI systems are designed and trained to perform a specific task. Most AI systems in use today fall into this category. These types of systems can perform well in a narrow range of activities, potentially even better than humans, but they cannot perform any other tasks. Examples include chess engines, recommender systems, medical diagnostic systems and facial recognition systems. \u2022 General-purpose AI systems are designed and trained to handle a broad range of tasks and are therefore flexible. Their use is not limited to a specific function, so they can be more easily used for purposes their designers may not have considered. Examples include large language models and systems such as Open AI\u2019s ChatGPT series. Both narrow and general-purpose AI systems are built and operate differently from traditional software systems. These differences mean that using an AI system for a particular task may amplify existing risks when compared with traditional software. For example, in traditional software systems, developers explicitly define all the logic governing a system\u2019s behaviour. This relies on explicit knowledge, with conscious human engagement at every stage of the software design and development process. Traditional software systems are easier for humans to control, predict and understand. In contrast, developers of AI systems take a different approach. This often involves defining an objective and constraints, selecting a dataset, and employing a \u2018machine learning algorithm\u2019. This creates an AI model which can achieve the specified objective. While such models often outperform comparable, traditional software systems, the different development approach means AI models are often less transparent, less interpretable, and more complex to test and verify. This amplifies risks and can lead to harm. This is more likely to happen in contexts where it is important to understand and explain how the output was achieved or to constrain the range of potential outputs for safety reasons. The specific characteristics of general AI systems can amplify risks and harms or pose new risks and harms to an organisation. General AI systems are more prone to unexpected and unwanted behaviour or misuse. This is because of their increased flexibility of interactions, the reduced predictability of their capabilities and behaviour and their reliance on large and diverse training data. For example, large language models can deliberately or inadvertently manipulate or misinform consumers. They can also pose novel intellectual property challenges for both training data and the outputs generated. Voluntary AI Safety Standard | industry.gov.au/NAIC 6 --- Page 14 --- The standard supports a risk-based approach to AI harm prevention As with all software, AI systems vary in the level of risk and the type of harm they pose. Some, like an algorithm on a website that suggests reordering based on stock levels, tend to be lower risk. The potential harms are confined to a customer taking longer to receive a product. Others, like a tool that prioritises job applicants for an interview process or makes financial lending decisions, have far greater potential to create harm. For instance, they may deny a suitable applicant the opportunity of a job or bank loan, or even systematically and unlawfully discriminate against a group of people. The standard supports a risk-based approach to managing AI systems. It does this by supporting organisations \u2013 starting with AI deployers \u2013 to take proactive steps to identify risks and mitigate the potential for harm posed by the AI systems they deploy, use or rely on. The standard prioritises safety and the mitigation of harms and risks to people and their rights. A human-centred perspective on the harms of AI systems Organisations should assess the potential for these risks and harms to people: \u2022 Harm to people. This includes infringements on personal civil liberties, rights, and physical or psychological safety. It can also include economic impacts, such as lost job opportunities because of algorithmic bias in AI recruitment tools or the unfair denial of services based on automated decision-making. \u2022 Harm to groups and communities. AI systems can exacerbate discrimination or unwanted bias against certain sub-groups of the population, including women, people with disability, and people from multicultural backgrounds. This can lead to social inequality, undermining of equality gains and unjust treatment. This is pertinent in recommender algorithms that amplify harmful content. \u2022 Harm to societal structures. AI systems\u2019 impact on broader societal elements, such as democratic participation or access to education, can be profound. AI systems that spread misinformation could undermine electoral processes, while those that affect educational algorithms could widen the digital divide. The standard is useful and applicable for identifying, preventing and minimising other risks that may affect an organisation. Organisations often analyse these risks against the potential for reputational damage, regulatory breach, and commercial losses (Figure 3). Figure 3: Organisational risks of AI Commercial \u2013 Commercial losses due to poor or biased AI system performance; adversarial attacks. Reputational \u2013 Damage to reputation and loss of trust due to harmful or unlawful treatment of consumers, employees or citizens. Regulatory \u2013 Breach of legal obligations that may result in fines, restrictions and require management focus. Voluntary AI Safety Standard | industry.gov.au/NAIC 7 --- Page 15 --- System factors and attributes that amplify risks and harms Several factors impact the likelihood of both narrow and general AI systems amplifying existing risks. These include why, when, where and how an AI system is deployed, as outlined in Table 1. The standard recognises that AI deployers may not have full knowledge or control over all these factors. However, the standard encourages organisations to understand the AI systems they use or rely on. This will help to identify and mitigate risks more accurately. Use the questions in Table 1 to assess if your system attributes suggest an elevated AI system risk. Voluntary AI Safety Standard | industry.gov.au/NAIC 8 --- Page 16 --- Table 1: System attributes and guiding questions for organisations to assess level of risk System Description Questions to help identify Examples attribute when an attribute may amplify risk (Answering \u2018yes\u2019 indicates a higher level of risk) AI system The choice of AI approach and Is the way the AI system A generative AI system is technical model can cause risk as well as operates inherently opaque used to create HR-related architecture improve performance. For to the provider, deployer or marketing materials. example, reduced transparency user? and greater uncertainty mean AI Does it rely on generative AI systems tend to need ongoing in ways that can lead to monitoring and meaningful harmful outputs? human oversight. They may be inappropriate for contexts where there is a legal requirement to provide a reason for a decision or output. General-purpose AI systems tend to have a higher risk profile than either narrow AI or traditional software solutions intended for the same task. Purpose AI systems can considerably Does the AI system create A bank uses a risk outperform traditional an output or decision assessment algorithm to approaches in many areas. This (intentional or not) that has decide whether to grant a means that organisations are a legal or significant effect home loan. increasingly adopting AI on an individual? systems to perform tasks that If so, will any harm caused have significant direct and be difficult to contest or indirect impacts for people. As manage redress? the impacts of an AI system rise, so too does the potential for significant harm if they fail or are misused. Context AI systems, being software, are Does the AI system interact A large retailer uses facial scalable as well as high with or affect people who recognition technology to performing for many tasks. have extra forms of legal identify shoplifters. However, their deployment in protection (such as certain contexts may be children)? inappropriate and their Will the system be deployed scalability may lead to in a public space? widespread harms. For example, the use of facial recognition systems in public spaces where children are likely to be present, or algorithms used to gather sensitive data about Australians from social media sites.9 Voluntary AI Safety Standard | industry.gov.au/NAIC 9 --- Page 17 --- System Description Questions to help identify Examples attribute when an attribute may amplify risk (Answering \u2018yes\u2019 indicates a higher level of risk) Data AI systems\u2019 performance is Is confidential, personal, An SME deploys a chatbot affected by the quality of data sensitive and/or biometric to confirm customer and how accurately that data information used either in contact details. represents people. Biased the AI system\u2019s training, its training data can lead to poor operation or as an input for quality or discriminatory making inferences? outputs. For example, health Is that data biased, non- diagnostic tools trained on representative or not a historically male-dominated comprehensive and non-diverse data may representation of the people produce outputs that lead to or contexts it is making a under-diagnosis or decision about? misdiagnosis of women and non-white patients. Level of Not all automated AI systems Does this system operate A construction site automation are risky. However, systems automatically? deploys autonomous that operate independently, or forklifts to move pallets in Does the system make that can be triggered or produce a warehouse. decisions without any outputs independent of human meaningful human oversight engagement, may increase or validation? risks if they fail or are misused. Risk further increases when there is a considerable period of time between the fault or malicious use happening and the harm being recognised by responsible teams. Voluntary AI Safety Standard | industry.gov.au/NAIC 10 --- Page 18 --- The legal landscape for AI in Australia This standard and the guardrails described in part 3 are voluntary. The standard does not seek to create new legal obligations for Australian organisations. It is designed to help organisations deploy and use AI systems in the bounds of existing Australian laws, emerging regulatory guidance and community expectations. Table 2 shows some of the existing laws of general application that will have an impact on how Australian organisations develop and deploy AI. Organisations deploying, using or relying on AI systems should be aware of these laws, and how they may constrain or inform the use of AI. There are also laws that may apply depending on the particular AI use case or application. These include product safety laws, motor vehicles and surveillance laws, and laws that may apply to particular sectors or organisations such as financial services or the medical sector. Organisations may also need to comply with laws of non-Australian jurisdictions (for example, where laws of another jurisdiction have extraterritorial application). As part of their duties, directors of organisations must have a sufficient understanding of both the risks and the laws that apply to their use of AI. Part 4 provides examples of use cases and potential risks and harms. Table 2: AI risks or harms and general laws AI risks or harms and general laws that may apply AI system not sufficiently secure \u2022 Directors\u2019 duties (e.g. to exercise powers and discharge duties with due care and diligence), to assess and govern risks to the organisation (including non-financial risk e.g. from AI and data). \u2022 Privacy laws, require steps that are reasonable in the circumstances to protect personal information and impose data minimisation obligations to destroy or deidentify information no longer needed. \u2022 The security of critical infrastructure act and sector specific laws (e.g. financial services), impose risk management and cybersecurity obligations. \u2022 Negligence, if a failure in risk management practices amounts to a failure to take reasonable steps to avoid foreseeable harm to people owed a duty of care, and that failure causes the harm. \u2022 Online safety laws, if certain online service providers fail to take pre-emptive and preventative actions to minimise harms from online services. Misleading outputs / statements \u2022 The Australian Consumer Law prohibitions against unfair practices (e.g. misleading and deceptive conduct and false and misleading representations) may apply: - if the outputs are misleading (e.g deceptive use of deepfakes) - to misleading representations or silence as to when AI is being used - to misleading statements as to the performance and outputs of the AI systems Voluntary AI Safety Standard | industry.gov.au/NAIC 11 --- Page 19 --- AI risks or harms and general laws that may apply Harmful outputs \u2022 Product liability (where the organisation is a manufacturer), if outputs result in harm caused by a safety defect (e.g. a defect in the design, model, manufacturing or testing of the system, including failure to address bias or cybersecurity risk) and other product safety laws (including recalls and reporting). \u2022 Negligence, if an organisation fails to exercise the standard of care of a reasonable person to avoid foreseeable harm to persons to whom it owes a duty of care, and that failure causes the harm. \u2022 Criminal laws, if the output resulted in, or aided or abetted the commission of a crime. \u2022 Online safety laws, if the outputs are restricted or harmful online content (such as cyberbullying or cyber-abuse material, or non-consensual sharing of intimate images or child sexual abuse material). \u2022 Defamation laws, if the outputs are defamatory and the organisation participated in the process of making the defamatory material available (such as through making the tool available or training) rather than merely disseminating the content. Misuse of data or infringement of model or system \u2022 Privacy laws, intellectual propriety laws (including copyright), duties of confidence and contract, protect the use, reproduction and/or disclosure of data (including training data, input data and outputs) and the model or system without the requisite consents or rights. \u2022 Privacy laws, restrict the collection of personal information for an improper purpose, and impose transparency and data minimisation requirements on the handling of personal information. \u2022 The Australian Consumer Law prohibitions against misleading and deceptive conduct, unconscionable conduct and false and misleading representations, may apply to unfair data collection and use practices. Bias, incorrect or poor-quality output \u2022 Privacy laws, impose quality and accuracy obligations that may apply to training and input data (that is personal information) and outputs (where new personal information is generated). \u2022 Systems that produce inaccurate or erroneous outputs such as \u2018AI hallucinations\u2019 may be in breach of statutory guarantees under the Australian Consumer Law (e.g. consumer goods be of acceptable quality and fit for purpose, or consumer services be rendered with due care and skill). \u2022 Anti-discrimination laws, if outputs exclude or disproportionately affect an individual or group on the basis of a protected attribute. AI system not accessible to individual or group \u2022 Anti-discrimination laws, if the exclusion is based on a protected attribute. \u2022 Prohibitions on unconscionable conduct under the Australian Consumer Law, if the exclusion of a consumer was so harsh that it goes against good conscience. \u2022 Essential services obligations, e.g. if used in energy and telecommunications essential services. Voluntary AI Safety Standard | industry.gov.au/NAIC 12 --- Page 20 --- Part 3: The guardrails Guardrails 1. Establish, implement and publish an Guardrail one creates the foundation for your accountability process including organisation\u2019s use of AI. Set up the required governance, internal capability and a accountability processes to guide your strategy for regulatory compliance. organisation\u2019s safe and responsible use of AI, including: \u2022 an overall owner for AI use \u2022 an AI strategy \u2022 any training your organisation will need. 2. Establish and implement a risk Set up a risk management process that management process to identify and assesses the AI impact and risk based on mitigate risks. how you use the AI system. Begin with the full range of potential harms with information from a stakeholder impact assessment (guardrail 10). You must complete risk assessments on an ongoing basis to ensure the risk mitigations are effective 3. Protect AI systems, and implement You must have appropriate data governance, data governance measures to privacy and cybersecurity measures in place manage data quality and to appropriately protect AI systems. These provenance. will differ depending on use case and risk profile, but organisations must account for the unique characteristics of AI systems such as: \u2022 data quality \u2022 data provenance \u2022 cyber vulnerabilities. 4. Test AI models and systems to Thoroughly test AI systems and AI models evaluate model performance and before deployment, and then monitor for monitor the system once deployed potential behaviour changes or unintended consequences. You should perform these tests according to your clearly defined acceptance criteria that consider your risk and impact assessment. 5. Enable human control or It is critical to enable human control or intervention in an AI system to intervention mechanisms as needed across achieve meaningful human oversight the AI system lifecycle. AI systems are across the life cycle. generally made up of multiple components supplied by different parties in the supply chain. Meaningful human oversight will let you intervene if you need to and reduce the potential for unintended consequences and harms. Voluntary AI Safety Standard | industry.gov.au/NAIC 13 --- Page 21 --- Guardrails 6. Inform end-users regarding AI- Create trust with users. Give people, society enabled decisions, interactions with and other organisations confidence that you AI and AI-generated content. are using AI safely and responsibly. Disclose when you use AI, its role and when you are generating content using AI. Disclosure can occur in many ways. It is up to the organisation to identify the most appropriate mechanism based on the use case, stakeholders and technology used. 7. Establish processes for people Organisations must provide processes for impacted by AI systems to challenge users, organisations, people and society use or outcomes impacted by AI systems to challenge how they are using AI and contest decisions, outcomes or interactions that involve AI. 8. Be transparent with other Organisations must provide information to organisations across the AI supply other organisations across the AI supply chain about data, models and chain so they can: systems to help them effectively address risks \u2022 understand the components used including data, models and systems \u2022 understand how it was built \u2022 understand and manage the risk of the use of the AI system. 9. Keep and maintain records to allow Organisations must maintain records to third parties to assess compliance show that they have adopted and are with guardrails. complying with the guardrails. This includes maintaining an AI inventory and consistent AI system documentation. 10. Engage your stakeholders and It is critical for organisations to identify and evaluate their needs and engage with stakeholders over the life of the circumstances, with a focus on AI system. This helps organisations to safety, diversity, inclusion and identify potential harms and understand if fairness. there are any potential or real unintended consequences from the use of AI. Deployers must identify potential bias, minimise negative effects of unwanted bias, ensure accessibility and remove ethical prejudices from the AI solution or component. Using the guardrails Adopting these guardrails will create a foundation for safe and responsible AI use. It will make it easier for any organisation to comply with any potential future regulatory requirements in Australia and emerging international practices. It will also help to uplift any organisation\u2019s AI maturity. When using the guardrails, start with guardrail 1 to create your core foundations. To completely adopt the standard, your organisation will need to adopt all 10 guardrails. Voluntary AI Safety Standard | industry.gov.au/NAIC 14 --- Page 22 --- Since most deployers rely on AI systems developed or provided by third parties, these guardrails offer procurement guidance (in aqua boxes) on how to work with your supplier to ensure their practice is aligned with the guardrails. The guardrails are not intended to be one-off activities. Instead, they are ongoing activities for organisations. The guardrails may contain organisational-level obligations to create the required processes and system-level obligations for each use case or AI system. How the guardrails support human-centred AI deployment Being voluntary, the standard does not create new legal duties about AI systems or their use. Rather, the guardrails ask organisations to commit to: \u2022 understanding the specific factors and attributes of their use of AI systems \u2022 meaningfully engaging with stakeholders \u2022 performing appropriately detailed risk and impact assessments \u2022 undertaking testing \u2022 adopting appropriate controls and actions so their AI deployment is safe and responsible. These activities will help organisations understand regulatory obligations and community expectations around AI use. For example, if an organisation deploys an AI system that uses data from or about First Nations communities, the organisation should respect Indigenous Data Sovereignty Principles. These principles draw on Article 32(2) of the United Nations Declaration on the Rights of Indigenous Peoples (UNDRIP). They affirm the inherent rights of First Nations peoples to govern the collection, ownership and use of their data. The principles require organisations to use this data in a way that respects the values and laws of First Nations communities. They also require that organisations secure free, prior and informed consent from relevant First Nations communities before starting AI projects that will engage First Nations data or impact First Nations communities. First Nations communities must have the capacity to withdraw consent should AI system data usage deviate from the initially agreed purposes. Guide to icons The icons in Figure 4 are designed to guide in mapping actions under each guardrail to Australia\u2019s AI Ethics principles. Human, societal and Reliability and safety environment wellbeing Human-centered values Transparency and explainability Fairness Contestability Privacy protection and Accountability security Figure 4: Mapping to the ethics principles Guidance for working with suppliers is provided in aqua boxes. Voluntary AI Safety Standard | industry.gov.au/NAIC 15 --- Page 23 --- 1. Guardrail 1: Establish, implement and publish an accountability process including governance, internal capability and a strategy for regulatory compliance. Guardrail 1 creates the foundation for your organisation\u2019s use of AI. Set up the required accountability processes to guide your organisation\u2019s safe and responsible use of AI, including: \u2022 an overall owner for AI use \u2022 an AI strategy \u2022 any training you will need to ensure broad understanding of these principles across the organisation. Incorporate AI governance into your existing processes or create new processes as you need them. Voluntary AI Safety Standard | industry.gov.au/NAIC 16 --- Page 24 --- 1.1 Organisational leadership and accountability Commit to appointing people in the leadership team who are accountable for the governance and outcomes of AI systems, as well as the safe and responsible use of AI within the organisation. Key concept: Leaders cannot delegate or outsource accountability for the safe and responsible deployment and use of AI systems. 1.1.1 Assign and communicate accountability and authority to relevant roles. These roles will ensure AI systems and the overall AI management system perform in the ways required. Having these roles also ensures AI systems meet external obligations and internal policies, including monitoring and reporting responsibilities.10 1.1.2 Staff these roles with appropriately empowered and skilled people. These people will need to meet specific obligations, such as handling personally identifiable information and legal and regulatory obligations.11 1.1.3 Clearly communicate the leadership commitment to, and accountability for, safe and responsible development and use of AI across the organisation. This includes the staff (including contractors and third-party providers) who you have made accountable for AI systems.12 1.1.4 Create and document overarching organisational responsibilities and accountabilities for AI deployment and use.13 1.1.5 Provide sufficient resources to deploy and use AI responsibly and safely throughout the organisation and throughout the lifecycle of AI systems in use.14 1.1.6 Maintain operational accountability, capability and meaningful human oversight throughout the lifecycle of AI systems in use.15 1.2 AI strategy and governance Commit to creating and documenting overarching objectives and policies for the deployment and use of AI. These should be in line with your organisation\u2019s strategic goals and values. Key concept: You should only adopt AI strategy and policies to address gaps in existing related policies, such as information security, data management and data privacy, or to include enhancements to existing policies to address the specific characteristics of AI systems. 1.2.1 Document and communicate the requirement that AI use in the organisation be assigned to an accountable owner with appropriate capability for this role. 1.2.2 Create and document the organisation\u2019s overarching strategic intent to deploy and use AI systems in line with the organisation\u2019s strategy and values.16 1.2.3 Create, document and communicate the organisation\u2019s strategy to comply with identified regulation related to the organisation\u2019s deployment and use of AI systems. 1.2.4 Create, document and communicate appropriately detailed AI policies, processes and goals for safe and responsible AI. Ensure these are compatible with the overall strategy. Create a process to set targets for AI systems to meet obligations for the safe and responsible use of AI.17 1.2.5 Review and revise cross-organisation AI strategies, policies and processes at appropriate intervals so they remain fit for purpose and meet the legal and regulatory obligations of the organisation.18 Make sure to appropriately plan any changes to the overarching AI management system.19 Voluntary AI Safety Standard | industry.gov.au/NAIC 17 --- Page 25 --- 1.2.6 Create and document a process to proactively identify deficiencies in the overarching AI management system. This includes instances of non-compliance in any AI systems or in use of AI systems in the organisation. Include documentation of any root causes, corrective action taken and revisions to the AI management system required.20 1.2.7 Create and document a process for deploying AI systems that supports mapping from business targets to system performance, with suggested metrics for internal and third-party developed systems.21 1.2.8 Identify and document any factors that may affect the organisation\u2019s ability to meet its responsibilities through the overarching AI management system.22 1.2.9 Where you anticipate developing AI systems internally, create and document the end-to-end process for AI system design and development.23 1.2.10 Document and perform a training needs analysis for broad AI understanding across the organisation. Source or deliver training to bridge any identified gaps. Regularly check AI skills are up to date as AI use and understanding evolves.24 1.3 Strategic AI training Commit to embedding responsible AI training and workplace practices. This provides people accountable or responsible for AI system performance with sufficient competence to perform their role. Key concept: Training requirements will depend on the nature of the role in relation to AI. At a leadership and governance level, staff need the skills to understand potential risks and benefits of AI in the context of the organisation. Product owners may need more in-depth technical skills relevant to specific characteristics of the AI system for which they are responsible. 1.3.1 Provide appropriate and up-to-date training so accountable people can perform their duties and responsibilities. Document the competencies of the accountable people.25 1.3.2 Adopt appropriate communication, training and leadership behaviour strategies to create a culture of broad accountability and address any gaps in understanding across the organisation. Offer a mechanism for staff to raise concerns or provide feedback about the use of AI systems.26 1.3.3 Monitor compliance and behaviours across the organisation to identify and address any gaps between leadership expectations and staff understanding of obligations about safe and responsible deployment and use of AI. 1.3.4 Document and communicate the consequences for people who act outside of the organisation\u2019s defined risk appetite and associated policies.27 1.3.5 Where applicable, evaluate the training needs for staff who deal with third-party AI systems that are being developed, procured or used. Provide the appropriate training to address skill gaps.28 Voluntary AI Safety Standard | industry.gov.au/NAIC 18 --- Page 26 --- 2. Guardrail 2: Establish and implement a risk management process to identify and mitigate risks AI impact and risk management processes need to consider how the AI system is used. Begin assessments with the full range of potential harms with information from the stakeholder impact assessment (guardrail 10). The impact and risk assessments must align with organisational risk appetite and tolerance levels. You must complete the risk assessments throughout the lifecycle of the AI system and on an ongoing basis to ensure the risk mitigations are effective. These assessments may be required to input into any future conformity assessments mandated for use in high-risk settings. Voluntary AI Safety Standard | industry.gov.au/NAIC 19 --- Page 27 --- 2.1. AI risk and impact management processes Commit to creating, documenting and applying an organisational-level risk management approach that considers the specific characteristics of AI systems. Key concept: Do not use the potential benefits to an organisation of deploying and using AI systems to overlook the risks and potential harms that could arise. Evaluate potential harms in relation to people, organisations and the environment. 2.1.1. Create an organisational-level risk tolerance for the use of AI systems.29 2.1.2. Create and document criteria to identify acceptable and unacceptable risks in relation to AI. Base this on the risk tolerance of the organisation, the likely risk of harms to users, and in line with AI policy.30 2.1.3. Create and document a suitable impact assessment, risk assessment and treatment approach to AI system deployment and use. This should cover both internal and third-party developed AI systems, with awareness of the specific characteristics and amplified risks of AI systems.31 Include criteria for reassessment over the lifecycle of an AI system. 2.1.4. Identify and document potential risks to the organisation and potential harms to people and groups that arise from the deployment and use of AI systems. Communicate these to relevant teams and third parties.32 2.1.5. Identify and document any specific use cases or qualities of AI systems that represent an unacceptable risk to stakeholders or the organisation, in line with the organisation\u2019s risk tolerance.33 2.1.6. Where indicated by risk, decide whether to require AI system developers to implement technology solutions for specific risk mitigation, such as industry-standard labelling and watermarking approaches. 2.1.7. Evaluate and document the high-level risks and liabilities related to the organisation\u2019s existing or planned use of third party-provided systems and components (including open-source software). 2.2. System risk and impact assessment Commit to rigorous risk and impact management processes for assessing AI systems against the organisational risk tolerance. Key concept: The level of risk of an AI system depends on the specific use case for that system. You should perform assessments for the system under the expected usage, and perform them again should that use evolve. This requires ongoing monitoring of the AI system. It may place extra responsibility on deployers and end users than more traditional technology systems. Key concept: A key risk is the over-reliance end users place on outputs or other responses from AI systems. Risk mitigation and treatment approaches should be put in place to address this risk, where appropriate, on an ongoing basis. The risk may evolve over the lifecycle of the system, particularly as users become more familiar with it. Voluntary AI Safety Standard | industry.gov.au/NAIC 20 --- Page 28 --- 2.2.1. Perform and document a risk assessment for each AI system, including systems developed by or procured from third parties. Assess and document risks with reference to specific, documented use cases, potential unintended use for that system and the unique requirements and characteristics of that system.34 2.2.2. As part of the risk assessment of systems where users, employees or other stakeholders may be exposed to potential harms, carry out and document an impact assessment process.35 2.2.3. Document and implement a system of controls to safeguard against risks and potential harms from AI systems and products as soon as is practical after your organisation has identified a risk.36 Reassess the risk after you\u2019ve implemented the controls to verify their effectiveness. 2.2.4. Perform risk assessments and treatment plans on a periodic basis or when a significant change to either the use case or system occurs, or you identify new risks. This includes responding to impact assessments or insufficient risk treatment plans.37 2.2.5. Implement, document and communicate a robust impact assessment approach relating to the deployment and use of AI systems.38 Procurement guidance for guardrail 2: Understand your suppliers\u2019 risk management processes. Make sure you have sufficient information about the system, such as identified risks and potential harms for the intended use of the system, to conduct your own risk and impact management process.39 Reflect agreed processes in your contracts. Voluntary AI Safety Standard | industry.gov.au/NAIC 21 --- Page 29 --- 3. Guardrail 3: Protect AI systems, and implement data governance measures to manage data quality and provenance. You must have appropriate data governance, privacy and cybersecurity measures in place to appropriately protect and manage AI systems. These will differ depending on use case and risk profile. Organisations must account for the unique characteristics of AI systems such as data quality, data provenance and cyber vulnerabilities. Voluntary AI Safety Standard | industry.gov.au/NAIC 22 --- Page 30 --- 3.1. Data governance, privacy and cybersecurity Commit to fit-for-purpose approaches to data governance, privacy and cybersecurity management of AI systems. This will help realise the value and mitigate the emerging and amplified risks. 3.1.1. Evaluate and adapt existing data governance processes to check they address the use of data with AI systems. Assess the risks arising from AI system use of and interaction with data. Focus on the potential for AI systems to create amplified and emerging risks.40 3.1.2. Review privacy policies to include the collection, use and disclosure of personal or sensitive information by AI systems, including for system training purposes.41 3.1.3. Review existing cybersecurity practices to verify they sufficiently address the risks arising from AI system use. 42 3.1.4. Create and document an organisation-wide process to support teams to apply the Australian Privacy Principles to all AI systems.43 3.1.5. Create and document an organisation-wide process to support teams in the management of data usage rights for AI, including intellectual property, Indigenous Data Sovereignty, privacy, confidentiality and contractual rights. 3.1.6. Create and document an organisation-wide process to support teams to apply the Essential Eight Maturity Model for cybersecurity risks to AI systems.44 3.1.7. Document how the Essential Eight Maturity Model for cybersecurity risks has been applied to each AI system in use, including those developed or provided by third parties. 45 3.2. Data governance measures to manage data quality and provenance Commit to evaluating the requirements of each AI system in relation to data quality, data provenance, information security and information management, including where systems are provided by third parties. Documentation of this activity may be required to input into any future conformity assessments mandated for use in high-risk settings Key concept: You should understand and document your data sources, put in place processes to manage your data and document the data used to train and test your AI model or system. 3.2.1. Define and document the requirements for each AI system relating to data quality, data/model provenance and data preparation.46 3.2.2. Evaluate the existing information/system security and management processes in the organisation. Make sure they are fit for purpose for AI system deployment and use. 3.2.3. Understand and document the sources, collection process and types of data on that the system was trained and tested on and the data that it relies on to function, including personal and sensitive data.47 3.2.4. Where appropriate, report to stakeholders on data, model sources and provenance for each AI system or product.48 Voluntary AI Safety Standard | industry.gov.au/NAIC 23 --- Page 31 --- 3.2.5. Document how you have applied the Australian Privacy Principles to each AI system in use, including those developed or provided by third parties.49 3.2.6. Document the data usage rights for each AI system, including intellectual property, Indigenous Data Sovereignty, privacy, confidentiality and contractual rights. 3.2.7. Consider and document data breach reporting requirements and liabilities from related standards for each AI system. For example, under the Notifiable Data Breach scheme of the Office of the Australian Information Commissioner.50 Procurement guidance for guardrail 3: Your suppliers must have appropriate data management (including data quality and data provenance), privacy, security and cybersecurity practices for the AI system or component.51 Reflect this in your contracts. Voluntary AI Safety Standard | industry.gov.au/NAIC 24 --- Page 32 --- 4. Guardrail 4. Test AI models and systems to evaluate model performance and monitor the system once deployed. Thoroughly test AI systems and AI models before you deploy them, and then monitor for potential behaviour changes or unintended consequences. Perform these tests according to the clearly defined acceptance criteria that considers the prior risk and impact assessment. Voluntary AI Safety Standard | industry.gov.au/NAIC 25 --- Page 33 --- 4.1. Organisational-level reporting, evaluation and continual improvement Commit to a robust process for timely and regular monitoring, evaluation and reporting of AI system performance. 4.1.1. Create and document organisation-wide processes and capability required for testing, monitoring, continuously evaluating, improving and reporting of AI systems.52 4.1.2. Create a formal process to review and approve evidence that systems are complying with their test requirements. 4.1.3. Apply appropriate document versioning, management and security practices.53 4.1.4. Create a process for determining whether an AI system requires regular auditing, appropriate to the level of risk identified by its risk assessment. 4.2. AI system acceptance criteria Commit to specifying, justifying and documenting acceptance criteria your organisation will need to meet to consider potential harms to be adequately controlled. 4.2.1. Create clear and measurable acceptance criteria for the AI system that, if met, should adequately control each of the identified harms. When appropriate, use industry and community general benchmarks. These criteria should be specific, objective and verifiable. Each acceptance criterion should link directly to one or more of the potential harms. For example, if the risk assessment raises fairness concerns, this implies fairness measures should be present in the acceptance criteria. Specify the thresholds or conditions under which you consider the potential harm to be adequately controlled. Record the acceptance criteria, with explicit justifications for why you chose the criteria and why you judged them to be adequate, in an acceptance criteria registry.54 4.2.2. Communicate the acceptance criteria and their justifications with all team members involved in the development, testing and deployment of the AI system.55 4.2.3. Regularly review and update the acceptance criteria to reflect any changes in the system, the identified harms or the broader context in which the system operates. Record any findings or changes in the acceptance criteria registry.56 4.3. Testing of AI systems or models to determine performance and mitigate any risks Commit to rigorously testing the system against the acceptance criteria before deployment, documenting the results and deciding whether to deploy. Voluntary AI Safety Standard | industry.gov.au/NAIC 26 --- Page 34 --- Key concept: AI model testing verifies and validates an AI system\u2019s underlying AI model(s). AI system testing verifies and validates the entire AI system, supporting expected behaviours in real-world scenarios. 4.3.1. Develop and carry out a test plan that covers all acceptance criteria. The plan should specify the testing methods, tools and metrics your organisation will use, as well as the roles and responsibilities of the testing team. \u2022 The plan should include both model and system testing. \u2022 When evaluating and testing your models, use data that is representative of the use of the system, but that has not been used in the training of the system. Where they exist, use industry and community benchmarks or datasets. \u2022 Design evaluation and testing processes that account for the possibility that there are multiple acceptable and unacceptable outputs. \u2022 For general-purpose AI systems, such as those based on large language models, include adversarial testing procedures such as red teaming. 4.3.2. Compile a complete test report, including: \u2022 a summary of the testing goals \u2022 methods and metrics used \u2022 detailed results for each test case \u2022 an analysis of the root causes of any identified issues or failures \u2022 recommendations for remediation or improvement \u2022 whether the improvements should be done before deployment or as a future release.57 4.3.3. Apply the organisational process for reviewing and approving the testing results to ensure the system meets all acceptance criteria before you deploy it.58 The system deployment authorisation must come from the person or people accountable for the AI system. 4.4. Ongoing system evaluation and monitoring Commit to implementing robust AI system performance monitoring and evaluation, and to ensuring each system remains fit for purpose. 4.4.1. Create continuous monitoring and evaluation mechanisms to gather evidence that the AI system continues to meet its acceptance criteria throughout its lifecycle. Directly monitor any measurable acceptance criteria, alongside other relevant metrics such as performance metrics or anomaly detection. Frequently evaluate the monitoring mechanisms to check they remain effective and aligned with evolving conditions.59 4.4.2. Create clear and accessible feedback channels for impacted people or groups to report problems or harms they may experience. You should actively solicit, systematically collect and carefully analyse this feedback.60 4.4.3. Follow organisational review processes to ensure accountable people review and interpret the monitoring data, reports and alerts. Keep auditable monitoring logs to document the activities, feedback you receive and actions you take. 4.4.4. Ensure that people who review individual-level feedback can trigger recourse and redress processes where there is an obligation to do so. High-impact decisions may warrant direct human oversight.61 Voluntary AI Safety Standard | industry.gov.au/NAIC 27 --- Page 35 --- 4.5. Regular system audit or assessments Commit to regular system audits for ongoing compliance with the acceptance criteria (or justify why you don\u2019t need to carry out audits). 4.5.1. Apply the organisation\u2019s process to determine whether the level of risk warrants a comprehensive system audit plan. Document this decision as a system audit requirement statement. If an audit is necessary: \u2022 Create a regular system auditing schedule based on factors such as the system\u2019s complexity, criticality and rate of change.62 \u2022 Ensure system audit teams have the necessary independence, expertise and authority to conduct a thorough, impartial evaluation against the organisation\u2019s audit criteria. Record their findings in a system audit report. The system\u2019s development team should not lead the audits.63 \u2022 Create review processes and response processes to address the findings of each system audit report. The reports should be reviewed by those accountable for the system, consulting with key stakeholders, and by management. Response processes should clearly lay out how to respond to the discovery of problems with the in-production system. Procurement guidance for guardrail 4: Clarify who is responsible and accountable for this monitoring and evaluation (between the supplier and the deployer). Regularly review with the accountable person and make sure each system remains fit for purpose. If the supplier is responsible for monitoring the AI system or its components, put an agreement in place. Voluntary AI Safety Standard | industry.gov.au/NAIC 28 --- Page 36 --- 5. Guardrail 5: Enable human control or intervention in an AI system to achieve meaningful human oversight. It is critical to ensure human control or intervention mechanisms are in place as needed across the AI system lifecycle. AI systems are generally made up of multiple components supplied by different parties in the supply chain. Meaningful human oversight will result in appropriate intervention and reduce the potential for unintended consequences and harms. Voluntary AI Safety Standard | industry.gov.au/NAIC 29 --- Page 37 --- 5.1. Accountability and human control to achieve meaningful human oversight. Commit to assigning accountability to a suitably competent and empowered person in the organisation for each AI system and product. 5.1.1. Assign accountability for each AI system to someone who shows suitable competence and has the necessary tools and resources. 64 5.1.2. Assign the accountable role sufficient authority to oversee, intervene and be effective in ensuring responsible AI use throughout the system lifecycle. 5.1.3. Create and document competency, oversight and intervention requirements and support needs for each AI system before implementation. Evaluate as part of the continuous improvement cycle.65 5.1.4. Create and document monitoring requirements for each AI system prior to implementation. Evaluate as part of the continuous improvement cycle. 5.1.5. Assign responsibility for developing, acquiring, deploying, operating, managing and maintaining each AI system to the teams and people best suited to supporting its safe and responsible use across the lifecycle.66 5.1.6. Assign accountability for oversight of third-party development and use of AI systems and components to appropriately skilled and empowered people in the organisation.67 5.1.7. Evaluate the training needs for end users for each AI system you deploy. Provide the required training to address any identified needs. 68 5.1.8. Evaluate the training needs for those responsible for the ongoing operation and monitoring for each AI system you deploy. Provide the required training to address any identified needs. Procurement guidance for guardrail 5: Develop a plan with your supplier for governance and oversight over the AI system or component, with clear responsibilities between parties. Reflect this in your contracts.69 Voluntary AI Safety Standard | industry.gov.au/NAIC 30 --- Page 38 --- 6. Guardrail 6: Inform end-users regarding AI-enabled decisions, interactions with AI and AI-generated content. Create trust with users. Provide people, society and other organisations with confidence that you are using AI safely and responsibly. Disclose when you use AI, its role and when content is AI-generated. Disclosure can occur in many ways. It is up to the organisation to identify the most appropriate mechanism based on the use case, stakeholders and technology used. Voluntary AI Safety Standard | industry.gov.au/NAIC 31 --- Page 39 --- 6.1. Transparency and contestability Commit to creating processes for stakeholders impacted by the decisions or behaviours of AI systems, so they understand when AI systems that could affect them are in use. Give stakeholders the opportunity to contest the decisions and outputs of those systems. Key concept: Technologies such as watermarking and labelling can help create transparency for stakeholders by making AI-generated content clearly identifiable to end users. For relevant AI systems, consider implementing or obtaining systems that comply with the Coalition for Content Provenance and Authority (C2PA) Technical Specification. 6.1.1. Create and communicate an organisational process through which people can understand the use of AI systems. This process should include when and how frequently to communicate, the level of detail to provide, and the level of AI knowledge of stakeholders. Evaluate communication obligations for both internal and external stakeholders and interested parties, including accessibility needs.70 6.1.2. Create and communicate an organisational requirement to disclose the use of AI to impacted parties in a direct interaction or in a decision-making process. 6.1.3. Create and document the level of transparency and evidence required for you to conduct an audit over the AI system lifecycle.71 6.1.4. Create and document a process to apply the organisation\u2019s responsibilities under this Standard to AI systems developed or provided by third parties. This should include appropriate transparency and detail of information for the organisation to make a sufficiently informed evaluation.72 6.1.5. Create and document a process to evaluate any specific reporting and disclosure obligations under the Online Safety Act relevant to AI systems usage. 6.2. Transparency for AI systems Commit to communicating with sufficient transparency to demonstrate safe and responsible use of AI systems. Key concept: Certain internal and external stakeholders may require different levels of transparency given existing social inequalities. For example, you may need to make extra considerations when using data owned by or about Aboriginal and Torres Strait Islander people and organisations to mitigate the perpetuation of existing social inequalities. 6.2.1. Evaluate the level of transparency that each AI system needs \u2013 including third-party-provided systems \u2013 dependent on the use case and external stakeholder expectations.73 Consider potential conflicts, such as privacy, intellectual property, AI systems presenting as a person, hallucinations or potential for misinformation. 6.2.2. Where applicable, document how the AI system indicates to impacted users that an AI system is being used in an interaction or in a decision-making process. Voluntary AI Safety Standard | industry.gov.au/NAIC 32 --- Page 40 --- 6.2.3. Evaluate and document how the required level of transparency with the key stakeholders varies by stakeholder group. When possible, choose more interpretable and explainable AI systems to ensure understandable transparency. 6.2.4. Implement the agreed transparency measures for each AI system.74 6.2.5. Where expected by stakeholders, implement approaches to communicate relevant information about AI-generated content to end users. Require associated third-party developers to do the same, with options such as labelling and watermarking. Evolve these approaches as new solutions become available. 6.2.6. Where required under the Online Safety Act, report on measures you have taken to ensure safety, such as notices or mandatory reporting. 6.2.7. Determine and document the expected level of technical detail required by different stakeholder groups to effectively explain the use of AI to the intended audience.75 Procurement guidance for guardrail 6: Agree with your supplier the transparency mechanisms required for the AI system or component.76 Reflect this in contracts and project documentation. Voluntary AI Safety Standard | industry.gov.au/NAIC 33 --- Page 41 --- 7. Guardrail 7: Establish processes for people impacted by AI systems to challenge use or outcomes . Organisations must provide processes for users, organisations, people and society impacted by AI systems to challenge how AI is used, contest decisions, outcomes or interactions that involve AI. Voluntary AI Safety Standard | industry.gov.au/NAIC 34 --- Page 42 --- 7.1. Contestability and related risk controls Commit to creating processes for stakeholders of AI systems to understand and challenge the use of those systems. 7.1.1. Create and communicate the process for potentially impacted stakeholders to understand how and for what purpose you are using AI, as well as raise concerns, challenges or requests for remediation.77 7.1.2. Embed stakeholder contestability of AI system use with the risk and control process of the organisation. 7.1.3. Create and communicate an organisational process through which people can raise concerns, challenges or requests for remediation and receive responses (for example, a human rights grievance and remediation mechanism). This process should include when and how frequently to communicate, the level of detail you need to provide, and the level of AI knowledge of stakeholders. Evaluate contestability requirements for both internal and external stakeholders and interested parties, including accessibility needs.78 7.1.4. Assign an accountable person to oversee concerns, challenges and requests for remediation. 7.1.5. Create and document a review process to evaluate stakeholder contests of AI system use across the organisation, including any concerns raised by stakeholder groups and requests for information. Procurement guidance for guardrail 7: Agree with your supplier a process to raise issues and contested outcomes. Reflect this in contracts and project documentation. Voluntary AI Safety Standard | industry.gov.au/NAIC 35 --- Page 43 --- 8. Guardrail 8: Be transparent with other organisations across the AI supply chain about data, models and systems to help them effectively address risks. Organisations must provide information to organisations downstream in the AI supply chain for them to understand the components of the AI system, how it was built and to understand and manage the risk of the use of the AI system. Voluntary AI Safety Standard | industry.gov.au/NAIC 36 --- Page 44 --- 8.1. Transparency between developers and deployers Commit to sharing information and establishing processes to provide sufficient transparency between developers and deployers of AI systems. Key concept: When using open-source AI models, deployers need to consider which safe and responsible AI measures the developer has implemented and their effectiveness. Developers using open-source AI models should be transparent about the safe and responsible AI practices they have implemented and what further practices they recommend for deployers of their AI system, AI model or component. 8.1.1. Organisations developing AI systems, AI models or components (systems) should supply deployers of their systems with as much of the following information as possible while protecting commercially sensitive information: \u2022 capabilities and limitations of the system \u2022 technical details of the system including architecture, description of components and characteristics \u2022 test use cases and results of the system relevant to the deployers use of the system \u2022 known risks and mitigations put in place related to the deployers use of the system \u2022 data management processes for training and testing data including data quality, known bias and provenance \u2022 privacy, security and cybersecurity practices including compliance to standards and best practice relevant to the deployers use of the system \u2022 transparency mechanisms implemented for AI generated content, interactions and decisions \u2022 any known potential bias, actions taken to minimise negative effects of unwanted bias and ethical prejudices from the AI solution or component. 8.1.2. Organisations deploying AI systems or components are required to share with their suppliers of AI models, systems or components the following information: \u2022 expected use of the AI system, component or model \u2022 any unexpected and unwanted bias resulting from use of the system. Where data privacy is a consideration, deployers should share as much as possible to highlight the issue and replicate the outcome without compromising data privacy or security such as data profiles or sample synthetic data. \u2022 issues, faults and incidents that occur with the system. 8.1.3. Agree with your suppliers of systems: \u2022 responsibility and accountability for monitoring and evaluation of system performance \u2022 responsibility and accountability for issue identification, resolution and system updates \u2022 responsibility and accountability for human oversight and intervention and when to take action \u2022 process for raising issues, faults and incidents including contested outcomes. Ensure your process protects user and stakeholder privacy. 8.1.4. Ensure you\u2019ve included the required information in contracts with suppliers of systems including when to update information. Voluntary AI Safety Standard | industry.gov.au/NAIC 37 --- Page 45 --- 8.1.5. Schedule regular reviews throughout the lifecycle of the system based on timed intervals and as a result of milestones or events. Procurement guidance for guardrail 8: Agree with your supplier roles, responsibilities and information flows across the lifecycle of the AI system from initial implementation through to end of life.Reflect in contracts and project documentation. Voluntary AI Safety Standard | industry.gov.au/NAIC 38 --- Page 46 --- 9. Guardrail 9: Keep and maintain records to allow third parties to assess compliance with guardrails. Organisations must maintain records to demonstrate that they have implemented and are complying with the guardrails, this includes maintaining an AI inventory and consistent AI system documentation. These records may be required to input into any future conformity assessments mandated for use in high-risk settings. Voluntary AI Safety Standard | industry.gov.au/NAIC 39 --- Page 47 --- 9.1. AI inventory and consistent documentation Commit to adopting an inventory of the AI systems you use and deploy. Define and apply documentation standards for these systems. 9.1.1. Create and maintain an up-to-date, organisation-wide inventory of each AI system, which includes79: \u2022 people accountable \u2022 purpose and business goals \u2022 capabilities and limitations of the AI system \u2022 technical requirements and components \u2022 datasets and their providence used for training and testing \u2022 technical specifications \u2022 acceptance criteria and test results \u2022 identified risks, potential impacts and relevant controls \u2022 any impact assessments and outcomes \u2022 any system audit requirements and outcomes \u2022 dates of review. 9.2. Critical system documentation Commit to understanding and documenting critical information about each AI system you deploy and use. Include the purpose, context, expected benefit and sufficient technical detail for the system to be understood. Be aware that the documentation you record will be the foundation to demonstrate compliance with future regulation in the form of conformity assessments. 9.2.1. Create and document the business goals, desired outcomes and obligations for each AI system the organisation deploys and uses. Periodically review this with reference to the organisation\u2019s strategy, values and risk tolerance.80 9.2.2. Document the scope for each AI system, including intended use cases, capabilities, limitations, expected contexts, and what responsible use looks like for an end user or affected stakeholder.81 Note that the unique characteristics of AI systems have the potential to go beyond intended use and context without explicit changes to the system or notice. 9.2.3. Document the risk management process including identified risks and mitigation implemented for the AI system or AI model. 9.2.4. Document or request from your system provider the relevant technical details of the system or model that you may need for others to understand the system. For example, expected use, overview of system architecture and design, information about the model and training data, overview of data flows, and reliance on or links to other digital systems.82 Voluntary AI Safety Standard | industry.gov.au/NAIC 40 --- Page 48 --- 9.2.5. Document the testing methodology applied and results of testing for the AI system or AI model. Request from your supplier the testing methodology and results during the development of the AI system and model. 9.2.6. Document the accountable people and the mechanisms for human control and oversight for the deployed AI systems. 9.2.7. Ensure documentation related to each AI system is recorded in the inventory at a sufficient and consistent level of detail to inform the accountable and responsible parties and any third-party stakeholders.83 This will enable completion of future conformity assessments to demonstrate compliance with mandated guardrails. Procurement guidance for guardrail 9: Work with your supplier to understand and document the expected use, capabilities and limitations of the AI system or component84. This should include technical details of the system and the data used in relation to the AI system (including the use of third-party data). Integrate expectations into contract, including ongoing scheduled reviews. Voluntary AI Safety Standard | industry.gov.au/NAIC 41 --- Page 49 --- 10. Guardrail 10: Engage your stakeholders and evaluate their needs and circumstances, with a focus on safety, diversity, inclusion and fairness. It is critical for AI deployers to identify and engage with stakeholders for the life the AI system. It helps in identifying potential harms and understanding if there are any potential or real unintended consequences from the use of AI. Deployers must identify potential bias, minimise negative effects of unwanted bias, ensure accessibility and remove ethical prejudices from the AI solution or component. Voluntary AI Safety Standard | industry.gov.au/NAIC 42 --- Page 50 --- 10.1. Organisational-level stakeholder engagement Commit to engaging with stakeholders \u2013 people and groups \u2013 potentially impacted by AI systems. 10.1.1. Identify and document which key stakeholder groups may be impacted by the organisation\u2019s use of AI in line with your AI strategy.85 10.1.2. Identify and document the needs of these key stakeholder groups in relation to your AI strategy.86 10.1.3. Identify and document which of the stakeholder needs your organisational-level AI policies and procedures will address.87 10.1.4. Create processes to support ongoing engagement with stakeholders about their experience of AI systems. Make sure you identify any marginalised groups and support them appropriately. Equip stakeholders with the skills and tools necessary to give meaningful feedback. 10.2. Organisational-level diversity, inclusion and fairness Commit to creating and documenting a process so any use of AI contributes to safe, fair and sustainable outcomes. 10.2.1. Define and document the organisation\u2019s responsibility to ensuring that AI systems do not undermine diversity, inclusion and fairness. 10.2.2. Define and document organisational-level goals relating to diversity, inclusion and fairness in the deployment and use of AI systems. 10.2.3. Evaluate whether and how the current or planned use of AI may impact the organisation\u2019s pre-existing responsibilities and programs related to creating a positive impact. For example, human rights, diversity and inclusion, accessibility and environmental responsibilities. 10.2.4. Document and operationalise a responsibility to prevent unwanted bias, discrimination and other risk factors that could impact diversity, inclusion and fairness in leadership responsibilities and the organisation\u2019s AI strategy. 10.3. System-level stakeholders, points of human interaction and impact of potential- harm Commit to system-level stakeholder engagement and evaluation of potential harm. Key concept: Stakeholder engagement is effective in responsible AI system deployment, particularly when carried out at the earliest possible stages in the AI lifecycle and embedded throughout the end-to-end lifecycle. 10.3.1. Identify and document where expected users interact with each AI system, including: \u2022 user interactions with the system or AI system-generated content Voluntary AI Safety Standard | industry.gov.au/NAIC 43 --- Page 51 --- \u2022 when the system processes an individual\u2019s personal data \u2022 when the system makes or influences a decision about a person or group of people. 10.3.2. Identify and document the stakeholder groups for each system.88 10.3.3. For each identified interaction with a human, evaluate and document if the interaction has the potential to cause harm to an individual, group or society at large.89 10.3.4. When this evaluation indicates that an AI system could harm people or groups, or pose a material risk to the organisation, perform and document an appropriate impact assessment.90 10.4. System-level diversity, inclusion and fairness Commit to relevant processes with fair and sustainable outcomes for AI systems and uses. Key concept: Organisations need to evaluate the potential impact of unwanted bias on the AI systems they deploy and use, including developing strategies to identify potential biases. Existing standards, guidance and technical reports, such as ISO Information technology \u2013 Artificial Intelligence (AI) \u2013 Bias in AI systems and AI aided decision making, ISO/IEC TR 24027:2021 may help. As understanding and expectations evolve, stay informed of new developments in this area, where relevant. 10.4.1. Evaluate and document the potential impact of each AI system in relation to diversity, inclusion and fairness. Identify and mitigate risks of unwanted bias or discriminatory outputs, including for marginalised groups. 10.4.2. Evaluate how each AI system may support or undermine any existing legal obligation or program with a positive, social impact. The include human rights, diversity and inclusion, accessibility and environmental responsibilities. 10.4.3. Define and document how you have embedded accessibility obligations (such as inclusive design) in the deployment and use of each AI system. 10.4.4. For each AI system, define and document the stages in the AI lifecycle where you will need meaningful human oversight to meet organisational, legal and ethical goals. Procurement guidance for guardrail 10: Work with your supplier to undertake AI impact assessments and understand the needs of system stakeholders.91 Know suppliers\u2019 actions to understand potential bias, minimise negative effects of unwanted bias, implement accessibility and remove ethical prejudices from the AI solution or component.92 Ensure you haven\u2019t reintroduced any unwanted bias during deployment. Voluntary AI Safety Standard | industry.gov.au/NAIC 44 --- Page 52 --- Part 4: Applying and adopting the standard through examples. The range of applications of AI is effectively infinite. While we can\u2019t give guidance on how the standard might apply to every use case, we can use examples to illustrate how you can use the guardrails to manage the risks and benefits of a specific AI system. We\u2019ve chosen 4 examples to show how individual guardrails might be apply in different use cases. The examples explore how organisations may use particular guardrails as part of their overall approach to deploying AI systems. The examples show that the guardrails can be applied in different situational contexts, for different technologies. These examples are not intended to represent a comprehensive application of all relevant guardrails, responsibilities or other legal obligations that may be relevant for the specified use cases. They are to provide examples of how the guardrails can be applied in a selection of fictional examples. Example 1: General-purpose AI chatbot A detailed example representing a common use case for organisations of all sizes, across all sectors. Due to the growing ubiquity of this technology, we\u2019ve provided extra detail on how an organisation could adopt a range of guardrails. As a point of contrast, this example includes potential outcomes where safe and responsible AI methodologies are not followed. Example 2: Facial recognition technology A simplified example on the use of facial recognition technology. It illustrates the use of the guardrails to decide that non-AI-based solutions will better achieve strategic and operational goals. Example 3: Recommender engine A simplified example of a common use case in which a recommender engine is used to improve customer experience and meet organisational goals. It includes reference to a court case in which a business using this kind of technology was ordered to pay a substantial financial penalty for not meeting legal obligations. Example 4: Warehouse accident detection A detailed example to outline obligations for testing of AI systems. In this example, we offer guidance on linking areas of concern with acceptance criteria. It covers testing at different stages during the AI system and governance lifecycle, due to the specific and technical nature of meeting relevant guardrails. Example 1: General-purpose AI Chatbot NewCo background NewCo is a fast-growing B2C company with 50 employees, selling a range of products in a niche market. It has an annual turnover of $3.5 million. The company is approaching a major product launch that they expect will create a significant increase in demand. NewCo\u2019s head of sales proposes to use the latest advances in AI and procure a new chatbot for their website. The chatbot would engage with customers to answer the most commonly asked questions. The company expects the new product to sell over 10,000 units in the first month because of an aggressive social media strategy featuring early-bird discounts. Voluntary AI Safety Standard | industry.gov.au/NAIC 45 --- Page 53 --- The new chatbot is meant to reduce the amount of time customers wait for a phone operator by shifting those with routine queries to the online chatbot. This should reduce the need to expand phone support and allow employees to spend more time on complex tasks. The most common customer queries include delivery times, returns and the application of time-limited discount codes. The head of sales suggests that a chatbot based on general-purpose AI would help the company respond to and resolve customer queries faster, leading to improved customer satisfaction scores (CSAT). CSAT scores are considered lead indicators for revenue growth goals, so NewCo hopes that a suitable customer query chatbot would also support growth in sales. Case study: Moffatt v Air Canada 2024 BCCRT 149 Air Canada deployed a chatbot on its website which made statements to a customer about the airline\u2019s bereavement fares. These statements were inconsistent with Air Canada\u2019s policy, to which the chatbot had provided a link. The customer sought a refund through legal proceedings. Air Canada claimed that the chatbot was a \u2018separate legal entity that is responsible for its own actions\u2019 and the customer was not entitled to a refund according to its bereavement policy. The tribunal rejected these arguments and found Air Canada responsible for all information provided on its website, whether from a static page or chatbot. Air Canada was found to have had a duty of care to take reasonable steps to ensure that information was accurate. There are similar protections in Australia for interactions with chatbots as part of an organisation\u2019s customer service offering. 93 Voluntary AI Safety Standard | industry.gov.au/NAIC 46 --- Page 54 --- NewCo\u2019s use of the standard: a comparison NewCo wants to procure a generative AI chatbot with the promise of: \u2022 reduced customer wait time \u2022 reduced customer service phone support time for staff. The table below compares what happens when NewCo follows the Voluntary AI Safety Standard, and what happens if it chooses not to follow the standard. Actions and outcomes Does not follow the standard Does follow the standard Organisational-level Head of sales (HOS) conducts Standard identified as basis for actions online research into potential effective governance of the new developers \u2013 decides an off-the- chatbot. shelf solution will allow NewCo NewCo commits to organisational-level to quickly launch and use the AI safe and responsible AI use that: system. \u2022 is aligned to business goals Developer selected and (guardrail 1) \u2018NewChat\u2019 launched within a week in parallel with the new \u2022 is safe, fair and sustainable product launch. (guardrail 10) \u2022 is supported by strategic AI training (guardrail 1) \u2022 is supported by risk and impact assessments (guardrail 2) \u2022 is supported by data and security governance (guardrail 3) \u2022 involves testing, evaluation monitoring and reporting (guardrail 4). Voluntary AI Safety Standard | industry.gov.au/NAIC 47 --- Page 55 --- Actions and outcomes Does not follow the standard Does follow the standard System-level actions None HOS takes overall responsibility for developer selection, contract negotiation, implementation and monitoring. She has recently undertaken training on deploying responsible and safe AI systems (guardrail 1). HOS engages with internal and external stakeholders to understand potential impacts and harms (guardrail 10). HOS tests the system with a planned promotional discount. The test detects unwanted bias in the outputs and the agreed fairness metric in the testing criteria is not met (guardrail 4). HOS conducts a risk assessment. Some risks and mitigating actions are identified (including NewCo modifying the system to minimise bias). Based on the risks HOS decides that only internal use of AI system as appropriate at this stage (guardrail 2). Voluntary AI Safety Standard | industry.gov.au/NAIC 48 --- Page 56 --- Actions and outcomes Does not follow the standard Does follow the standard Outcomes System behaviour and impacts Successful product launch NewChat holds convincing Customer Service teams use general- conversations with users and purpose AI as an internal resource to asks them for personal find relevant company documentation information, including gender. to answer customer queries more quickly. To maximise sales, NewChat offers customers discounts Customer satisfaction scores increase. above agreed promotional rates. Employee productivity increases. Customer Service team is unaware that NewChat is offering customers discounts and refuses to apply them to purchases at checkout. NewChat is only offering these discounts to people who report their gender as \u2018male\u2019. It does not otherwise offer any discounts. Because of a viral Reddit thread, thousands of customer complaints accuse NewCo of discrimination. They demand NewCo extend the chatbot- generated rate to all purchasers. Customer Service team overwhelmed with level of complaints from people whose discounts have been refused as well as those claiming they have been discriminated against. Harm to people and organisation Personal information is collected without being reasonably necessary for its functions. People who don\u2019t report their gender as \u2018male\u2019 miss out on the discount. Voluntary AI Safety Standard | industry.gov.au/NAIC 49 --- Page 57 --- Actions and outcomes Does not follow the standard Does follow the standard Financial, legal and reputational risks Customer satisfaction score drops significantly. Negative global media news coverage of incident. Potential breach of consumer laws for misleading or deceptive conduct in not honouring the offered discount. Potential breach of privacy laws for the collection of personal information that was not necessary for its functions. Potential complaints made to relevant regulatory bodies for unlawful discrimination based on a protected attribute (gender). Example 2: Facial recognition technology EcoRetail background EcoRetail has 20 permanent employees and over 100 casual workers across its nationwide chain of 15 stores. Its brand is heavily tied to advancing social good, including diversity and inclusion. Their customer base includes people from many different demographic groups. EcoRetail\u2019s AI system vendor, FRTCo Ltd, suggests installing facial recognition technology, which it states can: \u2022 identify known shoplifters and limit losses from shoplifting \u2022 identify other criminal activities (such as physical violence) to support staff safety. Facial recognition technology (FRT) is a type of AI that remotely captures sensitive biometric data to verify, identify or analyse people. This functionality poses heightened privacy and discrimination risks to human rights. While there is currently no specific Australian law governing the use of this technology, the Australian Government is considering the need for new guardrails for FRT as part of its broader Privacy Act reform process. Voluntary AI Safety Standard | industry.gov.au/NAIC 50 --- Page 58 --- How EcoRetail uses the standard EcoRetail wants to procure FRT to: \u2022 accurately identify and deter shoplifters \u2022 prevent violence, protecting customers, staff and assets. They use the guardrails to inform their actions. Guardrails Actions Guardrail 1: EcoRetail holds discussions with FRTCo Ltd (AI system vendor) to ensure that FRT Establish, aligns with business objectives (minimising loss from shoplifting) and strategic goals implement and (act in accordance with Australia\u2019s AI Ethics Principles and Australian legislation). publish an To understand how the use of FRT aligns with EcoRetail\u2019s organisational strategy and accountability risk appetite, EcoRetail evaluates the following characteristics of the technology and process how it will be deployed: including governance, \u2022 Spatial context of deployment: commercial, publicly accessible space. internal \u2022 Functionality of the FRT: facial identification \u2013 comparing a single face in the capability and a store to a large database of many faces to find a match. FRTCo Ltd is unable to strategy for provide detail as to where they have obtained the dataset, how representative regulatory it is or whether they followed privacy guardrails. compliance. \u2022 Performance: 99% performance accuracy applied to the estimated 300 people per day (foot traffic across all EcoRetail stores) equates to the potential for 3 people per day to be incorrectly identified. \u2022 Outcomes: the FRT would impact people\u2019s rights (including privacy of sensitive information and the potential for arbitrary detainment) and people\u2019s ability to access goods and services. \u2022 Free and informed consent: signs posted at store entry may not be sufficient for express and sufficiently informed consent. Guardrail 10: Senior leaders at EcoRetail held consultations with permanent and casual staff to Engage your understand how the use of FRTCo Ltd\u2019s FRT system might impact them and their stakeholders customers. and evaluate During the consultation, staff received FRTCo Ltd\u2019s reports on the accuracy of its their needs and product. circumstances, with a focus on The staff asked if the accuracy rate applied equally across different demographic safety, groups and discovered that the accuracy rate reduces to 95% for particular racial diversity, groups. FRTCo Ltd was unable to give any detail of methodologies used to reduce inclusion and outcomes based on unwanted bias or show the representation of its dataset. fairness. Although the staff indicated that they were sometimes concerned for their safety, they did not feel that the potential benefit from the AI system outweighed the level of surveillance. Outcomes EcoRetail decided that using FRT would not align with its strategic goals, risk appetite and legal obligations. Collecting sensitive biometric information posed too great a risk to the organisation from a legal perspective. EcoRetail also recognised that the scale and impact of potential harm to customers, particularly to those incorrectly identified as shoplifters, was too great. Voluntary AI Safety Standard | industry.gov.au/NAIC 51 --- Page 59 --- The possibility of reputational damage, exacerbated by potential regulatory activity for discrimination, was likely to have negative commercial outcomes. Example 3: Recommender engine TravelCo.com background TravelCo.com is a global hotel booking app that is paid by commission. Hotels will pay TravelCo.com a fee every time a user clicks on the offer for their hotel. Hotels are also able to pay a fee so their hotel appears higher up in search results. To meet shareholder expectations, TravelCo.com wants to increase market share by telling customers that they can get the cheapest possible price for the same hotel using the TravelCo.com app. Search results rely on recommender engines as an underlying technology. These use AI to analyse an individual\u2019s web browsing activities to give content suggestions based on inferences made about their demographic characteristics, behaviours and interests. TravelCo.com has engaged a company called XYZ to supply their recommender engine. Case study: Australian Competition and Consumer Commission v Trivago N.V. (No 2) [2022] FCA 417 Trivago stated it could help consumers find the \u2018best deal\u2019 or cheapest price by comparing hotel rates on different websites. The algorithm driving Trivago\u2019s recommender engine did not use the price of the room as the sole factor in ranking search results. Consumers were not aware that another significant factor was the value of the fee paid by the third-party booking site to have its search result ranking improved. Consumers were frequently not shown the cheapest price for a hotel in their top search result. In some cases, they were overpaying for the hotel listed as compared to other booking sites. Trivago was ordered to pay $44.7 million in penalties because of the Federal Court finding it had misled consumers.94 Voluntary AI Safety Standard | industry.gov.au/NAIC 52 --- Page 60 --- How TravelCo.com uses the standard TravelCo.com wants to procure a recommender engine to: \u2022 meet shareholder expectations of increasing market share \u2022 improve capabilities with AI and data analytics. They use the guardrails to inform their actions. Guardrails Actions Guardrail 2: XYZ notifies TravelCo.com of the challenge in providing a real-time \u2018cheapest price\u2019 Establish and because of the large and dynamic dataset of hotel pricing. implement a It would take at least 10 seconds to return a search result, which is not in line with risk customer expectations for instant information. management process to To minimise lag time for the customer, XYZ suggests updating a static version of the identify and data every 3 hours. mitigate risks. As a B2C organisation, TravelCo.com identifies the regulatory risk related to consumer law \u2013 that advertising cannot be misleading or deceptive. The pricing at the time the customer searches may no longer be the cheapest option, because of changes since the last update. Guardrail 6: The recommender engine uses several factors to create rankings of search results, Inform end- including alignment to TravelCo.com\u2019s business model. users regarding Another risk identified during the assessment is that the website does not clearly AI-enabled state that ranking of results is influenced by the commercial arrangements decisions, TravelCo.com has with the hotels. interactions with AI and AI- Customers could assume that the highest ranked result is the cheapest and generated therefore overpay. content. Outcome TravelCo.com decided to change its advertising materials from \u2018cheapest\u2019 or \u2018best\u2019 price to stating that it provides comparisons only. TravelCo.com also decided to include a clear and prominent notice with every search that reflects its commercial arrangements with hotels. Voluntary AI Safety Standard | industry.gov.au/NAIC 53 --- Page 61 --- Example 4: Warehouse accident detection ManufaxCo background ManufaxCo is a manufacturing company that has built an AI system in house called Safe Zone. SafeZone monitors high-risk factory environments for potential safety hazards and alerts staff to hazards in real- time to prevent accidents and keep workers and assets safe. SafeZone combines computer vision and Natural Language Processing (NLP) technologies. Cameras installed throughout the factory capture real-time video feeds, which AI analyses to detect safety hazards like spills, obstructions, or people entering unsafe zones. The NLP component allows the system to understand and process verbal commands or alerts from workers, creating a more interactive and complete safety monitoring approach. How ManufaxCo uses the standard Guardrails Actions Guardrail 2: ManufaxCo has carried out a risk assessment and found a set of concerns. The Establish and concerns (effectiveness and reliability, fairness, and privacy) are not an exhaustive list implement a for this AI system. For example, they do not cover concerns about accountability or risk potential misuse. management process to identify and mitigate risks. Voluntary AI Safety Standard | industry.gov.au/NAIC 54 --- Page 62 --- Guardrails Actions Guardrail 4.2: For each concern, the accountable owner in ManufaxCo sets acceptance criteria to Commit to control for the anticipated harms. specifying, 1. Effectiveness and reliability: system errors are highly impactful \u2013 both false justifying and positives (which stop work) and false negatives (where an accident may occur). documenting acceptance Set appropriate thresholds such as: criteria - fraction of hazards detected (recall) must be greater than 0.9 needed for the potential - frequency of unnecessary stop-works (false discovery rate) must be less harms to be than 0.3 adequately - raise an alarm if a camera view is significantly obstructed for more than controlled. 20 seconds. The system must fully integrate with existing safety guardrails and communication systems, with no reported compatibility issues during a 2-week trial period. The system must have an uptime of at least 99.5%, as measured over a 3-month period. At least 80% of staff must rate the system's user interface as \u2018easy to use\u2019 by in a user satisfaction survey. 2. Fairness: concerns the safety benefits offered by the system may not apply equally to all workers in the environment. For example, if there is a representation problem in the data. The NLP component must correctly understand and process commands or alerts from workers with at least a 90% accuracy rate across different accents and dialects. 3. Privacy: worker stakeholders raise concerns about their privacy at work. The system must pass a privacy compliance audit, ensuring adherence to relevant privacy regulations for handling video feeds and worker data. The system is designed and built to meet these criteria. A third-party vendor supplies voice recognition models for controlling the system. The hazard detection model is trained on historical data. Under normal operating conditions, occurrences of hazards may be rare, so controlled simulations of hazards augment the data. Voluntary AI Safety Standard | industry.gov.au/NAIC 55 --- Page 63 --- Guardrails Actions Guardrail 4.3: ManufaxCo develops a test plan to evaluate the system. Testing of AI They acquire the testing data to evaluate against the acceptance criteria under systems or controlled conditions. This includes evaluations specifically for the acceptance models to criteria: determine performance \u2022 hazard detection rates \u2013 tested using performed simulations for different types of and mitigate hazard any risks \u2022 false positive count \u2013 tested on operational data collected during a small pilot under full human oversight \u2022 functionality of failure alert system \u2013 inducing camera failures or placing obstructions. They design tests to identify implementation errors and system problems: \u2022 a team is assigned to design edge cases such as placing equipment to obscure potential hazards \u2022 tests are performed to ensure voice control is performing well enough in various working conditions of machinery \u2022 interactions with employees are observed to find out whether they are interacting correctly with the system and as it was intended in the initial design and tests. The tests find the system is functioning as intended, with the exception that initial testing reveals a problem with the false positive rate. The system has many false alarms during normal safe operation. The findings are reported, summarising the objectives, methods and metrics used. The accountable owners assign the development team to investigate, and they determine that the problem is because of differences in the environment between the training data and the pilot plant (such as lighting, camera angles, wall colours, specific equipment models). They acquire an updated dataset and re-test the system. Over this period, workers using the system report feedback about voice recognition issues, particularly for workers from multicultural backgrounds. The owners address this by acquiring and swapping in a voice recognition model from a different vendor with models that perform well across a more diverse set of accents. The accountable owners review the reporting to confirm the mitigations have been effective, and they approve the system for deployment. Guardrail 4.4: A month into deployment, ManufaxCo\u2019s monitoring indicates a reliability problem with Commit to the system. Timely investigation reveals a camera calibration issue that hardware implementing configuration and updating the computer vision pipeline\u2019s preprocessing stage fixes. robust AI ManufaxCo then rolls the system out across multiple warehouses. Initially, the system system proves effective in identifying common safety hazards, leading to a noticeable performance reduction in accidents and meeting all its acceptance criteria. monitoring and However, as the warehouse operations expand to include new types of machinery and evaluation, materials, the system experiences a dataset shift. It fails to recognise new hazards that and to were not present in its training data, resulting in several near-miss incidents that are ensuring reported through the feedback channels. each system The accountable owners examining the monitoring recognise this problem, and they remains fit assign the development team to address. The development team updates the training for purpose. dataset again to include the new hazards. The model is updated and re-tested. Voluntary AI Safety Standard | industry.gov.au/NAIC 56 --- Page 64 --- Guardrails Actions Guardrail 4.5: Considering the serious safety impacts of the systems, the accountable owner Commit to requests another independent internal technical team do an assessment before the regular final roll out across all warehouses. system During the assessment of the design documentation and pilot monitoring logs, the audits for independent assessors identify and recommend better camera placement to minimise ongoing chances of blind spots caused by machines and their operators. ManufaxCo applies compliance this recommendation as an update to the existing installed systems and records it as a with the consideration for any future deployment in other warehouses. acceptance criteria (or justify why audits aren\u2019t needed). Guardrail 4.1: Given this is a complex new system that could have significant safety impacts, Commit to a accountable owners decide to audit the system and its governance in 6 months. At this robust stage there will be an existing operational track record. process for timely and regular monitoring, evaluation and reporting of AI system performance. Voluntary AI Safety Standard | industry.gov.au/NAIC 57 --- Page 65 --- Acknowledgements The National AI Centre (NAIC) would like to thank: NAIC Responsible AI Network (RAIN) Partners for their guidance and input into the consultation process for the Voluntary AI Safety Standard: \u2022 Australian Industry (AI) Group \u2022 Australian Information Industry Association (AIIA) \u2022 Australian Institute of Company Directors (AICD) \u2022 Choice \u2022 Committee for Economic Development of Australia (CEDA) \u2022 Governance Institute of Australia \u2022 NAIC Responsible AI at Scale Think Tank \u2022 Tech Council of Australia \u2022 The Ethics Centre \u2022 Thinkplace. The organisations that contributed time and expertise to review the draft document: \u2022 ACCC \u2022 The Centre for Inclusive Design \u2022 Office for Women \u2022 Digital Transformation Agency \u2022 Standards Australia \u2022 eSafety Commissioner \u2022 Human Rights Commissioner \u2022 Diversity Council of Australia \u2022 National Indigenous Australians Agency \u2022 Kendra Vant \u2022 Creative Technologies Research Lab, UNSW NAIC partners that contributed extensive knowledge, experience and expertise into the creation and development of the standard: \u2022 CSIRO\u2019s Data61 \u2022 Human Technology Institute \u2022 Gradient Institute. Without all the expert input, the development of the standard would not have been possible. Voluntary AI Safety Standard | industry.gov.au/NAIC 58 --- Page 66 --- References 1 C Taylor, J Carrigan, H Noura, S Ungur, J van Halder and G Singh Dandona, Australia\u2019s automation opportunity: Reigniting productivity and inclusive income growth, McKinsey & Company, 3 March 2019, accessed 12 December 2023. 2 Department of Industry, Science and Resources, Australia\u2019s AI Ethics Principles, Australian Government, n.d. 3 UK Government, Policy paper: The Bletchley Declaration by Countries Attending the AI Safety Summit, 1\u2013 2 November 2023, UK Government, 1 November 2023. 4 Attorney-General\u2019s Department, Human rights protections, Australian Government, n.d. 5 S Gasson, \u2018Human-Centered Vs. User-Centered Approaches to Information System Design\u2019, The Journal of Information Technology Theory and Application (JITTA), 2023, 5(2):29\u201346. 6 D Cirillo, S Catuara-Solarz, C Morey, E Guney, L Subirats, S Mellino, A Gigante, A Valencia, MJ Rementeria, AS Chadha and N Mavridis, \u2018Sex and gender differences and biases in artificial intelligence for biomedicine and healthcare\u2019, npj Digital Medicine, 2020, 3, doi:10.1038/s41746-020-0288-5. 7 Attorney-General\u2019s Department, Australia\u2019s anti-discrimination law [web page], Australian Government, n.d. 8 ISO (International Organization for Standardization), AS ISO/IEC 42001:2023, Information technology \u2013 Artificial intelligence \u2013 Management system, ISO 2023 and NIST (National Institute of Science and Technology), AI Risk Management Framework 1.0, U.S. Department of Commerce, 2023. 9 See, for example, the determination by Australian Information Commissioner and Privacy Commissioner Angelene Falk, who found that Clearview AI, Inc. breached Australians\u2019 privacy by scraping their biometric information from the web and disclosing it through a facial recognition tool: https://www.oaic.gov.au/newsroom/clearview-ai-breached-australians- privacy#:~:text=Australian%20Information%20Commissioner%20and%20Privacy,through%20a%20facial %20recognition%20tool. 10 ISO, AS ISO/IEC 42001:2023, 4.1, 4.3, 5.1, A.3.2. 11 ISO, AS ISO/IEC 42001:2023, 4.1 note 3. 12 ISO, AS ISO/IEC 42001:2023, 5.1. 13 ISO, AS ISO/IEC 42001:2023, A.3.3. 14 ISO, AS ISO/IEC 42001:2023, A.3.2. 15 ISO, AS ISO/IEC 42001:2023, 5.1, 7.1. 16ISO, AS ISO/IEC 42001:2023, 5.2. 17 ISO, AS ISO/IEC 42001:2023, 5.2, A.2.4. 18 ISO, AS ISO/IEC 42001:2023, 5.2, A.2.4. 19 ISO, AS ISO/IEC 42001:2023, 6.3. 20 ISO, AS ISO/IEC 42001:2023, 10.2. 21 ISO, AS ISO/IEC 42001:2023, A.6.1.2. 22 ISO, AS ISO/IEC 42001:2023, 4.1. 23 ISO, AS ISO/IEC 42001:2023, A.6.1.3. 24 NIST AI Risk Management Framework (AI RMF 1.0), GOVERN 2.2. 25 ISO, AS ISO/IEC 42001:2023, 7.2. Voluntary AI Safety Standard | industry.gov.au/NAIC 59 --- Page 67 --- 26 ISO, AS ISO/IEC 42001:2023, 5.1, 7.2. 27 ISO, AS ISO/IEC 42001:2023, 7.3. 28 ISO, AS ISO/IEC 42001:2023, 7.2. 29 ISO, AS ISO/IEC 42001:2023, 6.1 and NIST, AI Risk Management Framework 1.0, 1.2.2, MAP 1.6. 30 ISO, AS ISO/IEC 42001:2023, 6.1.1. 31 ISO, AS ISO/IEC 42001:2023, 6.1.1, 6.1.2, 6.1.3, 6.1.4. 32 ISO, AS ISO/IEC 42001:2023, 6.1.1, 6.1.2, 6.1.3, 6.1.4. 33 ISO, AS ISO/IEC 42001:2023, 6.1.1. 34 ISO, AS ISO/IEC 42001:2023, 6.1.1, 6.1.2, 8.2 35 ISO, AS ISO/IEC 42001:2023, 6.1.1, 6.1.4, 8.4. 36 ISO, AS ISO/IEC 42001:2023, 6.1.1, 6.1.3, 8.3. 37 ISO, AS ISO/IEC 42001:2023, 8.2, 8.3. 38 ISO, AS ISO/IEC 42001:2023, 6.1.4. 39 World Economic Forum, Adopting AI Responsibly: Guidelines for Procurement of AI Solutions by the Private Sector, Insight Report June 2023, WEF, 2023. 40 ISO, AS ISO/IEC 38507:2022. 41 ISO, AS ISO/IEC 42001:2023, 4.1.1, B.2.3. 42 ISO, AS ISO/IEC 42001:2023, 4.1.1, B.2.3. 43 Office of the Australian Information Commissioner, Australian Privacy Principles, From Schedule 1 of the Privacy Amendment (Enhancing Privacy Protection) Act 2012, Australian Government, 2012. 44 Australian Signals Directorate, Essential Eight Maturity Model, Australian Government, 2017. 45 Australian Signals Directorate, Essential Eight Maturity Model. 46 ISO, AS ISO/IEC 42001:2023, A.7.4, A.7.5, A.7.6. 47 ISO, AS ISO/IEC 42001:2023, A.7.3. 48 ISO, AS ISO/IEC 42001:2023, A.8.5. 49 Office of the Australian Information Commissioner, Australian Privacy Principles, From Schedule 1 of the Privacy Amendment (Enhancing Privacy Protection) Act 2012. 50 Office of the Australian Information Commissioner, Notifiable data breaches [web page], Australian Government, n.d. 51 World Economic Forum, Adopting AI Responsibly: Guidelines for Procurement of AI Solutions by the Private Sector, Insight Report June 2023. 52 ISO, AS ISO/IEC 42001:2023, 9.1. 53 ISO, AS ISO/IEC 42001:2023, 7.5.1, 7.5.2, 7.5.3. 54ISO, AS ISO/IEC 42001:2023, 6.2, 8.1, A.6.2.3, A.6.2.4. 55 ISO, AS ISO/IEC 42001:2023, 6.2. 56 ISO, AS ISO/IEC 42001:2023, 6.2. 57 ISO, AS ISO/IEC 42001:2023, 8.1. 58 NIST, AI Risk Management Framework 1.0, MANAGE 1.1. Voluntary AI Safety Standard | industry.gov.au/NAIC 60 --- Page 68 --- 59 ISO, AS ISO/IEC 42001:2023, 9.1, A.6.2.6. 60 ISO, AS ISO/IEC 42001:2023, A.8.3 and NIST, AI Risk Management Framework 1.0, GOVERN 5.1, 5.2. 61 NIST, AI Risk Management Framework 1.0, MEASURE 3.3. 62 ISO, AS ISO/IEC 42001:2023, 9.2. 63 ISO, AS ISO/IEC 42001:2023, 9.2. 64 ISO, AS ISO/IEC 42001:2023, 7.1, 7.2. 65 ISO, AS ISO/IEC 42001:2023, 7.2. 66 ISO, AS ISO/IEC 42001:2023, 7.2, B.4.6. 67 ISO, AS ISO/IEC 42001:2023, 10.2. 68 NIST, AI Risk Management Framework (AI RMF 1.0), GOVERN 2.2. 69 UK Government, Guidelines for AI procurement, UK Government, 2020. 70 ISO, AS ISO/IEC 42001:2023, 7.4, A.8.3. 71 ISO, AS ISO/IEC 42001:2023, 9.2.1. 72 ISO, AS ISO/IEC 42001:2023, A.10.3. 73 ISO, AS ISO/IEC 42001:2023, A.8.5. 74 ISO, AS ISO/IEC 42001:2023, A.8.5. 75 ISO, AS ISO/IEC 42001:2023, A.6.2.7. 76 World Economic Forum, Adopting AI Responsibly: Guidelines for Procurement of AI Solutions by the Private Sector, Insight Report June 2023. 77 ISO, AS ISO/IEC 42001:2023, A.8.3. 78 ISO, AS ISO/IEC 42001:2023, 7.4, A.8.3. 79 ISO, AS ISO/IEC 42001:2023, 7.5.1 and NIST AI Risk Management Framework (AI RMF 1.0), GOVERN 1.6. 80 ISO, AS ISO/IEC 42001:2023, 4.1. 81 ISO, AS ISO/IEC 42001:2023, 4.3, A.9.3, A.9.4. 82 ISO, AS ISO/IEC 42001:2023, A.6.2.7. 83 ISO, AS ISO/IEC 42001:2023, 7.5. 84 World Economic Forum, Adopting AI Responsibly: Guidelines for Procurement of AI Solutions by the Private Sector, Insight Report June 2023. 85 ISO, AS ISO/IEC 42001:2023, 4.2. 86 See ISO, AS ISO/IEC 42001:2023, 4.2. 87 ISO, AS ISO/IEC 42001:2023, 4.2. 88 ISO, AS ISO/IEC 42001:2023, 6.1.4, A.5.3, A.5.4. 89ISO, AS ISO/IEC 42001:2023, 6.1.4, A.5.4, A.5.5. 90 ISO, AS ISO/IEC 42001:2023, 6.1.4, A.5.3, A.5.4. 91 UK Government, Guidelines for AI procurement. 92 World Economic Forum, Adopting AI Responsibly: Guidelines for Procurement of AI Solutions by the Private Sector, Insight Report June 2023. Voluntary AI Safety Standard | industry.gov.au/NAIC 61 --- Page 69 --- 93 LR Lifshitz and R Hung, \u2018BC Tribunal Confirms Companies Remain Liable for information Provided by AI Chatbot\u2019, American Bar Association Business Law Section [web page], ABA, 29 February 2024. 94 Federal Court of Australia, Australian Competition and Consumer Commission v Trivago N.V. (No 2) [2022] FCA 417, Federal Court of Australia, 22 April 2022.", "metadata": {"country": "Australia", "year": "2024", "legally_binding": "no", "binding_proof": "for the government", "date": "-/-/2024", "regulator": "Department of Industry, Science & Resources", "type": "", "status": "In force", "language": "EN", "use_cases": "[1, 3, 6]"}}
