{"_id": "686ac9bbe2af9fb1ff3ba990", "title": "National AI Strategy 2.0", "source": "https://file.go.gov.sg/nais2023.pdf", "text": "AI for the Public Good\nFor Singapore and the World\nCover image generated by Adobe Firefly\nCONTENTS | \n1\nContents\n03\nForeword\n05\nIntroduction\n13\nNational AI Strategy 2.0\nSYSTEM 1\nResearch\nIndustry\nGovernment\n17\nActivity Drivers\nSYSTEM 2\nPlacemaking\nTalent\nCapabilities\n33\nPeople & Communities\nSYSTEM 3\nTrusted Environment\nCompute\nData\n47\nInfrastructure & Environment\nLeader in Thought and Action\n65\nA Whole-of-Nation Movement\nOur Vision and Goals\nOur Plan\nFOREWORD | \n3\n2023 is a landmark year for Artificial Intelligence (AI).  \nRecent advances, especially in Generative AI, have opened doors to new possibilities. The \nmost powerful AI models provide human-like intelligence, or a close proxy of it, to do \nanything from clinical diagnoses to self-driving cars. \nThese advances have also precipitated many important conversations \u2013 about the \npotential of AI, its harms, and even on what it means to be human.\nSingapore believes in the transformative potential of AI. Our second National AI Strategy, \nor NAIS 2.0, represents Singapore\u2019s commitment to realise the benefits of AI, and to create \nexciting new opportunities.  It outlines our vision for Singapore to be a place where AI \nserves as a force for good, and where we harness AI to uplift and empower our people \nand businesses.  \nSingapore may be a small island state, but we have big dreams and plans. We will participate \nfully in AI research and deployment, to do things that were not possible before, and bring \nbenefits to ourselves and others outside Singapore. Hence, our vision for NAIS 2.0 is to \nachieve AI for the Public Good, for Singapore and the World. \nThis report spells out how we will achieve our vision. It sets out our plans to build a \nthriving AI ecosystem; develop our workforce to take on new opportunities; provide enough \ninfrastructural capacity to achieve our ambitions; and foster a trusted environment that \nprotects users and facilitates innovation. \nIn the end, our plans are only as good as our collective ability to implement them.  That\u2019s \nwhy we welcome all stakeholders and partners to join us in this shared endeavour.  Let us \nwork together to realise the full potential of AI in our lives. \nLAWRENCE WONG\nDeputy Prime Minister & Minister for Finance\nForeword\nINTRODUCTION | \n5\nSince the release of ChatGPT by OpenAI on 30 November 2022, Artificial Intelligence \n(AI)1 has gone mainstream. Recent breakthroughs, particularly those in the realm \nof Generative AI, have resulted in AI proliferating quickly. AI is moving beyond \nits traditional, narrow approaches, towards more general, almost human-like \ncharacteristics. The ability of ChatGPT to draft, depict, translate, understand \ncontext, and provide seemingly meaningful responses to our questions, has \ncaptured the imagination of not just scientists and experts, but all of us. For \ngovernments, these developments have raised new and critical questions around \nthe risks and responsibilities associated with the development and use of such a \npowerful technology, even as AI continues to reinforce and remind us of what it \nmeans to be human.\nIn Singapore, where the vast majority of our people already frequently interact \nwith digital technologies to transact with the Government and businesses, and are \nalso digitally engaged at work, AI has been both commonplace and an uncommon \nforce. \nIntroduction\n1 AI refers to a set of capabilities through which computer systems can demonstrate human-\nlike behaviour and complete tasks which typically require human intelligence. It is considered a \ngeneral-purpose technology which can be applied across a wide range of sectors. Some of its varied \napplications include advanced web search, recommendation and decision systems, advanced \nproblem-solving, understanding speech and natural language, perception (for applications like \nfacial recognition, image labelling, or autonomous vehicles), and Generative AI tools (including \nLarge Language Models) that can produce various types of content, including text, images, audio, \nand synthetic data.\n| INTRODUCTION\n6\nFundamentally, Singapore believes that:\n\u2003\n\u25b6\nAI can be a potent force for good, to uplift human potential:\n\u2003 \u2002\n\u25fc\nAI is a technology that will transform cognitive and physical tasks. We \nexpect its benefits to be immense, making today\u2019s jobs easier, while \nbringing within reach activities that were previously computationally \nimpossible.\n\u2003 \u2002\n\u25fc\nThis requires us to steer AI for the Public Good. We must harness AI \nin a sustainable way to create positive impact \u2013 for new opportunities, \nbetter jobs, and safer, more meaningful connections.\n\u2003\n\u25b6\nAI is strategically important. We must invest in it to:\n\u2003 \u2002\n\u25fc\nUnlock the next frontier of economic growth. We must master AI \nto overcome our labour and productivity challenges. This requires \ninvestments in our people, enterprises, and digital infrastructure, so \nthat we can create new jobs and participate in industries of the future.\n\u2003 \u2002\n\u25fc\nAddress risks from the potential abuse and mismanagement of AI. These \ninclude the misuse of AI to amplify harms (e.g. scams, cyberattacks, and \nmis/disinformation) across society, especially toward vulnerable groups.\nINTRODUCTION | \n7\nSingapore is not starting from scratch. Some earlier moves in AI include:\n\u2003\n\u25b6\nNational AI Strategy. In 2019, Singapore was one of the first countries to \nintroduce a National AI Strategy. We embarked on National AI Projects in \nareas like Education, Healthcare, and Safety & Security, and invested in \nenablers to strengthen our ecosystem.\n\u2003\n\u25b6\nDedicated investments in AI Research and Development (R&D). We have \ncommitted more than S$500 million through AI Singapore (AISG) under the \nResearch, Innovation and Enterprise (RIE) 2020 and 2025 plans.\n\u2003\n\u25b6\nAI Governance. Singapore launched the world\u2019s first Model AI Governance \nFramework in 2019. In 2022, we also launched AI Verify, an AI governance \ntesting framework and software toolkit, and made it open source for \ndevelopers in June 2023.\n| INTRODUCTION\n8\nThese early investments have borne fruit. We have:\n\u2003\n\u25b6\nBuilt strong foundations for our AI ecosystem. Singapore is well-regarded, \nranking among the top 10 in the world by several international metrics.2 \nOver 80 active AI research faculty, 150 AI R&D and product teams, and 1,100 \nAI start-ups call Singapore home.\n\u2003\n\u25b6\nHarnessed AI to improve Singaporeans\u2019 lives. Today, AI powers many public \nservices, such as adaptive learning systems in our schools, and chronic \nhealth management systems in our hospitals. We also use AI to support \nimmigration and customs clearance, and to detect and deter online scams.\n\u2003\n\u25b6\nForged international partnerships. Singapore is recognised as an active \ncontributor to global AI discourse, spanning issues from innovation to \ngovernance. We advocate for the responsible and ethical use of AI. We \nare active participants in multi-stakeholder platforms such as the Global \nPartnership on AI (GPAI), the World Economic Forum (WEF) AI Governance \nAlliance, and most recently the United Nations (UN) High-Level Advisory \nBody on AI. Singapore\u2019s initiatives, like AI Verify, have been welcomed as \npragmatic contributions to the growing body of work on AI governance.\n2 For instance, Singapore ranks 1st in per capita terms in the Stanford Global AI Vibrancy Index \n(2021); 2nd in the Oxford Insights Government AI Index (2021); and 3rd in the Tortoise Media Global \nAI Index (2022).\nINTRODUCTION | \n9\nThe recent breakthroughs in AI demand a renewed focus by Singapore to refine \nour national strategies for AI:\n\u2003\n\u25b6\nGreater capabilities. AI is now more powerful and accessible. We therefore \nneed to work with producers and users of AI in a more concerted manner, \nbecause responsible development and deployment do not happen by chance.\n\u2003\n\u25b6\nGreater concerns. There are growing concerns over the safety and security \nrisks of AI, particularly Generative AI models. These range from fears over \nmalicious attacks on AI models, to the current inscrutability of large language \nmodels (LLMs), which calls into question the validity, credibility, and legality \nof their output. Such concerns have garnered attention from many around \nthe world.\nThere are also new competitive realities around AI:\n \n\u2003\n\u25b6\nResources and talent for AI are scarce. These are concentrated within a few \ncompanies and countries, intensifying economic competition, as well as \ngeostrategic and resilience risks.\n\u2003\n\u25b6\nOther smaller countries are also moving quickly to attract AI investments \nand talent. While acquisitions for AI may be costly, they can confer enduring \nvalue and deepen capabilities, if properly integrated within the ecosystem.\nAgainst this backdrop, Singapore aspires to be a pace-setter \u2013 a global leader in \nchoice AI areas, that are economically impactful and serve the Public Good.\n| INTRODUCTION\n10\nWe will therefore make three shifts from our first National AI Strategy:\nFrom Opportunity to Necessity. People \u201cmust know\u201d AI, not just see it \nas a \u201cgood to have\u201d. We need both technical experts and savvy users to \nmaximise AI\u2019s potential for Singapore.\nFrom Local to Global. Our people and businesses should operate with the \nambition to be world-leading in AI. Singapore should be well-connected \nto global innovation networks, working with the best to overcome complex \nchallenges surrounding AI today (e.g. energy, data, and ethics). Singapore \nmust contribute to AI breakthroughs and products that the world values.\nFrom Projects to Systems. For AI to have widespread and positive impact \non our economy and society, Singapore has to move beyond flagship \nNational AI Projects. We will take a systems approach, bringing together \nstakeholders within and outside Singapore to add to our resources, \ncapabilities, and infrastructure, accelerate the exchange of ideas, and \nadminister AI-enabled solutions at scale.\nINTRODUCTION | \n11\nWe believe that Singapore can and will succeed, given (a) our position as a trusted \nglobal partner and major business hub, (b) our pro-innovation and business-\nfriendly operating environment, (c) our strong knowledge base and capable \nworkforce, and (d) our successful track record in transforming our economy. We \nare a nation of industrious people, known for being reliable and trustworthy, and \nin whose hands AI may flourish.\nIt is with these convictions that we set out Singapore\u2019s second National AI \nStrategy (NAIS 2.0), which will also drive the next bound of our Smart Nation \njourney.  Working together with our international network of friends and partners, \nwe shall use AI to be a force for good, generating new economic opportunities \nand improving societies everywhere.\nNATIONAL AI STRATEGY 2.0 | \n13\nNational AI \nStrategy 2.0\nDeveloped through extensive engagements, NAIS 2.0 starts with the conviction \nthat we must do our utmost to harness AI for the Public Good, for Singapore and \nthe World. Singapore will be a place where AI can:\n\u2003\n\u25b6\nAddress the needs and challenges of our time. For example, in areas of global \nimportance such as population health and climate change.\n\u2003\n\u25b6\nBe the great equaliser. We shall uplift and empower our people and \nbusinesses, equipping them with the capabilities and resources to thrive in \nan AI-enabled future.\nNAIS 2.0 seeks to attain the twin goals of:\nOur Vision and Goals\nExcellence. We will selectively develop peaks of excellence in AI, to \nadvance the field and maximise value creation.\nEmpowerment. We will raise up individuals, businesses, and communities \nto use AI with confidence, discernment, and trust.\n| NATIONAL AI STRATEGY 2.0\n14\nTo achieve our vision and goals, we will direct efforts under NAIS 2.0 toward three \nSystems, working through 10 Enablers.\n\u2003\n\u25b6\nSystem 1: Activity Drivers (Enablers: Industry, Government, Research). \nIndustry, Government, and public research performers have deep technical \ncapabilities that can be applied to deliver value. We need to orchestrate \nthem around meaningful use cases and problem statements to transform \nour economy and society.\n\u2003\n\u25b6\nSystem 2: People & Communities (Enablers: Talent, Capabilities, Placemaking). \nWe will attract more top-tier researchers and engineers to work with and \nfrom Singapore. More of our technology workforce should work to scale \nnovel AI solutions, that form part of the toolkit which a confident base of \nenterprises and workers can use.\n\u2003\n\u25b6\nSystem 3: Infrastructure & Environment (Enablers: Compute, Data, Trusted \nEnvironment, Leader in Thought and Action). We will ensure that Singapore \nhosts the necessary infrastructure and provides a trusted environment for \nAI innovation. This will make us a credible leader and preferred site for AI \ndevelopment, deployment, and adoption.\nThis strategy statement outlines 15 Actions that Singapore will undertake across \nthese systems and enablers, to support our ambitions over the next 3-5 years. We \nwill continually review these Actions to respond to fast-moving developments in \nAI, across domains.\nOur Plans\nNATIONAL AI STRATEGY 2.0 | \n15\nSYSTEM 1: ACTIVITY DRIVERS | \n17\nActivity Drivers\nSYSTEM 1\nIndustry, Government, and Research drive AI activity, from basic science to product \nR&D and adoption. Together, they help Singapore leverage the rapid innovation \ncycles in AI for the economy and our society.\nSingapore has built up a strong and promising base of AI capabilities, which we \nshould harness to significantly uplift the broader ecosystem. We will encourage \ngreater experimentation and collaboration, focusing on interesting and impactful \nchallenges, to reap greater benefits from AI.\nAI Trailblazers\nFeature Story\nIn July 2023, the Ministry of Communications and Information (MCI), Digital Industry \nSingapore (DISG), Smart Nation and Digital Government Office (SNDGO), together with \nGoogle Cloud, launched AI Trailblazers. This rapid prototyping initiative was a world\u2019s \nfirst, giving businesses and government agencies free access to Google Cloud\u2019s AI toolsets \nfor up to three months, with the aim of generating 100 Generative AI use cases in 100 \ndays, across the private and public sectors.\nMrs. Josephine Teo, Minister for Communications and \nInformation, at the launch event for the AI Trailblazers initiative \nat the Google Asia Pacific campus.\nBy working with the best-in-class to provide \nan innovation sandbox for AI development, \nSingapore was not only able to spark more \nexperimentation, but also gave many more \nbusiness owners the confidence to invest \nfurther resources once they were able to see \nthe potential of AI in delivering their business \nobjectives.\n| SYSTEM 1: ACTIVITY DRIVERS\n18\nWe shall steer AI toward developing select peaks of excellence, which can deliver \noutsized impact to Singapore and the lives of Singaporeans.\n\u2003\n\u25b6\nAI in Domains. We will encourage AI innovation and adoption in key domains, \nincluding:\n\u2003 \u2002\n\u25fc\nLeading Economic Sectors, which form a sizeable share of Singapore\u2019s real \nGross Domestic Product (GDP) and for which AI innovation could catalyse \nthe next bound of economic growth. Examples include Manufacturing, \nFinancial Services, Transport & Logistics, and Biomedical Sciences.\n\u2003 \u2002\n\u25fc\nSmart Nation Priorities, where AI assists our national development, \nand unlocks new value propositions for social impact. These include \nHealthcare, Education & Manpower, Trust & Safety, and Public Service \nDelivery.\n\u2003\n\u25b6\nCross-cutting Capabilities. We will invest in cross-cutting areas that can \naccelerate the development and deployment of AI solutions across all \ndomains, including:\n\u2003 \u2002\n\u25fc\nAI for Business Operations, where AI can optimise and transform business \nfunctions (e.g. customer relationship management, finance, human \nresources, legal, sales and marketing, and supply chain management).\n\u2003 \u2002\n\u25fc\nAI for Science, where the use of AI can accelerate research productivity \nacross scientific domains (e.g. drug discovery).\n\u2003 \u2002\n\u25fc\nFoundational AI, where scientific advancements are still needed to \nimprove AI\u2019s abilities. For a start, we will double down on three specific \nthemes \u2013 Reasoning AI, Resource-Efficient AI, and Responsible AI3 \u2013 with \na view to making AI less costly, more widely-used, and most importantly, \ntrusted.\n3 Reasoning AI improves AI systems\u2019 ability to understand logical and physical concepts, and \nexplain their output; Resource-efficient AI can help reduce AI\u2019s growing reliance on data and \ncompute, while still ensuring performance gains; Responsible AI is critical to mitigate the risks of \nAI and ensure that AI is trustworthy, safe, and secure. \nSYSTEM 1: ACTIVITY DRIVERS | \n19\n* E.g. customer relationship management, finance, human resources, legal, sales and marketing, and supply chain. \n| SYSTEM 1: ACTIVITY DRIVERS\n20\nSingapore\u2019s knowledge-intensive economy is especially exposed to AI. Our \nindustries and enterprises need to be responsive, and ready to create AI-driven \nopportunities that will strengthen Singapore\u2019s economic competitiveness. This \nwill help workers and industries set the pace and shape their own future, rather \nthan face untimely disruption.\nMany companies have already embarked on AI-enabled innovation projects, \nwhich are often focused on the Application layer. While these are useful and have \nmade a difference, there is scope for us to go further. Deep and transformative \nAI innovation today increasingly requires integrated capabilities from across the \nentire technology stack (i.e. including Model and Infrastructure layers). There \nmust also be a stronger nexus between sophisticated end-users and leading-\nedge AI innovators and producers.\nTo anchor transformative AI innovation and value creation in Singapore, the \nGovernment has a key role in curating the right incentives (e.g. targeted grants), \nresources (e.g. compute and talent), regulatory frameworks, and partners. For \na start, we will adopt a sector-specific, use case-centric approach, and we will \nfocus our initial efforts on Leading Economic Sectors assessed to be most ready \nfor AI-driven transformation.\nDone well, we will have a thriving industry ecosystem in Singapore, with significant \nvalue creation from AI, and capabilities across the AI technology stack. AI will \ncomplement our workforce, boost our productivity, and be a differentiating factor \nin attracting best-in-class companies to Singapore. We will also seed a virtuous \ncycle where industry end-users can tap into a dense network of AI producers, who \nin turn are motivated by the presence of strong, industry-relevant lead demand \nand product mandates.\nENABLER 1\nIndustry\nSYSTEM 1: ACTIVITY DRIVERS | \n21\nAnchor new AI Centres of Excellence (CoEs) across \ncompanies, and explore establishing Sectoral AI CoEs \nto drive sophisticated AI value creation and usage in \nkey sectors\nAction 1\nWe will attract and anchor new AI CoEs in Singapore-based companies that are \nleading-edge producers (e.g. technology companies, start-ups) and sophisticated \nend-users, in order to conduct value creation activities across the AI stack. These \nAI CoEs will (a) concentrate and deepen companies\u2019 innovation capabilities across \nthe AI stack; (b) own core AI product charters and functions; and (c) align with \nthe peaks of excellence that Singapore aims to build. Their roles could include \ncreating new intellectual property, products, and services, beyond optimising \nbusiness processes.\nIn addition, we will explore establishing a new model of Sectoral AI CoEs to \nintensify sophisticated AI value creation and usage in selected economic sectors. \nWe will work with Industry Champions4 to identify sector use cases and crowd in \na broader base of researchers and companies in the sector.\n4 Industry Champions refer to enterprise leaders that can form lead demand for AI solutions, \nboost their sector\u2019s competitiveness, uplift the capabilities of their supply chains, and generate \nspin-offs for the broader sector ecosystem.\n| SYSTEM 1: ACTIVITY DRIVERS\n22\nAmerican Express Decision Science CoE\nFeature Story\nIn December 2022, American Express set up a Decision \nScience CoE in Singapore focusing on data science \napplications in the areas of credit and fraud risk model \ndevelopment using AI, Machine Learning (ML), and \nnatural language processing (NLP). American Express \nexpanded the CoE in November 2023 to also use AI and \nML to optimise its customer marketing and service, and \ncreate personalised and relevant experiences across \ndigital channels. The CoE will also create a Generative \nAI R&D practice focused on new AI applications for \nservicing, risk, and technology.\nJacqueline Poh, Managing Director, Economic \nDevelopment Board (EDB) and Anna Marrs, \nGroup President, Global Commercial Services \nand Credit & Fraud Risk, American Express at \nthe official opening of the American Express \nDecision Science Center of Excellence.\n(Photo credit: American Express)\nSYSTEM 1: ACTIVITY DRIVERS | \n23\nStrengthen our AI start-up ecosystem, including \nattracting AI-focused accelerator programmes to spur \nrapid AI experimentation\nAction 2\nIn concert with private sector partners, we will strengthen the AI start-up \necosystem along several pillars. This includes attracting more venture builders \nand developing more accelerator programmes. We want to speed up AI value \ndiscovery across industry, and nurture a pipeline of disruptive, AI-native start-ups. \nThese accelerator programmes could be led by a mix of big technology companies \nand venture capital firms. They will provide the capital, business and technical \nexpertise, infrastructure, and market networks to spur rapid AI experimentation. \nWe will nurture globally-oriented AI innovators to create intellectual property \nand scale to more markets.\n| SYSTEM 1: ACTIVITY DRIVERS\n24\nThe Government is committed to the continuous improvement of public service \ndelivery for our people and businesses. We will harness AI to serve the public in \nmore impactful ways.\nENABLER 2\nGovernment\nOneService Chatbot\nFeature Story\nThe Municipal Services Office and the Government Technology Agency (GovTech) launched \nthe OneService chatbot to enable citizens to lodge complaints and provide information \non them via commonly used social messaging apps (i.e. WhatsApp and Telegram).\nPowered by AI, the chatbot can (a) automatically identify and classify complaints into the \nappropriate category (e.g. litter, illegal parking), (b) extract the relevant details of the \nincident that need attention, and (c) identify and inform the relevant Government agency \nto follow up on the case.\nWith this, citizens can easily provide municipal feedback without having to figure out \nwhich agency they should contact. Agencies can also attend to cases more quickly.\nWe also recognise the positive spin-offs from such efforts. These include providing \nlead demand for commercial AI tools, demonstrating AI-led transformation for \nthe rest of the economy and society, spurring private sector investment in AI, and \nmainstreaming AI adoption.\nSYSTEM 1: ACTIVITY DRIVERS | \n25\nFor the Government to better harness AI, we need to:\n\u2003\n\u25b6\nDevelop, deploy, and integrate more useful and powerful AI-enabled products. \nThese include customised or commercial off-the-shelf solutions. They can \nsupport the Government\u2019s needs, both general (e.g. smart transcription \ntools for citizen support services) and specialised (e.g. detecting anomalies \nin financial statements and transactions to assist anti-money laundering \nefforts).\n\u2003\n\u25b6\nDrive mass awareness and adoption of AI across the Public Service. This \nwill encourage public officers to use AI-enabled products and services \nconfidently, for more efficient and effective public service delivery.\n\u2003\n\u25b6\nWork with industry and the public to better identify use cases and address \npain points using AI. This can be done through well-structured ideathons, \nhackathons, and incubator programmes.\nFeature Story\nPair is a Large Language Model (LLM)-powered suite of tools for Government officers \nthat allows the safe and secure use of LLMs within the Government\u2019s IT systems. Its base \ncapabilities include ideation, drafting, text and natural language processing, coding, and \ndata analysis, making it a versatile tool for various tasks that public officers commonly \nundertake.\nPair Chat is the first application to be developed within the suite. It functions on a \nquestion-and-answer format, powered by the same LLMs underlying ChatGPT. Thousands \nof public servants regularly access Pair Chat to increase their productivity at work.\nPair\n| SYSTEM 1: ACTIVITY DRIVERS\n26\nImprove Public Service productivity, with new value \npropositions for our citizens\nAction 3\nThe Government will accelerate public sector adoption of AI to unlock new value \npropositions for our people and businesses. This will take place at two levels:\n\u2003\n\u25b6\nSmart Nation Priorities. Government agencies equipped with the specialised \nknowledge, technical capabilities, and regulatory tools for these domains \n(e.g. Healthcare and Education) will develop and lead sector-specific AI \nstrategies, to address the needs and challenges of these domains.\n\u2003\n\u25b6\nWhole-of-Government functional domains. These include Finance, Human \nResources, and Service Delivery, which are key to transforming government \nprocesses and services for the better. All Government agencies that lead \nsuch functions will identify and optimise specific business lines with AI, and \nbuild internal capabilities to deliver business outcomes.\nThe Government will coordinate the resources needed to support public sector AI \nadoption. For example, through central funding to support or scale-up novel and \nimpactful use cases, facilitating cross-agency data sharing, providing access to \nhigh-performance compute and associated engineering capabilities (e.g. through \nAI Trailblazers), and issuing facilitative policy guidelines.\nWe will also uplift the AI literacy baseline and sharpen the AI proficiency of all \npublic officers. We will develop and roll out targeted courses to impart AI skills, \nfor different public sector workforce segments (e.g. senior management, policy, \nand operational roles).\nSYSTEM 1: ACTIVITY DRIVERS | \n27\nA strong community of scientists and researchers contributes to a vibrant AI \necosystem in Singapore.\n\u2003\n\u25b6\nThey add to our overall technical heft, making us attractive to other top \nresearchers, investors, and entrepreneurs, to create new value in AI. This can \ngenerate other economic spin-offs, such as the creation of well-paying jobs, \nstart-up growth, and sophisticated venture capital investments.\n\u2003\n\u25b6\nTheir research breakthroughs can also translate into commercial products, \nwhich can be used by our companies and across government agencies to \nrealise better outcomes.\n\u2003\n\u25b6\nTheir presence helps secure an enduring competitive advantage in AI for \nSingapore. Over the last few decades, innovators in Silicon Valley and other \nAI research hubs (e.g. Beijing, Boston, Montreal, Seattle, and Toronto) have \nled these places\u2019 technological prowess. Singapore will similarly need strong \nresearch credentials.\nSingapore will take a pragmatic yet bold approach to building up and sustaining \nour leadership in select areas of AI research.\nENABLER 3\nResearch\n| SYSTEM 1: ACTIVITY DRIVERS\n28\nUpdate national AI R&D plans to sustain leadership in \nselect research areas\nAction 4\nWe intend to update our national AI R&D plans in five ways: Research Priorities, \nIndustry-Academia Nexus, Talent, Compute, and International Collaborations.\n\u2003\n\u25b6\nResearch Priorities. To optimise our limited resources for impact, we will \nbe selective in our AI research priorities, and align them to the peaks of \nexcellence that Singapore hopes to build.\n\u2003\n\u25b6\nIndustry-Academia Nexus. We will foster more R&D collaboration between \nacademia and industry, capitalising on the growing porosity of talent and \ninnovation activities across the two spheres.\nSYSTEM 1: ACTIVITY DRIVERS | \n29\nAISG\u2019s 100 Experiments programme\nFeature Story\n100 Experiments (100E) is AISG\u2019s flagship programme, aimed at solving AI problem \nstatements proposed by industry where no commercial off-the-shelf AI solutions exist.\nAs part of this, AISG helps pair companies with public researchers to develop AI solutions \nthat address the former\u2019s problem statement, and provides co-funding of up to S$330,000 \nper project.\nSeveral companies, from a range of sectors, have participated in the 100E programme.\nOne example is Q&M Dental Group\u2019s EM2AI, which worked with AISG to create an AI model \nthat helps dentists review X-ray images to detect disease, present findings as dental \ncharts, and recommend appropriate treatment plans. Q&M Dental Group has deployed \nthis model across more than 150 clinics in Singapore and Malaysia.\nAnother example is Tencent\u2019s Lightspeed Studios, a globally-leading game developer that \npublished games like PUBG Mobile. Under the project, Lightspeed Studios partnered AISG \nto develop a text-to-speech system to support the narration of game storylines, so that \ngamers who speak Malay can experience the games in their native language.\nThrough 100E, Q&M Dental Group\u2019s EM2AI has developed an AI \nmodel that helps dentists, and has deployed this across its clinics in \nSingapore and Malaysia.\n(Photo credit: Q&M Dental Group)\nThrough 100E, LightSpeed Studios is partnering with AISG to \ndevelop a new text-to-speech system for gamers.\n| SYSTEM 1: ACTIVITY DRIVERS\n30\n\u2003\n\u25b6\nTalent. We will strengthen efforts to recruit top AI researchers to work from \nand with Singapore (see Action 5).\n\u2003\n\u25b6\nCompute. We will secure access, and operate Graphics Processing Units (GPUs) \nfor Singapore\u2019s research community. This will also build up Singapore\u2019s \ninfrastructure engineering expertise.\n\u2003\n\u25b6\nInternational Collaboration. We will expand international research \ncollaboration in areas aligned with our research priorities. This could take \nthe form of joint grant calls or PhD training programmes with other countries, \nor involvement in international conferences. This will allow Singapore to \nlevel up our current capabilities by working with the best in the world and \nmake a meaningful contribution to global AI development.\nSingapore-Republic of Korea Joint Grant Call for AI Research\nFeature Story\nIn February 2023, AISG and the Republic of Korea (ROK)\u2019s Institute for Information & \nCommunication Technology Planning and Evaluation (IITP) launched a joint grant call to \nfund research on AI-based net-zero energy building management optimisation systems.\nThis grant call supported a research project between Nanyang Technological University \nand Korea University, to study AI-based energy management and optimisation frameworks, \nto enhance the energy efficiency of Heating, Ventilation, and Air Conditioning (HVAC) \nsystems. The project also drew in industry partners from both countries.\nSYSTEM 1: ACTIVITY DRIVERS | \n31\nSingapore Conference on AI\nFeature Story\nIn December 2023, MCI and Smart Nation Group convened the inaugural Singapore \nConference on AI (SCAI), to bring together a select group of top AI researchers to discuss \nand articulate the top research problems of AI research, development, and deployment \nthat, if solved, would allow humans and society to flourish.\nOver 40 local and overseas experts from academia, non-profit, industry \nand government gathered in Singapore for SCAI.\nSCAI participants identifying a list of critical questions that need to \nbe addressed to realise AI\u2019s full potential.\nSYSTEM 2: PEOPLE & COMMUNITIES | \n33\nPeople & \nCommunities\nSYSTEM 2\nTight-knit knowledge communities are critical for AI innovation, driving the \nexchange of ideas and expertise for research, product development, and \nthe impactful use of AI. Singapore must be home to exemplary AI Creators, \nPractitioners, and Users. We must:\n\u2003\n\u25b6\nAttract, anchor, and develop more AI Creators (top-tier AI talent) from industry \nand academia to work with and from Singapore, to generate cutting-edge AI \nresearch and drive novel use cases.\n\u2003\n\u25b6\nIncrease the number of AI Practitioners (tech workers) with the skillsets \nto create, implement and deploy AI systems, models, and algorithms in \norganisations, at scale.\n\u2003\n\u25b6\nBuild up a base of confident AI Users (enterprises and general workforce) \nthat are equipped to use AI-powered products and services to increase \nproductivity, and pursue better jobs and more impactful work.\n\u2003\n\u25b6\nAccelerate the exchange of ideas, both within our local AI community of \nCreators, Practitioners, and Users, and across global networks.\n| SYSTEM 2: PEOPLE & COMMUNITIES\n34\nSingapore will expand the pool and raise the quality of our AI Creators and \nPractitioners, to accelerate AI innovation and support AI Activity Drivers.\n \n\u2003\n\u25b6\nWe must increase the number and diversity of AI Creators working from \nand with Singapore, to drive leading-edge R&D activities and product \ndevelopment.\n\u2003\n\u25b6\nWe must also boost our pipeline of AI Practitioners. They are critical to \nsupporting top-tier AI activity, translating innovation into products and \nservices, and transforming our industries.\nENABLER 4\nTalent\nSYSTEM 2: PEOPLE & COMMUNITIES | \n35\nAttract world\u2019s top AI Creators to work from and with \nSingapore\nAction 5\nSingapore will engage and attract world-class AI Creators, from both the public \nresearch and industry spheres, to deepen their innovation activities here. We will:\n\u2003\n\u25b6\nSet up a dedicated team to identify, engage, and anchor AI Creators. This team \nwill be the primary interface to facilitate the integration of AI Creators into \nthe Singapore ecosystem, including through bespoke support mechanisms.\n\u2003\n\u25b6\nCreate novel value propositions to attract AI Creators. We will intensify \nlocal AI development activities and induct international experts into our \nAI ecosystem to create the building blocks for success. We will explore \npossible modalities for doing so, such as hybrid Singapore-overseas working \narrangements, part-time appointments across industry and academia, and \npartnerships with international research institutions or companies.\n| SYSTEM 2: PEOPLE & COMMUNITIES\n36\nBoost AI Practitioner pool to 15,000\nAction 6\nSingapore will boost the pool of AI Practitioners to support growing AI demand, \nby:\n \n\u2003\n\u25b6\nScaling up AI-specific training programmes. We will re-design the AI \nApprenticeship Programme (AIAP)5 to significantly increase the number of \napprentices we can train annually. We will also work with industry AI product \ndevelopment teams to expand the number of company attachments for our \nContinuing Education and Training programmes.\n\u2003\n\u25b6\nScaling up technology and AI talent pipelines, through Pre-Employment \nTraining and by reskilling and upskilling workers through Continuing \nEducation and Training.\n\u2003\n\u25b6\nRemaining open to global tech talent. We will continue to welcome global AI \ntalent to work and live in Singapore, and contribute to our national efforts.\n5 The AIAP is currently designed as a 9-month, full-time, deep-skilling company-led training \nprogramme, that helps trainees acquire AI capabilities by working alongside industry teams on \nreal business problems. \nSYSTEM 2: PEOPLE & COMMUNITIES | \n37\nTechSkills Accelerator\nFeature Story\nDriven by IMDA, in collaboration with industry, SkillsFuture Singapore (SSG), Workforce \nSingapore (WSG), and the National Trades Union Congress (NTUC), the TechSkills Accelerator \n(TeSA) aims to build and develop a skilled Information and Communications Technology \nworkforce, which includes AI talent. Through Company-Led Training Programmes and \nCareer Conversion Programmes, amongst others, TeSA has partnered industry to train \nindividuals in AI and Data Analytics, with more than 2,700 individuals placed in good jobs \nthus far.\n| SYSTEM 2: PEOPLE & COMMUNITIES\n38\nIt is important to uplift the AI capabilities of our industries and workforce, to \nenable our enterprises and workers to reap the benefits of AI, and minimise its \ndisruptive effects.\n\u2003\n\u25b6\nAI has the potential to help enterprises increase their productivity and stay \nrelevant, provided they have the capabilities to apply it well.\n\u2003\n\u25b6\nWorkers must also be equipped with the necessary skillsets to utilise AI \ntools, so that they can increase their productivity, avoid untimely disruptions \nto their jobs, and help enterprises optimise their operations and improve \ntheir competitiveness.\nWhile all economies must prepare for AI disruptions, we believe that Singapore \nis well-placed to navigate them. Singaporeans have persistently demonstrated \nresilience and adaptability in the face of change, and the Government has a \nstrong track record of managing such nation-wide transitions, whether structural \nor cyclical, through our policies and outreach. We will build upon our existing \ntoolkits, schemes, and training programmes, to empower enterprises and workers \nto adopt AI and stay ahead of the curve.\nENABLER 5\nCapabilities\nSYSTEM 2: PEOPLE & COMMUNITIES | \n39\nIntensify \nenterprise \nAI \nadoption \nfor \nindustry \ntransformation\nAction 7\nSingapore will intensify the promotion of AI adoption across all enterprises.\n\u2003\n\u25b6\nWe have made available tools that enterprises can use to evaluate their \nreadiness to adopt AI. For example, self-assessment tools like AISG\u2019s AI \nReadiness Index (AIRI) are useful in helping companies identify and map out \nthe capabilities and infrastructure they need for using AI.\n\u2003\n\u25b6\nWe will promote baseline digital adoption for our enterprises and enhance \nthe enterprise digitalisation toolkit, to support more sophisticated AI \nadoption. Businesses\u2019 first interactions with AI often happen through their \nexisting enterprise solutions. Only after they have had the foundational \ncapabilities in place, and are convinced of AI\u2019s benefits, do they then turn \nto more advanced or bespoke AI business solutions. Through programmes \nlike SMEs Go Digital and CTO-as-a-Service (CTOaaS), the Government has \nhelped businesses gain awareness of suitable AI-enabled solutions. We have \nalso encouraged the adoption of sector-relevant AI solutions, through our \nrefreshed Industry Digital Plans (IDPs).\n\u2003\n\u25b6\nFor more digitally mature enterprises, we will provide tailored support for \nAI-enabled business transformation. This includes leveraging IMDA and \nEnterpriseSG\u2019s Digital Leaders Programme (DLP), which helps companies \nbuild in-house digital capabilities, as well as the scoping of impactful AI \nprojects in partnership with AI solution providers.\n \n| SYSTEM 2: PEOPLE & COMMUNITIES\n40\nUpskill workforce through sector-specific AI training \nprogrammes\nAction 8\nTo help workers acquire the necessary skillsets to work with AI, Industry \nTransformation Map (ITM)6 Sector Leads will develop targeted interventions \nfor their sectoral workforce. AI has multiple applications in different contexts, \nand ITM Sector Leads are best placed to size the potential impact of AI on their \nrespective industries, and by extension their workforce. Sector Leads can ride \non existing frameworks like the Jobs Transformation Maps (JTMs) to identify the \nrelevant skillsets needed, and develop appropriate training programmes for AI \nupskilling and reskilling.\n \n6 ITMs are roadmaps developed for 23 industries under six clusters to systematically raise \nproductivity, develop skills, drive innovation, and promote internationalisation, so as to catalyse \ntransformation. ITMs will be refined over time to ensure relevance.\nSYSTEM 2: PEOPLE & COMMUNITIES | \n41\nJobs Transformation Maps\nFeature Story\nJTMs provide detailed insights on the impact of technology and automation on sectors \nand their workforce. They identify key technologies that are driving change, and their \nimpact on individual job roles. As existing job roles evolve and new job roles emerge, JTMs \nalso identify the pathways for employers to transform jobs and for workers to acquire \nrequisite skills. JTMs serve as a useful compass for employers and workers to prepare \nthemselves for the future of work as technology becomes more pervasive in their sectors.\n \nOne example is the Financial Services JTM, which studied the impact of AI, advanced \nanalytics, and automation on jobs and skills in the Financial Services sector in 2019. The \nJTM helped prioritise the industry\u2019s efforts to reskill and redeploy employees in segments \nthat were identified as likely to experience significant changes, such as consumer banking, \ninsurance, and operations.\nMAS and the Institute of Banking & Finance, with support from WSG, will be studying how \nGenerative AI will transform the financial sector, including where Generative AI might be \ndeployed, and by when. This will further inform our understanding of how Generative AI \nwill impact jobs in the sector, and provide advice on how the workforce should upskill and \nreskill to perform new and augmented roles.\n| SYSTEM 2: PEOPLE & COMMUNITIES\n42\nIn a vibrant ecosystem, diverse talent can readily connect with one another \nand forge meaningful partnerships. There are many talented AI Creators and \nPractitioners today, with good ideas that can expand the frontier of AI. The \nopportunity to spar and collaborate with like-minded peers can enrich these ideas, \nand accelerate the translation into products and new value. Such synergies are \nseen in global AI hubs such as San Francisco, where stakeholders working across \nall parts of the AI ecosystem are found in close proximity, and the vibrancy of the \ncommunity in turn attracts the participation of even more talented individuals, \ncompanies, and capital.\nTo realise similar benefits, Singapore intends to provide more platforms which \ncan bring our AI community together. We want to engage with more of our talent \npool, and connect them to global AI experts for greater opportunities to interact \nand collaborate. Over time, we hope these connections will create a sense of \nidentity and fraternity, and build up a broader Singapore AI community that can \nattest to the support and inspiration that this network provides.\nENABLER 6\nPlacemaking\nSYSTEM 2: PEOPLE & COMMUNITIES | \n43\nEstablish an iconic AI site to co-locate AI creators \nand practitioners, and nurture the AI community in \nSingapore\nAction 9\nSingapore will establish a dedicated physical place for AI in Singapore. This will \nbe a focal point for our community of AI Creators and Practitioners to form new \nconnections and spark new ideas:\n\u2003\n\u25b6\nThis place will serve as an intellectual home for both Singapore-based \nindividuals and visiting colleagues. We will welcome entrepreneurs, \nresearchers, engineers, apprentices, and students, who are passionate about \nAI and share our mission orientation, to this place. There, we will include a \nvariety of purpose-built spaces aimed at building up a sense of community \nand accelerating the exchange of ideas. We will co-create this place in close \nconsultation with AI ecosystem representatives.\n| SYSTEM 2: PEOPLE & COMMUNITIES\n44\nRAISE.SG\nSingapore organised the inaugural RAISE.SG in July 2023, gathering 27 Singapore-linked AI \nCreators and Practitioners (from across academia, industry, and non-profit organisations, \nbased locally and overseas), to discuss how Singapore could prepare and invest for the \nnext bound of our AI journey.\nThe inaugural RAISE.SG gathered a diverse collection of \nSingapore-linked AI Creators and Practitioners, to ex\u00ad\nchange ideas on the next bound of Singapore\u2019s AI journey.\nRAISE.SG participants also interacted with representa\u00ad\ntives from the local AI community, including through com\u00ad\nmunity-building events which drew in start-up founders, \ninvestors, and government agencies.\nSYSTEM 2: PEOPLE & COMMUNITIES | \n45\nParticipants emphasised the need to nurture a strong, tight-knit AI community in \nSingapore, concentrating community activities in a focal site to increase opportunities \nfor interactions and the meaningful exchange of ideas. They suggested that this site be:\n\u2003\n\u25b6\nSupported by a full calendar of AI-related programming, including community-run \nevents such as hackathons, demo days, guest lectures, seminars, and social functions, \nwhich create opportunities to build relationships across the community.\n\u2003\n\u25b6\nEnhanced with a \u201cdigital community\u201d layer, to enable online collaborations and \nremote participation in community events by Singapore-linked AI talent who are \nnot physically based in Singapore. Through such virtual interactions, the local \nAI community can connect more frequently, including with other AI innovation \ncommunities elsewhere.\nThe Government will explore such suggestions and other ways to support efforts that \nstrengthen our AI community. For instance, we will work with industry and public research \npartners to organise more AI-related events, and increase the cadence of AI community \nnetworking events such as the ongoing Neural Networking series. Building on the inaugural \nRAISE.SG, we will organise regular gatherings of AI experts with a Singapore nexus, as well \nas arrange engagements with prominent foreign-based AI experts and practitioners who \nvisit Singapore.\nFeature Story\nSYSTEM 3: INFRASTRUCTURE & ENVIRONMENT | \n47\nInfrastructure & \nEnvironment\nSYSTEM 3\nSuccessful AI value creation requires robust and conducive infrastructure and a \nfacilitative environment, where all stages of the AI life cycle are well supported, \nand everything works the way it should.\nFor Singapore, this will involve:\n\u2003\n\u25b6\nAvailing compute and data for AI innovation. These are important building \nblocks to serve our AI ambitions.\n\u2003\n\u25b6\nA trusted environment where AI-enabled innovations and systems are robust \nand safe, so that our people can engage with AI with confidence.\n\u2003\n\u25b6\nSafeguarding Singapore\u2019s AI interests in the international arena. We hope to \nparticipate in shaping the international rules of the road that are emerging \naround AI.\n| SYSTEM 3: INFRASTRUCTURE & ENVIRONMENT\n48\nThe increasing scale and proliferation of AI models have driven exponential \ngrowth in demand for chips that can support AI workloads (compute), such as \nGPUs. However, GPUs are in short supply, and we face intense global competition \nto access them. In addition, the resource-intensive nature of AI workloads requires \nadequate and sustainable infrastructure capacity, which we have to prepare for.\nTo support high value AI activities, Singapore must ensure reliable, localised \naccess to high-performance compute, so that industry, academia, and the \nGovernment have the means to innovate and build here.\n\u2003\n\u25b6\nCareful management is needed, as this infrastructure will take up significant \npower, carbon, water, and land, all of which are limited in Singapore.\n\u2003\n\u25b6\nWhile most of the compute supply is expected to go toward industry use, we \nwill direct a small proportion towards meritorious use cases that build up \nour local research and industry capabilities, or are in service of the Public \nGood.\nWe will also work with sustainability leaders in this space, to help Singapore \nachieve our AI ambitions while meeting our sustainability goals.\nENABLER 7\nCompute\nSYSTEM 3: INFRASTRUCTURE & ENVIRONMENT | \n49\nSignificantly increase high-performance compute \navailable in Singapore\nAction 10\nWe will actively crowd in a significant amount of compute to Singapore to support \nour growing AI needs, especially in areas of national interest. To this end, we will:\n\u2003\n\u25b6\nDeepen our substantive partnerships with major compute players, ranging \nfrom chipmakers to Cloud Service Providers (CSPs), to secure local access to \ncompute capacity.\n\u2003\n\u25b6\nSupport Singapore-based compute with the required resource envelope. \nWe will ensure that there is sufficient carbon budget and power allocated \nto support data centres, that house GPUs or their equivalents, to process \nAI workloads in the near term. In the medium to longer term, we will chart \na roadmap towards the growth of net-zero, green data centres that are \npowered by renewable energy. This includes continued collaborations with \nindustry on innovative proposals to push the sustainability envelope.\n\u2003\n\u25b6\nManage a small subset of Singapore-based GPUs or their equivalent, to \nsupport meritorious use cases for capability building, innovation, and the \nPublic Good.\n| SYSTEM 3: INFRASTRUCTURE & ENVIRONMENT\n50\nAI activity is built upon confident, reliable access to high-quality, relevant \ndatasets. Data remains one of the essential factors of production for AI.\nToday, there are more avenues for AI development to overcome the traditional \nobstacles of data availability and access. The open-source movement has made \nlarge quantities of data available, while private sector data marketplaces also \nfacilitate the commercial trading of datasets. Meanwhile, it has become easier \nto create and augment structured datasets for AI, including through the use of \nsynthetic data.\nNational efforts to maximise value from data should therefore pivot away \nfrom access and availability, to address other emerging factors. These include \nimproving the quality of datasets, and ensuring that data use for AI development \nis context-appropriate and operates within trusted data sharing frameworks. We \nare also keen to make more government datasets available to solve AI problems \nof high national priority. Done well, we will demonstrate to AI developers that \nSingapore is a conducive place for responsible AI development.\n\u2003\n\u25b6\nWe want to uncover and avail more good data that AI Activity Drivers can \nleverage. While Singapore may not have the biggest datasets where size of \npopulation is the key determinant, the breadth and depth of our economic \nbase suggest that meaningful datasets can still be mined to generate useful \ninsights and derive new value.\nENABLER 8\nData\nSYSTEM 3: INFRASTRUCTURE & ENVIRONMENT | \n51\n\u2003\n\u25b6\nSingapore should continue to lean forward to support industry efforts to \nemploy data for AI. Importantly, given increased concerns around data \nsecurity, we should invest in Privacy-Enhancing Technologies (PETs), and \nother novel approaches that can address barriers around data protection \nand sharing, to encourage more data flows.\n\u2003\n\u25b6\nThe Government should also support the data needs of meritorious use \ncases that serve the Public Good, particularly if these are aligned with our \nnational priorities but are not commercially attractive. This will ensure that \nsuch projects do not fall through the cracks.\n| SYSTEM 3: INFRASTRUCTURE & ENVIRONMENT\n52\nBuild capabilities in data services and Privacy-\nEnhancing Technologies\nAction 11\nSingapore will build up our data management capabilities so that more of us \ncan store, manage, share, and govern the data that we have, enabling us to draw \nnew insights and uncover more value. To do so, we will need to harness new \ntechnologies that will enable safe and trusted data sharing, and grow more \ncapabilities to do so at scale.\n\u2003\n\u25b6\nWe will develop capabilities in PETs. PETs enable businesses to access other \ndatasets in a privacy-preserving manner, hence extending the pool of data \nfrom which they can derive insights. The Government will support research \nand development for PETs, especially in targeted areas like synthetic \ndata generation, data annotation, federated learning, and homomorphic \nencryption, and expand practical measures like regulatory sandboxes and \nguidelines to promote experimentation with PETs.\nPET Sandbox\nFeature Story\nTo facilitate industry experimentation with PETs, IMDA launched Singapore\u2019s first PET \nSandbox in July 2022, to provide opportunities for companies to work with trusted PET \nsolution providers to develop use cases and pilot PETs. This recognises that PETs are still \nin their infancy, and there is much to learn about using them in a real-world environment.\nThe PET Sandbox matchmakes use case owners to a panel of PET digital solution providers, \nprovides grant support to user companies to scope and implement pilot projects, and \nprovides regulatory support to ensure that the deployment of PETs meets compliance \nguidelines.\nSYSTEM 3: INFRASTRUCTURE & ENVIRONMENT | \n53\nUnlock Government data for use cases that serve the \nPublic Good\nAction 12\nSingapore will selectively unlock more public sector data for AI development \nthat serves the Public Good. We will assess the feasibility of setting up a \u201cdata \nconcierge\u201d within the Government, to engage AI Activity Drivers and Creators, \nand identify and facilitate access to the datasets that they require for their AI \ndevelopment activities.\n\u2003\n\u25b6\nFor public sector datasets, the data concierge will help AI Activity Drivers and \nCreators discover and obtain access to appropriate public sector datasets. \nWhere these are not already open-access, the data concierge will help unlock \nclosed datasets through brokering data sharing agreements. It could also \nwork upstream with agencies to collect data based on the client\u2019s needs. \nThis will be underpinned by a data-sharing evaluation framework that takes \nthe \u201cPublic Good\u201d dimension into account.\n\u2003\n\u25b6\nFor private sector datasets, the Government can consider stepping in for \nselected high-value use cases where we can play a catalytic role. In such \ncases, the data concierge can explore facilitating handshakes between \nprivate industry entities as a trusted intermediary, to aggregate and produce \ndata derivatives.\nWe will continue our thought leadership around the progressive use of data \nfor AI development, such as advocating for trusted cross-border data flows and \nproviding guidelines to businesses to educate and facilitate greater data use.\n| SYSTEM 3: INFRASTRUCTURE & ENVIRONMENT\n54\nTrust and safety underwrite confidence in Singapore\u2019s AI landscape. They lie at \nthe heart of our interventions to deliver AI for the Public Good.\n\u2003\n\u25b6\nWe will endeavour for AI to be developed and deployed in a safe, trustworthy, \nand responsible manner. The Government will institutionalise appropriate \ngovernance and security frameworks for AI systems. The ultimate aim is to \nestablish a trusted environment for AI, where people can have the confidence \nthat their interests are protected when interacting with AI.\n\u2003\n\u25b6\nWe must retain agility in our regulatory approaches. AI will continue to \nevolve, and no party has full sight of the risks that might emerge. It is \nonly through experimentation and exploration that the AI community can \ndeepen its understanding of AI, and discover and address potential risks. \nThe Government must therefore take a pragmatic approach \u2013 supporting \nexperimentation and innovation, while still ensuring that AI is developed \nand used responsibly, in line with the rule of law and the safeguards we have \nput in place. Where existing regulatory frameworks need to be updated, we \nwill do so thoughtfully and in concert with others, accounting for the global \nnature of AI.\nENABLER 9\nTrusted Environment\nSYSTEM 3: INFRASTRUCTURE & ENVIRONMENT | \n55\nThere is a range of potential risks around AI, spanning from concerns around \nmodel quality and fair use, to fears around the loss of control and existential \nrisks. Singapore remains open to engaging with all perspectives, to enhance our \nunderstanding of the risk landscape and to inform our most urgent priorities for \nrisk mitigation. For a start:\n\u2003\n\u25b6\nWe must ensure that AI systems are well-developed, reliable, and resilient. \nThis requires paying close attention to the model development process, to \nensure that the output of models is not biased, inaccurate, or erroneous. \nAI models should also be aligned with the appropriate set of human and \ncultural values. Existing international conversations around AI governance \nand safety are largely centred on these concerns.\n\u2003\n\u25b6\nWe must prevent AI models from being used in malicious or harmful ways, \nand secure them against adversarial attacks. If AI is used carelessly, it can \npotentially amplify negative outcomes like discrimination, anti-competitive \nbehaviours, or intellectual property infringement. It can also be deliberately \nmisused against us, to supercharge existing threats (e.g. scams, cyber-\nattacks, and mis/disinformation) in terms of scale, speed, and sophistication. \nWe must remain vigilant against these risks to maintain digital trust.\nThe Government will take differentiated approaches to managing risks to and \nfrom AI, ranging from regulatory moves to voluntary guidelines, recognising that \nAI will continue to evolve. We will need a deeper understanding of how AI works, \nwhat benchmarks to use, and what testing is appropriate, and we look forward to \ndeveloping these perspectives together with other stakeholders.\n| SYSTEM 3: INFRASTRUCTURE & ENVIRONMENT\n56\nEnsure fit-for-purpose regulatory environment for AI\nAction 13\nThe Government will continue to maintain a regulatory environment for AI that is \npro-innovation while ensuring appropriate guardrails. We will:\n\u2003\n\u25b6\nUpdate AI governance frameworks to address novel risks. We will regularly \nreview and adjust frameworks like the Model AI Governance Framework \nand AI Verify to reflect emerging principles, concerns, and technological \ndevelopments (e.g. Generative AI). As part of this, it will be important to \nestablish clear responsibilities for actors across the AI supply chain. This \nbaseline guidance will give clarity to AI developers and users on how to be \nresponsible in the design and use of AI.\nAI Verify\nFeature Story\nAI Verify is an AI governance testing framework and software toolkit developed by IMDA. \nThe testing framework consists of 11 AI ethics principles which are consistent with \ninternationally recognised frameworks around the world. AI Verify helps organisations \nvalidate the performance of their AI systems against these principles through standardised \ntests. IMDA first released AI Verify in May 2022 for an international pilot.\nIn June 2023, IMDA also set up the AI Verify Foundation to harness the collective power \nand contributions of the global open-source community, in developing AI Verify testing \ntools. The not-for-profit Foundation will boost AI testing capabilities and assurance to \nmeet the needs of companies and regulators globally.\nSince its launch in June 2023, the AI Verify Foundation has \ngrown to have over 90 corporate members, who work to de\u00ad\nvelop AI testing tools to enable responsible AI.\nSYSTEM 3: INFRASTRUCTURE & ENVIRONMENT | \n57\n\u2003\n\u25b6\nContinue working with partners on R&D, particularly around alignment \nand evaluations. We will work with partners to translate guidelines into \nappropriate technical standards, tools, and services that can be practically \napplied. These will also be supported by policy measures including regulatory \nsandboxes, pilots for solutions such as watermarking and model cards, \nand capability development to nurture a domestic Testing, Inspection, and \nCertification (TIC) sector.\n\u2003\n\u25b6\nDesign interventions that are risk-based, tiered, and adapted for specific \nvertical sectors and horizontal applications. This recognises that every use \ncase carries a different set of considerations and risks, and would therefore \nrequire different risk thresholds and context-specific risk management \napproaches. For instance, using AI to improve the user experience for video \ngames would differ considerably from a use case where AI assists hiring \nmanagers in selecting potential candidates. To ensure coherence with our \nnational position on AI governance, the Government will also establish a \ncommon platform for regulatory agencies to coordinate on AI developments \nin their sectors, and share best practices when governing AI.\n\u2003\n\u25b6\nConsider updates to broader standards and laws to support effective AI use.\n\u2003\n\u25b6\nContribute actively to international discourse on AI governance, to raise \ncapacity, share best practices, and shape rules around AI, together with the \ninternational community.\nUN High-Level Advisory Body on AI\nFeature Story\nIn October 2023, the UN Secretary-General announced the creation of a new multi-\nstakeholder High-Level Advisory Body (HLAB) on the risks, opportunities, and international \ngovernance of AI. The HLAB comprises 39 experts from across UN Member States, and it \nwill support the international community\u2019s efforts to govern AI. Dr He Ruimin, the Chief AI \nOfficer and Deputy Chief Digital Technology Officer of the Government of Singapore, has \nbeen selected as one of the members of the HLAB.\n| SYSTEM 3: INFRASTRUCTURE & ENVIRONMENT\n58\nRaise security and resilience baseline for AI\nAction 14\nThe Government will also elevate the security and resilience baseline for all \nsystem owners using AI.\n\u2003\n\u25b6\nIn the short term, we will work with partners to update cybersecurity \ntoolkits for enterprises and individual users, to address AI-related risks. \nThese include advisories and actionable guidelines on how system owners \ncan augment their cybersecurity foundations to enable secure AI adoption.\n\u2003\n\u25b6\nIn the longer term, we will also coordinate efforts to share best practices and \nensure alignment to improve AI security, including through the development \nof standards and solutions for AI security. This will involve working with the \nprivate sector to co-create technical guidelines and solutions to secure AI use \ncases, and to leverage crowdsourcing initiatives to evaluate risks to AI (e.g. \njoint red teams to test AI systems). Such close partnerships with the private \nsector, including on capacity building and public education programmes, \nwill raise baseline AI security capabilities across the ecosystem.\nSYSTEM 3: INFRASTRUCTURE & ENVIRONMENT | \n59\nToday\u2019s international AI landscape is contested and fragmented.\n\u2003\n\u25b6\nAI has become a major front of competition. Global powers like the US and \nChina dominate the global AI landscape in research outcomes, innovation \ncapacity, and talent networks. In turn, critical and emerging technologies \nlike AI have become a pre-eminent domain for geostrategic contestation.\n\u2003\n\u25b6\nWhile the international community is working to develop consensus around \nthe responsible development and deployment of AI, efforts remain nascent \nat this point. All countries recognise the importance of managing AI well, \nand several have offered ideas on how the international community might \ncooperate on this. It is still early days, and it will take some time before the \ninternational community converges around any set of agreed principles.\nENABLER 10\nLeader in \nThought and Action\n| SYSTEM 3: INFRASTRUCTURE & ENVIRONMENT\n60\nNevertheless, it remains critical for Singapore to stay engaged internationally, in a \nmulti-party, multi-stakeholder fashion, to work towards a common understanding \non AI.\n\u2003\n\u25b6\nAI is a technology \u201cwithout a passport\u201d. Its impact cannot be easily contained \nwithin any single country. It is therefore crucial for all countries to converge \non efforts to make AI systems safer, and to avoid AI creating strategic risks \nand instability. This will require all countries to set aside the instincts of \nprotectionism, to steer AI as a force for good. Singapore is determined to play \nour part, and we will continue to participate constructively at international \nfora on AI.\n\u2003\n\u25b6\nSuch conversations on the trajectory of AI must be inclusive. AI will affect \nand potentially disrupt all of us. It is therefore important to widen the \nconversation, beyond the few who are developing cutting-edge models, \nto take in views from other countries and stakeholders, and build up the \ncapacity for meaningful efforts to address AI\u2019s impact.\n\u2003\n\u25b6\nIndustry also plays a critical role to shape more responsible AI. We welcome \nongoing industry-led efforts, including those by leading AI companies, as \nvaluable contributions to the broader conversation on how to govern AI \nmodels better. We look forward to learning together with them.\nSYSTEM 3: INFRASTRUCTURE & ENVIRONMENT | \n61\nEstablish Singapore as an ambitious and pragmatic \ninternational partner on AI innovation and governance\nAction 15\nSingapore is committed to being a serious and reliable international partner on \nAI. This is important for sustaining our international position as a trusted hub. \nWe will:\n\u2003\n\u25b6\nContinue to grow Singapore\u2019s international networks with key partner \ncountries and leading AI companies.\n\u2003\n\u25b6\nIncrease international mindshare in practical and risk-based approaches to \nAI. Singapore has gained global attention because of early efforts to integrate \nAI in key sectors such as finance, trade, and healthcare, as well as pragmatic \nGovernment tools such as Pair. We have developed governance frameworks \nand open-source testing toolkits such as AI Verify. These demonstrate \nSingapore\u2019s commitment to developing and deploying AI well, and we will \nleverage these gains to be a global pace-setter at the forefront of AI.\n| SYSTEM 3: INFRASTRUCTURE & ENVIRONMENT\n62\nWe will contribute to international AI developments by:\n\u2003\n\u25b6\nAnchoring key bilateral relationships with selected partners from government \nand industry, through substantive initiatives and technical cooperation. \nThese will allow Singapore to \u201cstart small and move quickly\u201d, to establish \ncommon ground with like-minded partners, as pathfinders to broad-based \nmultilateral cooperation.\n\u2003\n\u25b6\nDemonstrating alignment with key international fora and supporting \nworthwhile platforms. Singapore will support and actively participate in \nsubstantive multilateral, multi-stakeholder, or plurilateral initiatives, that \nseek to achieve an inclusive, practical, and rules-based global environment \nfor AI.\n\t\n\u2003\n\u25b6\nSharing Singapore\u2019s experience and curating meaningful partnerships for \ncapacity building. We will actively profile Singapore\u2019s approaches to AI \nthrough public engagements and conferences such as Asia Tech x Singapore \nand SCAI. We are the convenor of the Forum of Small States (FOSS)7, which now \nhas a digital pillar of engagement. Together with government and industry \npartners, we will develop AI-related capacity building initiatives to benefit \nthe 108 members of FOSS.\n7 Singapore established FOSS as an informal and non-ideological grouping of small states in \n1992, and has served as its Chair since. FOSS now comprises 108 countries across all geographical \nregions and at all levels of development, and meets several times a year to discuss issues of \nconcern to small states. In 2022, we introduced Digital FOSS as a new pillar of engagement within \nFOSS.\nSYSTEM 3: INFRASTRUCTURE & ENVIRONMENT | \n63\nATxSG brings together leaders and industry decision-mak\u00ad\ners across the global technology ecosystem to discuss the \nlatest trends in technologies including AI.\nAsia Tech x Singapore\nFeature Story\nAsia Tech x Singapore (ATxSG) is a platform to empower the technology community to \ndiscuss the latest trends and explore the critical intersections of technology, economy, \nand society. The third edition of ATxSG, organised by IMDA, was held in June 2023, and \nbrought together over 17,000 global leaders and industry decision-makers across the \nglobal technology ecosystem.\nA key track of ATxSG is the ATxAI Conference, where thought leaders, industry experts, \nand policy makers from leading AI countries convene and discuss strategic values of AI \ngovernance and technology trends, as well as showcase real-life implementations of AI. \n64 speakers and over 2,900 attendees from around the world have participated in ATxAI \nover the past three years.\nA WHOLE-OF-NATION MOVEMENT | \n65\nTo realise the broad and transformative impact of AI, we will actively forge multiple \npartnerships across our economy, society, and the world. Singapore\u2019s NAIS 2.0 is \ntherefore designed to unfold as a national movement on the global stage.\nAI will not only contribute to Singapore\u2019s sustained economic growth and the \nglobal competitiveness of our industries, it will also create better jobs, a more \nproductive labour force, and substantially improve the quality of life for our \npeople.\nSingapore is ready. We welcome all who share our vision of AI for the Public Good \nto join us on the next bound of our AI adventure.\nA Whole-of-Nation\nMovement\nPublished 4 December 2023\nCopyright \u00a9 2023 \nGovernment of the Republic of Singapore\nYou may download, view, print, and reproduce this \ndocument without modification, but only for non-\ncommercial use. All other rights are reserved.\n", "metadata": {"country": "Singapore", "year": "2023", "legally_binding": "no", "binding_proof": "This is a non-binding strategic roadmap released by the Singapore government to guide national AI development and deployment across public and private sectors.", "date": "12/04/2023", "regulator": "Smart Nation and Digital Government Office (SNDGO), Prime Minister\u2019s Office", "type": "national AI strategy", "status": "pending", "language": "English", "use_cases": "[2, 5, 6]"}}
{"_id": "686acbcee2af9fb1ff3ba991", "title": "Model AI Governance Framework", "source": "https://www.pdpc.gov.sg/-/media/Files/PDPC/PDF-Files/Resource-for-Organisation/AI/SGModelAIGovFramework2.pdf", "text": "MODEL ARTIFICIAL INTELLIGENCE GOVERNANCE FRAMEWORK   1\n \nARTIFICIAL INTELLIGENCE\nGOVERNANCE FRAMEWORK\nMODEL\nSECOND EDITION\nSUMMARY OF UPDATES  ...................................................................................  4\nFOREWORD  ..........................................................................................................  7\n1. PREAMBLE ........................................................................................................  9\n2. INTRODUCTION  ...............................................................................................  12\nObjectives ...............................................................................................................  13\nGuiding Principles for the Model Framework .......................................................  15\nAssumptions ...........................................................................................................  17\nDefinitions ....... ........................................................................................................  18\n3. MODEL AI GOVERNANCE FRAMEWORK  ..................................................  19\nInternal Governance Structures and Measures  ......................................................  21\nDetermining the Level of Human Involvement in AI-augmented Decision-making  .....  28\nOperations Management ......................................... ..............................................  35\nStakeholder Interaction and Communication ........................................................  53\nANNEX A  \nFor Reference: A Compilation of Existing AI Ethical Principles .............................  64\nANNEX B\nAlgorithm Audits  ......................................................................................................  67\nACKNOWLEDGEMENTS  .....................................................................................  68\nTABLE OF CONTENTS\n4\nEDITION\nDATE \nRELEASED\nSUMMARY\nFIRST\n23 January \n2019\nReleased the Model AI Governance Framework (First \nEdition) at the 2019 World Economic Forum Annual \nMeeting in Davos, Switzerland.\nSECOND\n21 January \n2020\nReleased the Model AI Governance Framework \n(Second Edition) at the 2020 World Economic Forum \nAnnual Meeting in Davos, Switzerland.\nThe key changes include:\n\u2022 \nAddition of industry examples in each section to \nillustrate how organisations have implemented AI \ngovernance practices in that section; \n\u2022 \nUpdating the titles of two sections to accurately \nreflect their content:\n \u00bb\n\u201cDetermining AI Decision-Making Model\u201d to \n\u201cDetermining the level of human involvement \nin AI-augmented decision-making\u201d;\n \u00bb\n\u201cCustomer Relationship Management\u201d to \n\u201cStakeholder  interaction  and  communication\u201d.\nSection-specific changes include the following:\nDetermining the level of human involvement in AI-\naugmented decision-making\n\u2022 \nClarified the \u201chuman-over-the-loop\u201d approach \nby explaining the human\u2019s supervisory role in AI-\naugmented decision-making.\n\u2022 \nClarified that organisations can consider other \nfactors such as the nature and reversibility of harm \nand operational feasibility in determining the \nlevel of human involvement in an organisation\u2019s \ndecision-making process involving AI.\nSUMMARY\nOF UPDATES\nMODEL ARTIFICIAL INTELLIGENCE GOVERNANCE FRAMEWORK   5\nEDITION\nDATE\nRELEASED\nSUMMARY\nSECOND\n21 January \n2020\nOperations management\n\u2022 \nProvided guidance to organisations to adopt a risk-\nbased approach when implementing measures by:\n \u00bb\nIdentifying features or functionalities with the \ngreatest impact on stakeholders;\n \u00bb\nConsidering which measure would be most \neffective in building trust with stakeholders.\n\u2022 \nProvided guidance on the necessity and relevance \nof the various measures: \n \u00bb\nClarified that datasets used for building AI \nmodels may include both personal and non-\npersonal data;\n \u00bb\nIncluded new measures such as robustness, \nreproducibility and auditability and provided \nexamples of helpful practices for these \nmeasures.\nStakeholder interaction and communication\n\u2022 \nHighlighted the importance of communication \nwith various internal and external stakeholders. \n\u2022 \nHighlighted the need to consider the purpose \nand context when interacting with the various \nstakeholders.\n\u2022 \nProvided suggestions on the level of information \nto be provided when interacting with various \nstakeholders. \nAnnex A \u2013 For reference: a compilation of existing \nAI ethical principles (Annex A)\n\u2022 \nClarified that the list of AI ethical principles \nprovided is a compilation of existing AI principles \nthat is for reference only. Not all listed principles \nare addressed in the Model AI Governance \nFramework. \nOrganisations \ncould \nconsider \nincorporating other principles in Annex A into \ntheir own corporate principles. \n6\nEDITION\nDATE\nRELEASED\nSUMMARY\nSECOND\n21 January \n2020\nAnnex B \u2013 Algorithm Audits\n\u2022 \nClarified that an algorithm audit is to be conducted \nonly if it is necessary to discover the actual \noperations of algorithms comprised in models, \nand only at the request of a regulator (as part of a \nforensic investigation). \nAnnex C \u2013 Use Case\n\u2022 \nAnnex C has been removed. Instead, a separate \nCompendium of Use Cases has been published \n(go.gov.sg/ai-gov-use-cases).\nIn 2019, the world saw significant advances in the sophistication and pervasive \nuse of artificial intelligence (\u201cAI\u201d). For instance, we witnessed the emergence \nof next-generation AI-powered natural text generators like GPT-2, which can \ngenerate passages that are difficult to distinguish from human writing. We \nalso saw the development of Dactyl, a robotic hand, which uses reinforcement \nlearning to grasp and manipulate common household objects with human-like \ndexterity. These examples attest to the speed of AI\u2019s advancement and how it\nwill become ubiquitous in our daily lives.\nThe discourse on AI ethics and governance has also moved forward. Over the \nlast two years, governments and international organisations have begun issuing \nprinciples, frameworks and recommendations on AI ethics and governance. \nIn January 2019, Singapore launched our Model AI Governance Framework \n(\u201cModel Framework\u201d) at the World Economic Forum in Davos. The Model \nFramework\u2019s unique contribution to the global discourse on AI ethics lies in \ntranslating ethical principles into practical recommendations that organisations \ncould readily adopt to deploy AI responsibly. We are heartened by the diversity \nof organisations that have adopted the practices outlined in the Model \nFramework, which underscores its ease-of-use and relevance.\nSingapore is proud to launch the second edition of the Model Framework. This \nedition incorporates the experiences of organisations that have adopted AI, and \nfeedback from our participation in leading international platforms, such as the \nEuropean Commission\u2019s High-Level Expert Group and the OECD Expert Group \non AI. Such input has enabled us to provide clearer and effective guidance for \norganisations to implement AI responsibly.\nFOREWORD\nMODEL ARTIFICIAL INTELLIGENCE GOVERNANCE FRAMEWORK   7\n8\nSingapore\u2019s Info-communications Media Development Authority (\u201cIMDA\u201d) \nand Personal Data Protection Commission (\u201cPDPC\u201d) have also partnered \nthe World Economic Forum Centre for the Fourth Industrial Revolution to \ndevelop an Implementation and Self-Assessment Guide for Organisations \n(\u201cISAGO\u201d). The ISAGO complements the Model Framework by allowing \norganisations to assess the alignment of their AI governance practices with the \nModel Framework, while providing useful industry examples and practices. \nWe are also publishing a Compendium of Use Cases, which features real-\nworld examples of how organisations have implemented or aligned their AI \ngovernance practices with the Model Framework. Together, these initiatives \nenable any organisation to establish and refine its AI governance practices in \nconcrete and practical ways.\nThese initiatives play a critical role in Singapore\u2019s National AI Strategy. \nThey epitomise our plans to develop a human-centric approach towards \nAI governance that builds and sustains public trust. They also reflect our \nemphasis on co-creating an AI ecosystem in a collaborative and inclusive \nmanner. The Model Framework and ISAGO will pave the way for future \ndevelopments, such as the training of professionals on ethical AI deployment, \nand laying the groundwork for Singapore, and the world, to better address \nAI\u2019s impact on society.\nThe steps we take today will leave an indelible imprint on our collective future. \nThe Model Framework has been recognised as a firm foundation for the \nresponsible use of AI and its future evolution. We will build on this momentum \nto advance a human-centric approach to AI \u2013 one that facilitates innovation \nand safeguards public trust \u2013 to ensure AI\u2019s positive impact on the world for \ngenerations to come.\nS Iswaran\nMinister for Communications and Information\nSingapore\nJanuary 2020\nMODEL ARTIFICIAL INTELLIGENCE GOVERNANCE FRAMEWORK   9\n1. PREAMBLE\nMODEL ARTIFICIAL INTELLIGENCE GOVERNANCE FRAMEWORK   9\n10\n1.1  \nThe Model Framework focuses primarily on four broad areas: \ninternal governance structures and measures, human involvement \nin AI-augmented decision-making, operations management and \nstakeholder interaction and communication.\n \nWhile the Model Framework is certainly not limited in ambition, \nit is ultimately limited by form, purpose and practical considerations \nof scope. With that in mind, several caveats bear mentioning. \nThe Model Framework is \u2013\na.  Algorithm-agnostic \n \nIt does not focus on specific AI or data analytics \nmethodology. It applies to the design, application and \nuse of AI in general.\nb. Technology-agnostic \n \nIt does not focus on specific systems, software or \ntechnology, and will apply regardless of development \nlanguage and data storage method.\nc. Sector-agnostic\n \nIt serves as a baseline set of considerations and \nmeasures for organisations operating in any sector to \nadopt. Specific sectors or organisations may choose \nto include additional considerations and measures or \nadapt this baseline set to meet their needs. The PDPC \nencourages and will collaborate with public agencies \nadapting the Model Framework for their sectors.\nd. Scale- and Business-model-agnostic \n \nIt does not focus on organisations of a particular scale \nor size. It can also be used by organisations engaging \nin business-to-business or business-to-consumer \nactivities and operations, or in any other business model.\nMODEL ARTIFICIAL INTELLIGENCE GOVERNANCE FRAMEWORK   11\n1.2 \nIt is recognised that there are a number of issues that are closely \ninterrelated to the ethical use and deployment of AI. This Model \nFramework does not focus on these specific issues, which are \noften sufficient in scope to warrant separate study and treatment. \nExamples of these issues include:\na. Articulating a new set of ethical principles for AI. There \nare already a number of attempts globally in \nestablishing a universal set of principles. While a \nconsistent core set of ethical principles is emerging, \nthere is also a penumbra of variation across cultures, \njurisdictions and industry sectors. The Model \nFramework uses existing and common AI ethical \nprinciples (a compilation of which is set out in Annex \nA) and converts them into implementable practices.\nb. Providing model frameworks and addressing issues \naround data sharing, whether between the public \nand private sectors or between organisations or \nwithin consortia. There are a number of guides that \nare relevant, such as the IMDA\u2019s Trusted Data \nSharing Framework and the Guide to Data Valuation \nfor Data Sharing.\nc. Discussing issues relating to the legal liabilities \nassociated with AI, intellectual property rights, and \nsocietal impacts of AI (e.g. on employment, \ncompetition, unequal access to AI products and \nservices by different segments of society, AI \ntechnologies falling into hands of wrong people), etc. \nThese issues are nevertheless pertinent and can be \nexplored separately through platforms such as the \nCentre for AI and Data Governance established in \nthe Singapore Management University School of Law.\n12\n2. INTRODUCTION\n12\nMODEL ARTIFICIAL INTELLIGENCE GOVERNANCE FRAMEWORK   13\nOBJECTIVES\n2.1 \nThe exponential growth in data and computing power has fuelled \nthe advancement of data-driven technologies such as AI. AI can \nbe used by organisations to provide new goods and services, \nboost productivity, enhance competitiveness, ultimately leading \nto economic growth and a better quality of life. As with any new \ntechnology, however, AI also introduces new ethical, legal and \ngovernance challenges. These include risks of unintended \ndiscrimination potentially leading to unfair outcomes, as well as \nissues relating to consumers\u2019 knowledge about how AI is involved \nin making significant or sensitive decisions about them.\n2.2 \nThe PDPC,1 with advice from the Advisory Council, proposes this \nsecond edition of the living and voluntary Model Framework as \na general, ready-to-use tool to enable organisations that are \ndeploying AI solutions at scale to do so in a responsible manner. \nThis Model Framework is not intended for organisations that are \ndeploying updated commercial off-the-shelf software packages \nthat happen to now incorporate AI in their feature set.\n2.3 \nThis voluntary Model Framework provides guidance on the key \nissues to be considered and measures that can be implemented. \nAdopting this Model Framework will require tailoring the measures \nto address the risks identified for the implementing organisation. \nThe Model Framework is intended to assist organisations to \nachieve the following objectives:\na. Build stakeholder confidence in AI through \norganisations\u2019 responsible use of AI to manage \ndifferent risks in AI deployment.\nb. Demonstrate reasonable efforts to align internal \npolicies, structures and processes with relevant \naccountability-based practices in data management \nand protection (e.g. the Personal Data Protection Act \n2012 (\u201cPDPA\u201d) and the OECD Privacy Principles).\n1  Under section 5 of Singapore\u2019s Personal Data Protection Act 2012, the IMDA is designated \n   as the PDPC.\n14\n2.4 \nTo assist organisations in implementing the Model Framework, \nthe PDPC has also prepared a complementary ISAGO. The ISAGO \nhelps organisations assess the alignment of their AI governance \npractices and processes with the Model Framework. It also provides \nadditional useful industry references and examples that further \nclarify the recommendations set out in this Model Framework. \n2.5 \nThe extent to which organisations adopt the recommendations \nin this Model Framework depends on several factors, including \nthe nature and complexity of the AI used by organisations, the \nextent to which AI is employed in the organisations\u2019 decision-\nmaking, and the severity and probability of the impact of the \nautonomous decision on individuals. \n2.6 \nTo elaborate: AI technologies may be used to augment a human \ndecision-maker or to autonomously make a decision. For instance, \nthe impact of an autonomous decision in medical diagnosis is \narguably greater than that in a product recommendation. The \ncommercial risks of AI deployment is therefore proportionate to \nthe impact on individuals. Generally, where the cost of \nimplementing AI technologies in an ethical manner outweighs \nthe expected benefits, organisations should consider whether \nalternative non-AI solutions should be adopted. The considerations \nand recommendations set out in this Framework are intended \nto guide organisations that have decided to deploy AI \ntechnologies at scale.\nMODEL ARTIFICIAL INTELLIGENCE GOVERNANCE FRAMEWORK   15\nGUIDING PRINCIPLES\n2.7 \nThe Model Framework is based on two high-level guiding \nprinciples that promote trust in AI and understanding of the use \nof AI technologies:\na. Organisations using AI in decision-making should \nensure that the decision-making process is explainable, \ntransparent and fair. \n \nAlthough perfect explainability, transparency and \nfairness are impossible to attain, organisations should \nstrive to ensure that their use or application of AI is \nundertaken in a manner that reflects the objectives \nof these principles as far as possible. This helps build \ntrust and confidence in AI. \nb. AI solutions should be human-centric.\n \nAs AI is used to amplify human capabilities, the \nprotection of the interests of human beings, including \ntheir well-being and safety, should be the primary \nconsiderations in the design, development and \ndeployment of AI.\nOrganisations should ensure that \nAI decision-making processes are \nexplainable, transparent and fair, \nwhile AI solutions should be \nhuman-centric.\n16\n2.8 \nLike other technologies, AI aims to increase human productivity. \nHowever, unlike earlier technologies, some aspects of autonomous \npredictions or decisions made by AI may not be fully explainable. \nAs AI technologies can make decisions that affect individuals, or \nhave a significant impact on society, markets or economies, \norganisations should consider using this Model Framework to \nguide their deployment of AI.\n2.9 \nOrganisations should detail a set of ethical principles when they \nembark on deployment of AI at scale within their processes or \nto empower their products and/or services. Where necessary, \norganisations may wish to refer to the compilation of AI ethical \nprinciples in Annex A. As far as possible, organisations should \nalso review their existing corporate values and incorporate the \nethical principles that they have articulated. Some of the ethical \nprinciples (e.g. safety) may be articulated as risks that can be \nincorporated into the corporate risk management framework. \nThe Model Framework is designed to assist organisations by \nincorporating ethical principles into familiar and pre-existing \ncorporate governance structures, and thereby aid in guiding the \nadoption of AI in an organisation. \nMODEL ARTIFICIAL INTELLIGENCE GOVERNANCE FRAMEWORK   17\nASSUMPTIONS\n2.10 The Model Framework aims to discuss good data management \npractices in general. The Model Framework is mainly applicable \nto machine learning models (as compared to pure decision \ntree-driven AI models).\n2.11 The Model Framework does not address the risk of catastrophic \nfailure due to cyber-attacks on an organisation heavily dependent \non AI. Organisations remain responsible for ensuring the \navailability, reliability, quality and safety of their products and \nservices, regardless of whether AI technologies are used. \n2.12 Adopting this voluntary Model Framework will not absolve \norganisations from compliance with current laws and regulations. \nHowever, as this is an accountability-based framework, adopting \nit will assist organisations in demonstrating that they had \nimplemented accountability-based practices in data management \nand protection, e.g. the PDPA and OECD Privacy Principles.\n2.13 Further, it should be noted that certain industry sectors (such \nas in the finance, healthcare, and legal sectors) may be regulated \nby existing sector-specific laws, regulations or guidelines \nrelevant to the sector. For example, the Monetary Authority of \nSingapore published the Principles to Promote Fairness, Ethics, \nAccountability and Transparency in the Use of Artificial \nIntelligence and Data Analytics in Singapore\u2019s Financial Sector \n(the \u201cFEAT Principles\u201d) to provide guidance to firms that use \nAI and data analytics to offer financial products and services.2 \nOrganisations are advised to remain mindful of such laws, \nregulations and guidelines, as adopting the Model Framework \ndoes not mean that organisations are in compliance with such \nsector-specific laws, regulations or guidelines.\n2  Monetary Authority of Singapore, \u201cPrinciples to Promote Fairness, Ethics, Accountability \n     and Transparency (FEAT) in the Use of Artificial Intelligence and Data Analytics in Singapore\u2019s \n     Financial Sector\u201d (12 November 2018) <https://www.mas.gov.sg/publications/monographs \n    or-information-paper/2018/FEAT>.\n18\nDEFINITIONS\n2.14 The following simplified diagram depicts the key stakeholders \nin an AI adoption process discussed in the Model Framework. \nThe adoption process does not distinguish between business-\nto-consumer (\u201cB2C\u201d), business-to-business (\u201cB2B\u201d), and business-\nto-business-to-consumer (\u201cB2B2C\u201d) relationships.\n2.15 Some terms used in AI may have different definitions depending \non context and use. The definitions of some key terms used in \nthis Model Framework are as follows:\nAI Solution Providers\nOrganisations\nIndividuals\nrefers to a set of technologies that seek to simulate human \ntraits such as knowledge, reasoning, problem solving, \nperception, learning and planning, and, depending on the AI model, produce an \noutput or decision (such as a prediction, recommendation, and/or classification). \nAI technologies rely on AI algorithms to generate models. The most appropriate \nmodel(s) is/are selected and deployed in a production system.3 \ndevelop AI solutions or application systems that make \nuse of AI technology. These include not just commercial \noff-the-shelf products, online services, mobile applications, and other software \nthat consumers can use directly, but also B2B2C applications, e.g. AI-powered \nfraud detection software sold to financial institutions. They also include device \nand equipment manufacturers that integrate AI-powered features into their \nproducts, and those whose solutions are not standalone products but are meant \nto be integrated into a final product. Some organisations develop their own AI \nsolutions and can be their own solution providers.\nrefers to companies or other entities that adopt or deploy \nAI solutions in their operations, such as backroom \noperations (e.g. processing applications for loans), front-of-house services (e.g. \ne-commerce portal or ride-hailing app), or the sale or distribution of devices \nthat provide AI-powered features (e.g. smart home appliances). \ncan, depending on the context, refer to persons to whom \norganisations intend to supply AI products and/or services, \nor persons who have already purchased the AI products and/or services. These \nmay be referred to as \u201cconsumers\u201d or \u201ccustomers\u201d as well.\n\u201cAI\u201d\n\u201cAI Solution \nProviders\u201d\n\u201cOrganisations\u201d\n\u201cIndividuals\u201d\nThis definition of AI was adapted from various sources, and contextualised accordingly for the purposes of this Model \nFramework. It should not be taken to be an authoritative or exhaustive definition.\n3\nMODEL ARTIFICIAL INTELLIGENCE GOVERNANCE FRAMEWORK   19\n3. MODEL AI \nGOVERNANCE \nFRAMEWORK\nMODEL ARTIFICIAL INTELLIGENCE GOVERNANCE FRAMEWORK   19\n20\nMODEL AI GOVERNANCE \nFRAMEWORK\n3.1 \nThis Model Framework comprises guidance on measures \npromoting the responsible use of AI that organisations should \nadopt in the following key areas:\na. Internal governance structures and measures \n \nAdapting existing or setting up internal \ngovernance structure and measures to \nincorporate values, risks, and responsibilities \nrelating to algorithmic decision-making.\nb. Determining the level of human involvement \nin AI-augmented decision-making \n \nA methodology to aid organisations in setting \nits risk appetite for use of AI, i.e. determining \nacceptable risks and identifying an appropriate \nlevel of human involvement in AI-augmented \ndecision-making.\nc. Operations management \n \nIssues to be considered when developing, \nselecting and maintaining AI models, including \ndata management.\nd. Stakeholder interaction and communication \n \nStrategies for communicating with an \norganisation\u2019s stakeholders, and the \nmanagement of relationships with them.\n3.2 \nOrganisations adopting this Model Framework may find that not \nall elements are relevant. This Model Framework is meant to be \nflexible, and organisations can adapt the Model Framework to \nsuit their needs and adopting those elements that are relevant.\n3.3 \nTo help organisations better understand the Model Framework, \nwe have included (in each section) illustrations demonstrating \nhow real-world companies have implemented certain practices \ndescribed in that specific section. In addition, the PDPC has \nalso released a Compendium of Use Cases that illustrates how \nvarious local and international organisations have put in place \nAI governance practices that are aligned to all sections of the \nModel Framework.\nMODEL ARTIFICIAL INTELLIGENCE GOVERNANCE FRAMEWORK   21\nINTERNAL GOVERNANCE \nSTRUCTURES AND MEASURES\n3.4 \nThis section is intended to guide organisations in developing \nappropriate internal governance structures that allow organisations \nto have appropriate oversight over how AI technologies are \nbrought into their operations and/or products and services. \n3.5 \nInternal governance structures and measures help to ensure robust \noversight over an organisation\u2019s use of AI. The organisation\u2019s \nexisting internal governance structures can be adapted, and/or \nnew structures can be implemented if necessary. For example, \nrisks associated with the use of AI can be managed within the \nenterprise risk management structure, while ethical considerations \ncan be introduced as corporate values and managed through \nethics review boards or similar structures. \nEthical considerations can be \nintroduced as corporate values and \nmanaged through ethics review \nboards or similar structures.\n3.6 \nOrganisations may also consider determining the appropriate \nfeatures in their internal governance structures. For example, \nwhen relying completely on a centralised governance mechanism \nis not optimal, a de-centralised one could be considered to \nincorporate ethical considerations into day-to-day decision-\nmaking at the operational level, if necessary. The sponsorship, \nsupport and participation of the organisation\u2019s top management \nand its board of directors in the organisation\u2019s AI governance \nare crucial.\n22\nKey roles and responsibilities that can be allocated \ninclude: \ni. Using any existing risk management framework and \napplying risk control measures (see \u201cRisk management \nand internal controls\u201d below) to: \n \n \no \nAssess and manage the risks of deploying AI, \n \nincluding any potential adverse impact on the \n \nindividuals (e.g. who are most vulnerable, how \n \nare they impacted, how to assess the scale of \n \nthe impact, how to get feedback from those \n \nimpacted, etc.). \n \n \no \nDecide on the appropriate level of human \n \ninvolvement in AI-augmented decision-making. \n \no  Manage the AI model training and selection \n \n process. \nc. \n3.7 \nOrganisations may wish to consider including features that \nare relevant to the development of their internal governance \nstructure, such as:\n \n1. Clear roles and responsibilities for the ethical  \n \ndeployment of AI\nResponsibility for and oversight of the various stages \nand activities involved in AI deployment should be \nallocated to the appropriate personnel and/or \ndepartments. If necessary and possible, consider \nestablishing a coordinating body, having relevant \nexpertise and proper representation from across \nthe organisation.   \nPersonnel and/or departments having internal AI \ngovernance functions should be fully aware of their \nroles and responsibilities, be properly trained, and be \nprovided with the resources and guidance needed for \nthem to discharge their duties. \na. \nb. \nMODEL ARTIFICIAL INTELLIGENCE GOVERNANCE FRAMEWORK   23\nii. Maintenance, monitoring, documentation and review \nof the AI models that have been deployed, with a \nview to taking remediation measures where needed. \niii. Reviewing communications channels and interactions \nwith stakeholders to provide disclosure and effective \nfeedback channels.\niv. Ensuring relevant staff dealing with AI systems are \nproperly trained. Where applicable and necessary, \nstaff who are working and interacting directly with \nAI models may need to be trained to interpret AI \nmodel output and decisions and to detect and \nmanage bias in data. Other staff whose work deals \nwith the AI system (e.g. a customer relationship \nofficer answering customer queries about the AI \nsystem, or a salesperson using an AI-enabled \nproduct to make a recommendation) should be \ntrained to be at least aware of and sensitive to the \nbenefits, risks and limitations when using AI, so that \nthey know when to alert subject-matter experts \nwithin their organisations.\n24\n \n2.  Risk management and internal controls\nOrganisations can consider implementing a sound \nsystem of risk management and internal controls that \nspecifically addresses the risks involved in the \ndeployment of the selected AI model. \nSuch measures include:\ni. Using reasonable efforts to ensure that the datasets \nused for AI model training are adequate for the \nintended purpose, and to assess and manage the \nrisks of inaccuracy or bias, as well as reviewing \nexceptions identified during model training. \nVirtually, no dataset is completely unbiased. \nOrganisations should strive to understand the ways \nin which datasets may be biased and address this \nin their safety measures and deployment strategies.\nii. Establishing monitoring and reporting systems as \nwell as processes to ensure that the appropriate \nlevel of management is aware of the performance \nof and other issues relating to the deployed AI. \nWhere appropriate, the monitoring can include \nautonomous monitoring to effectively scale human \noversight. AI systems can be designed to report \non the confidence level of their predictions, and \nexplainability features could focus on why the AI \nmodel had a certain level of confidence. \niii. Ensuring proper knowledge transfer whenever there \nare changes in key personnel involved in AI \nactivities. This will reduce the risk of staff movement \ncreating a gap in internal governance.\niv. Reviewing the internal governance structure and \nmeasures when there are significant changes to \norganisational structure or key personnel involved. \nv. Periodically reviewing the internal governance \nstructure and measures to ensure their continued \nrelevance and effectiveness.\na. \nb. \nMODEL ARTIFICIAL INTELLIGENCE GOVERNANCE FRAMEWORK   25\nCUJO AI is a network intelligence software company in the telecommunications \noperators\u2019 market. Headquartered in the US, it seeks to develop and deploy AI \nto improve security, control, privacy of connected devices in homes and businesses.\nCUJO AI has implemented clear internal governance structures and measures \nto ensure robust oversight of its use of AI. Its multi-stakeholder governance \nstructures facilitate decisions at appropriate levels: \nconsisting of the Chief Technology Officer, the \nHead of Labs and the Chief Data Scientist, approves \nthe AI development and deployment. In particular, the Chief Technology Officer \noversees four technical teams which consists of more than 100 employees. \nTheir roles and responsibilities are clearly defined: \na. Research team performs data analysis, research and develop Machine \nLearning (\u201cML\u201d) models and AI algorithms;\nb. Engineering team builds software, cloud services and applications;\nc. Operation team deploys the AI model and upgrade platform; and\nd. Delivery team engages with operators and integrate services.\nconsisting of the Chief Technology Officer, Chief \nArchitect Officer and lead engineers, ensures the \nrobustness of the AI/ML models before deployment. \nThe ASG has bi-weekly meetings where the research \nteam shares its findings on the ML models and AI algorithms (e.g. data, approach \nand assumptions).\noversee the AI development and deployment \nprocess, and strive to implement academic review \nstandards for each new feature development.\nIn addition, CUJO AI has developed a general Code of Ethics (\u201cCode\u201d) for its \nemployees. All new employees are introduced to the CUJO AI local country \ndocument and process repository. For example, CUJO AI\u2019s office in Finland provides \nits employees with an electronic \u201cCUJO employee handbook\u201d. The handbook \ndescribes in detail the Code, while covering other topics such as business ethics \nand conduct. Employees carry out their tasks and responsibilities on the basis of \nthe following ethical principles:\nILLUSTRATION ON INTERNAL GOVERNANCE \nSTRUCTURES AND MEASURES\nCUJO AI:\nA Research Board\nAn Architecture \nSteering Group \n(\u201cASG\u201d)\nPhD-level employees\n26\na. To conduct business in an honest and ethical manner across its various \noffices around the world;  \nb. To base decisions on honesty, fairness, respect, responsibility, integrity, \ntrust, and sound business judgment; \n \nc. That no illegal or unethical conduct on the part of officers, directors, \nemployees, or affiliates is in the company\u2019s best interest; and \n \nd. Not to compromise the company\u2019s principles for short-term advantage.\nMastercard is a technology company in the global payments industry. Its \nglobal payments processing network connects consumers, financial \ninstitutions, merchants, governments and businesses in more than 210 \ncountries and territories. To achieve its vision, Mastercard leveraged AI in \nmany applications such as fraud prevention, forecasting future spending \ntrends and improving user retail experience.\nTo ensure robust oversight of Mastercard\u2019s use of AI, Mastercard established \na Governance Council to review and approve the implementation of AI \napplications that are determined to be high risk. The Governance Council is \nchaired by its Executive Vice President of the Artificial Intelligence Center of \nExcellence, and whose members include the Chief Data Officer, Chief Privacy \nOfficer, Chief Information Security Officer, data scientists and representatives \nfrom business teams.\nMastercard has defined clear roles and responsibilities for the Governance \nCouncil. Each representative on the Council brings their expertise to the decision-\nmaking process:\nILLUSTRATION ON INTERNAL GOVERNANCE \nSTRUCTURES AND MEASURES\nMASTERCARD:\nMODEL ARTIFICIAL INTELLIGENCE GOVERNANCE FRAMEWORK   27\nwill review the proposal for implementation of \nAI to ensure that the:  \n \n \n \nwill ensure that security by design is \nimplemented.\nthat build and implement AI are in continued \ndialogue with the Data Office and the Privacy \nOffice, so that there is continued information \nsharing regarding the required governance and the lifecycle of a particular \nimplementation of an AI application.\nMastercard has also implemented risk management and internal controls to \naddress the risk involved in the AI deployment. For example, Mastercard \nconducts initial risk scoring to determine the risk of the proposed AI activity, \nwhich includes an evaluation of multiple factors including alignment with \ncorporate initiatives, the data types and sources utilised, and the impact on \nindividuals from AI decisions. \nIn addition, Mastercard will identify potential mitigants as part of the process \nto reduce the level of risk posed by the data being collected or potential biases \nin the activity. If an AI project has been identified as high risk, it will be referred \nto the Governance Council for review. Low risk projects will not be subjected \nto a review and can proceed to the model development stage.\nChief Information \nSecurity Officer\nb. \nc. \nChief Data Officer \nand Chief Privacy \nOfficer\na. \n\u2022 Data is fit for purpose for AI;\n\u2022 AI is used for an ethical purpose; and\n\u2022 Impact to an individual is appropriate and \npotential harms (including risks to privacy \nand data protection) are sufficiently \nmitigated. \nData Science teams\n28\nDETERMINING THE LEVEL OF \nHUMAN INVOLVEMENT IN AI-\nAUGMENTED DECISION-MAKING\n3.8 \nThis section is intended to help organisations determine the \nappropriate extent of human oversight in AI-augmented \ndecision-making.\n3.9 \nHaving clarity on the objective of using AI is a key first step in \ndetermining the extent of human oversight. Organisations can \nstart by deciding on their commercial objectives of using AI (e.g. \nensuring consistency in decision-making, improving operational \nefficiency and reducing costs, or introducing new product features \nto increase consumer choice). These commercial objectives can \nthen be weighed against the risks of using AI in the organisation\u2019s \ndecision-making. This assessment should be guided by \norganisations\u2019 corporate values, which in turn, could reflect the \nsocietal norms or expectations of the territories in which the \norganisations operate.\n3.10 It is also desirable for organisations operating in multiple countries \nto consider the differences in societal norms, values and/or \nexpectations, where possible. For example, gaming advertisements \nmay be acceptable in one country but not in another. Even within \na country, risks may vary significantly depending on where AI is \ndeployed. For example, risks to individuals associated with \nrecommendation engines that promote products in an online \nmall or automating the approval of online applications for travel \ninsurance may be lower than the risks associated with algorithmic \ntrading facilities offered to sophisticated investors. \nBefore deploying AI solutions, \norganisations should decide on their \ncommercial objectives of using AI, and \nthen weigh them against the risks of using \nAI in the organisation\u2019s decision-making.\nMODEL ARTIFICIAL INTELLIGENCE GOVERNANCE FRAMEWORK   29\n3.11 Some risks to individuals may only manifest at group level. For \nexample, widespread adoption of a stock recommendation \nalgorithm might cause herding behaviour, increasing overall \nmarket volatility if sufficiently large numbers of individuals make \nsimilar decisions at the same time. In addition to risks to individuals, \nother types of risks may also be identified (e.g. risk to an \norganisation\u2019s commercial reputation).\n3.12 Organisations\u2019 weighing of their commercial objectives against \nthe risks of using AI should ideally be guided by their corporate \nvalues. Organisations can assess if the intended AI deployment \nand the selected model for algorithmic decision-making are \nconsistent with their own core values. Any inconsistencies and \ndeviations should be conscious decisions made by organisations \nwith a clearly defined and documented rationale.\n3.13 As identifying commercial objectives, risks and determining the \nappropriate level of human involvement in AI-augmented \ndecision-making is an iterative and ongoing process, it is \ndesirable for organisations to continually identify and review \nrisks relevant to their technology solutions, mitigate those risks, \nand maintain a response plan should mitigation fail. Documenting \nthis process through a periodically reviewed risk impact \nassessment helps organisations develop clarity and confidence \nin using the AI solutions. It will also help organisations respond \nto potential challenges from individuals, other organisations or \nbusinesses, and regulators.\n30\na. Human-in-the-loop suggests that human oversight is active and involved, \nwith the human retaining full control and the AI only providing recommendations \nor input. Decisions cannot be exercised without affirmative actions by the \nhuman, such as a human command to proceed with a given decision. \n \nFor example, a doctor may use AI to identify possible diagnoses of and \ntreatments for an unfamiliar medical condition. However, the doctor will \nmake the final decision on the diagnosis and the corresponding treatment. \nThis model requires AI to provide enough information for the human to \nmake an informed decision (e.g. factors that are used in the decision, their \nvalue and weighting, correlations).\nb. Human-out-of-the-loop suggests that there is no human oversight over \nthe execution of decisions. The AI system has full control without the option \nof human override. \n \nFor example, a product recommendation solution may automatically suggest \nproducts and services to individuals based on pre-determined demographic \nand behavioural profiles. AI can also dynamically create new profiles, then \nmake product and service suggestions rather than relying on predetermined \ncategories. \n \nA machine learning model might also be used by an airline to forecast \ndemand or likely disruptions, and the outputs of this model are used by a \nsolver module to optimise the airline\u2019s scheduling, without a human in the \nloop.\nc. Human-over-the-loop (or human-on-the-loop) suggests that human oversight \nis involved to the extent that the human is in a monitoring or supervisory \nrole, with the ability to take over control when the AI model encounters \nunexpected or undesirable events (such as model failure). This approach \nallows humans to adjust parameters during the operation of the algorithm. \nFor example, a GPS navigation system plans the route from Point A to Point \nB, offering several possible routes for the driver to pick. The driver can alter \nparameters (e.g. due to unforeseen road congestions) during the trip without \nhaving to re-programme the route.\n3.14 Based on the risk management approach described above, the Model Framework \nidentifies three broad approaches to classify the various degrees of human oversight \nin the decision-making process:\nWHAT ARE THE THREE BROAD\nAPPROACHES OF HUMAN INVOLVEMENT\nIN AI-AUGMENTED DECISION-MAKING?\nMODEL ARTIFICIAL INTELLIGENCE GOVERNANCE FRAMEWORK   31\n3.17 The matrix, however, should not be taken to imply that the \nprobability of harm and severity of harm are the only factors to \nbe considered in determining the level of human oversight in an \norganisation\u2019s decision-making process involving AI (although \nthey are generally two of the more important factors).4\n3.18 For safety-critical systems, it would be prudent for organisations \nto ensure that a person be allowed to assume control, with the \nAI system providing sufficient information for that person to make \nmeaningful decisions or to safely shut down the system where \nhuman control is not possible. \nSeverity of Harm\nProbability of Harm\nHigh severity\nLow probability\nHigh severity\nHigh probability\nLow severity\nLow probability\nLow severity\nHigh probability\n3.15 The Model Framework also proposes a design framework \n(structured as a matrix) to help organisations determine the level \nof human involvement required in AI-augmented decision-making. \nThis design framework is structured along two axes: the (a) \nprobability; and (b) severity of harm to an individual (or organisation) \nas a result of the decision made by an organisation about that \nindividual (or organisation).\n3.16 The definition of \u201charm\u201d and the computation of probability and \nseverity will depend on the context and vary from sector to sector. \nFor example, the considerations of a hospital regarding the harm \nassociated with a wrong diagnosis of a patient\u2019s medical condition \nwill differ from the considerations of a clothing store\u2019s regarding the \nharm associated with a wrong product recommendation for apparels.\nOther factors that organisations in various contexts may consider relevant, could also include: \n(a) the nature of harm (i.e. whether the harm is physical or intangible in nature); (b) the \nreversibility of harm, and as a corollary to this, the ability for individuals to obtain recourse; \nand (c) whether it is operationally feasible or meaningful for a human to be involved in a \ndecision-making process (e.g. having a human-in-the-loop would be unfeasible in high-\nspeed financial trading, and be impractical in the case of driverless vehicles).\n4\n32\nHIGHLY\nRECOMMENDED!\nAn online retail store wishes to use AI to fully automate \nthe recommendation of food products to individuals based \non their browsing behaviours and purchase histories. The \nautomation will meet the organisation\u2019s commercial \nobjective of operational efficiency.\nProbability-severity assessment \nThe definition of harm can be the impact of making product recommendations \nthat do not address the perceived needs of the individuals. The severity of \nharm in making the wrong product recommendations to individuals may be \nlow since individuals ultimately decide whether to make the purchase. The \nprobability of harm may be high or low depending on the efficiency and \nefficacy of the AI solution.\nDegree of human intervention in decision-making process\nGiven the low severity of harm, the assessment points to an approach that \nrequires no human intervention (i.e. human-out-of-the-loop).\nRegular review \nThe organisation regularly reviews its approach (i.e. human-out-of-the-loop) \nto re-assess the severity and probability of harm, and as societal norms and \nvalues evolve.\nNote: This is a simple illustration using bright-line norms and values. Organisations can consider testing this method \nof determining the AI decision-making model against cases with more challenging and complex ethical dilemmas.\nSeverity of Harm\nProbability of Harm\nHigh severity\nLow probability\nHigh severity\nHigh probability\nLow severity\nLow probability\nLow severity\nHigh probability\nHuman-out-\nof-the-loop\nUSING THE PROBABILITY-SEVERITY OF HARM MATRIX\nMODEL ARTIFICIAL INTELLIGENCE GOVERNANCE FRAMEWORK   33\nSuade Labs (\u201cSuade\u201d) is a RegTech firm that operates globally and is a World \nEconomic Forum Technology Pioneer. Suade provides an AI-enabled solution \nthat allows financial institutions to process large volumes of granular data and \ngenerate the required regulatory data, calculations, and reports with the \nnecessary controls and governance. Suade\u2019s solution also allows users to analyse \nthe impact of the existing stock of regulation, including the impact of individual \npieces of legislation.\nIn determining the level of human involvement in decision-making using AI, \nSuade considered the following key factors:\na. Degree of domain knowledge (e.g. legal or policy-making knowledge) \nrequired to accurately interpret the results of the algorithm. \n \nb. Cost of non-compliance to regulation if the AI tool does not accurately \nanalyse the impact of regulation and provide correct suggestions for regulatory \ncompliance.\nAs Suade\u2019s solution requires a certain degree of domain knowledge from \nhuman experts, and given that the cost of regulatory non-compliance as a \nresult of incorrect recommendations made by the AI solution will be significant \nto users, Suade has thus adopted a human-in-the-loop approach for its AI \nsolution.\nOn the other hand, when it comes to tuning the AI model, Suade adopts a \nhuman-over-the-loop approach. In general, Suade tunes the AI model to \nautomatically favour the identification of false positives over false negatives. \nHowever, Suade conducted user research, which informed them that some \nusers prefer the model to favour false negatives over false positives. Therefore, \nSuade adopts a human-over-the-loop approach so that the AI model can \nbe tuned to account for the differing preferences of its users with respect \nto whether the algorithm produces results that favours false positives or \nfalse negatives.\nILLUSTRATION ON DETERMINING THE LEVEL OF HUMAN \nINVOLVEMENT IN AI-AUGMENTED DECISION-MAKING\nSUADE LABS:\n34\nGrab is a Singapore-based company that offers ride-hailing transport services, \nfood delivery and e-payment solutions. It uses AI across its platform, from ride \nallocation, detecting safety incidents, to identifying fraudulent transactions. In \nparticular, Grab uses AI to improve the overall quality of trip allocations and \nminimise trip cancellations.\nTo allocate trips successfully, Grab\u2019s AI model considers drivers\u2019 preferences \nbased on the following key factors:\na. Driver\u2019s preferences for certain trip types; \n \nb. Preferred locations where a driver start and end their day; and  \nc. Other selective driving behaviours.\nIn determining the level of human involvement in its AI\u2019s decision-making \nfor trip allocation, Grab considered the following key factors:\na. The scale of real-time decision-making required. As Grab has to make over \n5,000 trip allocations every minute, this would mean an impact to customers \nin terms of efficiency and cost if a human had to review each trip allocation; and \nb. The severity and probability to users should the AI model work in a sub-\noptimal manner. \nAmong other factors, Grab considered that: (1) it is not technically feasible for \na human to make such high volume of trip allocations in a short amount of time; \nand (2) there is often little or no harm to life should there be less than optimal \ntrip allocations. Hence, Grab decided to adopt a human-out-of-the-loop \napproach for its AI model deployed for trip allocation, while continuously \nreviewing the AI model to ensure optimal performance.\nILLUSTRATION ON DETERMINING THE LEVEL OF HUMAN \nINVOLVEMENT IN AI-AUGMENTED DECISION-MAKING\nGRAB:\nMODEL ARTIFICIAL INTELLIGENCE GOVERNANCE FRAMEWORK   35\nOPERATIONS MANAGEMENT \n3.19 This section is intended to help organisations adopt responsible \nmeasures in the operations aspect of their AI adoption process. \nA reference AI adoption process is set out in order to provide a \ncontext for the recommendations for good governance in respect \nof the organisation\u2019s data, algorithm and AI model.\n3.20 The Model Framework uses the following generalised AI model \ndevelopment and deployment process to describe phases in \nimplementing an AI solution by an organisation.5 It should be \nnoted that this process is not always uni-directional \u2013 it can, and \nusually is, a continuous process of learning.\nData Preparation\nAlgorithms\nChosen Model\nStage 1:\nRaw data is formatted \nand cleansed so \nconclusions can be \ndrawn accurately. \nGenerally, accuracy and \ninsights increase with \nrelevance and the \namount of data.\nStage 2:\nModels are trained on \nthe dataset and \nalgorithms may be \napplied. This includes \nstatistical or machine \nlearning models \nincluding decision trees \nand neural networks. The \nresults are examined and \nmodels are iterated until \nthe most appropriate \nmodel emerges.\nStage 3:\nThe chosen model is \nused to produce \nprobability scores that \ncan be incorporated \ninto applications to \noffer predictions, make \ndecisions, solve \nproblems and trigger \nactions. \nPrepared\nData\nApply \nAlgorithms \nand/or Train \nAI Model\nMachine\nLearning\nAlgorithms\nCandidate\nModel\nChosen\nModel\nApplication\nIterate until data is ready\nIterate for most appropriate model\nData pre-\nprocessing\nRaw\nData\nRaw\nData\nAdapted from \u201cMachine learning at scale\u201d Microsoft Azure (2 December 2018) <https://\ndocs.microsoft.com/en-us/azure/architecture/data-guide/big-data/machine-learning-at- \nscale> (accessed December 2019).\n5\n36\nDATA FOR MODEL DEVELOPMENT\n3.22 Datasets used for building models may come from multiple \nsources, and could include both personal and non-personal data. \nThe quality and selection of data from each of these sources are \ncritical to the success of an AI solution. If a model is built using \nbiased, inaccurate or non-representative data, the risks of \nunintended discriminatory decisions from the model will increase. \n3.23 The persons who are involved in training and selecting models \nfor deployment may be internal staff or external service providers. \nIt is ideal for the models deployed in an intelligent system to \nhave an internal departmental owner, who will be the one making \ndecisions on which models to deploy. To ensure the effectiveness \nof an AI solution, it would be helpful for relevant departments \nwithin the organisation with responsibilities over quality of data, \nmodel training and model selection to work together to put \nin place good data accountability practices. These include \nthe following:\nTo ensure the effectiveness of an \nAI solution, relevant departments \nwithin the organisation with \nresponsibilities over quality of data, \nmodel training and model selection \nmust work together to put in place \ngood data accountability practices.\n3.21 During deployment, algorithms such as linear regression \nalgorithms, decision trees, or neural networks are applied for \nanalysis on training datasets. The resulting algorithmic models \nare examined and algorithms are iterated until a model that \nproduces the most appropriate results for the use case emerges. \nThis model and its results are then incorporated into applications \nto offer predictions, make decisions, solve problems and trigger \nactions. The intimate interaction between data and algorithm/\nmodel is the focus of this part of the Model Framework.\nMODEL ARTIFICIAL INTELLIGENCE GOVERNANCE FRAMEWORK   37\nUnderstanding the lineage of data: This means \nknowing where the data originally came from, how it \nwas collected, curated and moved within the \norganisation, and how its accuracy is maintained over \ntime. Data lineage can be represented visually to trace \nhow the data moves from its source to its destination, \nhow the data gets transformed along the way, where \nit interacts with other data, and how the representations \nchange. There are three types of data lineage:\ni. \nBackward data lineage looks at the data from its \nend-use and backdating it to its source.\nii. \nForward data lineage begins at the data\u2019s source \nand follows it through to its end-use. \niii. End-to-end data lineage combines the two and \nlooks at the entire solution from both the data\u2019s \nsource to its end-use and from its end-use to its \nsource.\nKeeping a data provenance record allows an \norganisation to ascertain the quality of the data based \non its origin and subsequent transformation, trace \npotential sources of errors, update data, and attribute \ndata to their sources.\nIn some instances, the origin of data could be \ndifficult to establish. One example could be datasets \nobtained from a trusted third-party which may have \ncommingled data from multiple sources. It would be \nprudent for organisations to assess the risks of using \nsuch data and manage them accordingly.\na. \n38\nEnsuring data quality: Organisations are encouraged \nto understand and address factors that may affect the \nquality of data, such as:\ni. \nThe accuracy of the dataset, in terms of how well \nthe values in the dataset match the true \ncharacteristics of the entities described by the \ndataset;\nii. \nThe completeness of the dataset, both in terms \nof attributes and items; \niii. The veracity of the dataset, which refers to how \ncredible the data is, including whether the data \noriginated from a reliable source;\niv. \nHow recently the dataset was compiled or updated;\nv. \nThe relevance of the dataset and the context for \ndata collection, as it may affect the interpretation \nof and reliance on the data for the intended \npurpose;\nvi. The integrity of the dataset that has been joined \nfrom multiple datasets, which refers to how well \nextraction \nand \ntransformation \nhave \nbeen \nperformed;\nvii. The usability of the dataset, including how well \nthe dataset is structured in a machine-\nunderstandable form; and\nviii. Human interventions (e.g. if any human has filtered, \napplied labels, or edited the data).\nb. \nMinimising inherent bias: There are many types of \nbias relevant to AI. The Model Framework focuses on \ninherent bias in datasets, which may lead to undesired \noutcomes such as unintended discriminatory decisions. \nOrganisations should be aware that the data which \nthey provide to AI systems could contain inherent biases \nand are encouraged to take steps to mitigate such \nbias. The two common types of bias in data include:\nc. \nMODEL ARTIFICIAL INTELLIGENCE GOVERNANCE FRAMEWORK   39\ni. \nSelection bias: This bias occurs when the data \nused to produce the model are not fully \nrepresentative of the actual data or environment \nthat the model may receive or function in. Common \nexamples of selection bias in datasets are omission \nbias and stereotype bias. Omission bias describes \nthe omission of certain characteristics from the \ndataset. For example, a dataset consisting only \nof Asian faces will exhibit omission bias if it is used \nfor facial recognition training for a population that \nincludes non-Asians. A dataset of vehicle types \nwithin the central business district on a weekday \nmay exhibit stereotype bias weighted in favour of \ncars, buses and motorcycles but under-represent \nbicycles if it is used to model the types of \ntransportation available in Singapore.\nii. \nMeasurement bias: This bias occurs when the \ndata collection device causes the data to be \nsystematically skewed in a particular direction. For \nexample, the training data could be obtained using \na camera with a colour filter that has been turned \noff, thereby skewing the machine learning result.\nWhile identifying and addressing inherent bias in \ndatasets may not be easy, organisations can mitigate \nthe risk of inherent bias by having a heterogeneous \ndataset (i.e. collecting data from a variety of reliable \nsources). Another way is to ensure the dataset is as \ncomplete as possible, both from the perspective of \ndata attributes and data items. Premature removal of \ndata attributes can make it difficult to identify and \naddress inherent bias.\n40\nDifferent datasets for training, testing, and \nvalidation: Different datasets are required for training, \ntesting, and validation. The model is trained using \nthe training data, while the model\u2019s accuracy is \ndetermined using the test data. Where applicable, \nthe model could also be checked for systematic bias \nby testing it on different demographic groups to \nobserve whether any groups are being systematically \nadvantaged or disadvantaged. \nFinally, the trained model can be validated using the \nvalidation dataset.  It is considered good practice to \nsplit a large dataset into subsets for these purposes, \nif it does not lead to a significant reduction in the \nquality of data in terms of accuracy and representation. \nHowever, where this is not possible (e.g. if the \norganisation is not working with large datasets or are \nusing pre-trained models as in the case of transfer \nlearning), organisations are encouraged to be cognisant \nof the risks of systematic bias and put in place \nappropriate safeguards.\nPeriodic reviewing and updating of datasets: It \nwould be prudent for datasets (including training, \ntesting, and validation datasets) to be reviewed \nperiodically to ensure accuracy, quality, currency, \nrelevance and reliability. Where necessary, the datasets \ncan be updated with new input data obtained from \nactual use of the AI models deployed in production. \nWhen such new input data is used, organisations need \nto be aware of potential bias as using new input data \nthat has already gone through a model once could \ncreate a reinforcement bias.\nd. \ne. \n3.24 Even if only non-personal data are used for the training of AI \nmodels (including personal data that has been anonymised), the \ngood data accountability practices above remain relevant.\nMODEL ARTIFICIAL INTELLIGENCE GOVERNANCE FRAMEWORK   41\nAs the data used for Suade\u2019s AI model development directly affects its quality \nand performance, Suade has adopted several good data accountability practices. \nFor example, to ensure that regulatory data comes from a credible and reliable \nsource, Suade obtains and updates regulatory data only from the relevant \nregulators. In addition, Suade tags the datasets used with additional metadata. \nThis allows Suade to trace datasets back to their original source when needed, \nsuch as where inconsistencies are found. Further, in order to trace which particular \ndatasets were used in an AI model, Suade also documents and stores such \ninformation pertaining to model development on its database.\nSuade also minimises the inherent risks of AI models through responsible \ndata tagging. By using a larger number of taggers (i.e. people who tag data), \nSuade aims to make the output of its AI models as neutral as possible, and \nreduce the risk of its taggers being influenced by the context of the data (which \noften comprise of text) they are annotating. In other words, Suade uses as \nmany individuals as practicable to tag data to reduce the risk of tagger bias.\nIn addition, Suade developed a tagging system to facilitate the annotation of \ndata. This system is used to generate training data used by the algorithm. \nSuade will further develop this tagging system to enhance its ability to manage \nmultiple annotators and to better select datasets used for model training. \nSuade also periodically updates the tagging system with new data. New training \ndata is subsequently fed repeatedly back into the AI model. This way, the AI \nmodel is able to continuously learn from new sets of data.  \nAnother data accountability practice that Suade adopts is the use of validation \nschema checks at various stages of data transformation. This is a process in \nwhich Suade verifies that the data schema accurately represents the data from \nthe source, to ensure that there are no errors in factors such as the data\u2019s \nformatting and content.\nILLUSTRATION ON MANAGING DATA FOR MODEL DEVELOPMENT\nSUADE LABS:\nSuade (introduced above) has developed an AI-enabled solution that helps \nfinancial institutions generate the required data and reports to comply with \nregulatory requirements in the jurisdictions where they operate.  \n42\npymetrics is a technology provider that uses neuroscience insights and audited \nAI models to help evaluate applicants in a more predictive and less biased \nmanner. To develop an AI model, pymetrics:\n\u2022 Gets its clients\u2019 top-performing employees to go through pymetrics\u2019 \nassessments, and builds a trait profile of an employee that best fit the \nspecific job role;  \n \n\u2022 Validates the trait profile with the client\u2019s HR team; and \n \n\u2022 Collects behavioural data of applicants through pymetrics\u2019 gamified \nassessments, and assesses the suitability of the applicants based on the \ntrait profile. \nTo deal with socially sensitive features and mitigate the risk of inherent or \nunintentional bias in the datasets used by the AI model, pymetrics:\n\u2022 Uses objective data based on established neuroscience research (e.g. \nattention to detail, attention span and ability to recall), which are generally \nstable across gender, racial and age groups.  \n \n\u2022 Proactively de-biases all AI models to ensure that they are fully representative \nof the environment that they may function in, so that the AI models do not \ndisadvantage people on the basis of their demographic features: \n \u00bb\nThe standards for fairness are informed by legal requirements. As a pre-\nhire assessment, pymetrics models must pass a test known as \u201cthe four-\nfifths rule\u201d, which is commonly cited in employment law. According to \nUS\u2019 Equal Employment Opportunity Commission (\u201cEEOC\u201d), the selection \nrate for any legally protected group must be at least 80% of the selection \nrate for the majority group. For example, if an employer screens 200 \nqualified applicants (100 men and 100 women), a model that selects 50 \nmen must also select at least 40 women.  \n \n \u00bb\npymetrics will test the AI model against a dataset of users from diverse \ndemographics to ensure that random patterns in the data are not learned \nby the model and to address any potential for bias.  \nILLUSTRATION ON MANAGING BIASES \nIN DATASETS FOR MODEL DEVELOPMENT\nPYMETRICS:\nMODEL ARTIFICIAL INTELLIGENCE GOVERNANCE FRAMEWORK   43\n \u00bb\npymetrics would conduct further de-biasing on additional demographic \nbased on geographical relevance or legal requirements. pymetrics uses \na bias ratio to compare the proportional pass rates of the highest-passing \ndemographic group with the lowest-passing group for each demographic \ncategory (e.g. gender and ethnicity).  \n \n \n \u00bb\nThe AI model would be deployed only if they meet the EEOC \nstandards. \n \n\u2022 After AI deployment, pymetrics: \n \n \u00bb\nWill test the AI model\u2019s decisions on real applicants for adverse \nimpact; and  \n \u00bb\nRevisits the long-term impact of its system\u2019s predictions on retention \nfor the role.\nIf bias is found either before or after deployment, pymetrics will adjust the AI \nmodel to optimise for fairness towards applicants while ensuring the predictive \nperformance of the AI model.\nALGORITHM AND MODEL\n3.25 AI systems may have numerous features or functionalities enabled \nthrough algorithms in AI models. Measures such as explainability, \nrepeatability, robustness, regular tuning, reproducibility, \ntraceability, and auditability can enhance the transparency of \nalgorithms found in AI models. It may not be feasible or cost-\neffective to implement even the most essential of these measures \nfor all algorithms.\n \nOrganisations are encouraged to take a risk-based approach in \nmaking a two-fold assessment. First, identify the subset of \nfeatures or functionalities that have the greatest impact on \nstakeholders for which such measures are relevant. Second, \nidentify which of these measures will be most effective in building \ntrust with their stakeholders. Some of these measures like \nexplainability (or repeatability, when using models that are not \neasily explained), robustness and regular tuning are sufficiently \nessential that they could, to varying extents, be incorporated \nas part of the organisation\u2019s AI deployment process. Other \nmeasures, such as reproducibility, traceability and auditability \nare more resource-intensive and may be relevant for specific \nfeatures or in specific scenarios. \n44\nExplainability\n3.26 Explainability is achieved by explaining how deployed AI models\u2019 \nalgorithms function and/or how the decision-making process \nincorporates model predictions. The purpose of being able to \nexplain predictions made by AI is to build understanding and \ntrust. An algorithm deployed in an AI solution is said to be \nexplainable if how it functions and how it arrives at a particular \nprediction can be explained. When an algorithm cannot be \nexplained, understanding and trust can still be built by explaining \nhow predictions play a role in the decision-making process.\n3.27 Organisations deploying AI solutions are recommended to adopt \n \nthe following practices:\nModel training and selection are necessary for \ndeveloping an intelligent system (i.e. a system that \ncontains AI technologies). Documenting how the model \ntraining and selection processes are conducted, the \nreasons for which decisions are made, and measures \ntaken to address identified risks will enable the \norganisation to provide an account of the decisions \nsubsequently. \nIn this regard, the field of Automated Machine Learning \naims to automate a significant portion of machine \nlearning workflows, including feature engineering, \nfeature selection, model selection and hyper-parameter \ntuning. Organisations using these types of tools can \nconsider the transparency, explainability and traceability \nof the automated machine learning approach, as well \nas the models selected.\nIncorporating descriptions of the solutions\u2019 design and \nexpected behaviour into product or service descriptions \nand system technical specifications documentation \ndemonstrates accountability to individuals and/or \nregulators. This could also include design decisions in \nrelation to why certain features, attributes or models \nare selected in place of others. These steps can help \nprovide greater clarity on an AI model by giving \nunderstandable and digestible insights into how the \nmodel operates. \na. \nb. \nMODEL ARTIFICIAL INTELLIGENCE GOVERNANCE FRAMEWORK   45\n3.28 Technical explainability may not always be enlightening, especially \nto the man on the street. Implicit explanations of how the AI \nmodels\u2019 algorithms function may be more useful than explicit \ndescriptions of the models\u2019 logic. For example, providing an \nindividual with counterfactuals (such as \u201cyou would have been \napproved if your average debt was 15% lower\u201d) and/or \ncomparisons (such as \u201cthese are users with similar profiles to \nyours that received a similar decision\u201d) can be a powerful type \nof explanation that organisations could consider. \n3.29 Nevertheless, there may be scenarios where it might not be \npractical or reasonable to provide information in relation to an \nalgorithm. For example, disclosing algorithms deployed for anti-\nmoney laundering detection, information security, and fraud \nprevention may allow bad actors to avoid detection; likewise, \nproviding detailed information about proprietary algorithms or \nthe decisions made by the algorithms may expose confidential \nbusiness information.\nWhere an organisation\u2019s AI system was obtained or \nprocured from a third-party AI solution provider, the \norganisation can consider requesting assistance from \nthe AI solution provider as they may be better placed \nto explain how the solution functions.\nSupplementary explanation tools are helpful for \nexplaining  AI models,6 especially models that are less \ninterpretable (also known as \u201cblack box\u201d systems). \nThese tools help make the underlying rationale of an \nAI system\u2019s output more interpretable and intelligible \nto those who use the system. It is possible to use a \ncombination of these tools to improve the explainability \nof an AI model\u2019s decision.\nc. \nThese tools are known as \u201csupplementary\u201d as there is at present no single comprehensive \ntechnical solution for making AI models explainable. These tools thus play a supplementary \nrole in providing some level of interpretability on an AI model\u2019s operation. Examples of \nthese tools include the use of surrogate models, partial dependence plots, global variable \nimportance/interaction, sensitivity analysis, counterfactual explanations, or Self-Explaining \nand Attention-Based Systems.\n6\n46\nRepeatability\n3.30 Where explainability cannot practicably be achieved given the \nstate of technology, organisations can consider documenting \nthe repeatability of results produced by the AI model. Repeatability \nrefers to the ability to consistently perform an action or make a \ndecision, given the same scenario. While repeatability (of results) \nis not equivalent to explainability (of algorithm), some degree \nof assurance of consistency in performance could provide AI \nusers with a larger degree of confidence. Helpful practices include:\na. Conducting \nrepeatability \nassessments \nfor \ncommercial deployments in live environments to \nensure that deployments are repeatable; \n \nb.  Performing counterfactual \nfairness \ntesting. \nCounterfactual fairness testing ensures that a model\u2019s \ndecisions are the same in both the real world and \nin a counterfactual world where attributes deemed \nsensitive (such as race or gender) are altered;7 \nc. Assessing how exceptions can be identified and \nhandled when decisions are not repeatable, e.g. \nwhen randomness has been introduced by design; \nd. Ensuring exception handling is in line with \norganisations\u2019 policies;  \n \nIn this regard, it may be helpful to use AI models that \nare able to recognise when a given set of facts contains \nnew variables not previously considered and are able \nto highlight these new variables to a human;\ne. Identifying and accounting for changes over time \nto ensure that models trained on time-sensitive data \nremain relevant.\nJames Manyika, Jake Sitberg, and Brittany Presten, \u201cWhat Do We Do About the Biases in \nAI?\u201d Harvard Business Review (25 October 2019) <https://hbr.org/2019/10/what-do-we- \ndo-about-the-biases-in-ai> (accessed 31 October 2019).\n7\nMODEL ARTIFICIAL INTELLIGENCE GOVERNANCE FRAMEWORK   47\nRobustness\n3.31 Robustness refers to the ability of a computer system to cope \nwith errors during execution and erroneous input, and is assessed \nby the degree to which a system or component can function \ncorrectly in the presence of invalid input or stressful environmental \nconditions. Ensuring that deployed models are sufficiently robust \nwill contribute towards building trust in the AI system.\n3.32 The concept of robustness arises because it is not possible for \nmodels to be able to enumerate and set out all preconditions \nand consequences for an action. This creates the possibility of \nmodels producing insensible or unexpected results even with \nminor modifications to input data (that may not even be perceptible \nto humans). Testing for robustness can be achieved through \nscenario-based testing for foreseeable erroneous input.8 To ensure \nthat models are more robust, organisations can consider working \nwith AI developers to conduct adversarial testing on their models \nto ensure that their models are able to handle a broader range \nof unexpected input variables (especially for public-facing AI \nsystems). As this is a resource-intensive exercise, organisations \ncan take a risk-based approach towards identifying the subset \nof AI-powered features in their products or services that requires \nadversarial testing.\n3.33 No model can be perfectly robust as it is not possible to detect \nall possible modifications to a set of input data. For this reason, \norganisations intending to use continual learning (i.e. where the \nlearned parameters of a machine learning model are not fixed \nbut the model continues to change its learned parameters after \nbeing deployed into production) are encouraged to be aware \nof the risks of doing so, should the continual learning model \nbehave in an unpredictable manner.\nThis is distinct from user acceptance testing (\u201cUAT\u201d), which is a process where actual \nsoftware users test a piece of software to ensure that it can handle required tasks in real- \nworld scenarios, based on specifications. UAT is often a critical step that is taken before \na newly-developed software is released to the market.\n8\n48\nRegular tuning\n3.34 Establishing an internal policy and process to perform regular \nmodel tuning is effective for ensuring that deployed models \ncater for changes to customer behaviour over time. This allows \norganisations to refresh models based on updated training \ndatasets that incorporate new input data. Model tuning may also \nbe necessary when commercial objectives, risks, or corporate \nvalues change.\n3.35 Wherever possible, testing should reflect the dynamism of the \nplanned production environment. To ensure safety, testing may \nneed to assess the degree to which an AI solution generalises \nwell and fails gracefully. For example, a warehouse robot tasked \nwith avoiding obstacles to complete a task (e.g. picking up \npackages) could be tested with different types of obstacles and \nrealistically varied internal environments (e.g. workers wearing \na variety of different coloured shirts). Otherwise, models risk \nlearning regularities in the environment that do not reflect actual \nproduction environment conditions (e.g. assuming that all humans \nthat it must avoid will be wearing white lab coats). Once AI \nmodels are deployed in the real-world environment, active \nmonitoring, review and tuning are advised.\nTraceability\n3.36 An AI model is considered to be traceable if (a) its decisions, and \n(b) the datasets and processes that yield the AI model\u2019s decision \n(including those of data gathering, data labelling and the \nalgorithms used), are documented in an easily understandable \nway. The former refers to traceability of AI-augmented decisions, \nwhile the latter refers to traceability in model training. Traceability \nfacilitates transparency and explainability, and is also helpful for \nother reasons. First, the information might also be useful for \ntroubleshooting, or for an investigation into how the model was \nfunctioning or why a particular prediction was made. Second, \nthe traceability record (in the form of an audit log) can be a source \nof input data that can be used as a training dataset in the future.\nMODEL ARTIFICIAL INTELLIGENCE GOVERNANCE FRAMEWORK   49\n3.37 Practices that organisations may consider to promote traceability \ninclude:\na. Building an audit trail to document the model training and \nAI-augmented decision.\nb. Implementing a black box recorder that captures all input \ndata streams. For example, a black box recorder in a self-\ndriving car tracks the vehicle\u2019s position and records when and \nwhere the self-driving system takes control of the vehicle, \nsuffers a technical problem or requests the driver to take over \nthe control of the vehicle.9\nc. Ensuring that data relevant to traceability are stored \nappropriately to avoid degradation or alteration, and \nretained for durations relevant to the industry. \n3.38 As traceability measures may lead to an accumulation of a large \nvolume of activity data, organisations can consider which of their \nproduct features require traceability and which traceability \nmeasures might be sufficient for their needs, bearing in mind \nthe resources needed to document the AI model\u2019s decisions, \ndatasets and processes. Organisations could assess this based \non several factors, including: \na. Their assessment of the probability and/or severity of harm \narising from the use of the AI system;\nb. The extent to which the AI model had previously been trialled \nor used; and\nc. The regulatory needs of their industry. \nIt should be noted that a black box recorder does not refer to a \u201cblack box\u201d in the AI \nmodel sense (i.e. where the decision-making process of an AI model is inherently difficult \nto interpret and explain).\n9\n50\nReproducibility\n3.39 While repeatability refers to the internal repetition of results within \none\u2019s organisation, reproducibility refers to the ability of an \nindependent verification team to produce the same results using \nthe same AI method based on the documentation made by the \norganisation. Reproducibility can influence the trustworthiness \nof the AI product and the organisation deploying the AI model. \nAs implementing reproducibility entails the involvement of \nexternal parties, organisations can take a risk-based approach \ntowards identifying the subset of AI-powered features in their \nproducts or services that requires external reproducibility testing.\n3.40 The following practices contribute towards reproducibility:\na. Testing whether specific contexts or particular \nconditions would need to be taken into account to \nensure reproducibility;  \nb. Putting in place verification methods to ensure \ndifferent aspects of the AI model\u2019s reliability and \nreproducibility;  \n \nc. Making available replication files (i.e. files that \nreplicate each step of the AI model\u2019s developmental \nprocess) to facilitate the process of testing and \nreproducing behaviours; \n \nd. For companies that procure commercial off-the-shelf \nAI systems, checking with the original AI solution \nprovider about whether the model\u2019s results are \nreproducible; and\ne. Adopting points in paragraph 3.30 (c)-(e) under \nrepeatability (namely, assessing how exceptions can \nbe identified and handled, ensuring that exception-\nhandling is in line with organisational policies, and \nidentifying and accounting for changes over time).\nMODEL ARTIFICIAL INTELLIGENCE GOVERNANCE FRAMEWORK   51\nAuditability\n3.41 Auditability refers to the readiness of an AI system to undergo \nan assessment of its algorithms, data and design processes. The \nevaluation of the AI system by internal or external auditors (and \nthe availability of evaluation reports) can contribute to the \ntrustworthiness of the AI system as it demonstrates the \nresponsibility of design and practices and the justifiability of \noutcomes. It should, however, be noted that auditability does \nnot necessarily entail making information about business models \nor intellectual property related to the AI system publicly available.\n3.42 Implementing auditability not only entails the involvement of \nexternal parties but requires disclosure of commercially sensitive \ninformation to the auditors, who may be external. Organisations \ncan take a risk-based approach towards identifying the subset \nof AI-powered features in their products or services for which \nimplementing auditability is necessary, or where implementing \nauditability is necessary for an organisation to align itself with \nregulatory requirements or industry practice.\n3.43 To facilitate auditability, organisations can consider keeping a \ncomprehensive record of data provenance, procurement, pre-\nprocessing, lineage, storage and security. The record could also \ninclude qualitative input about data representations, data \nsufficiency, source integrity, data timelines, data relevance, and \nunforeseen data issues encountered across the workflow.\n3.44 Organisations may also wish to centralise such information digitally \nin a process log. This would enable the organisation to make \navailable, in one place, information that may assist in demonstrating \nto concerned parties and affected decision subjects both the \nresponsibility of design and practices and the justifiability of the \noutcomes of your system\u2019s processing behaviour. Such a log \nwould also enable better organisation of the accessibility and \npresentation of information yielded, assist in the curation and \nprotection of data that should be kept unavailable from public \nview, and increase the organisation\u2019s capacity to cater the \npresentation of results to different tiers of stakeholders with \ndifferent interests and levels of expertise.\n52\nThe solution \u2013 Ayasdi\u2019s Model Accelerator (the \u201cAMA\u201d) \u2013 first identifies relevant \nvariables to include in the model and then explains why they are selected. \nAMA does this by looking at possible relationships encoded within the enriched \nbase-level data, and finding hidden patterns that hold predictive value. The \nAMA then uses the variables selected to build AI models. It presents a candidate \nmodel and several viable challenger models for the clients\u2019 selection. Business \nunits within the clients\u2019 organisations will evaluate the candidate and challenger \nmodels and select those that best represent their business units.\nThe entire model creation process is documented automatically. The clients \ncan use AMA to institutionalise both their variable selection and modelling \nmethodology, systematically and deterministically, to produce a repeatable \nprocess with consistent supporting reports on model lineage, variable selection \nand cross-validation. This allows the clients to ensure that initial selections of \nfeatures and models are recorded and documented. At the same time, the \nentire modelling and approval process is tracked and catalogued, thus facilitating \nsubsequent processes from review to model re-use.\nThe ability to demonstrate the detailed process of model building and the \nrigour of evaluating challenger models allows Ayasdi\u2019s clients to explain to the \nUS Federal Reserve how their final models are selected.\nILLUSTRATION ON DOCUMENTING MODEL DEVELOPMENT\nSYMPHONY AYASDIAI:\nSymphony AyasdiAI (\u201cAyasdi\u201d) offers a solution that helps its clients, mainly \nin the US banking and finance sector, to build AI models that can adequately \nforecast revenues and the capital reserve required to absorb losses under \nstressed economic conditions. Its clients need to prove to the US Federal \nReserve that their AI models are accurate and defensible.\nMODEL ARTIFICIAL INTELLIGENCE GOVERNANCE FRAMEWORK   53\nSTAKEHOLDER INTERACTION AND \nCOMMUNICATION\n3.45 This section is intended to help organisations take appropriate \nsteps to build trust in the stakeholder relationship strategies \nwhen deploying AI.\nGeneral disclosure\n3.46 Organisations are encouraged to provide general information \non whether AI is used in their products and/or services. Where \nappropriate, this could include information on what AI is, how \nAI is used in decision-making in relation to consumers, what are \nits benefits, why your organisation has decided to use AI, how \nyour organisation has taken steps to mitigate risks, and the role \nand extent that AI plays in the decision-making process. For \nexample, an online portal may inform its users that they are \ninteracting with an AI-powered chatbot and not a human customer \nservice agent. \n3.47 Organisations can consider disclosing the manner in which an \nAI decision may affect an individual consumer, and whether the \ndecision is reversible. For example, an organisation may inform \nthe individuals that their credit ratings may lead to a loan refusal \nnot only from this organisation but also from other similar \norganisations, while also informing them that such a decision is \nreversible if individuals can provide more evidence on their credit \nworthiness.\nPolicy for explanation\n3.48 Organisations are encouraged to develop a policy on what \nexplanations to provide to individuals and when to provide them. \nSuch policies help ensure consistency in communication, and \nclearly sets out roles and responsibilities of different members \nof your organisation. These can include explanations on how AI \nworks in an AI-augmented decision-making process, how a specific \ndecision was made and the reasons behind that decision, and \nthe impact and consequence of the decision. The explanation \ncan be provided as part of general communication. It can also \nbe information in respect of a specific decision upon request. In \nthis regard, the principle of equivalence can provide some \nguidance such that the same standards of disclosure for human-\ndriven decisions is applied to decisions that have been made or \naugmented by an AI system.\n \n54\n3.49 Appropriate interaction and communication inspire trust and \nconfidence as they build and maintain open relationships between \norganisations and individuals (including employees). Stakeholder \nrelationship strategies should also not remain static. Companies \nare encouraged to test, evaluate and review their strategies for \neffectiveness. Further, the extent and mode of implementation \nof these factors could vary from scenario to scenario.\n3.50 As different stakeholders have different information needs, an \norganisation can start by first identifying its audience (i.e. its \nexternal and internal stakeholders). An organisation\u2019s external \nstakeholders may include consumers, regulators, other \norganisations it does business with, and society at large. Its \ninternal stakeholders may include the organisation\u2019s board, \nmanagement and employees. An organisation can also consider \nthe purpose and the context of the interaction with its \nstakeholders. For the purposes of illustration, this Model \nFramework provides considerations for interacting with consumers \nand other organisations.\nInteracting with consumers\n3.51 Organisations are encouraged to consider the information needs \nof consumers as they go through the journey of interacting with \nAI, from considering whether to use an AI solution, to \nunderstanding how the AI solution works as they use it, to \nrequesting for reviews on the decisions made by the AI solution. \nA typical consumer journey may entail meeting the following \ninformation needs of consumers:\nBringing explainability and transparency \ntogether in a meaningful way\nAs different stakeholders have \ndifferent information needs, an \norganisation can start by first \nidentifying its audience and \nconsidering the purpose and the \ncontext of the interaction.\nMODEL ARTIFICIAL INTELLIGENCE GOVERNANCE FRAMEWORK   55\na. Making sure that consumers are aware that the \nproducts or services that they are considering are \nAI-enabled. Such information could be provided \nas part of a general product description. \n \nb. Providing information so that consumers know how \nthe AI-enabled features are expected to behave \nduring normal use. The information could be \nprovided in more detailed descriptions or \nspecifications of product features.  \n \n \nThis, however, may not be necessary for every \nfeature that is AI-enabled. Organisations are \nencouraged to identify those features where \nproviding additional information in this manner will \nenhance consumer trust. Similarly, if AI is used in \ndecision-making, information may be provided so \nthat consumers understand how decisions made \nwith the assistance of AI may affect them. This can \nlikewise be provided through descriptions of how \nthe service will be provided.  \n \nc. For AI-enabled features that consumers interact \nwith regularly, providing information so that they \nunderstand why the AI-enabled feature is behaving \nin a certain way, and providing preference settings \nto allow consumers some influence over future \nbehaviour where possible. As doing so requires \nmore engineering effort (such as in the providing \nadditional user interfaces to user history), and the \nlevel of information provided may be somewhat \nmore detailed and personalised than feature \ndescriptions, organisations will have to decide which \nof their product features will benefit from provision \nof this level of detail. \n \nd. For AI-augmented decisions that affect consumers, \nconsider providing additional information so that \nthey understand why the decisions were made; and \nfor certain categories of such decisions, providing \nan appropriate channel to contest such decisions. \nThe level of information that is provided will \nnecessarily be detailed but this may not be necessary \nexcept for those scenarios where a customer is \naffected by the decision.\n56\nOption to opt-out\n3.52 Organisations may wish to consider carefully when deciding \nwhether to provide individuals with the option to opt out from \nthe use of the AI product or service, and whether this option \nshould be offered by default or only upon request. Relevant \nconsiderations include:\na. Degree of risk/harm to the individuals;\nb. Reversibility of the decision made;\nc. Availability of alternative decision-making mechanisms;\nd. Cost or trade-offs of alternative mechanisms;\ne. Complexity and inefficiency of maintaining parallel systems; \nand\nf. Technical feasibility.\n3.53 Where an organisation has weighed the factors above and decided \nnot to provide an option to opt out, it is prudent for the organisation \nto consider providing modes of recourse to the consumer such \nas providing a channel for reviewing the decision. Where \nappropriate, organisations may also wish to keep a history of \nchatbot conversations when facing complaints or seeking recourse \nfrom consumers.\nCommunication channels\n3.54 Organisations are encouraged to put in place the following \ncommunications channels for their customers:\nMODEL ARTIFICIAL INTELLIGENCE GOVERNANCE FRAMEWORK   57\nThis channel could be used for \ncustomers to raise feedback or \nraise queries. It could be managed by an organisation\u2019s Data \nProtection Officer (\u201cDPO\u201d) if this is appropriate. Where \ncustomers find inaccuracies in their personal data which has \nbeen used for decisions affecting them, this channel can also \nallow them to correct their data. Such correction and feedback, \nin turn, maintain data veracity. It could also be managed by \nan organisation\u2019s Quality Service Manager (\u201cQSM\u201d) if \nstakeholders wish to raise feedback and queries on material \ninferences made about them.  \nApart from existing review \nobligations, organisations can \nconsider providing an avenue \nfor individuals (such as an aggrieved consumer) to request a \nreview of material AI decisions that have affected them. Where \nthe effect of a fully-autonomous decision on a consumer may \nbe material, it would be reasonable to provide an opportunity \nfor the decision to be reviewed by a human. \nTesting the user interface\n3.55 Organisations are encouraged to test user interfaces and address \nusability problems before deployment, so that the user interface \nserves its intended purposes. If applicable, organisations are \nalso encouraged to inform individuals that their responses would \nbe used to train the AI system (e.g. a chatbot). Organisations \nshould be aware of the risks of using such responses as some \nindividuals may intentionally use \u201cbad language\u201d or \u201crandom \nreplies\u201d which would affect the training of the AI system.\nEasy-to-understand communications\n3.56 Organisations are encouraged to communicate in an easy-to-\nunderstand manner to increase transparency. There are existing \ntools to measure readability, such as the Fry readability graph, \nthe Gunning Fog Index, the Flesch-Kincaid readability tests, etc. \nIt would be helpful for decisions with higher impact to be \ncommunicated in an easy-to-understand manner, with the need \nto be transparent about the technology being used. Besides \ntextual communications, organisations can also consider using \nvisualisation tools, graphical representations, summary tables, \nor a combination of these. The priority is to convey your \ninformation, such as an explanation or interpretation, in a way \nthat is understandable by an organisation\u2019s consumers and other \nstakeholders.\nFeedback channels\na. \nDecision review \nchannels\nb. \n58\nAcceptable user policies\n3.57 In certain cases, organisations may be implementing AI-powered \nsolutions that are also trained on real-life input data (i.e. active \nlearning). These organisations may wish to consider setting out \ncertain acceptable user policies (\u201cAUPs\u201d) to ensure that users \ndo not maliciously introduce input data that unacceptably \nmanipulates the performance and/or results of the solution\u2019s \nmodel. This is pertinent, given past examples of AI chatbot \nsystems that have been unduly manipulated to issue publicly-\nunacceptable responses.\n3.58 In this regard, AUPs serve to set broad boundaries for the \ninteractions that individuals can perform with the AI system, such \nas restrictions with regard to intentional actions or attempts to \nreverse engineer, disable, interfere or disrupt the functionality, \nintegrity or performance of the AI-powered service. \nInteracting with other organisations\n3.59 Some of the approaches and methodologies described in the \npreceding section are also relevant when organisations interact \nwith AI solution providers (such as procuring AI solutions and \nobtaining regulatory approval), or other organisations (such as \nfacilitating industry collaboration, enabling interoperability of \nsystems). Organisations would thus need to obtain sufficient \ninformation from AI solution providers to help them meet their \nbusiness objectives (for example, this could be a back-to-back \narrangement for providing the information described in paragraph \n3.51). This could be as straightforward as obtaining the AI solution \nproviders\u2019 support to provide the information10 and to build the \nfeatures11 necessary such that the deploying organisation can \nalign itself to the Model Framework.\nFor example, information related to lineage of the training dataset and documenting the \nkey steps in the model training and selection process.\n10\nFor example, the user-facing interactions providing information about the expected behavior \nof an AI-powered feature and building the function to allow users to manage preference \nsettings that influence how the AI-powered feature will perform for them in future.\n11\nMODEL ARTIFICIAL INTELLIGENCE GOVERNANCE FRAMEWORK   59\n3.60 Organisations may have to consider the level of support and \ndetailed information that they may need to obtain from AI solution \nproviders pertaining to:\na. Data (e.g. types and range of data used in training the \nalgorithm, source and quality of external training data);\nb. Model training and selection (e.g. features and variables used \nand weights of the commercial models supplied, documenting \nthe key decisions made with respect to the model training \nand selection);\nc. Human elements (e.g. nature of human involvement in \ndeveloping the algorithm, or in the decision-making process);\nd. Inferences (e.g. predictions made by the algorithm and how \nthese are incorporated into product features or decision-\nmaking);\ne. Algorithmic presence (e.g. where in the solution that an \nalgorithm is used); and\nf. Measures and safeguards in place to mitigate biases in data \nand algorithms.\n3.61 Depending on the purpose and context, the type and level of \ndetail of information required may be different. For example, a \nregulator may require a regulated entity to demonstrate that its \nmodel development and selection process is sufficiently rigorous, \nand the AI solution provider may be required to provide more \ninformation and be involved in the clarification process with the \nregulator. An industry collaborator, on the hand, may be more \nconcerned with factors pertaining to compatibility and \ninteroperability. \nEthical evaluation\n3.62 Finally, as ethical standards governing the development and use \nof AI evolve, organisations are encouraged to evaluate whether \ntheir AI governance practices and processes are in line with \nevolving AI standards, and make available the outcome of such \nevaluations to relevant stakeholders.\n60\nIn particular, Facebook strives to be meaningfully transparent with its \nusers by:\nProviding a general disclosure about Facebook\u2019s collection and use of \ndata in an easy-to-understand manner. This is achieved through its \nTerms of Service and Data Policy, accompanied by explanatory and user-\nfriendly videos;  \nb. Giving users easy-to-use and meaningful control over how their information \nwill be used and shared; \n \nc. Publishing a policy for explanation through a series of blogposts to discuss \ncomplex subjects, explain the rationale of Facebook\u2019s decisions and invite \nexperts to share their opinions. For example, Facebook published a \u201cHard \nQuestions\u201d blogpost on Face Recognition, which discussed how Facebook \nused face recognition to help users to tag photos and the controls \nimplemented;\nd. Promoting a series of AI educational initiatives and campaigns to help users \nlearn about the technology that underlies the various products and features. \nFor example, Facebook\u2019s Artificial Intelligence Research Lab developed and \npublished a series of AI Education videos to explain Machine Learning \nalgorithms and how it is being used.  \nFacebook currently provides users with a customised News Feed that shows \nposts that are most relevant to them. The content of the News Feed is determined \nby the people and pages a user chooses to friend and follow.\nAs part of Facebook\u2019s consultation on its News Feed feature, Facebook took \ninto consideration:\na. The need to be transparent and provide more information about the \nalgorithms behind the feature; \nILLUSTRATION FOR STAKEHOLDER \nINTERACTION AND COMMUNICATION\nFACEBOOK:\nAs a social media and technology company, Facebook is committed to being \ntransparent with the public and users on its operations and services, which \nincludes the use of AI. \na.\nMODEL ARTIFICIAL INTELLIGENCE GOVERNANCE FRAMEWORK   61\nb. The type of information that would be most valuable to users. For example, \nFacebook included examples of people\u2019s interactions that contributed to \nposts on News Feed; and \n \nc. The need for users to take control or manage its News Feed.\nWith this, Facebook put in place the following to build trust with users:   \na. Implement a \u201cWhy am I seeing this post?\u201d feature to explain how users\u2019 \npast interactions impacted the ranking of posts in the News Feed. Specifically, \nusers were able to learn: \n \ni. The reason for viewing a certain post in the News Feed. For example, \nthe post could be from a friend or Group or Page that the user has \nfollowed. \n \nii. Information that had the largest influence over the order of posts, \nincluding: (a) the frequency in which the user interacts with posts from \npeople, Pages or Groups; (b) the frequency in which the user interacts \nwith a specific type of post (e.g. videos, photos or links); and (c) the \npopularity of the posts shared by the people, Pages and Groups that \nthe user follows. \n \niii. Shortcuts to controls that help users personalise its News Feed such as \nSee First, Unfollow, News Feed Preferences and Privacy Shortcuts.  \nb. Publish a series of \u201cNews Feed FYI\u201d blog posts that highlighted and explained \nthe rationale for key updates to News Feed.  \n \nc. Launch a new \u201cInside Feed\u201d website that provided greater detail on how \nFacebook\u2019s systems worked and the way Facebook evaluated the changes.\n62\nPrior to deployment, MSD\u2019s User Experience (\u201cUX\u201d) team tested the human-AI \ninterface and addressed usability problems to ensure optimal user interaction \nwith Jennie. In particular, three tenets guided the development and deployment \nof Jennie:\na. Understanding a user\u2019s mental model: The UX team conducted user \nresearch with representative users to understand users\u2019 expectations \nwhen interacting with a chatbot. The research covered the scope of IT \nquestions, expected answers and the kinds of answers provided (e.g. \nhow technical the answers should be phrased). By building a user-friendly \ninterface, users would be more comfortable to interact and use the chatbot. \nb. Taking a human-centric approach: To understand patterns of human \nbehaviour, the team analysed how its employees reacted when faced with \nchallenges in interacting with the chatbot. Examples include how users \nformulate questions, what types of answers satisfy users and how many times \na chatbot should be allowed to attempt an answer. After understanding \nthe human interaction touchpoints, the team used these insights to create \nan information flow architecture to deliver a better experience for users. \nc. Managing the bot-human handover: There could be instances where \nJennie might not be able to provide a satisfactory answer. In such instances, \nthe UX team determined that the chatbot would have a maximum of \nthree attempts to provide a satisfactory reply, before forwarding the user\u2019s \nrequest and chat logs to a customer care executive for follow-up. \nWhen employees engage Jennie, MSD will disclose on the landing page that \nJennie is AI-powered and is a beta version which will improve over time (see \nfigure below).\nILLUSTRATION FOR STAKEHOLDER \nINTERACTION AND COMMUNICATION\nMSD:\nMSD is a multinational pharmaceutical company that deploys an in-house \nchatbot, Jennie, to answer queries on IT-related matters. \nMODEL ARTIFICIAL INTELLIGENCE GOVERNANCE FRAMEWORK   63\nCONCLUSION\n3.63 This Model Framework is by no means complete or exhaustive \nand remains a document open to feedback. As AI technologies \nevolve, so would the related ethical and governance issues. It is \nthe PDPC\u2019s aim to update this Model Framework periodically \nwith the feedback received, to ensure that it remains relevant \nand useful to organisations deploying AI solutions.\nAppropriate communication regarding the \nuse of AI inspires trust as it builds and \nmaintains open relationships between \norganisations and individuals.\n64\nANNEX A\nFOR REFERENCE: A COMPILATION OF EXISTING AI ETHICAL PRINCIPLES \nThis annex comprises a collection of foundational AI ethical principles, distilled from various \nsources.12 Not all are included or addressed in the Model Framework. Organisations \nmay consider incorporating these principles into their own corporate principles, where \nrelevant and desired.\n1. Accountability: Ensure that AI actors are responsible and accountable for the proper \nfunctioning of AI systems and for the respect of AI ethics and principles, based on \ntheir roles, the context, and consistency with the state of art. \n \n \n \n \n2. Accuracy: Identify, log, and articulate sources of error and uncertainty throughout \nthe algorithm and its data sources so that expected and worst-case implications \ncan be understood and can inform mitigation procedures. \n \n \n3. Auditability: Enable interested third parties to probe, understand, and review the \nbehaviour of the algorithm through disclosure of information that enables monitoring, \nchecking or criticism. \n \n \n \n \n \n \n \n \n \n4. Explainability: Ensure that automated and algorithmic decisions and any associated \ndata driving those decisions can be explained to end-users and other stakeholders in \nnon-technical terms. \n \n \n \n \n \n \n \n \n \n5. Fairness: \na. \nEnsure that algorithmic decisions do not create discriminatory or unjust impacts \nacross different demographic lines (e.g. race, sex, etc.). \n \n \n \n \nb. To    develop    and    include     monitoring    and    accounting    mechanisms    to    avoid \nunintentional discrimination when implementing decision-making systems. \n \nc. \nTo consult a diversity of voices and demographics when developing systems, \napplications and algorithms.\nThese include the Institute of Electrical and Electronics Engineers (\u201cIEEE\u201d) Standards Association\u2019s Ethically Aligned \nDesign (https://standards.ieee.org/industry-connections/ec/ead-v1.html), Software and Information Industry Association\u2019s \nEthical Principles for Artificial Intelligence and Data Analytics (https://www.siia.net/Portals/0/pdf/Policy/Ethical%20\nPrinciples%20for%20Artificial%20Intelligence%20and%20Data%20Analytics%20SIIA%20Issue%20Brief.\npdf?ver=2017-11-06-160346-990) and Fairness, Accountability and Transparency in Machine Learning\u2019s Principles for \nAccountable Algorithms and a Social Impact Statement for Algorithms (http://www.fatml.org/resources/principles-for-\naccountable-algorithms). There is also the European Commission\u2019s Communication From The Commission To The European \nParliament, The Council, The European Economic And Social Committee And The Committee Of The Regions - Building \nTrust in Human-Centric Artificial Intelligence (https://ec.europa.eu/newsroom/dae/document.cfm?doc_id=58496), and \nthe OECD\u2019s Recommendation of the Council on Artificial Intelligence (https://legalinstruments.oecd.org/en/instruments/\nOECD-LEGAL-0449). They also include principles raised through consultation feedback from the industry.\n12\nMODEL ARTIFICIAL INTELLIGENCE GOVERNANCE FRAMEWORK   65\n6. Human Centricity and Well-being:\na. To aim for an equitable distribution of the benefits of data practices and avoid data \npractices that disproportionately disadvantage vulnerable groups.  \n \n \nb. To aim to create the greatest possible benefit from the use of data and advanced \nmodelling techniques. \n \n \n \n \n \n \n \n \nc. Engage in data practices that encourage the practice of virtues that contribute to \nhuman flourishing, human dignity and human autonomy. \n \n \n \n \nd. To give weight to the considered judgements of people or communities affected \nby data practices and to be aligned with the values and ethical principles \nof the people or communities affected. \n \n \n \n \n \n \ne. To make decisions that should cause no foreseeable harm to the individual, or \nshould at least minimise such harm (in necessary circumstances, when weighed \nagainst the greater good). \n \n \n \n \n \n \n \n \nf. To allow users to maintain control over the data being used, the context such data \nis being used in and the ability to modify that use and context. \n \n \n \ng. To ensure that the overall well-being of the user should be central to the AI \nsystem\u2019s functionality. \n \n \n \n \n \n \n \n \n \n7. Human rights alignment: Ensure that the design, development and implementation of \n \ntechnologies do not infringe internationally recognised human rights.  \n \n  \n8. Inclusivity: Ensure that AI is accessible to all. \n \n9. Progressiveness: Favour implementations where the value created is materially better \n \nthan not engaging in that project.\n66\n10.  Responsibility, accountability and transparency:\na. Build trust by ensuring that designers and operators are responsible and accountable \nfor their systems, applications and algorithms, and to ensure that such systems, \napplications and algorithms operate in a transparent and fair manner. \n \n \nb. To make available externally visible and impartial avenues of redress for adverse \nindividual or societal effects of an algorithmic decision system, and to designate a \nrole to a person or office who is responsible for the timely remedy of such issues. \nc. Incorporate downstream measures and processes for users or stakeholders to verify \nhow and when AI technology is being applied. \n \n \n \n \n \nd. To keep detailed records of design processes and decision-making.\n11. Robustness and Security: AI systems should be safe and secure, not vulnerable to \n \ntampering or compromising the data they are trained on. \n \n12. Sustainability: Favour implementations that effectively predict future behaviour and \n \ngenerate beneficial insights over a reasonable period of time.\nMODEL ARTIFICIAL INTELLIGENCE GOVERNANCE FRAMEWORK   67\nANNEX B\nALGORITHM AUDITS\n1. Algorithm audits are conducted if it is necessary to discover the actual operations of \nalgorithms comprised in models. This would have to be carried out at the request of a \nregulator (as part of a forensic investigation) having jurisdiction over the organisation or \nby an AI technology provider to assist its customer organisation which has to respond to \na regulator\u2019s request. Conducting an algorithm audit requires technical expertise which \nmay require engaging external experts. The audit report may be beyond the understanding \nof most individuals and organisations. The expense and time required to conduct an \nalgorithm audit should be weighed against the expected benefits obtained from the \naudit report. Ultimately, algorithm audits should normally be used when it is reasonably \nclear that such an audit will yield clear benefits for an investigation. \n \n2. The following factors may be relevant when considering an algorithm audit:  \na. The purpose for conducting an algorithm audit. The Model Framework promotes \nthe provision of information about how AI models function as part of explainable AI. \nBefore embarking on an algorithm audit, it is advisable to consider whether the \ninformation that has already been made available to individuals, other organisations \nor businesses, and regulators is sufficient and credible (e.g. product or service \ndescriptions, system technical specifications, model training and selection records, \ndata provenance record, audit trail). \n \nb. Target audience of audit results. This refers to the expertise required of the target \naudience to effectively understand the data, algorithm and/or models. The information \nrequired by different audience vary. When the audience consists of individuals, \nproviding information on the decision-making process and/or how the individuals\u2019 \ndata is used in such processes will achieve the objective of explainable AI more \nefficaciously. When the audience consists of regulators, information relating to data \naccountability and the functioning of algorithms should be examined first. An algorithm \naudit can prove how an AI model operates if there is reason to doubt the veracity or \ncompleteness of information about its operation. \n \nc. General data accountability. Organisations can provide information on how general \ndata accountability is achieved within the organisations. This includes all the good \ndata practices described in the Model Framework under Data for Model Development \nsection such as maintaining data lineage through keeping a data provenance record, \nensuring data accuracy, minimising inherent bias in data, splitting data for different \npurposes, determining data veracity and reviewing and updating data regularly.  \nd. Algorithms in AI models can be commercially valuable information that can affect \nmarket competitiveness. For example, the algorithm may be a trade secret or may embody \nbusiness rules that are trade secrets. If a technical audit is contemplated, corresponding \nmitigation measures should also be considered (e.g. non-disclosure agreements).\n68\nACKNOWLEDGEMENTS\nThe PDPC expresses its sincere appreciation to the following individuals and organisations \nfor their valuable feedback to the Model Framework (in alphabetical order):\nA*STAR\nAccenture\nAIG Asia Pacific Insurance Pte. Ltd.\nApple\nAsia Cloud Computing Association\nAsiaDPO\nBSA | The Software Alliance\nCambrian AI\nCUJO.AI\nData Synergies\nDBS\nElement AI\nEmerging Technologies Policy Forum\nFacebook \nFountain Court Chambers\nGoogle\nGrab\nGreat Eastern\nGSK\nIBM Asia Pacific\nLawTech.Asia \nMastercard\nMicrosoft Asia\nMSD International GmBH (Singapore branch)\nNon-Profit Working Group on AI\nOCBC Bank\nPwC\npymetrics\nSalesforce\nSingtel\nStandard Chartered Bank\nSuade Labs\nSymphony AyasdiAI\nTelenor Group \nTemasek International\nTookitaki\nUCARE.AI\nUntangle AI\nMODEL ARTIFICIAL INTELLIGENCE GOVERNANCE FRAMEWORK   69\n70\nBROUGHT TO YOU BY\n#SGDIGITAL\nSingapore Digital (SG:D) gives Singapore\u2019s digitalisation \nefforts a face, identifying our digital programmes and \ninitiatives with one set of visuals, and speaking to our \nlocal and international audiences in the same language. \nThe SG:D logo is made up of rounded fonts that evolve \nfrom the expressive dot that is red. SG stands for \nSingapore and :D refers to our digital economy. The :D \nsmiley face icon also signifies the optimism of Singaporeans \nmoving into a digital economy. As we progress into the \ndigital economy, it\u2019s all about the people - empathy and \nassurance will be at the heart of all that we do.\nCopyright 2020 \u2013 Info-communications Media Development Authority (IMDA) and \nPersonal Data Protection Commission (PDPC)\nThis publication is intended to foster responsible development and adoption of \nArtificial Intelligence. The contents herein are not intended to be an authoritative \nstatement of the law or a substitute for legal or other professional advice. The PDPC \nand its members, officers and employees shall not be responsible for any inaccuracy, \nerror or omission in this publication or liable for any damage or loss of any kind as \na result of any use of or reliance on this publication. \nThe contents of this publication are protected by copyright, trademark or other forms \nof proprietary rights and may not be reproduced, republished or transmitted in any \nform or by any means, in whole or in part, without written permission.\nwww.imda.gov.sg\nwww.pdpc.gov.sg\n", "metadata": {"country": "Singapore", "year": "2020", "legally_binding": "no", "binding_proof": "Voluntary governance principles issued by IMDA and PDPC for responsible AI development. Not legally binding.", "date": "01/22/2020", "regulator": "Infocomm Media Development Authority (IMDA) & Personal Data Protection Commission (PDPC)", "type": "AI governance framework", "status": "pending", "language": "English", "use_cases": "[1, 3, 5, 6]"}}
{"_id": "686acc67e2af9fb1ff3ba992", "title": "Model AI Governance Framework for Generative AI", "source": "https://aiverifyfoundation.sg/downloads/Discussion_Paper.pdf", "text": "Executive Summary\b\n3\n\t\nAccountability\b\n6\n\t\nData\b\n9\n\t\n\u0007Trusted Development\b\n12 \nand Deployment\n\t\nIncident Reporting\b\n16\n\t\nTesting and Assurance\b\n19\n\t\nSecurity\b\n21\n\t\nContent Provenance\b\n23\n\t\nSafety and Alignment R&D\b\n26\n\t\nAI for Public Good\b\n28\nConclusion\b\n31\nAcknowledgements\b\n32\nFurther Development\b\n34\nCONTENTS\n3\nMODEL AI GOVERNANCE FRAMEWORK FOR GENERATIVE AI\nGenerative AI has captured the world\u2019s imagination. While it holds significant \ntransformative potential, it also comes with risks. Building a trusted ecosystem is \ntherefore critical \u2014 it helps people embrace AI with confidence, gives maximal space \nfor innovation, and serves as a core foundation to harnessing AI for the Public Good. \nAI, as a whole, is a technology that has been developing over the years. Prior \ndevelopment and deployment is sometimes termed traditional AI.1 To lay the \ngroundwork to promote the responsible use of traditional AI, Singapore released \nthe first version of the Model AI Governance Framework in 2019, and updated it \nsubsequently in 2020.2 The recent advent of generative AI 3 has reinforced some \nof the same AI risks (e.g., bias, misuse, lack of explainability), and introduced new \nones (e.g., hallucination, copyright infringement, value alignment). These concerns \nwere highlighted in our earlier Discussion Paper on Generative AI: Implications for \nTrust and Governance,4 issued in June 2023. The discussions and feedback have \nbeen instructive. \nExisting governance frameworks need to be reviewed to foster a broader trusted \necosystem. A careful balance needs to be struck between protecting users and \ndriving innovation. There have also been various international discussions pulling \nin the related and pertinent topics of accountability, copyright and misinformation, \namong others. These issues are interconnected and need to be viewed in a practical \nand holistic manner. No single intervention will be a silver bullet. \nThis Model AI Governance Framework for Generative AI therefore seeks to set \nforth a systematic and balanced approach to address generative AI concerns \nwhile continuing to facilitate innovation. It requires all key stakeholders, including \npolicymakers, industry, the research community and the broader public, to collectively \ndo their part. There are nine dimensions which the Framework proposes to be looked \nat in totality, to foster a trusted ecosystem.\na)\t Accountability \u2014 Accountability is a key consideration to incentivise players \nalong the AI development chain to be responsible to end-users. In doing so, we \nrecognise that generative AI, like most software development, involves multiple \nlayers in the tech stack, and hence the allocation of responsibility may not be \nimmediately clear. While generative AI development has unique characteristics, \nuseful parallels can still be drawn with today\u2019s cloud and software development \nstacks, and initial practical steps can be taken.\nEXECUTIVE SUMMARY\n1\t \u0007Traditional AI refers to AI models that make predictions by leveraging insights derived from historical data. Typical traditional AI models include \nlogistic regression, decision trees and conditional random fields. Other terms used to describe this include \u201cdiscriminative AI\u201d.\n2\t \u0007The focus of the Model AI Governance Framework is to set out best practices for the development and deployment of traditional AI solutions. This \nhas been incorporated into and expanded under the Trusted Development and Deployment dimension of the Model AI Governance Framework \nfor Generative AI.\n3\t \u0007Generative AI are AI models capable of generating text, images or other media types. They learn the patterns and structure of their input training data \nand generate new data with similar characteristics. Advances in transformer-based deep neural networks enable generative AI to accept natural \nlanguage prompts as input, including large language models (LLM) such as GPT-4, Gemini, Claude and LLaMA.\n4\t \u0007The Discussion Paper was jointly published by the Infocomm Media Development Authority of Singapore (IMDA), Aicadium and AI Verify Foundation. \nSee https://aiverifyfoundation.sg/downloads/Discussion_Paper.pdf\nMODEL AI GOVERNANCE FRAMEWORK FOR GENERATIVE AI\n4\nb)\t Data \u2014 Data is a core element of model development. It significantly impacts \nthe quality of the model output. Hence, what is fed to the model is important \nand there is a need to ensure data quality, such as through the use of trusted \ndata sources. In cases where the use of data for model training is potentially \ncontentious, such as personal data and copyright material, it is also important \nto give business clarity, ensure fair treatment, and to do so in a pragmatic way.\nc)\t Trusted Development and Deployment \u2014 Model development, and the application \ndeployment on top of it, are at the core of AI-driven innovation. Notwithstanding \nthe limited visibility that end-users may have, meaningful transparency around \nthe baseline safety and hygiene measures undertaken is key. This involves \nindustry adopting best practices in development, evaluation, and thereafter \n\u201cfood label\u201d-type transparency and disclosure. This can enhance broader \nawareness and safety over time.\nd)\t Incident Reporting \u2014 Even with the most robust development processes and \nsafeguards, no software we use today is completely foolproof. The same \napplies to AI. Incident reporting is an established practice, and allows for timely \nnotification and remediation. Establishing structures and processes to enable \nincident monitoring and reporting is therefore key. This also supports continuous \nimprovement of AI systems. \ne)\t Testing and Assurance \u2014 For a trusted ecosystem, third-party testing and \nassurance plays a complementary role. We do this today in many domains, \nsuch as finance and healthcare, to enable independent verification. Although \nAI testing is an emerging field, it is valuable for companies to adopt third-party \ntesting and assurance to demonstrate trust with their end-users. It is also \nimportant to develop common standards around AI testing to ensure quality \nand consistency.\nf)\t Security \u2014 Generative AI introduces the potential for new threat vectors against \nthe models themselves. This goes beyond security risks inherent in any software \nstack. While this is a nascent area, existing frameworks for information security \nneed to be adapted and new testing tools developed to address these risks.\ng)\t Content Provenance \u2014 AI-generated content, because of the ease with which \nit can be created, can exacerbate misinformation. Transparency about where \nand how content is generated enables end-users to determine how to consume \nonline content in an informed manner. Governments are looking to technical \nsolutions like digital watermarking and cryptographic provenance. These \ntechnologies need to be used in the right context.\nh)\t Safety and Alignment Research & Development (R&D) \u2014 The state-of-the-\nscience today for model safety does not fully cover all risks. Accelerated \ninvestment in R&D is required to improve model alignment with human intention \nand values. Global cooperation among AI safety R&D institutes will be critical to \noptimise limited resources for maximum impact, and keep pace with commercially \ndriven growth in model capabilities.\ni)\t AI for Public Good \u2014 Responsible AI goes beyond risk mitigation. It is also about \nuplifting and empowering our people and businesses to thrive in an AI-enabled \nfuture. Democratising AI access, improving public sector AI adoption, upskilling \nworkers and developing AI systems sustainably will support efforts to steer AI \ntowards the Public Good.\n5\nMODEL AI GOVERNANCE FRAMEWORK FOR GENERATIVE AI\nThis Framework builds on the policy ideas highlighted in our Discussion Paper on Generative AI and draws \nfrom insights and discussions with key jurisdictions, international organisations, research communities \nand leading AI organisations. The Framework will evolve as technology and policy discussions develop.\nFostering a Trusted AI Ecosystem\n1. Accountability\n Putting in place the right incentive structure for different players in the\nAI system development life cycle to be responsible to end-users\n8. Safety and Alignment R&D\nAccelerating R&D through global cooperation among AI Safety Institutes to\nimprove model alignment with human intention and values\n9. AI for Public Good\nResponsible AI includes harnessing AI to benefit the public by democratising access,\nimproving public sector adoption, upskilling workers and developing AI systems sustainably\n6. Security\nAddressing new threat vectors that arise \nthrough generative AI models\n7. Content Provenance\nTransparency about where content comes\nfrom as useful signals for end-users\n2. Data\nEnsuring data quality \nand addressing \npotentially contentious \ntraining data in a \npragmatic way, as \ndata is core to model \ndevelopment\n3. Trusted \nDevelopment and \nDeployment\nEnhancing \ntransparency around \nbaseline safety and \nhygiene measures \nbased on industry \nbest practices \nin development, \nevaluation and \ndisclosure\n4. Incident \nReporting\nImplementing an \nincident management \nsystem for timely \nnotification, \nremediation \nand continuous \nimprovements, as no \nAI system is foolproof\n5. Testing and \nAssurance\nProviding external \nvalidation and \nadded trust through \nthird-party testing, \nand developing \ncommon AI testing \nstandards for \nconsistency\nACCOUNTABILITY\n7\nMODEL AI GOVERNANCE FRAMEWORK FOR GENERATIVE AI\nAccountability is a key consideration in fostering a trusted ecosystem. Players \nalong the AI development chain need to be responsible towards end-users,5 and \nthe structural incentives should align with this need. These players include model \ndevelopers, application deployers6 and cloud service providers (who often provide \nplatforms on which AI applications are hosted). Generative AI, like most software \ndevelopment, involves multiple layers in the tech stack. While the allocation of \nresponsibility may not be immediately clear, useful parallels can be drawn with \ntoday\u2019s cloud and software development, and practical steps can be taken.\nDesign \nTo do this comprehensively, there should be consideration for how responsibility \nis allocated both upfront in the development process (ex-ante) as best practice, \nand guidance on how redress can be obtained if issues are discovered thereafter \n(ex-post). \nEx Ante \u2014 Allocation Upfront\nResponsibility can be allocated based on the level of control that each stakeholder \nhas in the generative AI development chain, so that the able party takes necessary \naction to protect end-users. As a reference, while there may be various stakeholders \nin the development chain, the cloud industry7 has built and codified comprehensive \nshared responsibility models over time. The objective is to ensure overall security \nof the cloud environment. These models allocate responsibility by explaining \nthe controls and measures that cloud service providers (who provide the base \ninfrastructure layer) and their customers (who host applications on the layer above) \nrespectively undertake. \nThere is value in extending this approach to AI development. Cloud service \nproviders have recently extended some elements of their cloud shared responsibility \nmodels to cover AI, placing initial focus on security controls.8 This is a good start, \nand a similar approach can be taken to address other safety concerns. The AI \nshared responsibility approach may also need to consider different model types \n(e.g., closed-source, open-source9 or open-weights10), given the different levels \nof control that application deployers have for each model type. Responsibility in \nFOSTERING A TRUSTED AI ECOSYSTEM\nACCOUNTABILITY\n5\t \u0007While the Framework places emphasis on allocating responsibilities for AI development, end-users have separate responsibilities for AI use \n(e.g., abiding by terms of use).\n6\t \u0007We recognise that the generative AI development chain is complex. Application developers (who develop solutions or applications that make use \nof AI technology) and application deployers (who provide AI solutions or applications to end-users) can sometimes be two different parties. For \nsimplicity, this paper uses the term \u201capplication deployers\u201d to refer to both application developers and deployers. \n7\t \u0007This includes Google Cloud, Microsoft Azure and Amazon Web Services.\n8\t \u0007Microsoft, which is both a cloud and model service provider, has initiated some elements of this. See https://learn.microsoft.com/en-us/azure/\nsecurity/fundamentals/shared-responsibility-ai\n9\t \u0007Open-sourcing makes available the full source code and information required for re-training the model from scratch, including model architecture \ncode, training methodology and hyperparameters, original training dataset and documentation. Models that are closer to this end of the spectrum \n(but not fully open) include Dolly and BLOOMZ. \n10\t\u0007Open-weights makes available pre-trained parameters or weights of the model itself, but not the training code, dataset, methodology, etc. Existing \nopen-weights models include LlaMa2, Falcon-40B-Instruct and Mistral 7B-Instruct.\nMODEL AI GOVERNANCE FRAMEWORK FOR GENERATIVE AI\n8\n11\t \u0007The details of how responsibilities will be allocated are key and will need to be worked out gradually. \n12\t\u0007For example, Adobe, Anthropic, Google, Microsoft and OpenAI. \n13\t\u0007Under a no-fault insurance approach, stakeholders\u2019 expenses are covered regardless of who is at fault. It is currently adopted in the US for some \ntypes of motor accident claims. This insurance approach in the AI context warrants further study. \nthis case, for example when using open-source or open-weights models, should \nrequire application deployers to download models from reputable platforms to \nminimise the risk of tampered models. Being the most knowledgeable about their \nown models and how they are deployed, model developers are well-placed to \nlead this development in a concerted manner.11 This will provide stakeholders with \ngreater certainty upfront, and foster a safer ecosystem.\nEx Post \u2014 Safety Nets\nShared responsibility models serve as an important foundation for accountability \n\u2014 they provide clarity on redress when issues occur. However, they may not be \nable to cover all possible scenarios. Allocating responsibility when there are new or \nunanticipated issues may also be practically challenging. It will be worth considering \nadditional measures \u2014 including concepts around indemnity and insurance \u2014 to \nbetter cover end-users.\nThis exists in a limited form today. In clearer areas where redress is needed, the \nindustry has moved accordingly. Some model developers12 have begun to underwrite \ncertain risks, such as third-party copyright claims arising from the use of their \nAI products and services. In doing so, developers implicitly acknowledge their \nresponsibility for model training data and how their models are used.\nThere will inevitably be other areas that are not as clear and not well-covered. This \nmay include risks that have disproportionate impact on society as a whole, and \nwhich may only emerge as AI is used. It is therefore useful to consider updating \nlegal frameworks to make them more flexible, and to allow emerging risks to be \neasily and fairly addressed. This is akin to how end-users of physical products \ntoday enjoy safety protections. One example of such efforts is the EU\u2019s proposed AI \nLiability Directive and soon-to-be approved Revised Product Liability Directive. These \nDirectives aim to make it simpler for end-users to prove damage caused by AI-\nenabled products and services. This ensures that no party is unfairly disadvantaged \nby the compensation process.\nFinally, there are bound to be residual issues that fall through the cracks. This is \na very nascent discussion, and alternative solutions such as no-fault insurance13 \ncould be considered as a safety net.\nDATA\nMODEL AI GOVERNANCE FRAMEWORK FOR GENERATIVE AI\n10\nData is a core element of model and application development. A large corpus of data \nis needed to train robust and reliable AI models. Given its importance, businesses \nrequire clarity and certainty on how they can use data in model development. This \nincludes potentially contentious areas such as publicly available personal data and \ncopyright material, which are typically included in web-scraped datasets. In such \ncases, it is important to recognise competing concerns, ensure fair treatment, and \nto do so in a pragmatic way. In addition, developing a model well requires good \nquality data, and in some circumstances, representative data. It is also important \nto ensure the integrity of available datasets.14 \nDesign\nTrusted Use of Personal Data\nAs personal data operates within existing legal regimes, a useful starting point is \nfor policymakers to articulate how existing personal data laws apply to generative \nAI. This will facilitate the use of personal data in a manner that still protects the \nrights of individuals.15 For example, policymakers and regulators can clarify consent \nrequirements or applicable exceptions, and provide guidance on good business \npractices for data use in AI.16\nAn emerging group of technologies, known collectively as Privacy Enhancing \nTechnologies (PETs), has the potential to allow data to be used in the development \nof AI models while protecting data confidentiality and privacy. Some PETs such as \nanonymisation techniques are not new, while other technologies are still nascent \nand evolving.17 The understanding of how PETs can be applied to AI will be an \nimportant area to advance.\nBalancing Copyright with Data Accessibility\nFrom a model development perspective, the use of copyright material in training \ndatasets and the issue of consent from copyright owners is starting to raise \nconcerns, particularly as to remuneration and licensing to facilitate such uses. \nModels are also increasingly being used for generating creative output \u2014 some \nof which mimic the styles of existing creators and give rise to considerations of \nwhether this would constitute fair use.18\n14\t\u0007Data poisoning attacks training datasets by introducing, modifying or deleting specific data points. For example, with knowledge of the exact time \nmodel developers collect content (e.g., via snapshots) from sources like Wikipedia, bad actors can \u201cpoison\u201d the Wikipedia webpages with false \ncontent, which will be scraped and used to train the generative AI model. Even if the source moderators undo the changes made to the webpages, \nthe content would have been scraped and used.\n15\t\u0007The collection and use of personal data is already protected under many existing data regimes. \n16\t\u0007One example of this is the Singapore Personal Data Protection Commission\u2019s Advisory Guidelines on Use of Personal Data in AI Recommendation \nand Decision Systems. See https://www.pdpc.gov.sg/guidelines-and-consultation/2024/02/advisory-guidelines-on-use-of-personal-data-in-\nai-recommendation-and-decision-systems\n17\t\u0007IMDA\u2019s PET Sandbox helps to facilitate experimentation based on real-world use cases, including using PETs for AI. This enables industry to explore \ninnovative uses of this emerging technology while ensuring PETs are deployed in a safe and compliant manner. See https://www.imda.gov.sg/\nhow-we-can-help/data-innovation/privacy-enhancing-technology-sandboxes\n18\t\u0007The copyright issue has given rise to varied interests and concerns amongst different stakeholders, with policymakers studying to find the best way \nforward. Copyright owners have requested for renumeration for use of their works to train models, concerned that such systems may compete \nwith them and impact their livelihood. They have advocated for licensing-based solutions to facilitate text and data mining activities for machine \nlearning (ML), as well as an opt-out system for copyright owners from statutory exceptions for text and data mining, and ML activities to avoid \nunduly impinging on their commercial interests. Others have argued that text and data mining, and ML do not infringe copyright because training \ndoes not involve the copying and use of the creative expression in works. There are also practical considerations surrounding obtaining consent \nfrom every copyright owner, as well as trade-offs in model performance.\nFOSTERING A TRUSTED AI ECOSYSTEM\nDATA\n11\nMODEL AI GOVERNANCE FRAMEWORK FOR GENERATIVE AI\nGiven the large volume of data involved in AI training, there is value in developing \napproaches to resolve these difficult issues in a clear and efficient manner. Today, \nlegal frameworks have not yet coalesced around such an approach. Some copyright \nowners have instituted lawsuits against generative AI companies in the US and \nUK courts. Various countries are also exploring non-legislative solutions such as \ncopyright guidelines19 and codes of practice for developers and end-users.20 \nGiven the various interests at stake, policymakers should foster open dialogue \namongst all relevant stakeholders to understand the impact of the fast-evolving \ngenerative AI technology, and ensure that potential solutions are balanced and \nin line with market realities.\nFacilitating Access to Quality Data\nAs an overall hygiene measure at an organisational level, it would be good \ndiscipline for AI developers to undertake data quality control measures and adopt \ngeneral best practices in data governance, including annotating training datasets \nconsistently and accurately, and using data analysis tools to facilitate data cleaning \n(e.g., debiasing and removing inappropriate content).\nGlobally, it is worth considering a concerted effort to expand the available pool \nof trusted datasets. Reference datasets are important tools in both AI model \ndevelopment (e.g., for fine-tuning) as well as benchmarking and evaluation.21 \nGovernments can also consider working with their local communities to curate \na repository of representative training datasets for their specific context (e.g., in \nlow resource languages). This helps to improve the availability of quality datasets \nthat reflect the cultural and social diversity of a country, which in turn supports the \ndevelopment of safer and more culturally representative models.\n19\t\u0007Japan and the Republic of Korea have announced the development of copyright guidelines to address generative AI issues, though they have not \nyet been issued.\n20\t\u0007UK has announced that it is developing a voluntary code of practice between end-users and rights holders through a working group with diverse participation \nfrom technology, creative and research sectors. The stated aims of the working group are to make licenses for data mining more available, to help to \novercome barriers that AI firms and end-users currently face, and to ensure there are protections for rights holders.\n21\t\u0007This is akin to reference standards in, for example, the pharmaceutical industry, which are used as a basis for evaluation for drugs. \nTRUSTED \nDEVELOPMENT AND \nDEPLOYMENT\n13\nMODEL AI GOVERNANCE FRAMEWORK FOR GENERATIVE AI\nModel development, and the application deployment on top of it, are at the core \nof AI-driven innovation. Today, however, there is a lack of information on the \napproaches being taken to ensure trustworthy models. Even in cases of \u201copen-\nsource\u201d models, some important information like the methodology and datasets \nmay not be made available. \nGoing forward, it is important that the industry coalesces around best practices in \ndevelopment and safety evaluation. Thereafter, meaningful transparency around \nbaseline safety and hygiene measures undertaken will also be key. This will enable \nsafer model use by all stakeholders in the AI ecosystem. Such transparency will \nneed to be balanced with legitimate considerations such as safeguarding business \nand proprietary information, and not allowing bad actors to game the system. \nDesign\nSafety best practices need to be implemented by model developers and application \ndeployers across the AI development lifecycle, around development, disclosure \nand evaluation. Groundwork for this has been laid in the 2020 version of the Model \nAI Governance Framework, which sets out best practices for the development and \ndeployment of traditional AI solutions.22 The principles articulated there continue \nto be relevant and are extended here for generative AI.\nDevelopment \u2014 Baseline Safety Practices\nSafety measures are developing rapidly, and model developers and application \ndeployers are best placed to determine what to use. Even so, industry practices \nare starting to coalesce around some common safety practices. \nFor example, after pre-training, fine-tuning techniques such as Reinforcement \nLearning from Human Feedback (RLHF)23 can guide the model to generate safer \noutput that is more aligned with human preferences and values. A crucial step for \nsafety is also to consider the context of the use case and conduct a risk assessment. \nFor example, further fine-tuning or using user interaction techniques (such as input \nand output filters) can help to reduce harmful output. Techniques like Retrieval-\nAugmented Generation (RAG)24 and few-shot learning are also commonly used \nto reduce hallucinations and improve accuracy.\nDisclosure \u2014 \u201cFood Labels\u201d\nTransparency around these safety measures undertaken, that form the core of \nthe AI model\u2019s make-up is then key. This is akin to \u201cfood or ingredient labels\u201d. By \nproviding relevant information to downstream users, they can make more informed \ndecisions. While leading model developers already disclose some information, \n22\t\u0007See https://pdpc.gov.sg/-/media/Files/PDPC/PDF-Files/Resource-for-Organisation/AI/SGModelAIGovFramework2.pdf\n23\t\u0007RLHF is a technique used to improve LLMs by using human feedback to train a preference model, that in turns trains the LLM using reinforcement \nlearning. RLHF can be complemented with mechanisms to assess confidence during content generation to alert model developers or application \ndeployers to risks where human verification and validation is required.\n24\t\u0007RAG is a technique that helps models provide more contextually appropriate and current responses that are specific to an organisation or industry. \nThis is done by linking generative AI services to external resources, thereby giving models sources to cite and enhancing the accuracy and reliability \nof generative AI models with facts fetched from trusted sources.\nFOSTERING A TRUSTED AI ECOSYSTEM\nTRUSTED DEVELOPMENT \nAND DEPLOYMENT\nMODEL AI GOVERNANCE FRAMEWORK FOR GENERATIVE AI\n14\nstandardising disclosure will facilitate comparability across models and promote \nsafer model use. Relevant areas of disclosure may include:\na)\t Data Used: An overview of the types of training data sources and how data was \nprocessed before training.\nb)\t Training Infrastructure: An overview of the training infrastructure used and, \nwhere possible, estimated environmental impact.25\nc)\t Evaluation Results: Overview of evaluations done and key results.\nd)\t Mitigations and Safety Measures: Safety measures implemented (e.g., bias \ncorrection techniques and safeguarding the exfiltration of sensitive data).\ne)\t Risks and Limitations: Model\u2019s known risks and moves to address these risks.\nf)\t Intended Use: Clear statement setting out the scope of the model\u2019s intended use.\ng)\t User Data Protection: Outline of how user data will be used and protected.\nSuch disclosure provides a standard baseline for all models. Developers of customised \nor advanced models can consider disclosing additional information.\nThe level of detail disclosed can be calibrated based on the need to be transparent \nvis-\u00e0-vis protecting proprietary information. One step forward would be for the \nindustry to agree on the baseline transparency to be provided as part of general \ndisclosure to all parties. This involves both the model developers and application \ndeployers. Alternatively, the development of such a baseline can be facilitated by \ngovernments and third parties.\nGreater transparency to government will also be needed for models that pose \npotentially high risks, such as advanced models that have national security or \nsocietal implications. There is therefore space for policymakers to define the model \nrisk thresholds, above which additional oversight measures would apply.\nEvaluation\nThere are generally two main approaches to evaluate generative AI today \u2014 (i) \nbenchmarking tests models against datasets of questions and answers to assess \nperformance and safety; and (ii) red teaming, where a red team acts as an \nadversarial user to \u201cbreak\u201d the model and induce safety, security and other violations. \nAlthough benchmarking and red teaming are commonly adopted today, they still \nfall far short in terms of providing a robust assessment of model performance and \nsafety (refer to the dimension of Safety and Alignment R&D). \nEven within the benchmarking and red teaming framework, most evaluation today \nfocuses on generative AI\u2019s front-end performance, and less on its back-end safety. \nThere is also a lack of evaluation tools (e.g., for multi-modal models), as well as \ntesting for dangerous capabilities. Another issue is in consistency \u2014 many tests \nand evaluations today need to be customised to a specific model and at times, \ncomparability is a challenge.\nAs such, there is a need to work towards a more comprehensive and systematic \napproach to safety evaluations. This will yield more useful and comparable insights. \nTo provide additional assurance, the standardised approach could also include \ndefining a baseline set of required safety tests and developing shared resources,26 \nin consultation with policymakers.\n25\t\u0007More so as AI training and the use of accelerated compute is driving up carbon emissions.\n26\t\u0007For example, documenting best practices for initiating and developing red teams.\n15\nMODEL AI GOVERNANCE FRAMEWORK FOR GENERATIVE AI\nA Starting Point for Standardised Safety Evaluations\nAI Verify Foundation and IMDA recommended an initial set of standardised \nmodel safety evaluations for LLMs, covering robustness, factuality, propensity \nto bias, toxicity generation and data governance. It can be found in the \npaper titled Cataloguing LLM Evaluations issued in October 2023.27 The paper \nprovides both a landscape scan as well as practical guidance on what \nsafety evaluations may be considered. These recommendations have to be \ncontinuously improved, given rapid advances in the generative AI space.\nSectors and domains may have unique needs that require additional evaluations \n(e.g., mandating stringent accuracy thresholds for high-risk use cases such as \nmedical diagnosis). Moreover, application deployers are more likely to focus on \ndomain-specific assessments that address their use cases. In some cases, such \nas for models with very niche capabilities, customised evaluations may be needed. \nIndustry and sectoral policymakers will therefore need to jointly improve evaluation \nbenchmarks and tools, while still maintaining coherence between baseline and \nsector-specific requirements.28\n27\t\u0007See https://aiverifyfoundation.sg/downloads/Cataloguing_LLM_Evaluations.pdf\n28\t\u0007For example, aligning safety principles and using common terminologies.\nINCIDENT \nREPORTING\n17\nMODEL AI GOVERNANCE FRAMEWORK FOR GENERATIVE AI\nEven with the most robust development processes and safeguards, no software that \nwe use today is foolproof. The same applies to AI. Incident reporting is an established \npractice, including in critical domains such as telecommunications, finance and \ncybersecurity. It allows for timely notification and remediation. Establishing the \nstructures and processes to enable incident reporting is therefore key. This, in turn, \nsupports the continuous improvement of AI systems through insights, remediation \nand patching. \nDesign\nVulnerability Reporting \u2014 Incentive to Act Pre-Emptively\nBefore incidents happen, software product owners adopt vulnerability reporting as \npart of an overall proactive security approach. They co-opt and support white hats \nor independent researchers to discover vulnerabilities in their software, sometimes \nthrough a curated bug-bounty programme. Once discovered, the vulnerability is \nreported and the product owner is then given time (typically 90 days, based on \nindustry practice) to patch their software, publish the vulnerability (such as by filing \na CVE \u2014 see box below) and crediting the white hat or independent researcher. \nThis allows both the software product owners and users to undertake proactive \nsteps to enhance overall security.\nCommon Vulnerabilities and Exposures (CVE) Programme\nThe CVE programme, managed by the MITRE Corporation, compiles a list of \npublicly known security vulnerabilities and exposures. This list is widely referred \nto by cybersecurity teams around the world to look for new vulnerabilities \nthat might affect one\u2019s organisation. Software product owners may file \nvulnerabilities as a CVE. The ability to discover zero-day CVEs is also viewed \nas an achievement within the white hat community.\nAI developers can apply this similar concept, by allowing reporting channels for \nuncovered safety vulnerabilities in their AI systems. They can apply the same best \npractices for vulnerability reporting, including a time-window to assess the incident, \npatch and publish. This should also be complemented by ongoing monitoring \nefforts to detect malfunctions before they are noticed by end-users.\nIncident Reporting\nAfter incidents happen, organisations need internal processes to report the \nincident for timely notification and remediation. Depending on the impact of the \nincident and how extensively AI was involved, this could include notifying both \nthe public as well as governments. Defining \u201csevere AI incidents\u201d or setting the \nmateriality threshold for formal reporting is therefore key.29 AI incidents can also be \n29\t\u0007OECD\u2019s AI paper on Defining AI Incidents and Related Terms illustrates ongoing efforts to develop common definitions. See https://www.oecd.org/\ngovernance/defining-ai-incidents-and-related-terms-d1a8d965-en.htm\nFOSTERING A TRUSTED AI ECOSYSTEM\nINCIDENT REPORTING\nMODEL AI GOVERNANCE FRAMEWORK FOR GENERATIVE AI\n18\nwide-ranging. Principles will therefore need to be harmonised with the principles \nof existing reporting regimes. Borrowing from cybersecurity, AI incidents can be \nreported to the equivalent of \u201cInformation Sharing and Analysis Centres\u201d, which \nare trusted entities to foster information sharing and good practices, as well as to \nrelevant authorities, where required by law.\nReporting should be proportionate, which means striking a balance between \ncomprehensive reporting and practicality. This will need to be calibrated to suit \nthe specific local context. In this regard, the EU AI Act provides one reference point \nfor legal reporting requirements (see box below).\nIncident Reporting Under the EU AI Act\nProviders of high-risk AI systems are required to report serious incidents to \nthe market surveillance authorities of the Member States where that incident \noccurred, within 15 days after the AI system provider becomes aware of the \nincident. \u201cSerious incident\u201d is defined as any incident or malfunctioning of \nan AI system that directly or indirectly leads to the death of a person, serious \ndamage to a person\u2019s health, serious and irreversible disruption of critical \ninfrastructure, breaches of fundamental rights under Union law, or serious \nharm to property or the environment. \nTESTING AND \nASSURANCE\nMODEL AI GOVERNANCE FRAMEWORK FOR GENERATIVE AI\n20\nThird-party testing and assurance often play a complementary role in a trusted \necosystem. We do this today in many domains, such as finance and healthcare, \nto enable independent verification. While companies typically conduct audits to \ndemonstrate compliance with regulations, more companies are beginning to see \nexternal audits as a useful mechanism to provide transparency and build greater \ncredibility and trust with end-users.30 \nWhile this is an emerging field, we can draw from established audit practices to \ngrow the AI third-party testing ecosystem. Third-party testing will also benefit from \ncomprehensive and consistent standards around AI evaluations (discussed earlier \nin the Trusted Development and Deployment dimension).\nDesign\nFostering the development of a third-party testing ecosystem involves two \npivotal aspects: \na)\t How to Test: Defining a testing methodology that is reliable and consistent, and \nspecifying the scope of testing to complement internal testing. \nb)\t Who to Test: Identifying the entities to conduct testing that ensures independence.\nHow to Test \u2014 Standardisation\nIn the near term, third-party testing will comprise the same set of benchmarks and \nevaluation used by developers themselves.31 Eventually, this needs to be done in a \nstandardised way for third-party testing to be effective, and to facilitate meaningful \ncomparability across models. \nGreater emphasis should therefore be placed on setting common benchmarks \nand methodologies. This may be catalysed by having common tooling to reduce \nthe friction required to test across different models or applications. Thereafter, for \nmore mature areas, AI testing could be codified through standards organisations \nlike ISO/IEC and IEEE, to support more harmonised and robust third-party testing.\nAs the testing ecosystem develops, there is also room to standardise the scope of \nthird-party testing.32\nWho to Test \u2014 Trusted Accreditation \nIndependence is key to ensuring the objectivity and integrity of test results. Building \nup a pool of qualified third-party testers is critical. Concerted efforts by industry \nbodies and governments will be useful to grow capabilities in this area. Eventually, \nan accreditation mechanism could be developed to ensure independence and \ncompetency. This is common practice in many domains (e.g., finance). Many audit \nand professional services firms are understandably increasingly keen to grow initial \nAI audit capabilities and services.\n30\t\u0007For instance, in the White House Voluntary Commitments, several AI companies pledged to conduct external model red teaming as a means of \ndemonstrating trust. Benchmarking is another approach to third-party testing.\n31\t\u0007Stanford\u2019s Holistic Evaluation of Language Models is an example of a third-party conducting benchmark tests today.\n32\t\u0007Testing for robustness and fairness should form a starting baseline. Other elements to consider could include reproducibility and data governance.\nFOSTERING A TRUSTED AI ECOSYSTEM\nTESTING AND ASSURANCE\nSECURITY\nMODEL AI GOVERNANCE FRAMEWORK FOR GENERATIVE AI\n22\nGenerative AI has brought renewed focus on the security of AI itself. Many issues are \nfamiliar, such as supply chain risks in AI/ML middleware. In addressing AI security, it \nis useful to separate traditional software security concerns addressed via current \napproaches, from novel threat vectors against the AI model itself. The latter is a \nnascent space. Nevertheless, similar security concepts may still apply. \nDesign\nAdapt \u201cSecurity-by-Design\u201d\nSecurity-by-design is a fundamental security concept. It seeks to minimise system \nvulnerabilities and reduce the attack surface through designing security into every \nphase of the systems development life cycle (SDLC). Key SDLC stages include \ndevelopment, evaluation, operations and maintenance. \nHowever, refinements may be needed given the unique characteristics of generative \nAI. For example, the ability to inject natural language as input can pose challenges \nin designing appropriate security controls.33 Furthermore, the probabilistic nature \nof generative AI challenges traditional evaluation techniques that inform system \nrefinement and risk mitigation in the SDLC. Hence, new concepts have to be \ndeveloped or adapted for generative AI.\nDevelop New Security Safeguards \nNew tools have to be developed and may include:\na)\t Input Filters: Input moderation tools detect unsafe prompts (e.g., blocking malicious \ncode). The tools need to be tailored to understand domain-specific risks.\nb)\t Digital Forensics Tools for Generative AI: Digital forensics tools are used to \ninvestigate and analyse digital data (e.g., file contents) to reconstruct a \ncybersecurity incident. New forensics tools should be explored to help enhance \nthe ability to identify and extract malicious codes that might be hidden within \na generative AI model.\nApart from these tools, databases such as MITRE\u2019s Adversarial Threat Landscape for \nAI Systems provide information on adversary tactics, techniques and case studies \nfor ML systems, including generative AI. AI developers can use these to support risk \nassessment and threat modelling, and to identify useful tools or processes.\n33\t\u0007This is because existing security controls, such as next-generation firewalls and data loss protection typically rely on restricting communication \nprotocols between nodes and establishing pre-defined filters to detect and mitigate malicious attacks. They therefore do not perform well with \nwide-ranging communications that may span interactive and dynamic dialogue, long text and source code. In the case of multi-modal models, \nthis can even extend to various forms of content such as images, videos and audio.\nFOSTERING A TRUSTED AI ECOSYSTEM\nSECURITY\nCONTENT \nPROVENANCE\nMODEL AI GOVERNANCE FRAMEWORK FOR GENERATIVE AI\n24\nThe rise of generative AI, which enables the rapid creation of realistic synthetic \ncontent at scale, has made it harder for consumers to distinguish between AI-\ngenerated and original content. A common manifestation of such concern is \ndeepfakes. This has exacerbated harms like misinformation,34 and even potential \nsocietal threats like undermining the integrity of elections. \nThere is recognition across governments, industry and society on the need for \ntechnical solutions, such as digital watermarking and cryptographic provenance, to \ncatch up with the speed and scale of AI-generated content.35 Digital watermarking \nand cryptographic provenance both aim to label and provide additional information,36 \nand are used to flag content created with or modified by AI.\n34\t\u0007Other harms include non-consensual image use and reputational damage.\n35\t\u0007For example, China\u2019s Deep Synthesis Regulations require watermarking of AI-generated content, the US Executive Order on the Safe, Secure and \nTrustworthy Development and Use of AI commits the government to the development of effective labelling and content provenance mechanisms, \nand the EU AI Act imposes specific transparency obligations for deepfake systems.\n36\t\u0007Labelling of AI-generated content refers mainly to image, video and audio, although technologies to label text are maturing.\n37\t\u0007In the encoding process, a content creator inserts the invisible watermark via an algorithm into the digital image. For decoding, the image is \nscanned via an algorithm for the presence of an embedded watermark.\n38\t\u0007This is driven by several companies, including Adobe and Microsoft.\nDigital watermarking techniques embed information within the content \nand can be used to identify AI-generated content. There are several digital \nwatermarking solutions to label AI-generated content today (e.g., Google \nDeepMind\u2019s SynthID and Meta\u2019s Stable Signature). However, it is only possible \nto decode a watermark through the same company that encodes the \nwatermark,37 due to the current lack of interoperable standards.\nCryptographic provenance solutions track and verify the digital content origin \nand any edits made, with the records cryptographically protected. The Coalition \nfor Content Provenance and Authenticity (C2PA)38 is driving development of \nan open standard to enable the tracking of content provenance. \nFOSTERING A TRUSTED AI ECOSYSTEM\nCONTENT PROVENANCE\nTechnical solutions alone may not be sufficient and will likely have to be complemented \nby enforcement mechanisms.\n25\nMODEL AI GOVERNANCE FRAMEWORK FOR GENERATIVE AI\nDesign\nPolicies need to be carefully designed to enable practical use in the right contexts. \nPractically, it may not be feasible for all content creation, editing or display tools \nto include these technologies in the near term. Provenance information can also \nbe stripped.39 In addition, consumer understanding of these tools is low. Malicious \nactors will also find ways to circumvent these tools, or worse, use them to create \na false sense of authenticity.\nThere is therefore a need to work with key parties in the content life cycle, such as \nworking with publishers to support the embedding and display of digital watermarks \nand provenance details. As most digital content is consumed through social media \nplatforms, browsers or media outlets, publishers\u2019 support is critical to provide end-\nusers with the ability to verify content authenticity across various channels. There \nis also a need to ensure proper and secure implementation to circumvent bad \nactors trying to exploit it in any way. \nDifferent types of edits (e.g., whether an image is entirely AI-generated or only a \nsmall portion of it is) will impact how the content is perceived by the end-user. To \nimprove end-user experience and enable consumers to discern between non-AI \nand AI-generated content, standardising the types of edits to be labelled would \nbe helpful. \nEnd-users need greater understanding of content provenance across the content \nlife cycle and to learn to utilise tools to verify for authenticity. Key stakeholders \n(e.g., content creators, publishers, solution providers) can partner policymakers to \nraise awareness. Provenance details to be displayed should also be simplified to \nthe extent possible to facilitate end-user understanding.\n39\t\u0007For example, removed by online tools or when uploaded on some online platforms.\nSAFETY AND \nALIGNMENT R&D\n27\nMODEL AI GOVERNANCE FRAMEWORK FOR GENERATIVE AI\nSafety techniques, and evaluation tools today do not fully address all potential \nrisks. For example, even RLHF, the primary method for value alignment today, has \nlimitations. Existing large models also lack interpretability and may not be consistently \nreproducible. Given the speed of model advancement, there is a need to ensure that \nhuman capacity to align and control generative AI keeps pace with the potential risks, \nincluding both present risks (e.g., bias, hallucination) and future catastrophic risks.\nDesign\nWhile the call to invest more in R&D is a no-regrets move, there may be practical \nsteps to enhance the speed of translation and use of new R&D insights. There is a \nneed to, for example, understand and systematically map the diversity of research \ndirections and methods that have emerged in safety and alignment \u2014 and apply \nthem in a concerted manner.\na)\t One broad area of research entails the development of more aligned models \n(also known by some as \u201cforward alignment\u201d),40 such as through Reinforcement \nLearning from AI Feedback (RLAIF).41 RLAIF seeks to improve on RLHF by enhancing \nfeedback efficiency and quality, and enabling scalable oversight of advanced \nmodels. However, it too comes with its own drawbacks.42 \nb)\t Another area of research is the evaluation of a model after it is trained, to validate \nits alignment (also known by some as \u201cbackward alignment\u201d). This includes \ntesting for emergent capabilities so that potentially dangerous abilities, such \nas autonomous replication and long horizon planning, can be detected early. \nMechanistic interpretability, which seeks to understand the neural networks of \na model to find the source of problematic behaviours, is also gaining traction \nas a research area.\nTo keep pace with advancements in model capabilities, R&D in model safety and \nalignment needs to be accelerated. Today, the majority of alignment research is \nconducted by AI companies. The setting up of AI safety R&D institutes or equivalents \nin UK, US, Japan and Singapore43 is therefore a positive development that signals \ncommitment to leverage the existing R&D ecosystem as well as invest additional \nresources (which could include compute and access to models) to drive research \nfor the global good. \nHowever, global cooperation will be critical to optimise limited talent and resources \nfor maximum impact. Impactful areas of research can be collectively identified and \nprioritised based on the landscape map. The goal is to enable more impactful R&D \nefforts to develop safety and evaluation mechanisms ahead of time.\n40\t\u0007A November 2023 paper on the overview of safety and alignment research termed \u201cforward alignment\u201d and \u201cbackward alignment\u201d as the two key \ncategories of research in this field (Ji et al., 2023, AI Alignment: A Comprehensive Survey). See https://doi.org/10.48550/arXiv.2310.19852\n41\t\u0007RLAIF uses AI to generate feedback to train the preference model, based on parameters defined by humans. Anthropic\u2019s Constitutional AI is an \nexample of RLAIF.\n42\t\u0007In addition to developing more aligned models, it is important that models are safe. One relevant research area is robustness, which concerns \nmodel performance in unfamiliar or adversarial settings.\n43\t\u0007Singapore\u2019s Digital Trust Centre (DTC) looks at overall Digital Trust, including Trusted AI R&D. The DTC is funded by a S$50 million initial investment \nfrom IMDA and the National Research Foundation, and was set up in June 2022 to lead Singapore\u2019s R&D efforts for trustworthy AI technologies and \nother trust technologies. \nFOSTERING A TRUSTED AI ECOSYSTEM\nSAFETY AND \nALIGNMENT R&D\nAI FOR PUBLIC GOOD\n29\nMODEL AI GOVERNANCE FRAMEWORK FOR GENERATIVE AI\nThe transformative potential of generative AI is powerful. If we get the approach \ncorrect, global communities will reap exponential benefits. The imperative is to \nturbocharge growth and productivity for developed and developing countries alike, \nwhile empowering people and businesses globally with the potentially democratising \npower of AI. In this regard, countries must come together to support each other, \nespecially through international and regional groupings. Beyond the large and \ndeveloped countries (e.g., through G7), this is especially pertinent for developing \ncountries and small states, through key platforms like the Digital Forum of Small \nStates (Digital FOSS) at the United Nations and the Association of Southeast Asian \nNations (ASEAN). The aim is to establish a global Digital Commons \u2014 a place with \ncommon rules-of-the-road and equal opportunities for all citizens to flourish, \nregardless of their geographical location.\nDesign\nThere are four concrete touchpoints where AI can have beneficial and long-\nterm effects.\nDemocratising Access to Technology\nAll members of society should have access to generative AI, done in a trusted \nmanner. Generative AI is inherently intuitive given the natural language focus, \nbut it is still important that the overall product (of which generative AI is just one \ncomponent) is designed in a human-centric way. Most citizens of the world may \nnot understand the technology and the \u201cblack-box\u201d underpinning the applications \nthey are using. Therefore, designing applications to elicit the intended social and \nhuman outcomes is key.44\nTo more broadly support this, governments can partner companies and communities \non digital literacy initiatives to encourage safe and responsible AI use. Topics \ncould include educating end-users on how to use chatbots safely, sensitising them \nagainst \u201canthropomorphising\u201d AI, and identifying deepfakes. \nThe adoption of generative AI can also be challenging, especially for small and \nmedium enterprises (SMEs). Governments and industry partners can improve \nawareness and provide support to drive innovation and AI use among SMEs. An \nexample is Singapore\u2019s Generative AI Sandbox, which provides SMEs with tools and \ntraining on generative AI enterprise solutions.45\nPublic Service Delivery\nAI should serve the public in impactful ways. Today, AI powers many public services, \nsuch as adaptive learning systems in schools and health management systems in \nhospitals. This unlocks new value propositions, creates efficiencies and improves \nuser experience.\n44\t\u0007For example, model developers like OpenAI support the development of AI solutions to enhance the delivery of healthcare, education and other \npublic services.\n45\t\u0007See https://www.imda.gov.sg/resources/press-releases-factsheets-and-speeches/press-releases/2023/generative-ai-evaluation-sandbox\nFOSTERING A TRUSTED AI ECOSYSTEM\nAI FOR PUBLIC GOOD\nMODEL AI GOVERNANCE FRAMEWORK FOR GENERATIVE AI\n30\nIt is desirable for governments to coordinate resources to support public sector \nAI adoption. This includes facilitating responsible data sharing across different \ngovernment agencies,46 access to high performance compute and other related \npolicies. AI developers play a contributing role by helping governments identify use \ncases and providing AI solutions to address citizen pain points.\nWorkforce \nFor the productive value of AI to be unlocked, concerted upskilling of the workforce is \nimportant. This is key to countering the potentially negative outcomes of technology \nreplacing labour. Beyond the specific skill sets in using AI tools, other core skills \nsuch as creativity, critical thinking and complex problem-solving, are important \nto helping people harness AI effectively. \nIndustry, governments and educational institutions can work together to redesign \njobs and provide upskilling opportunities for workers. As organisations adopt \nenterprise generative AI solutions, they can also develop dedicated training \nprogrammes for their employees. This will enable them to navigate the transitions \nin their jobs and enjoy the benefits which result from job transformations.\nSustainability\nSustainable growth is key. The resource requirements of generative AI \n(e.g., energy and water) are non-trivial and will likely impact sustainability goals. \nStakeholders in the generative AI ecosystem therefore need to work together to \ndevelop suitable technology (e.g., energy efficient compute) in support of our \nclimate responsibilities.\nTo inform such plans, the carbon footprint of generative AI (e.g., for model training \nand inference) will also need to be tracked and measured. AI developers and \nequipment manufacturers are better placed to conduct R&D on green computing \ntechniques and adopt energy-efficient hardware. In addition, AI workloads can \nbe hosted in data centres that drive best-in-class energy-efficient practices, with \ngreen energy sources or pathways.\n46\t\u0007There is a clear data governance framework for the Singapore Public Service.\n31\nMODEL AI GOVERNANCE FRAMEWORK FOR GENERATIVE AI\nAs generative AI continues to develop and evolve, there is a need for global \ncollaboration on policy approaches. The nine dimensions in this Framework provide \na basis for global conversation to address generative AI concerns while maximising \nspace for continued innovation. The ideas proposed seek to also further the core \nprinciples of accountability, transparency, fairness, robustness and security. They \nreiterate the need for policymakers to work with industry, researchers and like-\nminded jurisdictions. We hope that this serves as a next step towards developing \na trusted AI ecosystem, where AI is harnessed for the Public Good, and people \nembrace AI safely and confidently.\nCONCLUSION\nMODEL AI GOVERNANCE FRAMEWORK FOR GENERATIVE AI\n32\nWe express our sincere appreciation to the following individuals and organisations \nfor their valuable feedback to the Model AI Governance Framework for Generative \nAI (in alphabetical order): \nACKNOWLEDGEMENTS\n\u2022\t Aimodels.org\n\u2022\t Adobe\n\u2022\t Alteryx\n\u2022\t Amazon Web \nServices\n\u2022\t Apollo \nResearch\n\u2022\t BIGO Technology\n\u2022\t Braithwate\n\u2022\t Cisco\n\u2022\t Data Protection \nSchemes \nLimited\n\u2022\t Ernst & Young\n\u2022\t Google\n\u2022\t HM\n\u2022\t IBM\n\u2022\t Kaspersky\n\u2022\t KPMG\n\u2022\t LexisNexis\n\u2022\t LLMware.ai\n\u2022\t Mastercard\n\u2022\t Mediacorp\n\u2022\t Meta\n\u2022\t Microsoft\n\u2022\t MNT3\n\u2022\t NovaSync Labs \n\u2022\t NCS \n\u2022\t OpenAI\n\u2022\t Salesforce\n\u2022\t SAP\n\u2022\t Singapore Airlines\n\u2022\t SymphonyAI\n\u2022\t Telenor Group\n\u2022\t Temasek \n\u2022\t TT-Logic \n\u2022\t Visa\n\u2022\t Workday\n\u2022\t Z\u00fchlke\n\u2022\t A*STAR\n\u2022\t Academy of Medicine \nSingapore \n\u2022\t AI Professionals \nAssociation\n\u2022\t AI 2030\n\u2022\t APAC Gates\n\u2022\t Asia Internet Coalition\n\u2022\t Asia Securities and \nFinancial Markets \nAssociation\n\u2022\t Association of \nChartered Certified \nAccountants\n\u2022\t BSA | The Software \nAlliance\n\u2022\t Centre for AI and \nDigital Policy\n\u2022\t Chartered Software \nDeveloper Association\n\u2022\t Computer & \nCommunications \nIndustry Association\n\u2022\t Copyright Licensing \nand Administration \nSociety\n\u2022\t Department of \nCommerce, \nUnited States\n\u2022\t Digital Prosperity for \nAsia Coalition\n\u2022\t Digital Trust Centre \n\u2022\t Future of Privacy Forum\n\u2022\t International \nFederation of the \nPhonographic Industry\nIndustry\nGovernment, Research Institutions, Associations\n\u2022\t Ministry of Health of \nSingapore\n\u2022\t Motion Picture \nAssociation\n\u2022\t Pragmagility\n\u2022\t Recording Industry \nAssociation Singapore\n\u2022\t Rhythmisis Institute\n\u2022\t SHE\n\u2022\t The American \nChamber of \nCommerce in \nSingapore\n\u2022\t The App Association\n\u2022\t The Dialogue\n\u2022\t US-ASEAN Business \nCouncil\n\u2022\t Vibrations\n33\nMODEL AI GOVERNANCE FRAMEWORK FOR GENERATIVE AI\n\u2022\t Aidan O\u2019Gara\n\u2022\t Alex Toh \n\u2022\t Jamie Bernardi\n\u2022\t Lim Zheng Xiong\n\u2022\t Merlin Stein\n\u2022\t Nicholas Ni\n\u2022\t Oliver Guest \n\u2022\t Oscar Delaney\n\u2022\t Pankaj Jasal \n\u2022\t Raymond Chan \n\u2022\t Saad Siddiqui\n\u2022\t Shaun Ee\n\u2022\t Soh Teck Foo \n\u2022\t Srikanth Mahankali \n\u2022\t Steven David Brown\n\u2022\t Will Hodgkins\n\u2022\t Xavier Tan\n\u2022\t Zaheed Kara\nOther Contributors \nMODEL AI GOVERNANCE FRAMEWORK FOR GENERATIVE AI\n34\nThe Model AI Governance Framework for Generative AI is the first step towards \nfostering a trusted ecosystem for generative AI. Building on the feedback received, \nthere is further work to be done in providing greater certainty through implementation \nguidelines and resources. Referencing the Framework\u2019s nine dimensions, we will \ncontinue to engage key stakeholders to develop these guidelines and resources \nto enable a systematic and balanced approach towards building guardrails while \nenabling maximal space for generative AI innovation.\nFURTHER DEVELOPMENT\n35\nMODEL AI GOVERNANCE FRAMEWORK FOR GENERATIVE AI\nRecognising the importance of collaboration and crowding in expertise, \nSingapore set up the AI Verify Foundation to harness the collective power and \ncontributions of the global open-source community to build AI governance \ntesting tools. The mission of the AI Verify Foundation is to foster and coordinate \na community of developers to contribute to the development of AI testing \nframeworks, code base, standards and best practices. It will establish a \nneutral space for the exchange of ideas and open collaboration, as well \nas nurture a diverse network of advocates for AI testing and drive broad \nadoption through education and outreach. The vision is to build a community \nthat will contribute to the broader good of humanity, by enabling trusted \ndevelopment of AI. \nAt IMDA, we see ourselves as Architects of Singapore\u2019s Digital Future. \nWe cover the digital space from end to end, and are unique as a government \nagency in having three concurrent hats \u2014 as Economic Developer (from \nenterprise digitalisation to funding R&D), as a Regulator building a trusted \necosystem (from data/AI to digital infrastructure), and as a Social Leveller \n(driving digital inclusion and making sure that no one is left behind). Hence, \nwe look at the governance of AI not in isolation, but at that intersection with \nthe economy and broader society. By bringing the three hats together, \nwe hope to better push boundaries, not only in Singapore, but in Asia and \nbeyond, and make a difference in enabling the safe and trusted use of this \nemerging and dynamic technology.\n\u00a9 COPYRIGHT IMDA AND\nAI VERIFY FOUNDATION 2024.\nALL RIGHTS RESERVED.\n", "metadata": {"country": "Singapore", "year": "2024", "legally_binding": "no", "binding_proof": "Voluntary framework developed by IMDA and AI Verify Foundation to guide responsible generative AI development. Final version published in May 2024.", "date": "05/30/2024", "regulator": "Infocomm Media Development Authority (IMDA) & AI Verify Foundation", "type": "generative AI governance", "status": "pending ", "language": "English", "use_cases": "[1, 3, 5, 6]"}}
{"_id": "686acd1ee2af9fb1ff3ba993", "title": "GUIDELINES ON SECURING AI SYSTEMS", "source": "https://isomer-user-content.by.gov.sg/36/e05d8194-91c4-4314-87d4-0c0e013598fc/Guidelines%20on%20Securing%20AI%20Systems.pdf", "text": " \n \n \n \n \n \n \n \n  \n \n \n \n \n \n \n \n \n \n \n \nGUIDELINES  \nON \nSECURING \nAI SYSTEMS \nCYBER SECURITY AGENCY OF SINGAPORE \nOCTOBER 2024 \n \n \n2   \n \nGUIDELINES ON SECURING AI SYSTEMS \n \nTABLE OF \nCONTENTS \n \n \n1. \nINTRODUCTION ....................................................................................................... 3 \n1.1. \nPURPOSE AND SCOPE OF THIS DOCUMENT ..................................................... 4 \n2. \nUNDERSTANDING AI THREATS ................................................................................. 5 \n3. \nSECURING AI  .......................................................................................................... 7 \n3.1. \nTAKE A LIFECYCLE APPROACH ......................................................................... 7 \n3.2. \nSTART WITH A RISK ASSESSMENT ..................................................................... 8 \n3.3. \nGUIDELINES FOR SECURING AI SYSTEMS ....................................................... 10 \nGLOSSARY .................................................................................................................... 14 \nANNEX A ........................................................................................................................ 18 \n \n \n \n \n \n \n3   \n \nGUIDELINES ON SECURING AI SYSTEMS \n1. INTRODUCTION \n \nArtificial Intelligence (AI) poses benefits for economy, society, and \nnational security. It has the potential to drive efficiency and innovation \nin almost every sector \u2013 from commerce and healthcare to \ntransportation and cybersecurity.  \n \n \nTo reap the benefits of AI, users must have confidence that the AI will behave as designed, \nand outcomes are safe and secure. However, in addition to safety risks, AI systems can \nbe vulnerable to adversarial attacks, where malicious actors intentionally manipulate or \ndeceive the AI system. The adoption of AI can introduce or exacerbate existing \ncybersecurity risks to enterprise systems. These can lead to risks such as data \nleakage or data breaches, or result in harmful or otherwise undesired model \noutcomes. \n \nAs such, as a key principle, AI should be secure by design and secure by default, as \nwith all software systems. This will enable system owners to manage security risks \nupstream. This will complement other controls and mitigation strategies that system \nowners may take to address the safety of AI, and other attendant considerations such as \nfairness or transparency, which are not addressed here.  \n \nThe Cyber Security Agency of Singapore (CSA) has developed Guidelines on Securing AI \nSystems for system owners to secure the use of AI throughout its lifecycle. As AI is \nincreasingly integrated into enterprise systems, security should be considered holistically \nat the system level. As such, these guidelines should be used alongside existing security \nbest practices and requirements for IT environments. While these guidelines are not \nmandatory, we strongly encourage system owners to consider these key principles, so \nthat they can make informed decisions on their adoption of AI vis-\u00e0-vis the potential risks. \n \nAI security is a developing field of work, and mitigation controls continue to evolve. As \nsuch, CSA is also collaborating with AI and cybersecurity practitioners on the Companion \nGuide on Securing AI Systems. This is intended as a community-driven resource, and the \nCompanion Guide complements the Guidelines as a useful reference containing practical \nmeasures and controls that system owners may consider as part of adopting the \nGuidelines, depending on their use case. The Companion Guide is not mandatory, \nprescriptive, or exhaustive. As the field of AI security continues to evolve rapidly, the \nCompanion Guide will be updated to account for material developments in this space.  \n \n4   \n \nGUIDELINES ON SECURING AI SYSTEMS \n1.1. PURPOSE AND SCOPE OF \nTHIS DOCUMENT \n \n \n \nPurpose \nThese guidelines are designed to support systems owners that are adopting, or considering \nthe adoption of AI systems. It identifies potential security risks associated with the use of AI \nand sets out guidelines for mitigating security risks at each stage of the AI lifecycle.  \nThis document can be read together with the Companion Guide on Securing AI Systems, \nwhich provides an informative compilation of practical security control measures, that \nsystem owners may consider in implementing these guidelines.  \n \n \nScope \nThese guidelines address the cybersecurity risks to AI systems. It does not seek to address \nAI safety, or other common attendant considerations for AI such as fairness, transparency \nor inclusion, or cybersecurity risks introduced by AI systems, although some of the \nrecommended actions may overlap. It also does not address the misuse of AI in \ncyberattacks (AI-enabled malware), mis/disinformation, and scams (deepfakes).  \n \n \n5   \n \nGUIDELINES ON SECURING AI SYSTEMS \n2. UNDERSTANDING \nAI THREATS  \n \nAI is a type of software system, and is itself vulnerable to cyber threats, \nwhile also posing a new attack surface for the broader enterprise \nsystem that it is integrated to, or interfaces with. As such, securing AI \nis in addition to practising good \u2018classical\u2019 cybersecurity hygiene. \n \n \nSecuring an AI system introduces new challenges that may be unfamiliar in traditional IT \nsystems. In addition to classical cybersecurity risks, the AI itself is vulnerable to novel \nattacks such as Adversarial Machine Learning (ML) that set out to distort the model\u2019s \nbehaviour. For more details on the security threats to AI, refer to Annex A. \nFigure 1. Classical and AI-specific risks of AI systems\u2013 diagram adapted from OWASP1 \n \n \n \n \n \n \n1 Threats overview - https://owaspai.org/docs/ai_security_overview/ \n \n6   \n \nGUIDELINES ON SECURING AI SYSTEMS \nCLASSICAL CYBERSECURITY RISKS TO AI SYSTEMS \n \nAI systems require vast amounts of data for training; some also require importing \nexternal models and libraries. If inadequately secured, AI systems can be \nundermined by supply chain attacks, or may be susceptible to intrusion or \nunauthorised access, through vulnerabilities in the AI model or the underlying IT \ninfrastructure. In addition, organisations and users risk losing the ability to \naccess and use AI tools if there are disruptions to cloud services, data centre \noperations, or other digital infrastructure (e.g. through Denial of Service attacks), \nthis could in turn disable systems that depend on AI tools to function.  \n \n \n \n \nADVERSARIAL MACHINE LEARNING \n \nMalicious actors may use novel Adversarial ML techniques to attack AI models and \ndata, influencing machine learning models to produce inaccurate, biased, or \nharmful output; and/or reveal confidential information. Adversarial ML 2 attacks \ninclude: data poisoning (injecting malicious or corrupted data into training data \nsets) or evasion attacks (on trained models) to distort outcomes, inference attacks \nor extraction attacks (probing the model) to expose sensitive or restricted data, or \nto steal the model. \n \n \n \n \n \n \n2 A Taxonomy and Terminology of Attacks and Mitigations https://csrc.nist.gov/pubs/ai/100/2/e2023/final. The MITRE ATLAS is a \nuseful reference to understand and situate classical cybersecurity risks from Adversarial ML. \n \n7   \n \nGUIDELINES ON SECURING AI SYSTEMS \n3. SECURING AI  \n \nThe security of AI is a widely cited concern, but this field of work is still \nrelatively nascent. While practitioners continue to grow the body of \nresearch and resources on the security threats to AI, these guidelines \nlay out key considerations that system owners should take to support \nsecure adoption of AI. Given the rapid speed of AI development, \nsystem owners should continue to apprise themselves on the latest \ndevelopments in AI security, and refresh their risk management \nstrategies accordingly. \n \n \n \n3.1. TAKE A LIFECYCLE \nAPPROACH  \n \nThere are five key stages \u2013 Planning and Design, Development, Deployment, Operations \nand Maintenance, and End of Life.   \n \nAs with good cybersecurity practice, CSA recommends that system owners take a lifecycle \napproach to consider security risks. Hardening only the AI model is insufficient to ensure \na holistic defence against AI related threats. All stakeholders involved across the lifecycle \nof an AI system should seek to better understand the security threats and their potential \nimpact on the desired outcomes of the AI system, and what decisions or trade-offs will \nneed to be made. \n \nThe AI lifecycle represents the iterative process of designing an AI solution to meet a \nbusiness or operational need. As such, system owners will likely revisit the planning and \ndesign, development, and deployment steps in the lifecycle many times in the delivery of \nan AI solution.  \n \n \n8   \n \nGUIDELINES ON SECURING AI SYSTEMS \nFigure 2: AI System Development Lifecycle (AI SDLC) \n \n \nSome organisations may have implemented the Machine Learning Operations (ML Ops) \npipeline, which may not map exactly to the AI SDLC. Nonetheless, ML Ops teams that run \na dev ops pipeline comprising ML Design, Development and Operation stages (similar to \nFigure 3), will find the guidelines across the AI SDLC's stages of Planning & Design, \nDevelopment, Deployment and Operations relevant. \n \nFigure 3: Example of ML-DevOps (source: Nvidia blog) \n \n \n \n3.2. START WITH A RISK \nASSESSMENT \n \nGiven the diversity of AI use cases, there is no one-size-fits-all solution to implementing \nsecurity. As such, effective cybersecurity starts with conducting a risk assessment. This \nwill enable organisations to identify potential risks, priorities, and subsequently, the \nappropriate risk management strategies. \n \nA fundamental difference between AI and traditional software is that while traditional \nsoftware relies on static rules and explicit programming, AI uses machine learning and \nneural networks to autonomously learn and make decisions without the need for detailed \ninstructions for each task. As such, organisations should consider conducting risk \nassessments more frequently than for conventional systems, even if they generally base \ntheir risk assessment approach on existing governance and policies. These assessments \nmay also be supplemented by continuous monitoring and a strong feedback loop. \n \nWe recommend these four steps to tailor a systematic defence plan that best addresses \nyour organisation\u2019s highest priority risks \u2013 protecting the things you care about the most.  \n \n9   \n \nGUIDELINES ON SECURING AI SYSTEMS \nSTEP 1 \nConduct risk assessment, focusing on security risks to AI systems \n \nConduct a risk assessment, focusing on security risks related to AI systems, either based \non best practices or your organisation\u2019s existing Enterprise Risk Assessment/Management \nFramework.  \n Risk assessment can be done with reference to CSA published guides, if applicable: \n\u25aa \nGuide To Cyber Threat Modelling \n\u25aa \nGuide To Conducting Cybersecurity Risk Assessment for Critical Information \nInfrastructure \n \n \n \n \nSTEP 2 \nPrioritise areas to address based on risk/impact/resources \n \nPrioritise which risks to address, based on risk level, impact, and available resources. \n \n \n \n \nSTEP 3 \nIdentify and implement the relevant actions to secure the AI \nsystem \n \nIdentify relevant actions and control measures to secure the AI system, such as by \nreferencing those outlined in the Companion Guide on Securing AI Systems and \nimplement these across the AI life cycle. \n \n \n \n \nSTEP 4 \nEvaluate residual risks for mitigation or acceptance \n \nEvaluate the residual risk after implementing security measures for the AI system to inform \ndecisions about accepting or addressing residual risks. \n \n10  \n  \nGUIDELINES ON SECURING AI SYSTEMS \n3.3. \nGUIDELINES FOR \nSECURING AI SYSTEMS \n \nThese guidelines apply across the various lifecycle stages of the AI \nsystem. System owners should read these as key issues to consider \nin securing their adoption of AI. In view of the diversity of use cases \nand developments in AI security, these guidelines do not provide \nprescriptive controls or requirements.  \n \nSystem owners should apply these to their specific context, and can \nreference the Companion Guide to Securing AI systems for potential \ncontrols. \n \n \n1. PLANNING AND DESIGN  \n \n1.1.  Raise awareness and competency on security risks  \n \nOrganisations should understand the potential security risks posed by AI, in order to \nmake informed decisions about adoption. Provide adequate training and guidance on \nthe security risks of AI to all personnel, including developers, system owners and senior \nleaders. \n \n1.2.  Conduct security risk assessments  \n \nRisk management strategies should be informed by a security risk assessment, which  \nwill help to determine key risks and priorities. Apply a holistic process to model threats \nand risks to an AI system, in accordance with relevant industry standards/best \npractices.  \n \n \n \n \n \n \n \n \n11  \n  \nGUIDELINES ON SECURING AI SYSTEMS \n2. DEVELOPMENT  \n \n2.1. Secure the supply chain  \n \nThe AI supply chain includes (but is not limited to) the training data, models, APIs, and \nsoftware libraries. Each of these components may introduce new vulnerabilities (e.g, \nmodels may carry malware encoded as model parameters that could enable attackers \nto extract and inject malicious software onto user machines). Assess and monitor \npotential security risks of the AI system\u2019s supply chain across its life cycle. Ensure that \nsuppliers adhere to security policies and internationally recognised standards, or that \nrisks are otherwise appropriately managed. Consider evaluating supply chain \ncomponents (e.g. through Software Bills of Material [SBOM], code checking, or against \nvulnerability databases). \n \n2.2. Consider security benefits and trade-offs when selecting \nthe appropriate model to use \n \nDifferent AI models (e.g. machine learning, deep learning, generative) pose unique \ncharacteristics and risks (e.g. LLMs can be vulnerable to input manipulation attacks) \nand as such require different security measures. When developing or selecting an \nappropriate AI model for your system, consider factors which may affect its security \n(such as complexity, explainability, interpretability, and sensitivity of training data).  \n \n2.3. Identify, track and protect AI-related assets \n \nAs AI systems become increasingly integrated into business operations, they will \nbecome part of an organisation\u2019s strategic assets and should be secured accordingly.  \nOtherwise, sensitive data, intellectual property and organisational assets are at risk of \npotential threats and breaches. Understand the value of AI-related assets, including \nmodels, data, prompts, logs and assessments. Have processes to track, authenticate, \nversion control, and secure assets. \n \n2.4. Secure the AI development environment \n \nAI models require access to large amounts of training data, and an insecure \ndevelopment environment can introduce risks of data breaches (e.g. exposure of \nPersonally Identifiable Information or confidential business information). Insecure \ndevelopment can also make AI models vulnerable to attacks (e.g. poisoning) that result \nin compromised model behaviour, or expose models and other intellectual property to \ntheft, unauthorised replication or misuse. Apply standard infrastructure security \nprinciples, such as implementing appropriate access controls and logging/monitoring, \nsegregation of environments, and secure-by-default configurations. \n \n12  \n  \nGUIDELINES ON SECURING AI SYSTEMS \n3. DEPLOYMENT  \n \n3.1. Secure the deployment infrastructure and environment of AI \nsystems \n \nSimilar considerations as with 2.4 \u201cSecure the AI development environment\u201d. Apply \nstandard infrastructure security principles, such as access controls and \nlogging/monitoring, segregation of environments, secure-by-default configurations, \nand firewalls. \n \n3.2. Establish incident management procedures \n \nAI systems are complex and adaptive, and this can sometimes result in unpredictable \nbehaviour. Given the diversity in AI use cases, incidents can range from minor issues \nsuch as malfunctioning chat bots to critical outcomes such as disruption in the \noperation of critical infrastructure. System owners should put in place appropriate \nincident response, escalation and remediation plans. \n \n3.3. Release AI systems responsibly \n \nAI systems can be vulnerable to the risks described above, including misuse, data \nbreaches, and model manipulation. These have impact on the trust and confidence of \nusers, and may have reputational implications for organisations. A good practice is to \nrelease models, applications or systems only after subjecting them to appropriate and \neffective security checks and evaluation. \n \n \n4. OPERATIONS AND MAINTENANCE  \n \n4.1. Monitor AI system inputs \n \nAI systems are dynamic and adaptive to input. There have already been real-life \nincidents, in which users/ attackers have deliberately crafted input to trick AI systems \ninto making incorrect or unintended decisions. AI system owners may wish to monitor \nand log inputs to the AI system, such as queries, prompts and requests, as third-party \nproviders may not do so due to privacy reasons. Proper logging allows for compliance, \naudit, investigation and remediation. \n \n \n \n \n \n13  \n  \nGUIDELINES ON SECURING AI SYSTEMS \n \n4.2. Monitor AI system outputs and behaviour \n \nAI systems can break or degrade in production phase. Monitoring models after \ndeployment will make sure that they are performing as intended, and alert system \nowners to potential issues (whether caused by adversarial attacks or otherwise). \nOperators should monitor for anomalous behaviour that might indicate intrusions, \ncompromise, or data drift.  \n \n4.3. Adopt a secure-by-design approach to updates and \ncontinuous learning  \n \nChanges to the data and model can lead to changes in behaviour. System owners \nshould ensure that risks associated to model updates have been considered and \nappropriately managed.  \n \n4.4. Establish a vulnerability disclosure process  \n \nEven with monitoring mechanisms in place, the adaptive nature of AI can make it \nchallenging to detect attacks and unintended behaviour. There should be a feedback \nprocess for users to share any findings of concern, which might uncover potential \nvulnerabilities to the system. \n \n \n \n5. END OF LIFE  \n \n5.1. Ensure proper data and model disposal \n \nAs models are trained on large amounts of training data (incl. potentially confidential \ninformation), improper disposal can lead to incidents such as data breaches. There \nshould be proper and secure disposal/destruction of data and models in accordance \nwith relevant industry standards or regulations. \n \n \n14  \n  \nGUIDELINES ON SECURING AI SYSTEMS \nGLOSSARY \n \nTerm \nBrief description \nAI system \nArtificial Intelligence.  \nA machine-based system that for explicit or implicit objectives, infers, from \nthe input it receives, how to generate outputs such as predictions, content, \nrecommendations, or decisions that can influence physical or virtual \nenvironments. Different AI systems vary in their levels of autonomy and \nadaptiveness after deployment.   \nAdversarial \nMachine \nLearning \nThe process of extracting information about the behaviour and characteristics \nof an ML system and/or learning how to manipulate the inputs into an ML \nsystem in order to obtain a preferred outcome. \nAnomaly \nDetection \nThe identification of observations, events or data points that deviate from what \nis usual, standard, or expected, making them inconsistent with the rest of \ndata. \nAPI \nApplication Programming Interface. \nA set of protocols that determine how two software applications will interact \nwith each other. \nBackdoor \nattack \nA backdoor attack is when an attacker subtly alters AI models during training, \ncausing unintended behaviour under certain triggers. \nChatbot \nA software application that is designed to imitate human conversation through \ntext or voice commands \nComputer \nVision \nAn interdisciplinary field of science and technology that focuses on how \ncomputers can gain understanding from images and videos. \nData Breach \nData Breach occurs when a threat actor gains unauthorised access to \nsensitive/confidential data.  \nData Integrity \nThe property that data has not been altered in an unauthorised manner. Data \nintegrity covers data in storage, during processing, and while in transit. \n \n15  \n  \nGUIDELINES ON SECURING AI SYSTEMS \nData Leakage \nUnintentional exposure of sensitive, protected, or confidential information \noutside its intended environment. \nData Loss \nPrevention \nA system\u2019s ability to identify, monitor, and protect data in use (e.g., endpoint \nactions), data in motion (e.g., network actions), and data at rest (e.g., data \nstorage) through deep packet content inspection, and contextual security \nanalysis of transaction (e.g., attributes of originator, data object, medium, \ntiming, recipient/destination, etc.) within a centralised management \nframework. \nData Poisoning \nControl a model with training data modifications. \nData Science \nAn interdisciplinary field of technology that uses algorithms and processes to \ngather and analyse large amounts of data to uncover patterns and insights that \ninform business decisions. \nDeep Learning \nA function of AI that imitates the human brain by learning from how it \nstructures and processes information to make decisions. Instead of relying on \nan algorithm that can only perform one specific task, this subset of machine \nlearning can learn from unstructured data without supervision. \nDefence-in-\nDepth \nDefence in depth is a strategy that leverages multiple security measures to \nprotect an organization's assets. The thinking is that if one line of defence is \ncompromised, additional layers exist as a backup to ensure that threats are \nstopped along the way. \nEvasion attack \nCrafting input to AI in order to mislead it into performing its task incorrectly. \nExtraction \nattack  \nCopy or steal an AI model by appropriately sampling the input space and \nobserving outputs to build a surrogate model that behaves similarly. \nGenerative AI \nA type of machine learning that focuses on creating new data, including text, \nvideo, code and images. A generative AI system is trained using large amounts \nof data, so that it can find patterns for generating new content. \nGuardrails \nRestrictions and rules placed on AI systems to make sure that they handle \ndata appropriately and don't generate unethical content. \n \n16  \n  \nGUIDELINES ON SECURING AI SYSTEMS \nHallucination \nAn incorrect response from an AI system, or false information in an output that \nis presented as factual information. \nImage \nRecognition \nImage recognition is the process of identifying an object, person, place, or text \nin an image or video. \nLLM \nLarge Language Model. \nA type of AI model that processes and generates human-like text. LLMs are \nspecifically trained on large data sets of natural language to generate human-\nlike output.   \nML \nMachine Learning. \nA subset of AI that incorporates aspects of computer science, mathematics, \nand coding. Machine learning focuses on developing algorithms and models \nthat can learn from data, and make predictions and decisions about new data.  \nMembership \nInference \nattack \nData privacy attacks to determine if a data sample was part of the \ntraining set of a machine learning model. \nNLP \nNatural Language Processing. \nA subset of AI that enables computers to understand spoken and written \nhuman language. NLP enables features like text and speech recognition on \ndevices. \nNeural Network \nA deep learning technique designed to resemble the human brain\u2019s structure. \nNeural networks require large data sets to perform calculations and create \noutputs, which enables features like speech and vision recognition. \nOverfitting \nOccurs in machine learning training when the algorithm can only work on \nspecific examples within the training data. A typical functioning AI model \nshould be able to generalise patterns in the data to tackle new tasks. \nPrompt \nA prompt is a natural language input that a user feeds to an AI system in order \nto get a result or output. \nReinforcement \nLearning \nA type of machine learning in which an algorithm learns by interacting with its \nenvironment and then is either rewarded or penalised based on its actions. \n \n17  \n  \nGUIDELINES ON SECURING AI SYSTEMS \n \nSDLC \nSoftware Development Life Cycle \nThe process of integrating security considerations and practices into the \nvarious stages of software development. This integration is essential to ensure \nthat software is secure from the design phase through deployment and \nmaintenance. \nTraining data \nTraining data is the information or examples given to an AI system to enable it \nto learn, find patterns, and create new content. \n \n18  \n  \nGUIDELINES ON SECURING AI SYSTEMS \nANNEX A  \nUNDERSTANDING AI THREATS  \n \n \nAdversarial threats are caused by threat actors with deliberate intention to cause harm. \nTypically, these threat actors are referred to as attackers or adversaries. \n \nTo understand these threats, system owners can refer to resources such as the OWASP \nTop 10 for Large Language Model Applications, or OWASP Machine Learning Security Top \n10, or the MITRE ATLAS\u2122 (Adversarial Threat Landscape for Artificial-Intelligence Systems). \nThe MITRE ATLAS in particular provides a structured knowledge base for AI and \ncybersecurity professionals to understand and defend against AI cyber threats. It \ncompiles adversary tactics, techniques, and case studies for AI systems based on real-\nworld observations, demonstrations from ML red teams and security groups, as well as \nstate-of-the-possible from academic research. \n \nAny attempt to secure an AI system should be on top of the \u2018traditional\u2019 good cybersecurity \nhygiene, such as implementing the principle of least privileges, multi-factor \nauthentication, continuous security monitoring and auditing.  \n \nThe ATLAS3 Matrix (see Table A1) covers 2 types of adversarial \u2018techniques\u2019. \n\u2022 \nTechniques specific to AI/ML systems (indicated in orange boxes), and  \n\u2022 \nTechniques that are conventional cybersecurity offensive techniques, but \napplicable to both AI and non-AI systems and come directly from the MITRE \nEnterprise ATT&CK Matrix (indicated in white boxes). \n \nSystem owners should continue to build their awareness of security threats using these \nresources, to better understand emerging risks that may have implications on their \nadoption of AI. As this space continues to evolve, such resources will aid both AI and cyber \nteams in their security risk assessment and management activities.  \n \n \n \n \n3 MITRE ALTAS Framework: https://atlas.mitre.org/. It leverages the same core principles and structure of the well-known \nMITRE ATT&CK (Adversarial Tactics, Techniques, and Common Knowledge) framework, which is widely used by cyber \ndefenders to map the terminologies of cybersecurity attacks. The ATLAS adapts these to the unique context of AI systems and \npotential adversarial attacks.   \n \n19  \n  \nGUIDELINES ON SECURING AI SYSTEMS \nTable A1: MITRE ATLAS Matrix \nReconnaissance \nSearch for \nVictim\u2019s \nPublicly \nAvailable \nResearch \nMaterials \nSearch for \nPublicly \nAvailable \nAdversarial \nVulnerability \nAnalysis \nSearch \nVictim-\nOwned \nWebsites \nSearch \nApplication \nRepositories \nActive \nScanning \n \n \nResource \nDevelopment \nAcquire \nPublic ML \nArtifacts \nObtain \nCapabilities \nDevelop \nCapabilities \nAcquire \nInfrastructure \nPublish \nPoison \nDatasets \nPoison \nTraining \nData \nEstablish \nAccounts \nInitial Access \nML Supply \nChain \nCompromise \nValid \nAccounts \nEvade ML \nModel \nExploit Public-\nFacing \nApplication \nLLM \nPrompt \nInjection \nPhishing \n \nML Model Access \nML Model \nInference API \nAccess \nML-Enabled \nProduct or \nService \nPhysical \nEnvironment \nAccess \nFull ML Model \nAccess \n \n \n \nExecution \nUser \nExecution \nCommand \nand Scripting \nInterpreter \nLLM Plugin \nCompromise \n \n \n \n \nPersistence \nPoison \nTraining Data \nBackdoor ML \nModel  \nLLM Prompt \nInjection \n \n \n \n \nPrivilege \nEscalation \nLLM Prompt \nInjection \nLLM Plugin \nCompromise \nLLM Jailbreak \n \n \n \n \nDefence Evasion \nEvade ML \nModel \nLLM Prompt \nInjection \nLLM Jailbreak \n \n \n \n \nCredential Access \nUnsecured \nCredentials \n \n \n \n \n \n \nDiscovery \nDiscover ML \nModel \nOntology \nDiscover ML \nModel Family \nDiscover ML \nArtifacts \nLLM Meta \nPrompt \nExtraction \n \n \n \nCollection \nML Artifact \nCollection \nData from \nInformation \nRepositories \nData from \nLocal System \n \n \n \n \nML Attack \nStaging \nCreate Proxy \nML Model \nBackdoor ML \nModel \nVerify Attack \nCraft \nAdversarial \nData  \n \n \n \nExfiltration \nExfiltration \nvia ML \nInference API \nExfiltration \nvia Cyber \nMeans \nLLM Meta \nPrompt \nExtraction \nLLM Data \nLeakage \n \n \n \nImpact \nEvade ML \nModel \nDenial of ML \nService \nSpamming \nML System \nwith Chaff \nData \nErode ML \nModel \nIntegrity \nCost \nHarvesting \nExternal \nHarms \n \n \n \n \n", "metadata": {"country": "Singapore", "year": "2024", "legally_binding": "no", "binding_proof": "Evergreen guidelines issued by the Cyber Security Agency (CSA) of Singapore to ensure AI systems are secure throughout their lifecycle.", "date": "03/01/2024", "regulator": "Cyber Security Agency of Singapore (CSA)", "type": "AI cybersecurity guidelines", "status": "consultation", "language": "English", "use_cases": "[3, 6]"}}
{"_id": "686acdffe2af9fb1ff3ba994", "title": "Global AI Assurance Pilot", "source": "https://assurance.aiverifyfoundation.sg/wp-content/uploads/2025/06/Assurance-Pilot-Main-Report.pdf", "text": "Table of Contents\nTitle\nPage\nExecutive Summary\n00\nChapter 1 - Introduction\n01\n1.1 Rationale\n01\n1.2 Target outcomes\n02\n1.3 Ground rules\n02\nChapter 2 - Pilot participants and use cases\n03\n2.1 Participant profile\n03\n2.2 Use cases\n04\n2.3 Patterns of LLM usage\n05\nChapter 3 - Risk Assessment and Test Design\n06\n3.1 Risk Assessment\n06\n3.2 Metrics\n07\n3.3 Testing approach: Test datasets\n07\n3.4 Testing approach: Evaluators\n08-09\nChapter 4 - Test Implementation\n10\n4.1 Test Environment\n10\n4.2 Test data and effort\n10\n4.3 Implementation challenges\n10\nChapter 5 - Lessons learnt\n11\n5.1 Test what matters\n11-12\n5.2 Don\u2019t expect test data to be fit for purpose\n13\n      Guest Blog: Learning from self-driving cars: Simulation Testing\n14\n      Guest Blog: Synthetic Data for Adversarial Testing\n15\n5.3 Look under the hood\n16\n5.4 Use LLMs as judges, but with skill and caution\n17\n      Guest Blog: LLM-as-a-judge: Pros and Cons\n18\n5.5 Keep your human SMEs close!\n19\n      Guest Blog: LLMs can\u2019t read your mind\n20\nChapter 6 - What\u2019s next?\n21-22\nExecutive Summary\nFrom Model Safety to Application Reliability\nAs Generative AI (\u201cGenAI\u201d) transitions \nfrom personal productivity tools and \nconsumer-facing chatbots into real-world \nenvironments like hospitals, airports and \nbanks, it faces a higher bar on quality \u2028\nand confidence. \n01\nRisk assessments depend heavily on the context of the \u2028\nuse case \u2013 e.g., lower tolerance for error in a clinical \u2028\napplication than a customer service chatbot.\n02\nGiven the higher complexity involved in integrating foundation \nmodels with existing data sources, processes and systems, there \nare more potential points of failure.\nHowever, much of the current work around AI testing focuses on the safety of foundation models, rather than the reliability of \u2028\nend-to-end applications. The Global AI Assurance Pilot was an attempt to address this gap: not through academic research, \u2028\nbut by building upon real-life experiences of practitioners around the world.\nLearning by doing\nThe pilot matched 17 deployers of GenAI applications with 16 specialist AI testing firms. These organisations were based in \nSingapore and 8 other geographies, providing a significant international lens. The primary objective was to surface and codify \nemerging norms in technical testing of GenAI applications. The 17 applications were aimed at a mix of internal (12) and \u2028\nexternal (5) users. There was a human in the loop (12) in most cases. 10 industries were represented, including banking, \nhealthcare and technology. Large Language Models (LLMs) were utilised in a variety of ways in these applications: summarisation, \nretrieval augmented generation, data extraction, chatbots, classification, translation, agentic workflows and code generation.\nThe \u201cwhat\u201d and \u201chow\u201d of testing GenAI applications\nDeciding what to test (or not!)\nDeciding what to test (or not!) \nwas a non-trivial exercise. \u2028\nThe 3 risks that interested most \ndeployers were accuracy and \nrobustness, use case specific \nregulation and compliance \nrequirements, and content safety.\nTest Datasets\nOff-the-shelf LLM benchmark test \ndatasets were rarely used to \nconduct the tests, except to test \ncontent safety in external facing \napplications. Use-case specific test \ndata sets were used most often, \nthough many decided to \nsupplement these with adversarial \nred-teaming or simulation testing \nfor edge-case scenarios.\nEvaluate test results\nThe 2 most popular ways to \nevaluate test results were human \nreview and LLMs-as-judges. Many \nparticipants highlighted that while \nthe latter are versatile, scalable \nand accessible, they carry risks and \nrequire mitigating controls.\n00\nTesting Real-World Generative AI systems\nGetting GenAI testing right: 4 practical recommendations\n01\nTest what matters\nYour context will determine what risks you should (or \nshould not!) care about. Spend time upfront to design \neffective tests for those.\n02\nDon\u2019t expect test data to be fit for purpose\nNo one has the \u201cright\u201d test dataset to hand. Human and \nAI effort is needed to generate realistic, adversarial and \nedge case test data.\n03\nLook under the hood\nTesting just the outputs may not be enough. Interim \ntouchpoints in the application pipeline can help with \ndebugging and redundancy. With agentic AI \napplications, this becomes a necessity.\n04\nUse LLMs as judges, but with skill and caution\nHuman-only evaluations will not scale. LLMs-as-judges \nare necessary but require careful design and human \ncalibration. Cheaper, faster and simpler alternatives \nexist, in some situations.\nThere was also an overwhelming reinforcement of the critical role of human experts, at every stage of the GenAI testing lifecycle.\nWhat comes next?\nPilot participants suggested 4 areas for future collaboration:\nBuilding awareness \nand sharing emerging \nbest practices around \nGenAI testing\nMoving towards \nindustry standards \naround \u201cwhat to test\u201d \nand \u201chow to test\u201d\nCreating an \naccreditation \nframework for testing\nSupporting greater \nautomation for \ntechnical testing\nThe launch of IMDA Starter Kit \u2013 for consultation \u2013 is an initial step to address some of these requests.\nThe journey towards making GenAI applications reliable in real-world settings has just started. IMDA and AIVF look \nforward to continued collaboration with AI builders, deployers and testers, and policy makers, on this important \ninitiative.\n00\nTesting Real-World Generative AI systems\n01\nTesting Real-World Generative AI systems\n ! Introduction\nThe AI Verify Foundation (AIVF) is a non-profit subsidiary of Singapore\u2019s Infocomm Media Development Authority (IMDA).  \u2028\nIts mission is to support the creation of a trusted AI ecosystem through access to reliable AI testing capabilities. \u2028\u2028\nTogether with its parent IMDA, the AIVF launched the Global AI Assurance Pilot in February 2025, to help codify emerging norms \nand best practices around technical testing of Generative AI (\u201cGenAI\u201d) applications. Existing, real-world GenAI applications were \nput to the test, pairing organisations that had deployed them with specialist AI testing firms.\n1.1 Rationale\nThe pilot was motivated by three core beliefs:\n001\nGenAI can have a massive, \npositive impact on our society \nand economy \u2013 if it is adopted \nat scale in public and private \nsector organisations.\n002\nSuch \u201creal-world\u201d adoption \nrequires GenAI applications to \noperate at a much higher level \nof quality and reliability (vs. the \ngeneral-purpose models that \nunderpin them).\n003\nThe extensive work underway on \nAI model safety and capability is \nnecessary, but not sufficient, to \nhelp meet that higher bar.\nLarge Language Models (LLMs) and their multi-modal equivalents are being adopted extensively as personal productivity tools. \nHowever, to have real transformational impact, GenAI must get embedded in the public and private sector organisations that \u2028\ndrive critical parts of the economy, such as health, finance, utilities and government services.\nUsing GenAI in such real-world situations, \nat scale, raises the quality and reliability \nbar significantly. Two factors account for \nthis difference: Context and Complexity. \nMost academic and technology industry efforts \naround AI testing have tended to focus on Model \nsafety and alignment. A shift is required \u2013 from the \nSafety of Foundation Models to the Reliability of \nthe end-to-end Systems or Applications in which \nthey are embedded.\nModels\nE2E Systems\nAI Safety\nAI Reliability\n01\nContext\nUnlike a general purpose LLM chatbot application or personal \nproductivity tool, a GenAI-enabled application must operate in the \nspecific context of a use case, organisation, industry and/or socio-\ncultural expectations. For example, a GenAI application in a \u2028\nhealthcare setting may have very low levels of tolerance for \n\u201challucination\u201d compared to one used as an internal employee helpdesk. \n02\nComplexity\nReal-life GenAI applications are also likely to have more layers of \ncomplexity. They may use LLMs in conjunction with existing data \nsources, processes and systems, creating additional potential \u2028\npoints of failure beyond the LLM.\nThe pilot was an attempt to start enabling that shift \u2013 not through new academic research or technical development, \u2028\nbut through real-world experience.\n02\nTesting Real-World Generative AI systems\n1.2 Target outcomes\nThe pilot was launched with 3 target outcomes\nTesting norms and practices\u2028\na Inputs into future standards for \nTechnical Testing of Gen AI \napplications\nFoundations for a viable \nassurance market\na Greater awareness of the ways \nin which external assurance can \nbuild trust in GenAI \napplications and enable \nadoption at scale\u008c\na A foundation for potential \naccreditation programmes in \nthe future\nAI testing tool roadmaps\na Inputs into the product \nroadmaps for open source and \nproprietary AI testing softwar\u00ba\na Specific focus areas for AIVF\u2019s \nMoonshot platform\n1.3 Ground rules\nThe pilot had three ground rules:\n01\nThe application must involve the \nuse of at least one LLM or multi-\nmodal model \n02\nThe application must be live or \nintended to go-live (not Proofs-\nof-Concept)\n03\nThe exercise must focus on \ntechnical testing (not process \ncompliance)\n04\nTesting should be conducted on the GenAI application \n(not just the underlying foundation model)\n05\nTesting must be conducted by an external party \u2013 i.e., \nan organisation different from the one that has built \nand/or deployed the application.\nIMDA and AIVF sought no access to the actual results of the technical tests. The focus was on understanding how the deployer \nsaw the associated risks, how technical tests were designed and implemented to assess them, and the lessons learnt from the \nexercise.\n03\nTesting Real-World Generative AI systems\n&\u001f Pilot participants and use cases\n33 organisations from ~10 geographies and industries participated in the pilot. The use cases spanned a broad range of \u2028\nfunctional areas and LLM usage archetypes. Almost all were already in production, though mostly with humans in the loop.\n2.1 Participant profile\nGenAI applications from 17 organisations were put to the test during the pilot\nHealthcare/ Pharma\nBanking, Insurance, Fintech\nEuropean Insurer\nMultinational Bank\nWealth management \u2028\nInstitution\nIT/ Software\nOthers\nHigh-tech Manufacturer\nEach of these organisations was paired with 1 (or 2) of 16 specialist firms that provide software and/or services to test \u2028\nGenAI applications. In some cases, the \u201cpairing\u201d was done by the participants themselves, whereas in others, \u2028\nAIVF helped match deployers with testers.\n16\nLeading AI testing \nspecialists\nAbout half of these 33 organisations were based in Singapore. The remaining came from 8 other geographies\u2013 \u2028\nCanada, France, Germany, Hong Kong, Switzerland, Taiwan, UK, US.\n1Applications were deployed by the named organisations- except in the case of MIND Interview, Tookitaki, Unique, ultra mAInds and Fourtitude . All of these were intended to be \ndeployed at their B2B clients.\n2 In two cases, more than one testing firm was involved (Changi Airport with PRISM Eval and Guardrails, and ultra mAInds with Aiquris and AIDX). One testing firm-Ragas-provided \nsupport and expert advice without directly partnering with a deployer\n04\nTesting Real-World Generative AI systems\n2.2 Use cases\nBackground\n9\n7\n1\nIs The Application Live?\nIn production\nIn Beta and/or with\u2028\nselected users\nIn testing\n16 of the 17 use cases were already live in production.\n7 of them were in beta or/or rolled out to a limited group of users. \n10\n5\n2\nWho Are The Target \nUsers?\nInternal (all staff)\nInternal (specialist)\nExternal\nA majority were targeted at specialist users inside an organisation \u2028\n(e.g., software engineers at NCS). 5 were customer/ citizen-facing. \n5\n12\nIs A Human In The Loop?\nYes\nNo\nA human was \u201cin the loop\u201d in more than 2/3rd of the cases. \u2028\nEven in the remaining 5, there was significant human involvement \u2028\noutside the immediate workflow of the application.\nFull list of use cases\n#\nTester(s)\nDeployer\nUse case\n1\nAdvai\nCheckmate\nOn-demand Scam and Online Fact-checker\n2\nAIDX\nFourtitude\nCustomer Service Chatbot (\u201cAssure.ai\u201d) for publicsector and utility clients\n3\nAIDX\nSynapxe\nHealthHub AI Conversational Assistant\u00a0\n4\nAIDX/Aiquris\nultra mAInds\nNo-code AI-powered Retrieval Augmented Generation platform for Enterprise search and data connectivity\n5\nFairly\nMIND Interview\nAI-enabled Candidate Screening and Evaluation tool\n6\nGuardrails\nPRISM Eval\nCAG\nAskMax Virtual Concierge Chatbot\n7\nKnovel\nHTX\nProductivity Co-pilot\n8\nLatticeFlow\nConfidential\nInvestment Insights for Relationship Managers\n9\nParasoft\nNCS\nAI-enabled Coding Assistant for refactoring code\n10\nPwC\nSCB\nClient Engagement Email Generator for Wealth Management Relationship Managers\n11\nPwC\nUOB\nInternal Q&A Chatbot\n12\nQuantpi\nUnique\nInvestment Research Assistant\n13\nResaro\nMSD\nConfidential\n14\nResaro\nTookitaki\nFinMate Anti-Money Laundering Assistant\n15\nSoftserve\nCGH\nMedical Reports Summarisation,\n16\nVerify AI\nConfidential\nPublic Road Safety Chatbot\n17\nVulcan\nHigh-tech \nManufacturer\nMulti-lingual Internal Knowledge Bot\n05\nTesting Real-World Generative AI systems\n2.3 Patterns of LLM usage \nAcross the 17 applications, LLMs3 \u2028\nwere used in diverse ways.\u2028\n \nThe top 3 usage patterns were \nSummarisation, Retrieval Augmented \nGeneration and Data Extraction from \nunstructured sources. These patterns \nalign with the focus of many of these \napplications on staff productivity \nimprovement. \u2028\nLLMs were also used to power multi-\nturn chatbots, and to help translate \nbetween languages. Relatively few used \nLLMs as part of agentic workflows \u2013 yet.\nHow are LLMs used in the application?\n16\n12\n08\n04\n00\n15\nSummarisation\n13\nRetrieval Augmented Generation - RAG\n12\nData extraction from unstructured sources\n10\nMulti-turn chatbot\n8\nClassification or Recommendation\n6\nTranslation\n3\nOrchestrator for an agentic flow\n2\nDrafting email\n2\nMultimodal (video/ audio to text or vice versa)\n1\nCode re-factoring\nThe table below maps each of the 17 applications to the different LLM usage modalities:\nTable - How Are LLMs Used in the Applications?   \nTester\nDepoyer\nSUM\nTRA\nDAT\nCLA\nMUL\nRAG\nCHT\nCOD\nAGE\nEML\nAdvai\nCheckmate\nAIDX\nFourtitude\nAIDX\nSynapxe\nAIDX/Aiquris\nultra mAInds\nFairly\nMind Interview\nGuardrails\nPRISM Eval\nCAG\nKnovel\nHTX\nLatticeFlow\nEuropean FI\nParasoft\nNCS\nPwC\nSCB\nPwC\nUOB\nQuantpi\nUnique\nResaro\nMSD\nResaro\nTookitaki\nSoftserve\nCGH\nVerify AI\nConfidential\nVulcan\nHigh-tech \nManufacturer\nLegend\nSUM\nSummarisation\nTRA\nTranslation\nDAT\nData extraction from unstructured sources\nMUL\nMultimodal (video/ audio to text or Vice versa)\nRAG\nRetrieval Augmented Generation\nCHT\nMulti-turn chatbot\nCLA\nClassification or Recommendation\nCOD\nCode Refactoring\nAGE\nCode Refactoring\nEML\nDrafing email\n3 In a couple of cases, multi-modal models were also used\n06\nTesting Real-World Generative AI systems\n3. Risk Assessment and Test Design\nThere are 4 key choices to be made when designing tests for a Generative AI application:\nRisks that matter the \u2028\nmost for the application\nMetrics to help assess \u2028\nthe prioritised risks in a \nquantifiable manner\nDataset provided as \u2028\ninput to the application\u00a0\nEvaluator to assess \u2028\nthe output from \u2028\nthe application\n3.1 Risk Assessment\nAt the outset, each deployer defined the risks that mattered most to them. A subset was selected for testing during the pilot timelines.\nWhat risks were prioritised and tested during pilot?\n(number of use cases)\n20\n15\n10\n05\n00\n2\n2\n1\n3\n2\n3\n1\n1\n15\nIn-accuracy, incl (lack of) robustness & completeness\n9\nBreach of Use-case specific compliance or regulation\n9\nContent safety\n5\nReputation risk\n5\nUnfair bias\n3\nSecurity\n3\nInappropriate data disclosure\n1\nInadequate User Trans-parency\nRisks tested during pilot\nHigh priority but not tested during pilot\nIn line with the focus on summarisation, \u2028\nRAG and data extraction as the top LLM use \npatterns, deployers saw the highest risk in \u2028\noutputs that were inaccurate, incomplete or \ninsufficiently robust.\u2028\nWith many use cases in regulated industries, \u2028\nthe risk of not meeting existing, non-AI-specific \nregulations or internal compliance requirements \ncame next. Content safety was also considered \nimportant, particularly for applications facing off \nto external users.\nThe following examples illustrate how the specific context of individual use cases led to the risk prioritisation by the deployers.\nDeployer\nUse case\nExample of prioritised risk\nCheckmate\nOn-demand Scam and Online Fact-checker\nMalicious attackers seeking to undermine its effectiveness - e.g., falsely \nlabelling fraudulent messages as authentic - or availability - e.g., denial of \nservice through prompt injection.\nFourtitude\nCustomer Service Chatbot (\u201cAssure.ai\u201d) for public \nsector and utility clients in Malaysia\nContent that potentially offends Malaysian religious, cultural and racial \nsensitivities\nSynapxe\nHealthHub AI Conversational Assistant\nContent that could pose a risk to an individual's wellbeing - e.g., mental \nhealth, healthcare habits and alcohol consumption\nMIND Interview\nAI-enabled Candidate Screening and Evaluation tool\nUnfair bias, which is a key consideration for recruitment-related laws in \nmany geographies\u00a0\nNCS\nAI-enabled Coding Assistant for refactoring code\nPoor quality and/or insecure refactored code\nStandard Chartered\nClient Engagement Email Generator for Wealth \nManagement Relationship Managers\nNon-adherence to relevant regulation & internal compliance requirements \naround provision of investment advice to clients\nChangiGeneral Hospital\nMedical Report Summarisation\nInaccurate fact extraction and/or surveillance recommendations for individual patient\n07\nTesting Real-World Generative AI systems\n3.2 Metrics\nOnce the priority risks have been identified, appropriate metrics need to be defined to quantify the results of the testing.\nFor example:\nDeployer\nPrioritised risk\nMetric(s) chosen\nMIND Interview\nUnfair bias\nImpact Ratios by sex, race, and sex + race\nStandard Chartered\nAccuracyRobustness\nHallucination and Contradiction rate (Accuracy)\nCosine similarity of generated drafts with the same inputs (Robustness)\nTookitaki\nAccuracy\nPresence and correctness of key entities (amounts, dates, names - post-masking) and critical \ninstructions in Narration generated by assistant (Precision/ Recall/ Faithfulness)\nSynapxe\u00a0\nUnsafe content\nPoint scale to judge how well the Synapxe/ Health Hub chatbot was able to block out-of-policy requests\nChangi Airport\nFalse refusal\n% of refused requests subsequently found to be within application\u2019s mandate and RAG context\nUnique\nAccuracy/ Irrelevance\nWord Overlap Rate, Mean Reciprocal Rank, Lenient Retrieval Accuracy to assess Search layer\n3.3 Testing approach: Test datasets\nThere are 4 alternatives \nwhen it comes to sourcing or \ncreating the datasets needed \nto test the GenAI \napplication. Testers in the \npilot used all four.\nHow did they conduct the tests?\n(Number of Use Case)\n00\n05\n10\nSimulation testing (eg for edge cases)\n10\nUse-case specific test data\n9\nRed-teaming (adversarial)\n7\nBenchmarking\n5\n01\nBenchmarking\nDefinition\u00a0\nBenchmarking involves presenting the application with a standardised set of \ntask prompts and then comparing the generated responses against pre-\ndefined answers or evaluation criteria\nWas used in instances where the application was to be tested for \ngeneralisable risks such as content toxicity, data disclosure or security.\nWas not used when application was to be tested for context-specific \nrisks, such as accuracy and completeness of answers sourced from the \ndeployer\u2019s internal knowledge base\nExample\nParasoft: Testing of NCS\u2019 AI-refactored \u2028\ncode against its standard security and \u2028\ncode compliance requirements.\nAIDX: Testing of Synapxe\u2019s and ultra \nmAInds\u2019 applications vs. generic content \nsafety benchmarks.\n08\nTesting Real-World Generative AI systems\n02\n(Adversarial) Red-Teaming\nDefinition\u00a0\nRed-teaming is the practice of probing applications for system failures or \nrisks such as content safety or sensitive data leakage. Can be done \nmanually, or automated using another model.\nWas used when dynamic testing - e.g. through creative prompt \nstrategies, multi-turn conversations - was required, compared to static/ \nstructured benchmarks \nWas used not just in external-facing applications, but also where the \npotential harm from malicious internal actors was significant\nExample\nPRISM Eval: Use of proprietary Behavioural \nElicitation Tool to map the responses of \nChangi Airport\u2019s Virtual Assistant across \u2028\n6 content safety areas\nVulcan: Attempts to make the knowledge \nbot at high-tech manufacturer disclose \nconfidential IP or the meta-prompts \nunderpinning the application\n03\nUse-case specific test data\nDefinition\u00a0\nUse-case-specific test datasets are static and structured like benchmarks but \nrelate to only the specific application being tested. Such datasets can be \nhistorical, sourced from production runs or synthetically generated.\nDefault option in most pilot use cases\nConceptually familiar to business and data science teams\nLimited availability of historical data in most pilot use cases, but several \nused \u201crealistic\u201d synthetic data\nExample\nSoftserve: use of historical data to test \nChangi General Hospital\u2019s Medical Report \nSummarisation application\nVerify AI: use of an LLM to generate \nrepresentative questions from the original \ndocument used in the Road Safety Chatbot \nRAG application\n04\nSimulation tests (non-adversarial)\nDefinition\u00a0\nSimulation testing involves increasing test coverage, by simulating long tail \nor edge case scenarios and generating synthetic data corresponding to \nthem. Also referred to as \u201cstress testing\u201d.\nWas used where the application\u2019s ability to respond to out-of-distribution \ntest cases was to be tested\nRequired combination of human creativity - to come up with relevant \nscenarios - and automation \u2013 to generate synthetic test data at scale\nExample\nGuardrails AI: Large-scale simulation \u2028\ntesting on Changi Airport\u2019s Virtual Assistant \nto generate realistic, diverse scenarios that \nreveal critical failure modes around \nhallucination, toxic content and over-refusal\nResaro: Series of perturbation techniques - \ne.g., missing value imputation, \u2028\nerror injection, numeric and logical errors - \napplied to 100 \u201cin distribution\u201d queries from \ndeployer Tookitaki\n3.4 Testing approach: Evaluators\nEvaluators are tools or methods used to apply a selected metric to the application\u2019s output and generate a score or label.\u2028\n\u00a0\nHuman experts are often considered to be the \u201cgold standard\u201d when it comes to assessing whether the output from an \napplication meets defined criteria. However, by definition, this approach is not suited for automated assessments and thus, \u2028\nnot scalable.\u2028\nThe alternative is to use rule-based logic, traditional statistical measures such as semantic similarity, an LLM as a judge, or another \nsmaller model. Typically, the more probabilistic the technique, the greater the need for careful human review and calibration of the \ntest results.\n09\nTesting Real-World Generative AI systems\nHow did the pilot participants evaluate test results? (Number of Use Cases)\nTester\nDepoyer\nHuman \njudgement or \nreview\nRule-based logic\nSurface-level/ \nSemantic \nmetrics\nLLM-as-judge\nNon-LLM model \nas judge\nAdvai\nCheckmate\nAIDX\nFourtitude\nAIDX\nSynapxe\nAIDX/Aiquris\nultra mAInds\nFairly\nMind Interview\nGuardrails\nPRISM Eval\nCAG\nKnovel\nHTX\nLatticeFlow\nEuropean FI\nParasoft\nNCS\nPwC\nSCB\nPwC\nMNC Bank\nQuantpi\nUnique\nResaro\nMSD\nResaro\nTookitaki\n*\nSoftserve\nCGH\n*\nVerify AI\nInsurer\n^\nVulcan\nHigh-tech \nManufacturer\nTotal\n14\n9\n5\n13\n4\nLegend\n*\nLLM used to extract facts as part of eval\n^\nStatistical models used to check effectiveness of LLM-as-judge\n01\nMost testers in the pilot (14) \nused LLMs as judges, due to \ntheir versatility and accessibility.\n02\nHuman reviewers were used \noften (13) to evaluate bespoke, \nsmall-scale tests and to calibrate \nautomated evaluation scores \nparticularly when using LLM-as-\na-judge.\n03\nRule-based logic was popular \n(10) wherever LLMs were being \nused in data extraction.\n04\nSmaller models \u2013 as alternatives to LLMs \u2013 were used \nless frequently (4) in the pilot, but are more likely when \ntesting at scale, due to their simplicity and cost \neffectiveness.\n05\nStatistical measures like BLEU were less popular.\n10\nTesting Real-World Generative AI systems\n4. Test Implementation\u00a0\n4.1 Test Environment\nMost (10) testers used their own proprietary testing platform to execute the tests. Installing these within the deployer\u2019s network \nwas difficult within the short timeframe of the pilot. However, this option was still feasible if\u00a0\nThe GenAI application allowed external access via API, and/or\u00a0\nRelevant input/ output/ trace data could be shared externally by deployer with appropriate anonymising/ safeguards\nIn the remaining instances, a mix of bespoke testing scripts and tester\u2019s existing testing libraries were used. In at least two cases, \nthe tester was onboarded by the deployer into a staging environment for the testing exercise.\n4.2 Test data and effort\nGiven the time spent upfront to define \u201cwhat to test\u201d and \u201chow to test\u201d, limited time was available for actual test execution. As a \nresult, test sizes were relatively small. Most testers used a few hundred test cases, though two (PRISM Eval and Verify AI) went into \ntens or even hundreds of thousands.\n\nThe effort needed from the deployer and tester teams varied. Many required a total of 50-100 hours\u2019 worth of effort over 2-4 \nweeks, though a few required hundreds. Infrastructure and LLM costs were inconsistently shared, but do not appear to have been \nsignificant in the context of this limited pilot period.\n4.3 Implementation challenges\nThe difficulty of finding, or \ngenerating, test data that \nis realistic, able to cover \nedge cases and anticipate \nadversarial attacks, was \nseen as a common \nchallenge by most pilot \nparticipants.\nImplementation Challenges During Testing (Number of Use Cases)\n00\n05\n10\n15\nFinding/ generating relevant test data\n13\nAPI access/ throughput/ latency/ performance\n5\nConfidentiality, Privacy, Security constraints\n5\nLack of granular tracing inside app pipeline\n4\nAccess needed to SMEs\n4\nRepeatability of test results\n3\nBeyond that, testers also found the following aspects challenging:\nConfidentiality, Security and Privacy constraints: impacted access to relevant test data, system prompts and even the actual application. \nAPI Access, Throughput, Latency and Performance.\nLack of granular tracing access inside the application pipeline: resulting in limited ability to test and debug at interim points.\nHigh demand for access to human subject matter experts: e.g., to annotate \u201cground truth\u201d or calibrate results of automated testing.\nLack of consistency: Differences in response from the same application, to the same input, making it difficult to create consistent test results.\nSome testers were also concerned about not sharing too much publicly on their proprietary testing approach \u2028\n(particularly for adversarial benchmarking and red-teaming efforts) or on the internal architecture of the applications being tested. \nThese concerns have been incorporated when drafting the report.\u00a0\n11\nTesting Real-World Generative AI systems\n5. Lessons learnt\u00a0\nObserving the 17 sets of pilot participants as they went about testing the applications - prioritising risks, defining test metrics, \ncoming up with suitable test datasets and evaluators, and executing the tests in constrained conditions \u2013 provided invaluable \ninsights. These have been distilled into 4 practical recommendations.\n001\nTest what matters\nYour context will determine what risks you \nshould (and shouldn\u2019t!) care about. Spend time \nupfront to design effective tests for those.\n002\nDon\u2019t expect test data to be \u2028\nfit for purpose\nNo one has the \u201cright\u201d test dataset to hand. \nHuman and AI effort is needed to generate \nrealistic, adversarial and edge case test data.\n003\nLook under the hood\nTesting just the outputs may not be enough. \nInterim touchpoints in the application pipeline \ncan help with debugging and increase \nconfidence.\n004\nUse LLMs as judges, \u2028\nbut with skill and caution\nHuman-only evals don\u2019t scale. LLMs-as- judges \nare often necessary, but need careful design and \nhuman calibration. Cheaper, faster alternatives \nexist in some situations\nThe role of the human expert is still paramount for effective testing!\n5.1 Test what matters\nRunning some tests and computing some numbers, that is the easy part. But knowing what tests to \nexecute and how to interpret the results, that was the hard part.\u2028\nMartin Saerbeck \u2013 Aiquris\nIn theory, it should be easy to determine \u201cwhat to test\u201d in a GenAI application. In practice, three factors made it challenging for \nthe pilot participants.\n01\nBroad risk surface\nExtensive, rapidly evolving and \noften daunting list of risks \nassociated with GenAI \ntechnologies in public domain.\nDifficult for lay persons, or even \ntechnical/ functional experts, to \ndiscern what might apply to a \nspecific situation\n02\nUnfamiliar territory for \nautomation efforts\nGenAI use cases often in areas \nthat have traditionally not seen \nattempts at automation\nAs a result, there are fewer \nprecedents to call upon, when \ndefining good and bad outcomes\n03\nNon-quantitative nature \u2028\nof outputs\nSpecifying \u201cwhat good looks like\u201d \nis subjective and much harder, \nwhen the output is in free text - \ne.g., Is this summary of the \nsource text good enough?\nIn comparison, most traditional \nmodels have numeric or \ncategorical outputs, and suited \nfor clear-cut assessments\n12\nTesting Real-World Generative AI systems\nThe pilot provided useful tips around the most effective ways of addressing these challenges.\n01\nTo narrow down the risk surface, two approaches have been useful\nStructured down-selection\nStart with a comprehensive GenAI risk assessment \nframework - which are often mapped to relevant \nregulation/guidelines \u2013 and use a structured process to \nrate the relative importance of each risk for the \u2028\nspecific use cases. \u2028\nExamples: Aiquris-ultra mAInds, \u2028\nPwC \u2013 Standard Chartered and NCS \u2013 Parasoft.\nBottom-up approach\nStart with the perspective of what really matters to the \ndeployer and impacted stakeholders \u2013 without referring \nto regulatory or compliance frameworks in the first \ninstance. \u2028\nExamples; AIDX \u2013 Fourtitude and Guardrails/ \u2028\nPRISM Eval - Changi Airport (incidentally, \u2028\nboth public-facing customer chatbots).\nBoth options can work, sometimes even in conjunction. The former provides more comfort when regulatory compliance is a key \nconsideration for testing. The latter is often faster and more pragmatic, but could require follow-up to justify decisions.\n02\nTo overcome the lack of precedents to determine good and bad outcomes\nEngage early and extensively with Subject Matter \nExperts (SMEs) \u2013 e.g., with a designated medical \npractitioner at Changi General Hospital\nObserve outcomes from historical or live \nproduction experience where possible \u2013 e.g., \nassessing where the end-user or human in the loop \nis ignoring/ over-ruling the automated output\nConduct simulation testing to identify potential \nfailure points in edge cases \u2013 e.g., \u2028\nat Changi Airport\nLeverage the experience of specialist testers who \nhave built targeted benchmarks and red-teaming \ntechniques for similar risks\n03\nFinally, finding quantitative metrics to assess the qualitative outputs is the area that has seen significant practitioner activity \nalready. Tap on the experience of specialist testers and major open source LLM app eval projects.\n\nMake sure that the SME is engaged to shape, review and approve the definition and specific implementation of metrics. \u2028\nFor example:\nWhen using a standard \u201cfaithfulness\u201d metric to \nassess LLM application output vs. the context \nprovided to it. However, it is important to know \nwhether the metric is measuring the extent to \nwhich the output (a) can be backed up by the \ncontext, or (b) is not contradicted by the context. \nNeedless to say, these metrics measure very \ndifferent things!\nWhen using a standard \u201csummarisation quality\u201d \nmetric, the prompt used to assess the \ncompleteness of the summary may be equally \nweighted to all the key claims in the source \ncontext. However, in specific situations, getting a \nparticular piece of information \u2013 say, the number of \npolyps in a colonoscopy report \u2013 might be a \u201cdeal-\nbreaker\u201d, invalidating the summarisation score\n13\nTesting Real-World Generative AI systems\n5.2 Don\u2019t expect test data to be fit for purpose\nTest data is very hard to get in a highly regulated industry like Financial Services.\u2028\nSina Wulfmeyer \u2013 Unique AG\n13 out of the 17 pilot participants identified \u201cfinding the right test data\u201d as a major challenge during testing. \u2028\nExpect this challenge to exist by default in almost every GenAI testing situation. Budget design and engineering effort, \u2028\nand SME engagement, to address it.\nHistorical data\nAt Changi Hospital, historical records were available \u2028\nfor individual patients. However, a degree of fresh \nannotation was needed to make that data suitable for \nautomated testing. Anonymisation efforts may also be \nneeded sometimes, depending on how the historical \nrecords were stored.\nLive production data\nNot an option during the pilot but can be a relevant \noption after an application has been live for some time. \nHowever, not all applications retain sufficiently granular \ntraces from the application\u2019s responses in the production \nenvironment. Additional steps around data \nanonymisation may also be needed.\nAdversarial red teaming\nAt Changi Airport, PRISM Eval helped create \u2028\nthousands of adversarial attempts by applying their \nBehaviour Elicitation Tool to the multi-turn chatbot. \u2028\nAt Fourtitude, AIDX used seed prompts to create \nadversarial attacks specific to the Malaysian religious, \nracial and cultural context.\nSimulation testing\nAt Changi Airport, Guardrails AI created a series of \ntarget user personas with inputs from business, \u2028\nand then used a mix of human creativity and \u2028\nLLM-based automation to generate large volumes of \nprompts that could test the chatbot\u2019s likely responses \u2028\nin edge-case scenarios.\nThese guest blogs from Guardrails AI and Advai provide practical guidance on red teaming and simulation testing respectively.\n14\nTesting Real-World Generative AI systems\nGuest blogs\nLearning from self-driving cars: Simulation Testing\nBy Safeer Mohiuddin, Co-founder, Guardrails AI\nVisit San Francisco today, and you can summon a Waymo \u2028\nself-driving car - no human driver required. Surprisingly, \u2028\nthe fundamental technology enabling these self-driving cars \nhas changed little in the past decade. What's truly advanced is \nthe painstaking process of identifying and solving the 'long \ntail' of edge cases - those rare but potentially catastrophic \nscenarios that could lead to accidents. \n\nThis journey mirrors the challenges facing GenAI application \nbuilders today, where the non-deterministic nature of large \nlanguage models creates not only safety but reliability \nconcerns.  Effective GenAI systems require both rigorous \ntesting to identify edge cases during development and \nprotective guardrails once in production\u2014mirroring the \u2028\ndual approach that ultimately brought self-driving cars \u2028\nfrom concept to reality.\n\nCatching a 0.01% failure with 99.99% confidence requires \ntesting approximately 10,000 prompts per risk category\u2014\nmaking blind brute-force testing untenable.\nBuilding on lessons from autonomous vehicles, we've identified four complementary testing approaches that together form \u2028\na comprehensive strategy.\nTechnique\nGoal\nZones Tested\nMetrics\nStatic Dataset\nPrecision\nKnown-knowns\nPass-rate threshold (\u201c\u2265 95% must pass\u201d)\nSimulation Testing\nCoverage\nKnown-unknowns, Edge cases\nFailure density (\u201c1 critical per 5K runs\u201d)\nHuman Review\nAlignment\nSubjective failures\nHuman-quality mean\nRedteaming\nResilience\nAdversarial unknowns\nTime-to-bypass\nSimulation testing stands out in this ladder by generating thousands of diverse test cases at scale\u2014uncovering hallucinations, \u2028\noff-topic responses, and policy violations that manual test creation would miss. By mastering edge case generation, we can build \nAI systems that handle the unexpected with the same reliability that finally brought self-driving cars safely onto our roads.\n15\nTesting Real-World Generative AI systems\nGuest blogs\nSynthetic Data for Adversarial Testing\nBy David Sully, Co-Founder, Advai\nWhy do we break things?\u2028\nIf you ask a toddler, it\u2019s probably just for fun. But as we grow \nup, we break things to understand them better. You can only \nget so far with theory\u2014eventually, you need to smash \nsomething.\u2028\nWhether it\u2019s particles in an accelerator or a car with crash \ndummies, breaking things reveals how the universe works or \nhow a seatbelt can save your life. AI is no different \u2013 at some \npoint, you need to try and understand what will happen when \nit experiences things it should not. And for that, you need \nAdversarial Testing. Adversarial testing involves feeding in \ndata designed to break your AI model\u2014until it breaks. The \nease with which it fails helps you understand its true \nboundaries: what it handles well, where it struggles, what it \ndetects reliably, and what it cannot be trusted with. \n\u2028\nAdversarial testing isn\u2019t just a red-teaming trick\u2014it\u2019s the way \nto truly understand AI systems. If you\u2019re not doing it, you \nprobably don\u2019t know your system as well as you think.\nYes, this is an expert-led craft. But if you're going solo, here\u2019s a crash course:\n01\nDefine Your Use Case and Critical Failure Modes\nWhere failure is unacceptable? Bias, \nhallucinations, being tricked (e.g. prompt \ninjections), fairness, safety? Prioritise what \nmatters most.\n02\nUse Data Mutation Techniques\nModify real data to stretch model limits, \u2028\nfor example:\nText\ntypos, paraphrases, entity swaps, jailbreaks, injections\nImages\nocclusion, lighting, clutter, adversarial noise\n03\nLeverage Generative Models\nPrompt LLMs or diffusion models to create hard-\nedge examples\u2014corner cases, misleading \nphrasing, traps your model might fall into.\n04\nMeasure and Benchmark\nNumbers mean more in context. Benchmark \ndifferent models or versions side-by-side to see \nwhat truly improves.\n05\nAutomate It\nYou're in AI\u2014automate your adversarial pipeline!\u00a0\n16\nTesting Real-World Generative AI systems\n5.3 Look under the hood\nA key difference between testing an LLM and a GenAI application that uses it is the possibility, and sometimes necessity, \u2028\nof testing inside the application pipeline.\u2028\nFor example, consider this grossly simplified representation of a hospital\u2019s application to summarise medical reports, \u2028\nand recommend personalised surveillance protocols based on established industry guidelines.\nThe default approach to testing would be to look at the final \noutput, and assess whether the personalised recommendation \nfor the patient, as well as the key facts extracted from the \nsource medical reports, were in line with the \u201cground truth\u201d \nset by a human SME. An LLM-based summarisation quality \nscore could be used as the comparison metric.\nSource Reports\nMedical Guidelines\nKey Facts & Recommendations\nAt Changi General Hospital, this was the starting point. As part of the pilot exercise, tester Softserve introduced \u2028\ntwo additional tests:\n01\nAdditional test\nCompare the key facts extracted by the LLM from the \nsource reports with the ground truth version of the \nkey facts\n02\nAdditional test\nCompare the recommendation implied by the key \nfacts from #1 through the deterministic \u201cdecision \ntree\u201d underpinning the standard industry guideline\nSuch an approach can provide several advantages, though it also entails greater effort and is therefore more suited to \u2028\nhigh-stakes use cases.\nRedundancy in automated evaluations: additional triangulation points for the final output\u2019s summarisation score.\nAssistance in debugging application: additional traceability can help understand root causes for poor summarisation scores in the final output\nLower dependence on LLMs as judges: Python scripts rather than LLM-based evaluators used for the incremental two tests.\nAnother example of the advantages of looking \u201cunder the hood\u201d comes from the red teaming exercise conducted by Advai on \nCheckmate\u2019s multi-step agentic flow. By knowing the hand-offs at each step of the agentic workflow, the Advai team were able to \nrefine their adversarial attacks on the application.\nWhat about Agentic AI?\n\u201cLooking under the hood\u201d becomes even more important \u2028\nin the context of real-life applications that use agentic workflows. \u2028\nThe example on the left \u2013 from outside the pilot \u2013 illustrates a \nbasic agentic workflow to conduct fraud investigations, \u2028\nand the granular testing to which it may lend itself\n17\nTesting Real-World Generative AI systems\n5.4 Use LLMs as judges, but with skill and caution\nUsing LLMs as judges is unavoidable for evaluation of GenAI applications in many instances. \u2028\nFor example, when assessing a response from a GenAI application on:\n01\nNuanced \nconsiderations such as \nconsistency with \ncompany values\n02\nAppropriateness from a \nracial or religious \nsensitivity perspective\n03\nQuality of language \ntranslation\n04\nCompleteness and \naccuracy of \nsummarisation\nIn all these examples, it is possible to use a human SME as an alternative. However, this can be costly and difficult to scale even in \npre-production testing. It becomes practically impossible in real-time production environments, unless a decision is taken to \npermanently keep a human-in-the-loop.\nOf course, using an LLM as judge carries several risks as well. Mitigating them requires:\n01\nSkilful and careful prompting when \nconstructing the evaluator\n02\nExtensive human calibration\n03\nOngoing monitoring to ensure \nthat there are no \u201csilent failures\u201d\n04\nConcerted effort to explain how they work, and what are \ntheir limitations, to the non-technical stakeholders \naccountable for the final application\n05\nNon-trivial spending on LLM credits or compute capacity\nUnsurprisingly, almost every tester in the pilot has used LLMs as judges as part of their evaluator design. The detailed case studies \ndocument the steps they have taken to improve reliability of such automated evaluators. Most of them used extensive human \ncalibration to mitigate risks, with some using statistical approaches to ensure evaluation robustness.\u00a0\u2028\nBeyond the pilot stage, it is expected that several of them may find cheaper, simpler and more transparent alternatives such as \nsmaller language models, rule-based logic or some combination to replace or complement LLM-based evaluators.\n18\nTesting Real-World Generative AI systems\nThis guest blog from PwC provides a broader introduction to the pros and cons of using LLMs as judges.\nGuest blogs\nLLM-as-a-judge: Pros and Cons\nPowerful advantages in speed, scalability, and consistency, but effectiveness \ndepends on thoughtful design, human oversight, and awareness of limitations\nBy Leigh Bates, Partner PwC UK and Global Risk AI Leader\nTesting tools built on Large Language Models (LLMs) rely on testing and evaluating many prompt-answer pairs over different risk \nmetrics, such as accuracy, lack of hallucinations, coverage, robustness as well as adherence to any specific requirements \u2028\n(e.g. that an external chatbot shouldn\u2019t make commercial commitments).\u00a0\nSuch evaluation can be done \nusing Natural Language \nProcessing (NLP) and \nstatistical techniques as well \nas human SME evaluation, \nbut both pose challenges:\nNLP and statistical approaches can act as a good baseline for assessing the \naccuracy of LLMs outputs, but they are not flexible and sometimes fail to \u2028\ncapture linguistic nuance.\nHuman SME evaluations are more reliable and can add an important layer of \ntesting for higher risk use cases. However, obtaining statistically meaningful \nresults through human assessment is nearly impossible and impractical.\nTo address this, PwC has developed AI testing toolkits based on an \u201cLLM-as-a-Judge\u201d approach. LaJ typically involve prompting \nthe judge LLM to evaluate whether a given prompt\u2013response pair from an LLM-based system meets a specific requirement. \u2028\nThe LaJ prompt can be supplemented with ground truth or examples of appropriate outputs.\u00a0\n\nUsing LaJ in assessing LLM systems has benefits, including:\n01\nBetter ability to approximate \nhuman-like judgment vs NLP \nmethods, in areas where \nevaluation is qualitative (e.g., tone \nappropriateness, helpfulness).\n02\nSpeed & cost effectiveness of \nexecuting tests at scale, relative to \nhuman review.\n03\nNot susceptible to fatigue, unlike \nhuman reviewers, which could lead \nto fairer and more standardised \nevaluations across large datasets.\n04\nThe potential to use the same LaJ beyond initial testing - \nin ongoing monitoring. This can create dynamic \nperformance metrics and associated alerts.\n05\nThe potential to extend to Agentic AI system evaluation, \nwhere multiple LLMs with different instructions interact \nwith each other without human intermediaries.\n19\nTesting Real-World Generative AI systems\nGuest blogs\nLLM-as-a-judge: Pros and Cons\nContinued from previous page\nOf course, there are \nchallenges to consider when \nusing LaJ for evaluation \ntesting:\nHeavy reliance on effectiveness of the prompt used to guide it. The more \ncomplex the assessment, the more elaborate and specific the prompt will need to \nbe; this is hard to determine without an element of trial and error.\nLack of transparency in LaJ reasoning, making it difficult to audit decisions or \nunderstand failure modes, especially when evaluating edge cases/ novel inputs.\nRisk of biased assessments if using the same underlying model or family of \nmodels in the LaJ and the LLM application being tested (however, not observed \nin the LLM system assessments we have done so far).\nThere may be the temptation to rely solely on LaJ outputs due to their convenience. However, it is important to reinforce that \nthese tools should supplement (not replace) expert human judgment, especially in high-stakes evaluations. Having human experts \ntest a smaller sample of scenarios to ensure the LaJ is working as intended, and interpret some of the LaJ outcomes, is crucial.\u00a0\n5.5 Keep your human SMEs close!\nAt every stage of the \ntechnical testing lifecycle, \nhuman SMEs have a critical \nrole to play.\nNarrowing down the risks that genuinely matter in a specific use case.\nHelping choose or to refine the metrics that can best reflect those risks.\nAnnotating test data sets, or creating \u201cseed\u201d scenarios that form the basis for \nsynthetic test data generation.\nReviewing/ refining/ validating automated evaluators.\nInterpreting test results and deciding on corrective action if any.\nThere is widespread recognition of the theory of involving SMEs from an early stage. Unfortunately, there is inadequate \nappreciation of the scale of the demands to be placed on the human experts along the way. Additionally, non-technical \u2028\nusers often lack user-friendly tools to engage throughout this process.\n20\nTesting Real-World Generative AI systems\nGuest blogs\nLLMs can\u2019t read your mind\nBy Shahul E.S, Co-Founder, ragas\nThis guest blog from Ragas provides a practical perspective on how to engage human SMEs in a specific step \u2013 that of \u2028\nannotating / calibrating automated evaluators. Many teams underestimate the criticality of human review when setting up \nautomated evaluations. If your goal is to align your AI system with human expectations, you must first define those \u2028\nexpectations clearly. \nHere\u2019s how you can go about it:\nStep 01\nDefine what to measure\nAsk: What matters in a good response for your use case? \u2028\nPick 1\u20133 dimensions per input\u2013response pair. For example, response correctness and citation accuracy \u2028\nfor a RAG system, or syntactic correctness, runtime success and code style for a coding agent.\nStep 02\nChoose metric type\nFor each dimension, choose a metric that\u2019s both easy to apply and actionable:\nBinary (pass/fail): Use when \nthere\u2019s a clear boundary \nbetween acceptable and \nunacceptable. \u2028\nExample: Did the agent \ncomplete the task? Yes or no.\nNumerical (e.g. 0-1): Use when \npartial credit makes sense and \nyou can clearly define what \nscores mean. \u2028\nExample: Citation accuracy \u2028\nof 0.5 = half the citations \u2028\nwere incorrect.\nRanking: Use when outputs are \nsubjective and comparisons are \neasier than absolute judgments. \nExample: Summary A is better \nthan Summary B.\nStep 03\nReview and evaluate the automated evaluator\nOnce you are satisfied that you have defined what you \nreally want to measure and have found a way to \nautomate the calculation of your preferred metric, the \nnext step is to assess how well the automated evaluator \nis performing, and whether it is aligned to the human \nsubject matter expert\u2019s views. \u2028\u2028\nHowever, in doing so, it is essential to collect \njustification alongside it. \u201cFail\u201d without a reason isn\u2019t \nhelpful. On the other hand, a good justification can act \nas crucial training input for your LLM-as-judge.\nFinally, do not under-estimate the power of a good user \ninterface in making human reviews/ annotations \npainless. Looking at data can be tedious, but a user-\nfriendly \u201cdata viewer\u201d can make it less so. Your data \nviewer should ideally be: (a) tailored to your use case \n(RAG, summarization, agents, etc.); (b) fast to label; and \n(c) structured enough to store data and feedback \nconsistently.\n21\nTesting Real-World Generative AI systems\n6 What\u2019s next?\nPilot participants provided their views on potential areas for future work. 4 themes emerged:\n01\nBuilding Awareness and Sharing Best Practices\nMore training and awareness \u2028\nof the risks\nOn the risks of GenAI systems\u00a0\nOn testing and how that needs to \nbecome an integral part of the \ndevelopment process\nOpportunities to share experiences \namong testing practitioners and the \norganisations deploying GenAI apps\nMacro-level: (e.g., how to sensitise \nsenior leaders on risk)\nSpecific: (e.g., the best metrics to test \ntranslation quality)\nThe need for multi-stakeholder \nengagement around testing\nNot just with developers but also \nbusiness leaders, product owners, \nSubject Matter Experts and risk/ \ncompliance teams\nEven non-technical stakeholders (have) to be part of the AI assurance ecosystem. \u2028\nThat is where the opportunity is as well.\u2028\nFion Lee-Madan,Fairly AI\n02\nStandardisation of \u201cwhat to test\u201d and \u201chow to test\u201d\nAcross the test lifecycle: Risk \nassessment, test selection, test \nexecution, test configuration, and \nresult interpretation\nShould result in inter-operable/ \nportable tests and consistency in \nresults (same system, two testers = \nsame outcome) \nIdeally, also linked to policy/ \nregulation positions where it makes \nsense (e.g., on the use of automated \nred-teaming or LLMs as a judge)\nSome participants suggested the need for standards at a more granular level \u2013 e.g.,\nIndividual test metrics like accuracy \nof summarisation or translation)\nReal-world evaluation benchmarks \nfor specific use cases \nMachine readable outputs from \nGenAI systems to support \u2028\ntesting automation\nWe need standards around the mechanisms to assess accuracy or safety, so that results from different \ntools and vendors are comparable.\u2028\nYifan Jia \u2013 AIDX\n03\nAccreditation\nAccreditation scheme for AI testing / assurance \nproviders (services and software) \nAs a way of ensuring consistency, common \nassessment standards and greater confidence \u2028\namong deployers and end-users\nFormal accreditation of vendors and their test approaches could also help in assuring consistency and \nensuring a common standard of assessment.\u2028\nMiguel Fernandes \u2013 Resaro AI\n22\nTesting Real-World Generative AI systems\n04\nSupport for Automation as a way of scaling up\nScalable test environments with stable APIs and \nbroad platform support\nDemocratised access to testing technologies (not just \nlimited to frontier labs, big technology firms or the \nlargest enterprises) \nThere\u2019s too much headache over the cost and complexity of mobilising testing and assurance \ntechnology, particularly for actors who cannot rely on deep LLM expertise or large security budgets.\u2028\nNicolas Miailhe \u2013 PRISM Eval\nIMDA and AIVF will take these \ninputs into consideration as \nthey shape their roadmap. \u2028\nA few immediate actions\u2028\nare underway.\nSharing the outcomes from the Assurance Pilot widely, engaging with AIVF \nmembers (200 organisations) and the broader community.\nConsultation on the IMDA Starter Kit, containing a set of voluntary guidelines \nthat coalesces rapidly emerging best practices and methodologies for app \ntesting . At this stage, the starter kit covers 4 risks: hallucination, undesirable \ncontent, data disclosure, and vulnerability to adversarial attack.\nIncorporation of both the pilot findings and the Starter Kit into the AIVF open \nsource GenAI testing toolkit roadmap.\nContinuation of the collaboration platform provided by the pilot in a different \nform - e.g., an assurance clinic. The first members of the next cohort are \nalready on-board.\nThe journey towards making GenAI applications reliable in real-world \nsettings has just started. IMDA and AIVF look forward to continued \ncollaboration with AI builders, deployers and testers, as well as policy \nmakers locally and internationally, on this important initiative.\n", "metadata": {"country": "Singapore", "year": "2025", "legally_binding": "no", "binding_proof": "This is a voluntary pilot initiative by IMDA and AI Verify Foundation to establish international norms for testing and assurance of AI systems.", "date": "01/01/2025", "regulator": "IMDA & AI Verify Foundation", "type": "AI assurance pilot", "status": "ongoing", "language": "English", "use_cases": "[3, 5, 6]"}}
