{"_id": "68695c4c5520a4cc7989de80", "title": "National Strategy for Data and Artificial Intelligence", "source": "https://sdaia.gov.sa/en/SDAIA/SdaiaStrategies/Pages/NationalStrategyForDataAndAI.aspx", "text": "AI ETHICS\nARTIFICIAL INTELLIGENCE ETHICS\nPRINCIPLES & GUIDELINES\nPRINCIPLES & GUIDELINES\nDEC 2022Table of Content\nPage Page\n1. Overview 4 3.4 Explainable 29\n1.1 Introduction 5 3.4.1 Principle 29\nWe will make AI systems as explainable as technically possible\n1.2 Scope 6 3.4.2 Guidelines 29\n1.3 Strategic alignment 6 3.5 Robust, Safe and Secure AI 31\n1.4 Responsibility 8 3.5.1 Principles 31\nAI systems will be technically robust\n3.5.2 Guidelines 32\n1.5 Licensing 8\n3.6 Human Centred AI 34\n1.6 Toolkit 8\n3.6.1 Principles 34\nWe will give AI systems human values and make them beneficial to society\n3.6.2 Guidelines 35\n2. Definitions 10\n3.7 Sustainable and Environmentally Friendly AI 38\n3.7.1 Principle 38\n3. Principles and Guidelines 16\nWe will promote sustainabity and environemntly friendly AI 38\n3.7.2 Guidelines 38\n3.1 Fairness 17\n3.8 Privacy Preserving AI 38\n3.1.1 Principle 17\nWe will make AI systems fair\n3.8.1 Principle 40\n3.1.2 Guidelines 20\nWe will respect people\u2019s privacy\n3.8.2 Guidelines 40\n3.2 Accountable AI 20\n3.2.1 Principle 20\nWe will make AI systems accountable\n4. Bibliography 44\n3.2.2 Guidelines 20\n3.3 Transparent AI 26\n3.3.1 Principle 26\nWe will make AI systems transparent\n3.3.2 Guidelines 26\n2 301 1.1 Introduction\nAI\u2019s rapid advancement and innovation potential across a range of fields is incredibly exciting.\nYet a thorough and open discussion around AI ethics, and the principles organizations using this\nOverview\ntechnology must consider, is needed.\nThis document - AI Ethics: Principles and Guidelines \u2013 intends to meet this need to balance these\ntwo central considerations of AI. It is designed to offer detailed guidance to help AI actors adhere\nto eight principles of AI ethics.\nWe will make AI systems that are:\nexplainable transparent accountable fair\nprivacy preserving sustainable and human centered robust, safe and\nenvironmentally secure\nfriendly\nThe guidelines are non-binding, and are drafted as a collaborative multi-\nstakeholder effort, with full awareness of organizations\u2019 needs to innovate\nand protect their intellectual property. This is a collaborative process in\nwhich all stakeholders are invited to be part of an ongoing dialogue. We\nwould like to see the AI Ethics Guidelines evolve into a universal, practical\nand applicable framework informing ethical requirements for AI design and\nuse. The eventual goal is to reach widespread agreement and adoption of\ncommonly agreed policies to inform the ethical use of AI nationally and\naround the world.\n4 51.2 Scope Additionally, the AI Office is currently developing additional policies to be taken into consideration\nalong with this document:\nThis document gives non-mandatory guidelines for achieving the ethical design\nand deployment of AI systems in both the public and private sectors. AI already\nAI Seal AI Procurement Guidelines\nsurrounds us, but some applications are more visible and sensitive than others.\nThe UAE AI Seal brand (UAI) will be used The AI Procurement Guidelines will be\nThis document is applicable only to those AI systems which make or inform\nto attract talent and business from used to guide Federal Government entities\n\u2018significant decisions\u2019 \u2013 that is, those decisions which have the potential for\nacross the globe to come to the UAE to in the procurement of AI systems. They\nsignificant impact either on individuals or on society as a whole. They also apply to\ntest and develop AI. This includes a UAI will list the general principles, proposed\n\u2018critical decisions\u2019, which are a subset of significant decisions and are of especially\nmark recognizing high quality, ethical AI mechanisms for procurement and review\ncritical nature.\ncompanies. It would reward safe, efficient, of AI products between the federal\nverified AI technology with a \u2018UAI Seal government and vendors. The policy\nThis document guides the further developments of sector specific requirements\nof Approval\u2019. The UAE AI Seal brand is considers R&D for specific challenges\nand principles, hence every entity within its sector can further amend, use, or\ncurrently under development by the AI that do not currently have an AI solution\ncomplement its current frameworks to properly suit its context and the need of\nOffice. product that is market-ready. Based on\naffected stakeholders.\nglobal standards for AI Procurement, the\npolicy aims to incentivize AI adoption\nIt is a living document and will undergo further future reviews and enhancements.\nwithin government entities and promote\nAI R&D within the private sector for\nspecified government challenges. The\nprocurement policy is currently under\ndevelopment by the AI Office.\n1.3 Strategic alignment\nThis document comes as a fulfillment of the goals and aspirations set in the UAE National AI\nStrategy and the international Sustainable Development Goals (SDGs). The UAE has long been\nat the forefront of sustainable development. These shared global milestones have created global\nmovement and adoption in several disciplines and sectors. As an early adopter, the UAE has\nestablished a national committee on SDGs with a long-term realization of the UAE SDG agenda\n2030. These goals are further engrained and reflected in the AI Ethics Principles under the topics of\nfairness, inclusiveness, equality, human benefit, and rights.\n6 71.4 Responsibility\nThe AI Office will not be responsible for any misuse of the AI Ethics Principles and Guidelines. The\nuser bears all the consequences of their use.\n1.5 Licensing\nThis document is published under the terms of a Creative Commons Attribution 4.0 International\nLicense in order to facilitate its re-use by other governments and private sector organizations.\nIn summary this means you are free to share and adapt the material, including for commercial\npurposes, provided that you give appropriate credit to the UAE AI and Blockchain Council as its\nowner and do not suggest the Minister of State for Artificial Intelligence, Digital Economy and\nRemote Work Applications Office endorses your use.\n1.6 Toolkit\nThe document is part of a toolkit of self-governing nature that boosts awareness and enables\ngovernment and private institutions to pursue innovative use cases while maintaining human\nvalues and principles.\n8 92.1 AI developer organization\n02\nAn organization which does any of the following:\na determines the purpose of an AI system;\nDefinitions\nb designs an AI system;\nc builds an AI system, or:\nd performs technical maintenance or tuning on an AI system\nNote:\nThe definition applies regardless of whether the organization is the ultimate user of the system, or\nwhether they sell it on or give it away.\nExample: A company develops an artificially intelligent facial recognition system and\nsells it to a country\u2019s border force, who use it to identify suspicious personnel. The company\nis an AI developer organization and the border force is an AI operator organization.\n2.2 AI operator organization\nAn organization which does any of the following:\na uses AI systems in operations, backroom processes or decision-making;\nb uses an AI system to provide a service to an AI subject;\nc is a business owner of an AI system;\nd procures and treats data for use in an AI system; or\ne evaluates the use case for an AI system and decides whether to proceed.\nNotes:\n1. This definition applies regardless of whether the AI system was developed in-house or procured.\n2. It is possible for organizations to be both an AI developer organization and an AI operator organization.\n10 112.6 Critical decision\n2.3 Artificial Intelligence (also AI)\nThe capability of a functional unit to perform functions that are generally associated with human An individually significant decision which is deemed to either have a very large impact on an\nintelligence such as reasoning, learning and self-improvement1. individual or to have especially high stakes. These can be especially sensitive, have the potential\nto cause high loss or damage, be societally significant, or set an important precedent.\n2.4 Artificially Intelligent System (also AI system)\nNotes:\nThe types of decisions referred to here are the same as those in the definition of significant-at-\nA product, service, process or decision-making methodology whose operation or outcome is\nscale decisions, except in this case the effects are felt as a result of an individual decision rather\nmaterially influenced by artificially intelligent functional units.\nthan an aggregate of many decisions..\nNotes:\n1. It is not necessary for a system\u2019s outcome to be solely determined by artificially intelligent\nfunctional units in order for the system to be defined as an artificially intelligent system; and Example: A court determines whether a defendant is guilty of a criminal charge, with the\n2. A particular feature of AI systems is that they learn behavior and rules not explicitly punishment for guilt being a life sentence. This is a critical decision because it has a very\nprogrammed in. large impact on the life of the defendant and also sets precedent for similar cases in the\nfuture.\nExample 1: A small claims court uses Example 2: A government entity uses\n2.7 Functional Unit\nan artificially intelligent software package a chatbot which allows customers to ask\nto collect evidence pertaining to a case, routine questions, book appointments\nEntity of hardware or software, or both, capable of accomplishing a specified purpose3 .\ncompare it to similar cases in the past, and conduct minor financial transactions.\nand present a recommended decision The chatbot responds to customer\nto a judge. The judge determines the queries with pre-written responses and is\n2.8 Individually significant decision\nfinal outcome. This decision-making based on pre-programmed decision rules.\nA decision which has the potential for significant impact on at least one individual\u2019s\nmethodology is materially influenced Therefore, the chatbot is not an AI system.\ncircumstances, behavior or choices, or has legal or similarly significant effects on him or her.\nby an artificially intelligent functional If, however, the chatbot autonomously\nunit, and is therefore classified as an AI adjusted its treatment of customers\nsystem. based on the outcome of past cases, it\nwould be an AI system. Example: A company decides to make an employee redundant. This is an individually\nsignificant decision because of its potential impact on the employee\u2019s financial situation.\n2.5 Bias (of a system)\nInclination or prejudice for or against one person or group, especially in a way considered to be\nunfair2.\n2 Oxford Dictionary 2018, Oxford University Press, viewed online 4th October 2018, <https://en.oxforddictionaries.com>\n1 Consistent with ISO/IEC 2382:2015 3 From ISO/IEC 2382:2015\n12 132.9 Non-operational bias (of a system) 2.11 Significant decision\nBias that is either: A decision which is either individually significant or is part of a set of significant-at-scale\ndecisions.\na not a design feature; or\nb not important in achieving the stated purpose of the system. 2.12 Subject of an artificially intelligent system (also AI subject)\nA natural person who is any of the following:\n2.10 Set of significant-at-scale decisions\na an end-user of an AI system\nA set of decisions made by the same system or organization which, when taken in aggregate,\nb directly affected by the operation of or outcomes of an AI system, or:\nhave significant impact on society as a whole or groups within it.\nc a recipient of a service or recommendation provided by an AI system\nNotes:\n1. The decisions need not be individually significant in order to qualify, in aggregate, as a set of\nsignificant-at-scale decisions; and\n2. Examples of areas which have a large impact on society and which include but are not limited\nto: the large-scale allocation of resources or opportunities amongst groups; the structure of\ngovernment; the division of power between large entities or groups; the law, and its interpretation\nand enforcement; conflict and war; international relations, etc.\nExample: An AI system is used by a website to determine which content to show users.\nThis decision is not individually significant, since a user is not greatly affected by whether\na particular piece of media is shown to them. However, if the website is popular then the AI\nsystem may be making a set of significant-at-scale decisions, because any biases in the\nsystem will affect a large number of users.\n14 153.1 Fairness (Principle 1)\n03\n3.1.1 Principle\nPRINCIPLES AND We will make AI systems fair\nData ingested should, where possible, be accurate and representative of the affected\npopulation.\nGUIDELINES\nAlgorithms should avoid non-operational bias.\nSteps should be taken to mitigate and disclose the biases inherent in datasets.\nSignificant decisions should be provably fair.\nAll personnel involved in the development, deployment and use of AI systems have a role and\nresponsibility to operationalize AI fairness and should be educated accordingly.\n3.1.2 Guidelines\n3.1.2.1\nBenefits of AI systems should be available and accessible to all. AI systems should not discriminate\nagainst people or groups in a way that could have an adverse impact. Especially significant and\ncritical decisions made or assisted by AI should be provably fair.\na Considering that fairness can have many different definitions, an organization should\ndocument its own definition of fairness for the context that the AI system is going to be\nimplemented in. Organizations should document what the implemented fairness objective\nstands for and why this choice was considered the most suitable for the given scenario.\nb It is recommended to identify and document demographic groups that may be adversely\nimpacted and mitigate the risk where possible.\nc AI developers and AI operators could consider formal procedures such as Discrimination\nImpact Assessments as a means of ensuring fairness. This assessment should be\ndocumented as well as the mitigation measures that were implemented by the organization.\nThe impact assessment should be conducted pre-release and regularly after as an ongoing\nevaluation, results are advised to be documented.\n16 173.1.2.2 3.1.2.3\nConsideration should be given to whether the data ingested is accurate and representative Consideration should be given to whether decision-making processes introduce bias\nof the affected population\na When subjecting different groups to different decision-making processes, AI developers\nd Fairness has many different definitions in different cultures and for different contexts. should consider whether this will lead to non-operational bias.\nEncouraging a diverse and inclusive AI ecosystem is thus all the more crucial to ensure\nthat one definition of fairness does not contradict another, and that the process of defining b When evaluating the fairness of an AI system, AI developers and AI operators should\nfairness itself is fair, with under-represented groups represented in the discussion. consider whether AI subjects in the same circumstances receive equal treatment.\ne AI developers and AI operators should undertake reasonable data exploration and/or testing\nto identify potentially prejudicial decision-making tendencies in AI systems arising from\nExample: An organization uses an AI tool to automate the pre-screening of candidates\ndata inaccuracy and biases in the data.\nfor a job opening. It is trained on data from the company\u2019s existing employees, the majority\nof whom are from the same ethnic background. Therefore, the system learns to use name\nf AI developers and operators should evaluate all datasets to assess inclusiveness of and nationality as discriminating factors in filtering job applicants. This could have been\nidentified demographic groups and collect data to close any gaps.\nidentified through testing and rectified by, for example, balancing the training data or only\nusing relevant data fields for training.\ng AI developers and AI operators should refrain from training AI systems on data that is not\nlikely to be representative of the affected AI subjects, or is not likely to be accurate, whether\nthat be due to age, omission, method of collection, or other factors.\n3.1.2.4\nSignificant decisions informed by the use of AI should be fair\nExample: Following a natural disaster, a government relief agency uses an AI system to a AI developers and AI operators could consider formal procedures such as Discrimination\ndetect communities in greatest need by analyzing social media data from a range of websites. Impact Assessments as a means of ensuring fairness.\nHowever, those communities where smartphone penetration is lower having less presence on\n3.1.2.4\nsocial media, and so are at risk of receiving less attention. Therefore, the charity complements\nAI operators should consider whether their AI systems are accessible and usable in a fair\ntheir AI tool with traditional techniques to identify needy populations elsewhere.\nmanner across user groups\n3.1.2.5\nh AI developers should consider whether their AI systems can be expected to perform well Consideration should be given to the effect of diversity on the development and deployment\nwhen exposed to previously unseen data, especially when evaluating people who are not processes\nwell-represented in the training data.\nb Organizations should seek to include people from diverse demographic backgrounds across\nthe full lifecyle, to include design, development and deployment processes. Organizations\nshould seek to engage diverse internal and external groups also.\nc AI developers should consider whether the assumptions they make about AI subjects could\nbe wrong or are likely to lead to non-operational bias; if so, they should consider consulting\nthe AI subjects in a representative manner during the design, development and deployment\nto confirm these assumptions.\n18 193.2 Accountable AI (Principle 2)\n3.2.2.2\nPositive efforts should be made to identify and mitigate any significant risks inherent in the AI\nsystems designed\n3.2.1 Principle\na AI operators should only use AI systems that are backed by evidence-based academic or\nindustrial research, and AI developers should base their development on such research.\nWe will make AI systems accountable\nAccountability for the outcomes of an AI system lies not with the system itself but is\napportioned between those who design, develop and deploy it; Example: A foreign country has a government service which identifies parents who owe\nmoney in child maintenance. The data matching process is often incorrect due to misspelled\nDevelopers should make efforts to mitigate the risks inherent in the systems they design; names or missing data which results in some individuals being incorrectly targeted\nautomatically by the system with the result being a large bill, poor credit ratings and even\nAI systems should have built-in appeals procedures whereby users can challenge significant\nfreezing wages. The recourse for individuals who are incorrectly targeted is time-consuming\ndecisions;\nand not straightforward . If the potential impact of incorrect decisions had been assessed,\nmitigation measures such a user-friendly review procedure could have been set up.\nAI systems should be developed by diverse teams which include experts in the area in which\nthe system will be deployed; and\nAI systems should be subject to external audit and decision quality assurance. b AI operators should identify the likely impact of incorrect automated decisions on AI\nsubjects and, in the case where incorrect decisions are likely to cause significant cost or\ninconvenience, consider mitigating measures.\n3.2.2 Guidelines\nc AI operators should consider internal risk assessments or ethics frameworks as a means to\n3.2.2.1 facilitate the identification of risks and mitigating measures.\nAccountability for the outcomes of an AI system should not lie with the system itself\nd In designing AI systems to inform significant decisions, AI developers should consider\na Accountability for loss or damages resulting from the application of AI systems should not measures to maintain data accuracy over time, including:\nbe attributed to the system itself.\n\u2022 the completeness of the data.\nb AI operators and AI developers should consider designating individuals to be responsible for \u2022 timely update of the data, and.\ninvestigating and rectifying the cause of loss or damage arising from the deployment of AI\n\u2022 whether the context in which the data was collected affects its suitability for the\nsystems.\nintended use case.\nExample: A border camera scanning for predictors of risk may misinterpret a \u201ctic\u201d of an\nindividual with Tourette syndrome as suspicious. These can manifest in a diverse fashion, and\nshould not cause this person to undergo secondary inspection every time they pass through\nthe border . If the data is updated after the first case is encountered then it would avoid\ncausing inconvenience on subsequent visits.\n5 Government of Canada. (2018). Responsible AI in the Government of Canada. Digital Disruption White Paper Series. Version 2.0, p.26. Retrieved from:\nhttps://docs.google.com/document/d/1Sn-qBZUXEUG4dVk909eSg5qvfbpNlRhzIefWPtBwbxY/edit\n4 Cabinet Office (UK), Data Science Ethical Framework, Version 1.0, licensed under the Open Government License v3.0, p. 13 6 Stoica, I. et. al.,2017, A Berkeley View of Systems Challenges for AI, p. 2, https://arxiv.org/pdf/1712.05855.pdf\n20 21e AI developers and AI operators should tune AI models periodically to cater for changes to e Be mindful that there might be fundamental tensions between different principles and\ndata and/or models over time. requirements. Continuously identify, evaluate, document and communicate these trade-\noffs and their solutions.\nf AI operators should consider whether AI systems trained in a static environment will display\nmodel instability when deployed in dynamic environments. Adopt a Trustworthy AI assessment list when developing, deploying or using AI systems,\nf\nand adapt it to the specific use case in which the system is being applied.\nExample: AI systems will need to be able to adapt to the changes in the environment that g Keep in mind that such an assessment list will never be exhaustive. Ensuring Trustworthy\nthey are deployed in. For example, a self-driving car would need to quickly adapt to unexpected AI is not about ticking boxes, but about continuously identifying and implementing\nand dangerous road by learning in real time from other cars that have successfully dealt with requirements, evaluating solutions, ensuring improved outcomes throughout the AI\nthese conditions. In addition, such mission-critical applications must handle noisy inputs system\u2019s lifecycle, and involving stakeholders in this.\nand defend against malicious actors .\n3.2.2.4\nAI subjects should be able to challenge significant automated decisions concerning them and,\ng AI developers should collaborate with AI Operators to train models using historical data where appropriate, be able to opt out of such decisions\nfrom the Operator.\na AI operators using AI systems to inform significant decisions should provide procedures by\nwhich affected AI subjects can challenge a specific decision concerning them.\nh AI Operators should consider working with their vendors (AI developers) to continually\nmonitor performance.\nb AI operators should consider such procedures even for non-significant decisions.\ni AI Operators should subject AI systems informing significant decisions to quality checks\nat least as stringent as those that would be required of a human being taking the same c AI operators should make affected AI subjects aware of these procedures and should\ndesign them to be convenient and user-friendly.\ndecision.\nd AI operators should consider employing human case evaluators to review any such\n3.2.2.3\nchallenges and, when appropriate, overturn the challenged decision.\nAI systems informing critical decisions should be subject to appropriate external audit\na When AI systems are used for critical decisions, external auditing of the AI systems in\nquestion should be used as a means to ensure meaningful standards of transparency and Example: A bank allows customers to apply for a loan online by entering their data. The\naccountability are upheld. bank uses an AI system to automatically determine whether to give the loan and what the\ninterest rate should be. They provide users with an option to contest the decision and have\nb In the case that critical decisions are of civic interest, public release of the results of the it reviewed by a human . They also require that customers justify their challenge by filling in\naudit should be considered as a means of ensuring public processes remain accountable a form, which assists the case reviewer and deters customers from challenging a decision\nto those affected by them. without good reason.7\nIn the case that critical decision are life and death decisions, these decisions should be\nc\nsupported by further validation and verification via a human operator.\nd Facilitate traceability and auditability of AI systems, particularly in critical contexts or\nsituations.\n7 Adapted from EU Commission, Can I be subject to automated individual decision-making, including profiling? Retrieved from: https://ec.europa.eu/\ninfo/law/law-topic/data-protection/reform/rights-citizens/my-rights/can-i-be-subject-automated-individual-decision-making-including-profiling_\nen#example\n22 23e AI operators should consider instituting an opt-out mechanism for significant automated 3.2.2.7\ndecisions. AI operators should understand the AI systems they use sufficiently to assess their suitability\nfor the use case and to ensure accountability and transparency\nf AI operators could consider \u201ccrowd challenge\u201d mechanisms whereby a critical number of\ncomplaints triggers an investigation into the fairness and/or accuracy of a decision-making\na In the case of critical decisions, AI operators should avoid using AI systems that cannot be\nprocess as a whole.\nsubjected to meaningful standards of accountability and transparency.\n3.2.2.5 b AI developers should consider notifying customers and AI operators of the use cases for\nAI systems informing significant decisions should not attempt to make value judgements on which the system has been designed, and those for which it is not suitable.\npeople\u2019s behalf without prior consent\na When informing an AI subject about significant choices they will make, AI systems should\nnot unreasonably restrict the available options or otherwise attempt to influence their value\njudgements without the explicit consent of the AI subject in question.\n3.2.2.6\nAI systems informing significant decisions should be developed by diverse teams with\nappropriate backgrounds\na AI developers who develop AI systems which may be used to assist in making critical\ndecisions should involve in the process experts with a background in social science, policy,\nor another subject which prepares them to evaluate the broader societal impact of their\nwork.\nb Development of AI systems informing significant decisions should include consultation\nwith experts in the field in which the system will be deployed.\nExample: An app that uses AI to assess medical symptoms and has a large user base had\nto face regulatory scrutiny because of number of complaints from doctors. They warned\nthat the application can miss signs of serious illness. A number of different shortcomings\nwere identified by doctors, some of which the company could address and resolve.8\n8 Financial Times. 2018. High-profile health app under scrutiny after doctor\u2019s complaints. Retrieved from: https://www.ft.com/content/19dc6b7e-8529-\n11e896-dd-fa565ec55929\n24 253.3 Transparent AI (Principle 3)\nc To facilitate the above, AI developers and AI operators should consider documenting the\nfollowing information during the design, development and deployment phases, and retaining\n3.3.1 Principle this documentation for a length of time appropriate to the decision type or industry:\n\u2022 the provenance of the training data, the methods of collection and treatment, how the data was\nWe will make AI systems transparent\nmoved, and measures taken to maintain its accuracy over time;\n\u2022 the model design and algorithms employed; and\nDevelopers should build systems whose failures can be traced and diagnosed. \u2022 changes to the codebase, and authorship of those changes.\nPeople should be told when significant decisions about them are being made by AI. d Where possible given the model design, AI developers should consider building in a means\nby which the \u201cdecision journey\u201d of a specific outcome (i.e. the component decisions leading\nWithin the limits of privacy and the preservation of intellectual property, those who deploy to it) can be logged.\nAI systems should be transparent about the data and algorithms they use.\nResponsible disclosures should be provided in a timely manner, and provide reasonable\nExample: A technology company has a product which is designed to assist in medical\njustifications for AI systems outcomes. This includes information that helps people\ndiagnosis. It documents each stage of its reasoning and relates it back to the input data.9\nunderstand outcomes, like key factors used in decision making.\n3.3.2 Guidelines\n3.3.2.1\nTraceability should be considered for significant decisions, especially those that have the\npotential to result in loss, harm or damage.\na For AI systems which inform significant decisions, especially those with the potential to\ncause loss, harm or damage, AI developers should consider building in traceability (i.e. the\nability to trace the key factors leading to any specific decision).\nb Organizations should ensure that harms caused through AI systems are investigated and\nredressed, by enacting strong enforcement mechanisms and remedial actions, to make\ncertain that human rights and the rule of law are respected in the digital world and in the\nphysical world.\n9 See IBM WatsonPaths\n26 273.3.2.2 3.4 Explainable AI (Principle 4)\nPeople should be informed of the extent of their interaction with AI systems\n3.4.1 Principle\na AI operators should inform AI subjects when a significant decision affecting them has been\nmade by an AI system.\nWe will make AI systems as explainable as\ntechnically possible\nExample: A small claims court adjudicates minor civil matters such as debt collection and\nevictions. They introduce an AI system to suggest the outcome of a ruling. At the time of the\nDecisions and methodologies of AI systems which have a significant effect on individuals\nruling the plaintiff and defendant are notified that the decision was assisted by an AI system.\nshould be explainable to them, to the extent permitted by available technology.\nThe court also provides an explanation for the decision.\nIt should be possible to ascertain the key factors leading to any specific decision that\ncould have a significant effect on an individual.\nb If an AI system can convincingly impersonate a human being, it should do so only after\nIn the above situation we will provide channels through which people can request such\nnotifying the AI subject that it is an AI system.\nexplanations.\nExample: A technology company produces a conversational AI agent which can make 3.4.2 Guidelines\nsome phone calls on behalf of its users. Those who receive the calls may believe that they\n3.4.2.1\nare speaking to a human. Therefore, the company programs the agent to identify itself at\nAI operators could consider providing affected AI subjects with a high-level explanation of\nthe start of every conversation.\nhow their AI system works\na AI operators could consider informing the affected AI subjects in understandable, non-\ntechnical language of:\n\u2022 the data that is ingested by the system;\n\u2022 the types of algorithms employed;\n\u2022 the categories into which people can be placed; and\n\u2022 the most important features driving the outcomes of decisions\n\u2022 A comprehensive list of feature engineering and models that was considered during\nthe model building phase.\nExample: A person turned down for a credit card might be told that the algorithm took\ntheir credit history, age, and postcode into account, but not learn why their application was\nrejected10 .\n28 29b For non-sensitive public sector use cases designed for the common good, AI operators\ncould consider making source code, together with an explanation of the workings of the AI Example: The UK\u2019s NHS developed a tool called Predict, which allows women with breast\nsystem, available either publicly or upon request (this should be done only if there is low risk cancer to compare their case to other women who have had the same condition in the past,\nof people \u2018gaming the system\u2019). and visualize the expected survival rate under various treatment options. The website has an\nexplanation page which shows the weights behind various factors and contains a description\nc The AI operator should maintain necessary documentation that provides elaboration and of the underlying mathematics12 .\nclarification on how the algorithm works, for example documentation of processes, decision\nmaking flow charts of the system, and how the appeal process is embedded.\nd An individual should have the ability to contest and seek effective redress against decisions c In the case that such explanations are available, they should be easily and quickly accessible,\nmade by AI systems. These must be addressed by the group or team supporting these free of charge and user-friendly.\nmodels.\ne Develop appropriate impact indices for the evaluation of AI system technological 3.5 Robust, Safe and Secure AI (Principle 5)\ninterventions from multiple perspectives.\n3.5.1 Principles\n3.4.2.2\nAI operators should consider providing affected AI subjects with a means to request\nexplanations for specific significant decisions, to the extent possible given the state of present AI systems will be technically robust\nresearch and the choice of model\nAI systems should be technically robust with a preventative approach to risks which\na AI operators should consider providing a means by which people affected by a significant\noperates in a manner such that they reliably behave.\ndecision informed by AI can access the reasoning behind that decision.\nAI Developers should ensure AI systems will not cause any unintentional harm and\nadverse impacts.\nExample: The US Consumer Financial Protection Bureau requires that creditors in the US\nwho reject credit applications must explain to the applicant the principal reason(s) why their\nAI systems should be resilient to attacks and security such as data poisoning and\napplication was rejected (e.g. \u201clength of residence\u201d or \u201cage of automobile\u201d)11 . In particular,\nmodel leakage.\n\u201cstatements that the adverse action was based on the creditor`s internal standards or\npolicies or that the applicant, joint applicant, or similar party failed to achieve a qualifying AI systems should have safeguards that enable a fallback plan in case of problems.\nscore on the creditor`s credit scoring system are insufficient\u201d.\nAI system results should be reproducible.\nb Where such explainability is not possible given available technology, AI operators should\nconsider compromises such as counterfactual reasoning, or listing the most heavily\nweighted factors contributing to the decision.\n11 Consumer Financial Protection Bureau, 12 CFR Part 1002 - Equal Credit Opportunity Act (Regulation B), \u00a7 1002.9 Notifications, Retrieved from https://www.\n10 The Guardian. AI watchdog needed to regulate automated decision-making, say experts. Retrieved from: https://www.theguardian.com/technology/2017/ consumerfinance.gov/policy-compliance/rulemaking/regulations/1002/\njan/27/ai-artificial-intelligence-watchdog-needed-to-prevent-discriminatory-automated-decisions 12 Predict website, accessible at http://www.predict.nhs.uk/predict_v2.1/legal/algorithm\n30 313.5.2 Guidelines\n3.5.2.1 3.5.2.3\nAI Operators should continue conducting vulnerability assessments, verification of AI system AI Operators should assure end users of the system\u2019s reliability through documentation, and\ndifferent behaviors in unexpected situations and for any dual-use case, to include: operationalizing processes for testing and verification of desired outcomes , specifically:\na Putting measures to ensure integrity and resilience of IA system against potential attacks. a AI system should be designed with an approach to continue monitoring if the system meets\ngoals, purposes and intended application.\nb AI Operators should define an approach to ensure results are reproducible through clear\nExample: IBM researchers presented a novel approach to exploiting AI for malicious\nprocess and documentation.\npurposes. Their system, DeepLocker embeds AI capabilities within the malware itself in order\nto improve its evasion techniques. It uses an Artificial Intelligence model to identify its target\nc AI Operators should publish documentation to assure system robustness to end users.\nusing indicators like facial recognition, geolocation and voice recognition.\n3.5.2.4\nAI systems will be safe, secure and controllable by humans\nb Verifying how AI systems behave in unexpected situations and environments.\na Safety and security of people, be they operators, end-users or other parties, will be of\nc Taking preventative measures against any possible system dual-case use. paramount concern in the design of any AI system.\nb AI systems should be verifiably secure and controllable throughout their operational\n3.5.2.2\nlifetime, to the extent permitted by technology.\nAI operators should define a suitable fall back plan and test it to maintain readiness in\nunexpected situations and in high safety risks level, to include:\nc The continued security and privacy of users should be considered when decommissioning\na Developing a plan to measure and assess potential safety risks to you or any third party AI systems.\nfrom technology use accidental or malicious misuse.\nd AI systems that may directly impact people\u2019s lives in a significant way should receive\nb Define thresholds for system acceptable results and governance procure to fall back on commensurate care in their designs.\nalternative defined and tested plans.\ne Such systems should be able to be overridden or their decisions reversed by designated\nc Considering an insurance policy to mitigate risks arising from potential damages. individuals.\nExample: Adopting a risk-based approach to procurement and clearly communicating it to\nvendors can help address such issues by giving the procuring organization advance notice\nof the specific oversight capabilities it will need in future stages of the system lifecycle,\npreventing vendors from presenting intellectual property arguments against required testing,\nmonitoring and auditing of their AI systems going forward and rewarding vendors\u2014of all\nsizes\u2014that are more advanced and responsive in their responsible AI efforts.\n13 IBM. https://documents.trendmicro.com/assets/white_papers/wp-malicious-uses-and-abuses-of-artificial-intelligence.pdf\n32 333.6.2 Guidelines\n3.5.2.5\nAi systems should not be able to autonomously hurt, destroy or deceive humans\n3.6.2.1\na AI systems should be built to serve and inform, and not to deceive and manipulate.\nAI should be developed to align with human values, contribute to human flourishing and\nbenefit society.\nb Nations should collaborate to avoid an arms race in lethal autonomous weapons, and\nsuch weapons should be tightly controlled. a Society should be consulted in a representative fashion to inform the development of AI.\nc Active cooperation should be pursued to avoid corner-cutting on safety standards. b Stakeholders throughout society should be involved in the development of AI and its\ngovernance.\nSystems designed to inform significant decisions should do so impartially.\nd\nc Stakeholders should proactively engage in responsible stewardship of trustworthy AI in\npursuit of beneficial outcomes for people and the planet.\n3.6 Human Centered AI (Principle 6)\nd Organizations should prioritize having all their stakeholders learn about wellbeing metrics\nas a potential determinant of how they create, deploy, market and monitor their AI\n3.6.1 Principles technologies.\nWe will give AI systems human values and\nExample: IEEE P7010 recommended practice14 establishes wellbeing metrics relating to\nmake them beneficial to society human factors directly affected by intelligent and autonomous systems and establishes a\nbaseline for the types of objective and subjective data these systems should analyze and\nGovernment will support the research of the beneficial use of AI. include (in their programming and functioning) to proactively increase human wellbeing.\nStakeholders throughout society should be involved in the development of AI and its\ngovernance.\n3.6.2.2\nSystematic analyses that examine the ethics of designing affective systems to nudge human\nDesign AI systems to adopt, learn and follow the norms and values of the community\nbeings prior to deployment are needed.\nthey serve.\nSystematic analyses that examine the ethics of designing affective systems to nudge a Nudging in AI systems should have an opt-in system policy with explicit consent.\nhuman beings prior to deployment are needed.\nb We recommend that when appropriate, an affective system that nudges human beings\nDecisions related to lethal force, life and death should not be delegated to AI systems. should have the ability to accurately distinguish between users, including detecting\nRules and standards should be adopted to ensure effective human control over those characteristics such as whether the user is an adult or a child. Additional protections must\ndecisions. be put in place for vulnerable populations, such as children, when informed consent cannot\nbe obtained, or when it may not be a sufficient safeguard.\nc AI systems with nudging strategies must be carefully evaluated, monitored, and controlled.\n14 IEEE. https://sagroups.ieee.org/7010/\n34 353.6.2.3 f In situations where it is needed, human rights impact assessments and human rights due\nHumanity should retain the power to govern itself and make the final decision, with AI in an diligence, human determination codes of ethical conduct, or quality labels and certifications\nassisting role. intended to promote human centered values and fairness should be considered.\ng Mechanisms should be put into place to receive external feedback regarding AI systems\na Decisions related to lethal force, life and death should not be delegated to AI systems. Rules\nthat potentially infringe on fundamental rights.\nand standards should be adopted to ensure effective human control over those decisions.\nb Responsible parties (e.g., parents, nurse practitioners, social workers, and governments)\nshould be trained to detect the influence due to AI and ineffective mitigation techniques.\nIn the most extreme cases it should always be possible to shut down harmful AI system.\nc Those actions undertaken by an affective system that are most likely to generate an\nemotional response should be designed to be easily changed.\nd Users should be able to make informed autonomous decisions regarding AI systems. They\nshould be given appropriate knowledge and tools to comprehend and interact with AI\nsystems to satisfactory degree and, where possible to reasonably self-assess or challenge\nthe system.\n3.6.2.4\nAI systems should be designed in a way that respects the rule of law, human rights, society\nvalues, and should include appropriate safeguards to ensure a fair and just society.\na AI Developers should design AI systems to adopt, learn and follow the norms and values of\nthe community they serve.\nb Organizations could identify the norms of the specific community in which AI systems are\nto be deployed in. In particular, pay attention to the norms relevant to the kinds of tasks that\nthe AI systems are designed to perform. These could be documented as well as how these\nnorms are addressed by the AI system.\nc To respond to the dynamic change of norms in society the AI system could be able to adjust\nits existing norms and learn new ones, while being transparent about these changes.\nd Designers should consider forms and metrics for assessing an AI system\u2019s norm conformity\nover the lifetime of the system (e.g. human-machine agreement on moral decisions,\nverifiability of AI decisions, justified trust).\ne The norm identification process must document the similarities and differences between\nthe norms that humans apply to other humans and the norms they apply to AI systems.\nNorm implementations should be evaluated specifically against the norms that the\ncommunity expects the AI system to follow.\n36 373.7 Sustainable and Environmentally Friendly AI (Principle 7)\nd While there is no standardized means of reporting on the environmental impact of AI\nsystems, where possible, model development and commissioning should make carbon\n3.7.1 Principle impacts a core consideration alongside functional and business requirements.\ne Smaller models should be considered \u2013 shrinking down the model size and using fewer\nWe will promote sustainable and environmentally compute cycles (to balance financial and performance costs with the end performance of\nthe model).\nfriendly AI\nf Carbon awareness should be considered \u2013 adjusting the operational parameters of the AI\nAI systems should be used to benefit all human beings, including future generations. The\nsystem to dynamically select the best time and location for energy use from the grid can\nenvironment is fundamental to this.\nreduce its carbon footprint.\nThroughout the AI lifecycle, implementations should only be carried out on the basis of a full\nunderstanding and acknowledgement of implications for sustainability and environment.\nExample: In agriculture, AI can transform Example: Google`s DeepMind division\nAn AI system\u2019s development, deployment and use, should be assessed via critical examination production by better monitoring and has developed AI that teaches itself to\nof resource usage and energy consumption.\nmanaging environmental conditions minimize the use of energy to cool Google`s\nand crop yields. AI can help reduce both data centers. As a result, Google reduced\nMechanisms to measure environmental impact due to type of energy use and processing\nfertilizer and water, all while improving its data center energy requirements by\npower provided by data centers should be established.\ncrop yields. 40% 15.\n3.7.2 Guidelines\n3.7.2.1 Throughout the AI lifecycle, implementations should always be carried out after full\nunderstanding and acknowledgement of AI implications on sustainability and environment.\na The application of Artificial Intelligence to address sustainability challenges are well\nunderstood. Building and running green, sustainable AI systems is of increasing importance,\ngiven the large carbon footprint that they can generate and the wider context of addressing\nclimate change.\nb AI model development, and by extension commissioning, should therefore seek to balance\ntechnical performance with energy consumption and environmental impact.\nc Efforts should be made to estimate and understand sustainability and environmental\nimpact across the AI lifecycle (e.g. the energy consumption costs associated with model\ntraining, and CO2 emissions and cloud compute costs associated with the deployment\nand running of the system).\n15 https://www.deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-by-40\n38 393.8 Privacy Preserving AI (Principle 8)\n3.8.2.2\nEstablish an oversight mechanism for data collection, storage and processing and use across\nyour organization\n3.8.1 Principle\na Assess who can access data and under which conditions.\nWe will respect people\u2019s privacy\nb Assign specific responsibilities and role for Data Protection Officers.\nAI systems should respect privacy and use the minimum intrusion necessary.\nc Prevention of harm to privacy necessitates adequate data governance that covers the\nquality and integrity of the data used, its relevance in light of the domain in which the\nAI systems should uphold high standards of data governance and security,\nprotecting personal information. AI systems will be deployed, its access protocols and the capability to process data in a\nmanner that protects privacy.\nSurveillance or other AI-driven technologies should not be deployed to the extent\nof violating internationally and/or UAE\u2019s accepted standards of privacy and human d AI systems must guarantee privacy and data protection throughout a system\u2019s entire\ndignity and people rights. lifecycle. This includes the information initially provided by the user, as well as the\ninformation generated about the user over the course of their interaction with the system\nPrivacy by design should be embedded in AI systems and where possible AI\n(e.g. outputs that the AI systems generated for specific users or how users responded to\nalgorithm should have adequate privacy impact assessments.\nparticular recommendations). Digital records of human behavior may allow AI systems\nto infer not only individuals\u2019 preferences, but also their age, gender, religious or political\nAdequate data protection frameworks and governance mechanisms should be\nviews. To allow individuals to trust the data gathering process, it must be ensured that data\nestablished in a multi-stakeholder approach and ensured throughout the life cycle\ncollected about them will not be used to unlawfully or unfairly discriminate against them.\nof AI systems.\ne In any given organization that handles individuals\u2019 data (whether someone is a user of the\nAI developers and operators should strive to strike the balance between privacy\nsystem or not), data protocols governing data access should be put in place in line with\nrequirements, individual rights and innovation growth and society benefits.\nnational privacy legislation (be these universal or sector-specific). These protocols should\noutline who can access data and under which circumstances. Only duly qualified personnel\n3.8.2 Guidelines\nwith the competence and need to access individual\u2019s data should be allowed to do so.\n3.8.2.1\nEstablish mechanism for users to flag issues related to privacy or data protection\na AI Operators should review the system for proper consent logging, ability of users to revoke\npermission whenever applicable.\nb Consider training AI model without or with minimal user of potentially sensitive or personal\ndata.\nc Use measures to enhance privacy such as encryption, anonymization and aggregation.\n40 41f Algorithmic systems require adequate privacy impact assessments, which also include\nsocietal and ethical considerations of their use and an innovative use of the privacy by\ndesign approach. AI actors need to ensure that they are accountable for the design and\nimplementation of AI systems in such a way as to ensure that personal information is\nprotected throughout the life cycle of the AI system.\ng Establish data policies or equivalent frameworks, or reinforce existing ones, to ensure full\nsecurity for personal data and sensitive data, which, if disclosed, may cause exceptional\ndamage, injury or hardship to individuals. Examples include data relating to offences,\ncriminal proceedings and convictions, and related security measures; biometric, genetic\nand health data; and personal data such as that relating to race, descent, gender, age,\nlanguage, religion, political opinion, national origin, ethnic origin, social origin, economic or\nsocial condition of birth, or disability and any other characteristics.\nh Promote mechanisms, such as open repositories for publicly funded or publicly held data,\nsource code and data trusts, to support the safe, fair, legal and ethical sharing of data,\namong others.\ni Promote and facilitate the use of quality and robust datasets for training, development and\nuse of AI systems, and exercise vigilance in overseeing their collection and use.\nExample: with the help of AI & ML algorithms16 , Synthetic data can be created to enhance\nprivacy. The data created will have the same statistical characteristics for the testing\nenvironment and where a third party can have access.\n16 https://research.aimultiple.com/privacy-enhancing-technologies/\n42 43Microsoft. 2022. Microsoft Responsible AI Standard, v2. General Requirements. June 2022.\n04\nRetrieved from:\nhttps://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4ZPmV\nISO. 2022. Proposed ISO Standard on Risk Management of AI: What Businesses Should Know.\nBibliography\nMay 2022. Retrieved from:\nhttps://unesdoc.unesco.org/ark:/48223/pf0000381137/PDF/381137eng.pdf.multi\nGSMA 2022. The AI Ethics Playbook. Implementing ethical principles into everyday business.\nFebruary 2022. Retrieved from:\nhttps://www.gsma.com/betterfuture/wp-content/uploads/202201//The-Mobile-Industry-Ethics-\nPlaybook_Feb-2022.pdf\nOECD. 2022. AI Policy Observatory. Retrieved from:\nhttps://standards.ieee.org/wp-content/uploads/import/documents/other/ead_v2.pdf\nUNESCO. 2022. Recommendation on the Ethics of Artificial Intelligence. Adopted in November\n2021. Retrieved from:\nhttps://unesdoc.unesco.org/ark:/48223/pf0000381137/PDF/381137eng.pdf.multi\nEuropean Commission. 2021. Proposal for a Regulation of the European Parliament and of the\nCouncil Laying Down Harmonized Rules on Artificial Intelligence (Artificial Intelligence Act) and\nAmending Certain Union Legislative Acts. Retrieved from:\nhttps://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A52021PC0206\nEuropean Commission. High Level Expert Group on Artificial Intelligence ( AI HLEG ). Ethics\nGuidelines for Trustworthy AI . April 2019. Retrieved from:\nhttps://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai\nCentre for Data Ethics and Innovation. UK Government: The roadmap to an effective AI assurance\necosystem. December 2021. Retrieved from:\nhttps://www.gov.uk/government/publications/the-roadmap-to-an-effective-ai-assurance-\necosystem\nThe New York City Council. 2021. A Local Law to amend the administrative code of the city of New\nYork, in relation to automated employment decision tools. Retrieved from:\nhttps://legistar.council.nyc.gov/LegislationDetail.aspx?ID=4344524&GUID=%20B051915D-A9AC-\n451E-81F86596032-FA3F9&\nUNESCO, Recommendation on the Ethics of Artificial Intelligence. 2021. Retrieved from:\nhttps://unesdoc.unesco.org/ark:/48223/pf0000381137\n44 45AI NOW Institute. Algorithmic accountability for the public sector. August 2021. Retrieved from: CNIL. (2017). How can humans keep the upper hand? The ethical matters raised by algorithms\nhttps://www.opengovpartnership.org/documents/algorithmic-accountability-public-sector/ and artificial intelligence. Retrieved from:\nhttps://www.cnil.fr/sites/default/files/atoms/files/cnil_rapport_ai_gb_web.pdf\nIEEE 2019. Ethically Aligned Design. Version 2 \u2013 For Public Discussion. Retrieved from:\nhttps://standards.ieee.org/wp-content/uploads/import/documents/other/ead_v2.pdf Executive Office of the President of the United States. National Science and Technology Council.\nNetworking and Information Technology Research and Development Subcommittee. (2016). The\nnational artificial intelligence research and development strategic plan. Retrieved from:\nIEEE 2017. Ethically Aligned Designed : A vision for Prioritizing Human Well-being with\nhttps://www.nitrd.gov/PUBS/national_ai_rd_strategic_plan.pdf\nAutonomous and Intelligent Systems. Retrieved from:\nhttps://standards.ieee.org/wp-content/uploads/import/documents/other/ead_v2.pdf\nExecutive Office of the President of the United States National Science and Technology Council.\nCommittee on Technology. (2016). Preparing for the future of artificial intelligence. Retrieved from:\nGoogle. AI Principles 2021. Retrieved from:\nhttps://obamawhitehouse.archives.gov/sites/default/files/whitehouse_files/microsites/ostp\nhttps://ai.google/principles/\nNSTC/preparing_for_the_future_of_ai.pdf\nMicrosoft. Responsible AI Standard v2 (June 2022). Retrieved from:\nThe Headquarters for Japan\u2019s Economic Revitalization. (2015). New Robot Strategy. Japan\u2019s\nhttps://www.microsoft.com/en-us/ai/responsible-ai\nRobot Strategy. Vision, Strategy, Action Plan. Retrieved from:\nhttp://www.meti.go.jp/english/press/2015/pdf/0123_01b.pdf\nThe ECONOMIST Group 2022 . Pushing forward: the future of AI in the Middle East and North\nAfrica Report. Retrieved from:\nTreasury Board of Canada Secretariat. (2018). Responsible Artificial Intelligence in the\nhttps://impact.economist.com/perspectives/sites/default/files/google_ai_mena_report.pdf\nGovernment of Canada. Digital Disruption White Paper Series. Version 2.0.\nhttps://docs.google.com/document/d/1Sn-qBZUXEUG4dVk909eSg5qvfbpNlRhzIefWPtBwbxY/\nPDPC. (2018, June 5). Discussion paper on Artificial Intelligence (AI) and Personal Data.\nedit\nSingapore: Personal Data Protection Commission Singapore (PDPC). Retrieved from:\nhttps://www.pdpc.gov.sg/-/media/Files/PDPC/PDF-Files/Resource-for-Organisation/AI/\nThe Toronto Declaration: Protecting the right to equality and non-discrimination in machine\nDiscussion-Paper-on-AI-and-PD---050618.pdf\nlearning systems. Toronto, Canada: Amnesty International and Access Now. Retrieved from:\nhttps://www.accessnow.org/cms/assets/uploads/201808//The-Toronto-Declaration_ENG_08-\nITI. AI Policy Principles. Retrieved from:\n2018.pdf\nhttps://www.itic.org/public-policy/ITIAIPolicyPrinciplesFINAL.pdf\nhttp://data.parliament.uk/writtenevidence/committeeevidence.svc/evidencedocument/artificial-\nCabinet Office.(2016, May 19). Data Science Ethical Framework. Retrieved from: intelligence-committee/artificial-intelligence/oral/73546.html (house of lords select committee\nhttps://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/ oral evidence, Q61)oral evidence, Q61)\nfile/524298/Data_science_ethics_framework_v1.0_for_publication__1_.pdf\nHouse of Lords Select Committee on Artificial Intelligence. (2018). AI in the UK: ready, willing and\nEuropean Parliament. (2017, February 16). European Parliament resolution of 16 February 2017 able?\nwith recommendations to the Commission on Civil Law Rules on Robotics (20152103/(INL)). https://publications.parliament.uk/pa/ld201719/ldselect/ldai/100100/.pdf\nRetrieved from:\nhttp://www.europarl.europa.eu/sides/getDoc.do?pubRef=-//EP//TEXT+TA+P8-TA-2017- House of Commons Science and Technology Select Committee. Algorithms in Decision Making.\n0++0051DOC+XML+V0//EN https://publications.parliament.uk/pa/cm201719/cmselect/cmsctech/351351/.pdf\nVillani, C. (2018). For a meaningful Artificial Intelligence towards a French and European strategy.\nRetrieved from:\nhttps://www.aiforhumanity.fr/pdfs/MissionVillani_Report_ENG-VF.pdf\n46 47", "metadata": {"country": "Saudi Arabia", "year": "2020", "legally_binding": "no", "binding_proof": "None", "date": "10/01/2020", "regulator": "Saudi Data and Artificial Intelligence Authority (SDAIA)", "type": "national strategy", "status": "active", "language": "English, Arabic", "use_cases": "[1, 2, 3]"}}
{"_id": "68695c585520a4cc7989de81", "title": "National Strategy for Data and Artificial Intelligence", "source": "https://sdaia.gov.sa/en/SDAIA/SdaiaStrategies/Pages/NationalStrategyForDataAndAI.aspx", "text": "AI ETHICS\nARTIFICIAL INTELLIGENCE ETHICS\nPRINCIPLES & GUIDELINES\nPRINCIPLES & GUIDELINES\nDEC 2022Table of Content\nPage Page\n1. Overview 4 3.4 Explainable 29\n1.1 Introduction 5 3.4.1 Principle 29\nWe will make AI systems as explainable as technically possible\n1.2 Scope 6 3.4.2 Guidelines 29\n1.3 Strategic alignment 6 3.5 Robust, Safe and Secure AI 31\n1.4 Responsibility 8 3.5.1 Principles 31\nAI systems will be technically robust\n3.5.2 Guidelines 32\n1.5 Licensing 8\n3.6 Human Centred AI 34\n1.6 Toolkit 8\n3.6.1 Principles 34\nWe will give AI systems human values and make them beneficial to society\n3.6.2 Guidelines 35\n2. Definitions 10\n3.7 Sustainable and Environmentally Friendly AI 38\n3.7.1 Principle 38\n3. Principles and Guidelines 16\nWe will promote sustainabity and environemntly friendly AI 38\n3.7.2 Guidelines 38\n3.1 Fairness 17\n3.8 Privacy Preserving AI 38\n3.1.1 Principle 17\nWe will make AI systems fair\n3.8.1 Principle 40\n3.1.2 Guidelines 20\nWe will respect people\u2019s privacy\n3.8.2 Guidelines 40\n3.2 Accountable AI 20\n3.2.1 Principle 20\nWe will make AI systems accountable\n4. Bibliography 44\n3.2.2 Guidelines 20\n3.3 Transparent AI 26\n3.3.1 Principle 26\nWe will make AI systems transparent\n3.3.2 Guidelines 26\n2 301 1.1 Introduction\nAI\u2019s rapid advancement and innovation potential across a range of fields is incredibly exciting.\nYet a thorough and open discussion around AI ethics, and the principles organizations using this\nOverview\ntechnology must consider, is needed.\nThis document - AI Ethics: Principles and Guidelines \u2013 intends to meet this need to balance these\ntwo central considerations of AI. It is designed to offer detailed guidance to help AI actors adhere\nto eight principles of AI ethics.\nWe will make AI systems that are:\nexplainable transparent accountable fair\nprivacy preserving sustainable and human centered robust, safe and\nenvironmentally secure\nfriendly\nThe guidelines are non-binding, and are drafted as a collaborative multi-\nstakeholder effort, with full awareness of organizations\u2019 needs to innovate\nand protect their intellectual property. This is a collaborative process in\nwhich all stakeholders are invited to be part of an ongoing dialogue. We\nwould like to see the AI Ethics Guidelines evolve into a universal, practical\nand applicable framework informing ethical requirements for AI design and\nuse. The eventual goal is to reach widespread agreement and adoption of\ncommonly agreed policies to inform the ethical use of AI nationally and\naround the world.\n4 51.2 Scope Additionally, the AI Office is currently developing additional policies to be taken into consideration\nalong with this document:\nThis document gives non-mandatory guidelines for achieving the ethical design\nand deployment of AI systems in both the public and private sectors. AI already\nAI Seal AI Procurement Guidelines\nsurrounds us, but some applications are more visible and sensitive than others.\nThe UAE AI Seal brand (UAI) will be used The AI Procurement Guidelines will be\nThis document is applicable only to those AI systems which make or inform\nto attract talent and business from used to guide Federal Government entities\n\u2018significant decisions\u2019 \u2013 that is, those decisions which have the potential for\nacross the globe to come to the UAE to in the procurement of AI systems. They\nsignificant impact either on individuals or on society as a whole. They also apply to\ntest and develop AI. This includes a UAI will list the general principles, proposed\n\u2018critical decisions\u2019, which are a subset of significant decisions and are of especially\nmark recognizing high quality, ethical AI mechanisms for procurement and review\ncritical nature.\ncompanies. It would reward safe, efficient, of AI products between the federal\nverified AI technology with a \u2018UAI Seal government and vendors. The policy\nThis document guides the further developments of sector specific requirements\nof Approval\u2019. The UAE AI Seal brand is considers R&D for specific challenges\nand principles, hence every entity within its sector can further amend, use, or\ncurrently under development by the AI that do not currently have an AI solution\ncomplement its current frameworks to properly suit its context and the need of\nOffice. product that is market-ready. Based on\naffected stakeholders.\nglobal standards for AI Procurement, the\npolicy aims to incentivize AI adoption\nIt is a living document and will undergo further future reviews and enhancements.\nwithin government entities and promote\nAI R&D within the private sector for\nspecified government challenges. The\nprocurement policy is currently under\ndevelopment by the AI Office.\n1.3 Strategic alignment\nThis document comes as a fulfillment of the goals and aspirations set in the UAE National AI\nStrategy and the international Sustainable Development Goals (SDGs). The UAE has long been\nat the forefront of sustainable development. These shared global milestones have created global\nmovement and adoption in several disciplines and sectors. As an early adopter, the UAE has\nestablished a national committee on SDGs with a long-term realization of the UAE SDG agenda\n2030. These goals are further engrained and reflected in the AI Ethics Principles under the topics of\nfairness, inclusiveness, equality, human benefit, and rights.\n6 71.4 Responsibility\nThe AI Office will not be responsible for any misuse of the AI Ethics Principles and Guidelines. The\nuser bears all the consequences of their use.\n1.5 Licensing\nThis document is published under the terms of a Creative Commons Attribution 4.0 International\nLicense in order to facilitate its re-use by other governments and private sector organizations.\nIn summary this means you are free to share and adapt the material, including for commercial\npurposes, provided that you give appropriate credit to the UAE AI and Blockchain Council as its\nowner and do not suggest the Minister of State for Artificial Intelligence, Digital Economy and\nRemote Work Applications Office endorses your use.\n1.6 Toolkit\nThe document is part of a toolkit of self-governing nature that boosts awareness and enables\ngovernment and private institutions to pursue innovative use cases while maintaining human\nvalues and principles.\n8 92.1 AI developer organization\n02\nAn organization which does any of the following:\na determines the purpose of an AI system;\nDefinitions\nb designs an AI system;\nc builds an AI system, or:\nd performs technical maintenance or tuning on an AI system\nNote:\nThe definition applies regardless of whether the organization is the ultimate user of the system, or\nwhether they sell it on or give it away.\nExample: A company develops an artificially intelligent facial recognition system and\nsells it to a country\u2019s border force, who use it to identify suspicious personnel. The company\nis an AI developer organization and the border force is an AI operator organization.\n2.2 AI operator organization\nAn organization which does any of the following:\na uses AI systems in operations, backroom processes or decision-making;\nb uses an AI system to provide a service to an AI subject;\nc is a business owner of an AI system;\nd procures and treats data for use in an AI system; or\ne evaluates the use case for an AI system and decides whether to proceed.\nNotes:\n1. This definition applies regardless of whether the AI system was developed in-house or procured.\n2. It is possible for organizations to be both an AI developer organization and an AI operator organization.\n10 112.6 Critical decision\n2.3 Artificial Intelligence (also AI)\nThe capability of a functional unit to perform functions that are generally associated with human An individually significant decision which is deemed to either have a very large impact on an\nintelligence such as reasoning, learning and self-improvement1. individual or to have especially high stakes. These can be especially sensitive, have the potential\nto cause high loss or damage, be societally significant, or set an important precedent.\n2.4 Artificially Intelligent System (also AI system)\nNotes:\nThe types of decisions referred to here are the same as those in the definition of significant-at-\nA product, service, process or decision-making methodology whose operation or outcome is\nscale decisions, except in this case the effects are felt as a result of an individual decision rather\nmaterially influenced by artificially intelligent functional units.\nthan an aggregate of many decisions..\nNotes:\n1. It is not necessary for a system\u2019s outcome to be solely determined by artificially intelligent\nfunctional units in order for the system to be defined as an artificially intelligent system; and Example: A court determines whether a defendant is guilty of a criminal charge, with the\n2. A particular feature of AI systems is that they learn behavior and rules not explicitly punishment for guilt being a life sentence. This is a critical decision because it has a very\nprogrammed in. large impact on the life of the defendant and also sets precedent for similar cases in the\nfuture.\nExample 1: A small claims court uses Example 2: A government entity uses\n2.7 Functional Unit\nan artificially intelligent software package a chatbot which allows customers to ask\nto collect evidence pertaining to a case, routine questions, book appointments\nEntity of hardware or software, or both, capable of accomplishing a specified purpose3 .\ncompare it to similar cases in the past, and conduct minor financial transactions.\nand present a recommended decision The chatbot responds to customer\nto a judge. The judge determines the queries with pre-written responses and is\n2.8 Individually significant decision\nfinal outcome. This decision-making based on pre-programmed decision rules.\nA decision which has the potential for significant impact on at least one individual\u2019s\nmethodology is materially influenced Therefore, the chatbot is not an AI system.\ncircumstances, behavior or choices, or has legal or similarly significant effects on him or her.\nby an artificially intelligent functional If, however, the chatbot autonomously\nunit, and is therefore classified as an AI adjusted its treatment of customers\nsystem. based on the outcome of past cases, it\nwould be an AI system. Example: A company decides to make an employee redundant. This is an individually\nsignificant decision because of its potential impact on the employee\u2019s financial situation.\n2.5 Bias (of a system)\nInclination or prejudice for or against one person or group, especially in a way considered to be\nunfair2.\n2 Oxford Dictionary 2018, Oxford University Press, viewed online 4th October 2018, <https://en.oxforddictionaries.com>\n1 Consistent with ISO/IEC 2382:2015 3 From ISO/IEC 2382:2015\n12 132.9 Non-operational bias (of a system) 2.11 Significant decision\nBias that is either: A decision which is either individually significant or is part of a set of significant-at-scale\ndecisions.\na not a design feature; or\nb not important in achieving the stated purpose of the system. 2.12 Subject of an artificially intelligent system (also AI subject)\nA natural person who is any of the following:\n2.10 Set of significant-at-scale decisions\na an end-user of an AI system\nA set of decisions made by the same system or organization which, when taken in aggregate,\nb directly affected by the operation of or outcomes of an AI system, or:\nhave significant impact on society as a whole or groups within it.\nc a recipient of a service or recommendation provided by an AI system\nNotes:\n1. The decisions need not be individually significant in order to qualify, in aggregate, as a set of\nsignificant-at-scale decisions; and\n2. Examples of areas which have a large impact on society and which include but are not limited\nto: the large-scale allocation of resources or opportunities amongst groups; the structure of\ngovernment; the division of power between large entities or groups; the law, and its interpretation\nand enforcement; conflict and war; international relations, etc.\nExample: An AI system is used by a website to determine which content to show users.\nThis decision is not individually significant, since a user is not greatly affected by whether\na particular piece of media is shown to them. However, if the website is popular then the AI\nsystem may be making a set of significant-at-scale decisions, because any biases in the\nsystem will affect a large number of users.\n14 153.1 Fairness (Principle 1)\n03\n3.1.1 Principle\nPRINCIPLES AND We will make AI systems fair\nData ingested should, where possible, be accurate and representative of the affected\npopulation.\nGUIDELINES\nAlgorithms should avoid non-operational bias.\nSteps should be taken to mitigate and disclose the biases inherent in datasets.\nSignificant decisions should be provably fair.\nAll personnel involved in the development, deployment and use of AI systems have a role and\nresponsibility to operationalize AI fairness and should be educated accordingly.\n3.1.2 Guidelines\n3.1.2.1\nBenefits of AI systems should be available and accessible to all. AI systems should not discriminate\nagainst people or groups in a way that could have an adverse impact. Especially significant and\ncritical decisions made or assisted by AI should be provably fair.\na Considering that fairness can have many different definitions, an organization should\ndocument its own definition of fairness for the context that the AI system is going to be\nimplemented in. Organizations should document what the implemented fairness objective\nstands for and why this choice was considered the most suitable for the given scenario.\nb It is recommended to identify and document demographic groups that may be adversely\nimpacted and mitigate the risk where possible.\nc AI developers and AI operators could consider formal procedures such as Discrimination\nImpact Assessments as a means of ensuring fairness. This assessment should be\ndocumented as well as the mitigation measures that were implemented by the organization.\nThe impact assessment should be conducted pre-release and regularly after as an ongoing\nevaluation, results are advised to be documented.\n16 173.1.2.2 3.1.2.3\nConsideration should be given to whether the data ingested is accurate and representative Consideration should be given to whether decision-making processes introduce bias\nof the affected population\na When subjecting different groups to different decision-making processes, AI developers\nd Fairness has many different definitions in different cultures and for different contexts. should consider whether this will lead to non-operational bias.\nEncouraging a diverse and inclusive AI ecosystem is thus all the more crucial to ensure\nthat one definition of fairness does not contradict another, and that the process of defining b When evaluating the fairness of an AI system, AI developers and AI operators should\nfairness itself is fair, with under-represented groups represented in the discussion. consider whether AI subjects in the same circumstances receive equal treatment.\ne AI developers and AI operators should undertake reasonable data exploration and/or testing\nto identify potentially prejudicial decision-making tendencies in AI systems arising from\nExample: An organization uses an AI tool to automate the pre-screening of candidates\ndata inaccuracy and biases in the data.\nfor a job opening. It is trained on data from the company\u2019s existing employees, the majority\nof whom are from the same ethnic background. Therefore, the system learns to use name\nf AI developers and operators should evaluate all datasets to assess inclusiveness of and nationality as discriminating factors in filtering job applicants. This could have been\nidentified demographic groups and collect data to close any gaps.\nidentified through testing and rectified by, for example, balancing the training data or only\nusing relevant data fields for training.\ng AI developers and AI operators should refrain from training AI systems on data that is not\nlikely to be representative of the affected AI subjects, or is not likely to be accurate, whether\nthat be due to age, omission, method of collection, or other factors.\n3.1.2.4\nSignificant decisions informed by the use of AI should be fair\nExample: Following a natural disaster, a government relief agency uses an AI system to a AI developers and AI operators could consider formal procedures such as Discrimination\ndetect communities in greatest need by analyzing social media data from a range of websites. Impact Assessments as a means of ensuring fairness.\nHowever, those communities where smartphone penetration is lower having less presence on\n3.1.2.4\nsocial media, and so are at risk of receiving less attention. Therefore, the charity complements\nAI operators should consider whether their AI systems are accessible and usable in a fair\ntheir AI tool with traditional techniques to identify needy populations elsewhere.\nmanner across user groups\n3.1.2.5\nh AI developers should consider whether their AI systems can be expected to perform well Consideration should be given to the effect of diversity on the development and deployment\nwhen exposed to previously unseen data, especially when evaluating people who are not processes\nwell-represented in the training data.\nb Organizations should seek to include people from diverse demographic backgrounds across\nthe full lifecyle, to include design, development and deployment processes. Organizations\nshould seek to engage diverse internal and external groups also.\nc AI developers should consider whether the assumptions they make about AI subjects could\nbe wrong or are likely to lead to non-operational bias; if so, they should consider consulting\nthe AI subjects in a representative manner during the design, development and deployment\nto confirm these assumptions.\n18 193.2 Accountable AI (Principle 2)\n3.2.2.2\nPositive efforts should be made to identify and mitigate any significant risks inherent in the AI\nsystems designed\n3.2.1 Principle\na AI operators should only use AI systems that are backed by evidence-based academic or\nindustrial research, and AI developers should base their development on such research.\nWe will make AI systems accountable\nAccountability for the outcomes of an AI system lies not with the system itself but is\napportioned between those who design, develop and deploy it; Example: A foreign country has a government service which identifies parents who owe\nmoney in child maintenance. The data matching process is often incorrect due to misspelled\nDevelopers should make efforts to mitigate the risks inherent in the systems they design; names or missing data which results in some individuals being incorrectly targeted\nautomatically by the system with the result being a large bill, poor credit ratings and even\nAI systems should have built-in appeals procedures whereby users can challenge significant\nfreezing wages. The recourse for individuals who are incorrectly targeted is time-consuming\ndecisions;\nand not straightforward . If the potential impact of incorrect decisions had been assessed,\nmitigation measures such a user-friendly review procedure could have been set up.\nAI systems should be developed by diverse teams which include experts in the area in which\nthe system will be deployed; and\nAI systems should be subject to external audit and decision quality assurance. b AI operators should identify the likely impact of incorrect automated decisions on AI\nsubjects and, in the case where incorrect decisions are likely to cause significant cost or\ninconvenience, consider mitigating measures.\n3.2.2 Guidelines\nc AI operators should consider internal risk assessments or ethics frameworks as a means to\n3.2.2.1 facilitate the identification of risks and mitigating measures.\nAccountability for the outcomes of an AI system should not lie with the system itself\nd In designing AI systems to inform significant decisions, AI developers should consider\na Accountability for loss or damages resulting from the application of AI systems should not measures to maintain data accuracy over time, including:\nbe attributed to the system itself.\n\u2022 the completeness of the data.\nb AI operators and AI developers should consider designating individuals to be responsible for \u2022 timely update of the data, and.\ninvestigating and rectifying the cause of loss or damage arising from the deployment of AI\n\u2022 whether the context in which the data was collected affects its suitability for the\nsystems.\nintended use case.\nExample: A border camera scanning for predictors of risk may misinterpret a \u201ctic\u201d of an\nindividual with Tourette syndrome as suspicious. These can manifest in a diverse fashion, and\nshould not cause this person to undergo secondary inspection every time they pass through\nthe border . If the data is updated after the first case is encountered then it would avoid\ncausing inconvenience on subsequent visits.\n5 Government of Canada. (2018). Responsible AI in the Government of Canada. Digital Disruption White Paper Series. Version 2.0, p.26. Retrieved from:\nhttps://docs.google.com/document/d/1Sn-qBZUXEUG4dVk909eSg5qvfbpNlRhzIefWPtBwbxY/edit\n4 Cabinet Office (UK), Data Science Ethical Framework, Version 1.0, licensed under the Open Government License v3.0, p. 13 6 Stoica, I. et. al.,2017, A Berkeley View of Systems Challenges for AI, p. 2, https://arxiv.org/pdf/1712.05855.pdf\n20 21e AI developers and AI operators should tune AI models periodically to cater for changes to e Be mindful that there might be fundamental tensions between different principles and\ndata and/or models over time. requirements. Continuously identify, evaluate, document and communicate these trade-\noffs and their solutions.\nf AI operators should consider whether AI systems trained in a static environment will display\nmodel instability when deployed in dynamic environments. Adopt a Trustworthy AI assessment list when developing, deploying or using AI systems,\nf\nand adapt it to the specific use case in which the system is being applied.\nExample: AI systems will need to be able to adapt to the changes in the environment that g Keep in mind that such an assessment list will never be exhaustive. Ensuring Trustworthy\nthey are deployed in. For example, a self-driving car would need to quickly adapt to unexpected AI is not about ticking boxes, but about continuously identifying and implementing\nand dangerous road by learning in real time from other cars that have successfully dealt with requirements, evaluating solutions, ensuring improved outcomes throughout the AI\nthese conditions. In addition, such mission-critical applications must handle noisy inputs system\u2019s lifecycle, and involving stakeholders in this.\nand defend against malicious actors .\n3.2.2.4\nAI subjects should be able to challenge significant automated decisions concerning them and,\ng AI developers should collaborate with AI Operators to train models using historical data where appropriate, be able to opt out of such decisions\nfrom the Operator.\na AI operators using AI systems to inform significant decisions should provide procedures by\nwhich affected AI subjects can challenge a specific decision concerning them.\nh AI Operators should consider working with their vendors (AI developers) to continually\nmonitor performance.\nb AI operators should consider such procedures even for non-significant decisions.\ni AI Operators should subject AI systems informing significant decisions to quality checks\nat least as stringent as those that would be required of a human being taking the same c AI operators should make affected AI subjects aware of these procedures and should\ndesign them to be convenient and user-friendly.\ndecision.\nd AI operators should consider employing human case evaluators to review any such\n3.2.2.3\nchallenges and, when appropriate, overturn the challenged decision.\nAI systems informing critical decisions should be subject to appropriate external audit\na When AI systems are used for critical decisions, external auditing of the AI systems in\nquestion should be used as a means to ensure meaningful standards of transparency and Example: A bank allows customers to apply for a loan online by entering their data. The\naccountability are upheld. bank uses an AI system to automatically determine whether to give the loan and what the\ninterest rate should be. They provide users with an option to contest the decision and have\nb In the case that critical decisions are of civic interest, public release of the results of the it reviewed by a human . They also require that customers justify their challenge by filling in\naudit should be considered as a means of ensuring public processes remain accountable a form, which assists the case reviewer and deters customers from challenging a decision\nto those affected by them. without good reason.7\nIn the case that critical decision are life and death decisions, these decisions should be\nc\nsupported by further validation and verification via a human operator.\nd Facilitate traceability and auditability of AI systems, particularly in critical contexts or\nsituations.\n7 Adapted from EU Commission, Can I be subject to automated individual decision-making, including profiling? Retrieved from: https://ec.europa.eu/\ninfo/law/law-topic/data-protection/reform/rights-citizens/my-rights/can-i-be-subject-automated-individual-decision-making-including-profiling_\nen#example\n22 23e AI operators should consider instituting an opt-out mechanism for significant automated 3.2.2.7\ndecisions. AI operators should understand the AI systems they use sufficiently to assess their suitability\nfor the use case and to ensure accountability and transparency\nf AI operators could consider \u201ccrowd challenge\u201d mechanisms whereby a critical number of\ncomplaints triggers an investigation into the fairness and/or accuracy of a decision-making\na In the case of critical decisions, AI operators should avoid using AI systems that cannot be\nprocess as a whole.\nsubjected to meaningful standards of accountability and transparency.\n3.2.2.5 b AI developers should consider notifying customers and AI operators of the use cases for\nAI systems informing significant decisions should not attempt to make value judgements on which the system has been designed, and those for which it is not suitable.\npeople\u2019s behalf without prior consent\na When informing an AI subject about significant choices they will make, AI systems should\nnot unreasonably restrict the available options or otherwise attempt to influence their value\njudgements without the explicit consent of the AI subject in question.\n3.2.2.6\nAI systems informing significant decisions should be developed by diverse teams with\nappropriate backgrounds\na AI developers who develop AI systems which may be used to assist in making critical\ndecisions should involve in the process experts with a background in social science, policy,\nor another subject which prepares them to evaluate the broader societal impact of their\nwork.\nb Development of AI systems informing significant decisions should include consultation\nwith experts in the field in which the system will be deployed.\nExample: An app that uses AI to assess medical symptoms and has a large user base had\nto face regulatory scrutiny because of number of complaints from doctors. They warned\nthat the application can miss signs of serious illness. A number of different shortcomings\nwere identified by doctors, some of which the company could address and resolve.8\n8 Financial Times. 2018. High-profile health app under scrutiny after doctor\u2019s complaints. Retrieved from: https://www.ft.com/content/19dc6b7e-8529-\n11e896-dd-fa565ec55929\n24 253.3 Transparent AI (Principle 3)\nc To facilitate the above, AI developers and AI operators should consider documenting the\nfollowing information during the design, development and deployment phases, and retaining\n3.3.1 Principle this documentation for a length of time appropriate to the decision type or industry:\n\u2022 the provenance of the training data, the methods of collection and treatment, how the data was\nWe will make AI systems transparent\nmoved, and measures taken to maintain its accuracy over time;\n\u2022 the model design and algorithms employed; and\nDevelopers should build systems whose failures can be traced and diagnosed. \u2022 changes to the codebase, and authorship of those changes.\nPeople should be told when significant decisions about them are being made by AI. d Where possible given the model design, AI developers should consider building in a means\nby which the \u201cdecision journey\u201d of a specific outcome (i.e. the component decisions leading\nWithin the limits of privacy and the preservation of intellectual property, those who deploy to it) can be logged.\nAI systems should be transparent about the data and algorithms they use.\nResponsible disclosures should be provided in a timely manner, and provide reasonable\nExample: A technology company has a product which is designed to assist in medical\njustifications for AI systems outcomes. This includes information that helps people\ndiagnosis. It documents each stage of its reasoning and relates it back to the input data.9\nunderstand outcomes, like key factors used in decision making.\n3.3.2 Guidelines\n3.3.2.1\nTraceability should be considered for significant decisions, especially those that have the\npotential to result in loss, harm or damage.\na For AI systems which inform significant decisions, especially those with the potential to\ncause loss, harm or damage, AI developers should consider building in traceability (i.e. the\nability to trace the key factors leading to any specific decision).\nb Organizations should ensure that harms caused through AI systems are investigated and\nredressed, by enacting strong enforcement mechanisms and remedial actions, to make\ncertain that human rights and the rule of law are respected in the digital world and in the\nphysical world.\n9 See IBM WatsonPaths\n26 273.3.2.2 3.4 Explainable AI (Principle 4)\nPeople should be informed of the extent of their interaction with AI systems\n3.4.1 Principle\na AI operators should inform AI subjects when a significant decision affecting them has been\nmade by an AI system.\nWe will make AI systems as explainable as\ntechnically possible\nExample: A small claims court adjudicates minor civil matters such as debt collection and\nevictions. They introduce an AI system to suggest the outcome of a ruling. At the time of the\nDecisions and methodologies of AI systems which have a significant effect on individuals\nruling the plaintiff and defendant are notified that the decision was assisted by an AI system.\nshould be explainable to them, to the extent permitted by available technology.\nThe court also provides an explanation for the decision.\nIt should be possible to ascertain the key factors leading to any specific decision that\ncould have a significant effect on an individual.\nb If an AI system can convincingly impersonate a human being, it should do so only after\nIn the above situation we will provide channels through which people can request such\nnotifying the AI subject that it is an AI system.\nexplanations.\nExample: A technology company produces a conversational AI agent which can make 3.4.2 Guidelines\nsome phone calls on behalf of its users. Those who receive the calls may believe that they\n3.4.2.1\nare speaking to a human. Therefore, the company programs the agent to identify itself at\nAI operators could consider providing affected AI subjects with a high-level explanation of\nthe start of every conversation.\nhow their AI system works\na AI operators could consider informing the affected AI subjects in understandable, non-\ntechnical language of:\n\u2022 the data that is ingested by the system;\n\u2022 the types of algorithms employed;\n\u2022 the categories into which people can be placed; and\n\u2022 the most important features driving the outcomes of decisions\n\u2022 A comprehensive list of feature engineering and models that was considered during\nthe model building phase.\nExample: A person turned down for a credit card might be told that the algorithm took\ntheir credit history, age, and postcode into account, but not learn why their application was\nrejected10 .\n28 29b For non-sensitive public sector use cases designed for the common good, AI operators\ncould consider making source code, together with an explanation of the workings of the AI Example: The UK\u2019s NHS developed a tool called Predict, which allows women with breast\nsystem, available either publicly or upon request (this should be done only if there is low risk cancer to compare their case to other women who have had the same condition in the past,\nof people \u2018gaming the system\u2019). and visualize the expected survival rate under various treatment options. The website has an\nexplanation page which shows the weights behind various factors and contains a description\nc The AI operator should maintain necessary documentation that provides elaboration and of the underlying mathematics12 .\nclarification on how the algorithm works, for example documentation of processes, decision\nmaking flow charts of the system, and how the appeal process is embedded.\nd An individual should have the ability to contest and seek effective redress against decisions c In the case that such explanations are available, they should be easily and quickly accessible,\nmade by AI systems. These must be addressed by the group or team supporting these free of charge and user-friendly.\nmodels.\ne Develop appropriate impact indices for the evaluation of AI system technological 3.5 Robust, Safe and Secure AI (Principle 5)\ninterventions from multiple perspectives.\n3.5.1 Principles\n3.4.2.2\nAI operators should consider providing affected AI subjects with a means to request\nexplanations for specific significant decisions, to the extent possible given the state of present AI systems will be technically robust\nresearch and the choice of model\nAI systems should be technically robust with a preventative approach to risks which\na AI operators should consider providing a means by which people affected by a significant\noperates in a manner such that they reliably behave.\ndecision informed by AI can access the reasoning behind that decision.\nAI Developers should ensure AI systems will not cause any unintentional harm and\nadverse impacts.\nExample: The US Consumer Financial Protection Bureau requires that creditors in the US\nwho reject credit applications must explain to the applicant the principal reason(s) why their\nAI systems should be resilient to attacks and security such as data poisoning and\napplication was rejected (e.g. \u201clength of residence\u201d or \u201cage of automobile\u201d)11 . In particular,\nmodel leakage.\n\u201cstatements that the adverse action was based on the creditor`s internal standards or\npolicies or that the applicant, joint applicant, or similar party failed to achieve a qualifying AI systems should have safeguards that enable a fallback plan in case of problems.\nscore on the creditor`s credit scoring system are insufficient\u201d.\nAI system results should be reproducible.\nb Where such explainability is not possible given available technology, AI operators should\nconsider compromises such as counterfactual reasoning, or listing the most heavily\nweighted factors contributing to the decision.\n11 Consumer Financial Protection Bureau, 12 CFR Part 1002 - Equal Credit Opportunity Act (Regulation B), \u00a7 1002.9 Notifications, Retrieved from https://www.\n10 The Guardian. AI watchdog needed to regulate automated decision-making, say experts. Retrieved from: https://www.theguardian.com/technology/2017/ consumerfinance.gov/policy-compliance/rulemaking/regulations/1002/\njan/27/ai-artificial-intelligence-watchdog-needed-to-prevent-discriminatory-automated-decisions 12 Predict website, accessible at http://www.predict.nhs.uk/predict_v2.1/legal/algorithm\n30 313.5.2 Guidelines\n3.5.2.1 3.5.2.3\nAI Operators should continue conducting vulnerability assessments, verification of AI system AI Operators should assure end users of the system\u2019s reliability through documentation, and\ndifferent behaviors in unexpected situations and for any dual-use case, to include: operationalizing processes for testing and verification of desired outcomes , specifically:\na Putting measures to ensure integrity and resilience of IA system against potential attacks. a AI system should be designed with an approach to continue monitoring if the system meets\ngoals, purposes and intended application.\nb AI Operators should define an approach to ensure results are reproducible through clear\nExample: IBM researchers presented a novel approach to exploiting AI for malicious\nprocess and documentation.\npurposes. Their system, DeepLocker embeds AI capabilities within the malware itself in order\nto improve its evasion techniques. It uses an Artificial Intelligence model to identify its target\nc AI Operators should publish documentation to assure system robustness to end users.\nusing indicators like facial recognition, geolocation and voice recognition.\n3.5.2.4\nAI systems will be safe, secure and controllable by humans\nb Verifying how AI systems behave in unexpected situations and environments.\na Safety and security of people, be they operators, end-users or other parties, will be of\nc Taking preventative measures against any possible system dual-case use. paramount concern in the design of any AI system.\nb AI systems should be verifiably secure and controllable throughout their operational\n3.5.2.2\nlifetime, to the extent permitted by technology.\nAI operators should define a suitable fall back plan and test it to maintain readiness in\nunexpected situations and in high safety risks level, to include:\nc The continued security and privacy of users should be considered when decommissioning\na Developing a plan to measure and assess potential safety risks to you or any third party AI systems.\nfrom technology use accidental or malicious misuse.\nd AI systems that may directly impact people\u2019s lives in a significant way should receive\nb Define thresholds for system acceptable results and governance procure to fall back on commensurate care in their designs.\nalternative defined and tested plans.\ne Such systems should be able to be overridden or their decisions reversed by designated\nc Considering an insurance policy to mitigate risks arising from potential damages. individuals.\nExample: Adopting a risk-based approach to procurement and clearly communicating it to\nvendors can help address such issues by giving the procuring organization advance notice\nof the specific oversight capabilities it will need in future stages of the system lifecycle,\npreventing vendors from presenting intellectual property arguments against required testing,\nmonitoring and auditing of their AI systems going forward and rewarding vendors\u2014of all\nsizes\u2014that are more advanced and responsive in their responsible AI efforts.\n13 IBM. https://documents.trendmicro.com/assets/white_papers/wp-malicious-uses-and-abuses-of-artificial-intelligence.pdf\n32 333.6.2 Guidelines\n3.5.2.5\nAi systems should not be able to autonomously hurt, destroy or deceive humans\n3.6.2.1\na AI systems should be built to serve and inform, and not to deceive and manipulate.\nAI should be developed to align with human values, contribute to human flourishing and\nbenefit society.\nb Nations should collaborate to avoid an arms race in lethal autonomous weapons, and\nsuch weapons should be tightly controlled. a Society should be consulted in a representative fashion to inform the development of AI.\nc Active cooperation should be pursued to avoid corner-cutting on safety standards. b Stakeholders throughout society should be involved in the development of AI and its\ngovernance.\nSystems designed to inform significant decisions should do so impartially.\nd\nc Stakeholders should proactively engage in responsible stewardship of trustworthy AI in\npursuit of beneficial outcomes for people and the planet.\n3.6 Human Centered AI (Principle 6)\nd Organizations should prioritize having all their stakeholders learn about wellbeing metrics\nas a potential determinant of how they create, deploy, market and monitor their AI\n3.6.1 Principles technologies.\nWe will give AI systems human values and\nExample: IEEE P7010 recommended practice14 establishes wellbeing metrics relating to\nmake them beneficial to society human factors directly affected by intelligent and autonomous systems and establishes a\nbaseline for the types of objective and subjective data these systems should analyze and\nGovernment will support the research of the beneficial use of AI. include (in their programming and functioning) to proactively increase human wellbeing.\nStakeholders throughout society should be involved in the development of AI and its\ngovernance.\n3.6.2.2\nSystematic analyses that examine the ethics of designing affective systems to nudge human\nDesign AI systems to adopt, learn and follow the norms and values of the community\nbeings prior to deployment are needed.\nthey serve.\nSystematic analyses that examine the ethics of designing affective systems to nudge a Nudging in AI systems should have an opt-in system policy with explicit consent.\nhuman beings prior to deployment are needed.\nb We recommend that when appropriate, an affective system that nudges human beings\nDecisions related to lethal force, life and death should not be delegated to AI systems. should have the ability to accurately distinguish between users, including detecting\nRules and standards should be adopted to ensure effective human control over those characteristics such as whether the user is an adult or a child. Additional protections must\ndecisions. be put in place for vulnerable populations, such as children, when informed consent cannot\nbe obtained, or when it may not be a sufficient safeguard.\nc AI systems with nudging strategies must be carefully evaluated, monitored, and controlled.\n14 IEEE. https://sagroups.ieee.org/7010/\n34 353.6.2.3 f In situations where it is needed, human rights impact assessments and human rights due\nHumanity should retain the power to govern itself and make the final decision, with AI in an diligence, human determination codes of ethical conduct, or quality labels and certifications\nassisting role. intended to promote human centered values and fairness should be considered.\ng Mechanisms should be put into place to receive external feedback regarding AI systems\na Decisions related to lethal force, life and death should not be delegated to AI systems. Rules\nthat potentially infringe on fundamental rights.\nand standards should be adopted to ensure effective human control over those decisions.\nb Responsible parties (e.g., parents, nurse practitioners, social workers, and governments)\nshould be trained to detect the influence due to AI and ineffective mitigation techniques.\nIn the most extreme cases it should always be possible to shut down harmful AI system.\nc Those actions undertaken by an affective system that are most likely to generate an\nemotional response should be designed to be easily changed.\nd Users should be able to make informed autonomous decisions regarding AI systems. They\nshould be given appropriate knowledge and tools to comprehend and interact with AI\nsystems to satisfactory degree and, where possible to reasonably self-assess or challenge\nthe system.\n3.6.2.4\nAI systems should be designed in a way that respects the rule of law, human rights, society\nvalues, and should include appropriate safeguards to ensure a fair and just society.\na AI Developers should design AI systems to adopt, learn and follow the norms and values of\nthe community they serve.\nb Organizations could identify the norms of the specific community in which AI systems are\nto be deployed in. In particular, pay attention to the norms relevant to the kinds of tasks that\nthe AI systems are designed to perform. These could be documented as well as how these\nnorms are addressed by the AI system.\nc To respond to the dynamic change of norms in society the AI system could be able to adjust\nits existing norms and learn new ones, while being transparent about these changes.\nd Designers should consider forms and metrics for assessing an AI system\u2019s norm conformity\nover the lifetime of the system (e.g. human-machine agreement on moral decisions,\nverifiability of AI decisions, justified trust).\ne The norm identification process must document the similarities and differences between\nthe norms that humans apply to other humans and the norms they apply to AI systems.\nNorm implementations should be evaluated specifically against the norms that the\ncommunity expects the AI system to follow.\n36 373.7 Sustainable and Environmentally Friendly AI (Principle 7)\nd While there is no standardized means of reporting on the environmental impact of AI\nsystems, where possible, model development and commissioning should make carbon\n3.7.1 Principle impacts a core consideration alongside functional and business requirements.\ne Smaller models should be considered \u2013 shrinking down the model size and using fewer\nWe will promote sustainable and environmentally compute cycles (to balance financial and performance costs with the end performance of\nthe model).\nfriendly AI\nf Carbon awareness should be considered \u2013 adjusting the operational parameters of the AI\nAI systems should be used to benefit all human beings, including future generations. The\nsystem to dynamically select the best time and location for energy use from the grid can\nenvironment is fundamental to this.\nreduce its carbon footprint.\nThroughout the AI lifecycle, implementations should only be carried out on the basis of a full\nunderstanding and acknowledgement of implications for sustainability and environment.\nExample: In agriculture, AI can transform Example: Google`s DeepMind division\nAn AI system\u2019s development, deployment and use, should be assessed via critical examination production by better monitoring and has developed AI that teaches itself to\nof resource usage and energy consumption.\nmanaging environmental conditions minimize the use of energy to cool Google`s\nand crop yields. AI can help reduce both data centers. As a result, Google reduced\nMechanisms to measure environmental impact due to type of energy use and processing\nfertilizer and water, all while improving its data center energy requirements by\npower provided by data centers should be established.\ncrop yields. 40% 15.\n3.7.2 Guidelines\n3.7.2.1 Throughout the AI lifecycle, implementations should always be carried out after full\nunderstanding and acknowledgement of AI implications on sustainability and environment.\na The application of Artificial Intelligence to address sustainability challenges are well\nunderstood. Building and running green, sustainable AI systems is of increasing importance,\ngiven the large carbon footprint that they can generate and the wider context of addressing\nclimate change.\nb AI model development, and by extension commissioning, should therefore seek to balance\ntechnical performance with energy consumption and environmental impact.\nc Efforts should be made to estimate and understand sustainability and environmental\nimpact across the AI lifecycle (e.g. the energy consumption costs associated with model\ntraining, and CO2 emissions and cloud compute costs associated with the deployment\nand running of the system).\n15 https://www.deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-by-40\n38 393.8 Privacy Preserving AI (Principle 8)\n3.8.2.2\nEstablish an oversight mechanism for data collection, storage and processing and use across\nyour organization\n3.8.1 Principle\na Assess who can access data and under which conditions.\nWe will respect people\u2019s privacy\nb Assign specific responsibilities and role for Data Protection Officers.\nAI systems should respect privacy and use the minimum intrusion necessary.\nc Prevention of harm to privacy necessitates adequate data governance that covers the\nquality and integrity of the data used, its relevance in light of the domain in which the\nAI systems should uphold high standards of data governance and security,\nprotecting personal information. AI systems will be deployed, its access protocols and the capability to process data in a\nmanner that protects privacy.\nSurveillance or other AI-driven technologies should not be deployed to the extent\nof violating internationally and/or UAE\u2019s accepted standards of privacy and human d AI systems must guarantee privacy and data protection throughout a system\u2019s entire\ndignity and people rights. lifecycle. This includes the information initially provided by the user, as well as the\ninformation generated about the user over the course of their interaction with the system\nPrivacy by design should be embedded in AI systems and where possible AI\n(e.g. outputs that the AI systems generated for specific users or how users responded to\nalgorithm should have adequate privacy impact assessments.\nparticular recommendations). Digital records of human behavior may allow AI systems\nto infer not only individuals\u2019 preferences, but also their age, gender, religious or political\nAdequate data protection frameworks and governance mechanisms should be\nviews. To allow individuals to trust the data gathering process, it must be ensured that data\nestablished in a multi-stakeholder approach and ensured throughout the life cycle\ncollected about them will not be used to unlawfully or unfairly discriminate against them.\nof AI systems.\ne In any given organization that handles individuals\u2019 data (whether someone is a user of the\nAI developers and operators should strive to strike the balance between privacy\nsystem or not), data protocols governing data access should be put in place in line with\nrequirements, individual rights and innovation growth and society benefits.\nnational privacy legislation (be these universal or sector-specific). These protocols should\noutline who can access data and under which circumstances. Only duly qualified personnel\n3.8.2 Guidelines\nwith the competence and need to access individual\u2019s data should be allowed to do so.\n3.8.2.1\nEstablish mechanism for users to flag issues related to privacy or data protection\na AI Operators should review the system for proper consent logging, ability of users to revoke\npermission whenever applicable.\nb Consider training AI model without or with minimal user of potentially sensitive or personal\ndata.\nc Use measures to enhance privacy such as encryption, anonymization and aggregation.\n40 41f Algorithmic systems require adequate privacy impact assessments, which also include\nsocietal and ethical considerations of their use and an innovative use of the privacy by\ndesign approach. AI actors need to ensure that they are accountable for the design and\nimplementation of AI systems in such a way as to ensure that personal information is\nprotected throughout the life cycle of the AI system.\ng Establish data policies or equivalent frameworks, or reinforce existing ones, to ensure full\nsecurity for personal data and sensitive data, which, if disclosed, may cause exceptional\ndamage, injury or hardship to individuals. Examples include data relating to offences,\ncriminal proceedings and convictions, and related security measures; biometric, genetic\nand health data; and personal data such as that relating to race, descent, gender, age,\nlanguage, religion, political opinion, national origin, ethnic origin, social origin, economic or\nsocial condition of birth, or disability and any other characteristics.\nh Promote mechanisms, such as open repositories for publicly funded or publicly held data,\nsource code and data trusts, to support the safe, fair, legal and ethical sharing of data,\namong others.\ni Promote and facilitate the use of quality and robust datasets for training, development and\nuse of AI systems, and exercise vigilance in overseeing their collection and use.\nExample: with the help of AI & ML algorithms16 , Synthetic data can be created to enhance\nprivacy. The data created will have the same statistical characteristics for the testing\nenvironment and where a third party can have access.\n16 https://research.aimultiple.com/privacy-enhancing-technologies/\n42 43Microsoft. 2022. Microsoft Responsible AI Standard, v2. General Requirements. June 2022.\n04\nRetrieved from:\nhttps://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4ZPmV\nISO. 2022. Proposed ISO Standard on Risk Management of AI: What Businesses Should Know.\nBibliography\nMay 2022. Retrieved from:\nhttps://unesdoc.unesco.org/ark:/48223/pf0000381137/PDF/381137eng.pdf.multi\nGSMA 2022. The AI Ethics Playbook. Implementing ethical principles into everyday business.\nFebruary 2022. Retrieved from:\nhttps://www.gsma.com/betterfuture/wp-content/uploads/202201//The-Mobile-Industry-Ethics-\nPlaybook_Feb-2022.pdf\nOECD. 2022. AI Policy Observatory. Retrieved from:\nhttps://standards.ieee.org/wp-content/uploads/import/documents/other/ead_v2.pdf\nUNESCO. 2022. Recommendation on the Ethics of Artificial Intelligence. Adopted in November\n2021. Retrieved from:\nhttps://unesdoc.unesco.org/ark:/48223/pf0000381137/PDF/381137eng.pdf.multi\nEuropean Commission. 2021. Proposal for a Regulation of the European Parliament and of the\nCouncil Laying Down Harmonized Rules on Artificial Intelligence (Artificial Intelligence Act) and\nAmending Certain Union Legislative Acts. Retrieved from:\nhttps://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A52021PC0206\nEuropean Commission. High Level Expert Group on Artificial Intelligence ( AI HLEG ). Ethics\nGuidelines for Trustworthy AI . April 2019. Retrieved from:\nhttps://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai\nCentre for Data Ethics and Innovation. UK Government: The roadmap to an effective AI assurance\necosystem. December 2021. Retrieved from:\nhttps://www.gov.uk/government/publications/the-roadmap-to-an-effective-ai-assurance-\necosystem\nThe New York City Council. 2021. A Local Law to amend the administrative code of the city of New\nYork, in relation to automated employment decision tools. Retrieved from:\nhttps://legistar.council.nyc.gov/LegislationDetail.aspx?ID=4344524&GUID=%20B051915D-A9AC-\n451E-81F86596032-FA3F9&\nUNESCO, Recommendation on the Ethics of Artificial Intelligence. 2021. Retrieved from:\nhttps://unesdoc.unesco.org/ark:/48223/pf0000381137\n44 45AI NOW Institute. Algorithmic accountability for the public sector. August 2021. Retrieved from: CNIL. (2017). How can humans keep the upper hand? The ethical matters raised by algorithms\nhttps://www.opengovpartnership.org/documents/algorithmic-accountability-public-sector/ and artificial intelligence. Retrieved from:\nhttps://www.cnil.fr/sites/default/files/atoms/files/cnil_rapport_ai_gb_web.pdf\nIEEE 2019. Ethically Aligned Design. Version 2 \u2013 For Public Discussion. Retrieved from:\nhttps://standards.ieee.org/wp-content/uploads/import/documents/other/ead_v2.pdf Executive Office of the President of the United States. National Science and Technology Council.\nNetworking and Information Technology Research and Development Subcommittee. (2016). The\nnational artificial intelligence research and development strategic plan. Retrieved from:\nIEEE 2017. Ethically Aligned Designed : A vision for Prioritizing Human Well-being with\nhttps://www.nitrd.gov/PUBS/national_ai_rd_strategic_plan.pdf\nAutonomous and Intelligent Systems. Retrieved from:\nhttps://standards.ieee.org/wp-content/uploads/import/documents/other/ead_v2.pdf\nExecutive Office of the President of the United States National Science and Technology Council.\nCommittee on Technology. (2016). Preparing for the future of artificial intelligence. Retrieved from:\nGoogle. AI Principles 2021. Retrieved from:\nhttps://obamawhitehouse.archives.gov/sites/default/files/whitehouse_files/microsites/ostp\nhttps://ai.google/principles/\nNSTC/preparing_for_the_future_of_ai.pdf\nMicrosoft. Responsible AI Standard v2 (June 2022). Retrieved from:\nThe Headquarters for Japan\u2019s Economic Revitalization. (2015). New Robot Strategy. Japan\u2019s\nhttps://www.microsoft.com/en-us/ai/responsible-ai\nRobot Strategy. Vision, Strategy, Action Plan. Retrieved from:\nhttp://www.meti.go.jp/english/press/2015/pdf/0123_01b.pdf\nThe ECONOMIST Group 2022 . Pushing forward: the future of AI in the Middle East and North\nAfrica Report. Retrieved from:\nTreasury Board of Canada Secretariat. (2018). Responsible Artificial Intelligence in the\nhttps://impact.economist.com/perspectives/sites/default/files/google_ai_mena_report.pdf\nGovernment of Canada. Digital Disruption White Paper Series. Version 2.0.\nhttps://docs.google.com/document/d/1Sn-qBZUXEUG4dVk909eSg5qvfbpNlRhzIefWPtBwbxY/\nPDPC. (2018, June 5). Discussion paper on Artificial Intelligence (AI) and Personal Data.\nedit\nSingapore: Personal Data Protection Commission Singapore (PDPC). Retrieved from:\nhttps://www.pdpc.gov.sg/-/media/Files/PDPC/PDF-Files/Resource-for-Organisation/AI/\nThe Toronto Declaration: Protecting the right to equality and non-discrimination in machine\nDiscussion-Paper-on-AI-and-PD---050618.pdf\nlearning systems. Toronto, Canada: Amnesty International and Access Now. Retrieved from:\nhttps://www.accessnow.org/cms/assets/uploads/201808//The-Toronto-Declaration_ENG_08-\nITI. AI Policy Principles. Retrieved from:\n2018.pdf\nhttps://www.itic.org/public-policy/ITIAIPolicyPrinciplesFINAL.pdf\nhttp://data.parliament.uk/writtenevidence/committeeevidence.svc/evidencedocument/artificial-\nCabinet Office.(2016, May 19). Data Science Ethical Framework. Retrieved from: intelligence-committee/artificial-intelligence/oral/73546.html (house of lords select committee\nhttps://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/ oral evidence, Q61)oral evidence, Q61)\nfile/524298/Data_science_ethics_framework_v1.0_for_publication__1_.pdf\nHouse of Lords Select Committee on Artificial Intelligence. (2018). AI in the UK: ready, willing and\nEuropean Parliament. (2017, February 16). European Parliament resolution of 16 February 2017 able?\nwith recommendations to the Commission on Civil Law Rules on Robotics (20152103/(INL)). https://publications.parliament.uk/pa/ld201719/ldselect/ldai/100100/.pdf\nRetrieved from:\nhttp://www.europarl.europa.eu/sides/getDoc.do?pubRef=-//EP//TEXT+TA+P8-TA-2017- House of Commons Science and Technology Select Committee. Algorithms in Decision Making.\n0++0051DOC+XML+V0//EN https://publications.parliament.uk/pa/cm201719/cmselect/cmsctech/351351/.pdf\nVillani, C. (2018). For a meaningful Artificial Intelligence towards a French and European strategy.\nRetrieved from:\nhttps://www.aiforhumanity.fr/pdfs/MissionVillani_Report_ENG-VF.pdf\n46 47", "metadata": {"country": "Saudi Arabia", "year": "2020", "legally_binding": "no", "binding_proof": "None", "date": "10/01/2020", "regulator": "Saudi Data and Artificial Intelligence Authority (SDAIA)", "type": "national strategy", "status": "active", "language": "English, Arabic", "use_cases": "[1, 2, 3]"}}
{"_id": "68695c625520a4cc7989de82", "title": "AI Adoption Framework", "source": "https://sdaia.gov.sa/en/SDAIA/about/Files/AIAdoptionFramework.pdf", "text": "AI ETHICS\nARTIFICIAL INTELLIGENCE ETHICS\nPRINCIPLES & GUIDELINES\nPRINCIPLES & GUIDELINES\nDEC 2022Table of Content\nPage Page\n1. Overview 4 3.4 Explainable 29\n1.1 Introduction 5 3.4.1 Principle 29\nWe will make AI systems as explainable as technically possible\n1.2 Scope 6 3.4.2 Guidelines 29\n1.3 Strategic alignment 6 3.5 Robust, Safe and Secure AI 31\n1.4 Responsibility 8 3.5.1 Principles 31\nAI systems will be technically robust\n3.5.2 Guidelines 32\n1.5 Licensing 8\n3.6 Human Centred AI 34\n1.6 Toolkit 8\n3.6.1 Principles 34\nWe will give AI systems human values and make them beneficial to society\n3.6.2 Guidelines 35\n2. Definitions 10\n3.7 Sustainable and Environmentally Friendly AI 38\n3.7.1 Principle 38\n3. Principles and Guidelines 16\nWe will promote sustainabity and environemntly friendly AI 38\n3.7.2 Guidelines 38\n3.1 Fairness 17\n3.8 Privacy Preserving AI 38\n3.1.1 Principle 17\nWe will make AI systems fair\n3.8.1 Principle 40\n3.1.2 Guidelines 20\nWe will respect people\u2019s privacy\n3.8.2 Guidelines 40\n3.2 Accountable AI 20\n3.2.1 Principle 20\nWe will make AI systems accountable\n4. Bibliography 44\n3.2.2 Guidelines 20\n3.3 Transparent AI 26\n3.3.1 Principle 26\nWe will make AI systems transparent\n3.3.2 Guidelines 26\n2 301 1.1 Introduction\nAI\u2019s rapid advancement and innovation potential across a range of fields is incredibly exciting.\nYet a thorough and open discussion around AI ethics, and the principles organizations using this\nOverview\ntechnology must consider, is needed.\nThis document - AI Ethics: Principles and Guidelines \u2013 intends to meet this need to balance these\ntwo central considerations of AI. It is designed to offer detailed guidance to help AI actors adhere\nto eight principles of AI ethics.\nWe will make AI systems that are:\nexplainable transparent accountable fair\nprivacy preserving sustainable and human centered robust, safe and\nenvironmentally secure\nfriendly\nThe guidelines are non-binding, and are drafted as a collaborative multi-\nstakeholder effort, with full awareness of organizations\u2019 needs to innovate\nand protect their intellectual property. This is a collaborative process in\nwhich all stakeholders are invited to be part of an ongoing dialogue. We\nwould like to see the AI Ethics Guidelines evolve into a universal, practical\nand applicable framework informing ethical requirements for AI design and\nuse. The eventual goal is to reach widespread agreement and adoption of\ncommonly agreed policies to inform the ethical use of AI nationally and\naround the world.\n4 51.2 Scope Additionally, the AI Office is currently developing additional policies to be taken into consideration\nalong with this document:\nThis document gives non-mandatory guidelines for achieving the ethical design\nand deployment of AI systems in both the public and private sectors. AI already\nAI Seal AI Procurement Guidelines\nsurrounds us, but some applications are more visible and sensitive than others.\nThe UAE AI Seal brand (UAI) will be used The AI Procurement Guidelines will be\nThis document is applicable only to those AI systems which make or inform\nto attract talent and business from used to guide Federal Government entities\n\u2018significant decisions\u2019 \u2013 that is, those decisions which have the potential for\nacross the globe to come to the UAE to in the procurement of AI systems. They\nsignificant impact either on individuals or on society as a whole. They also apply to\ntest and develop AI. This includes a UAI will list the general principles, proposed\n\u2018critical decisions\u2019, which are a subset of significant decisions and are of especially\nmark recognizing high quality, ethical AI mechanisms for procurement and review\ncritical nature.\ncompanies. It would reward safe, efficient, of AI products between the federal\nverified AI technology with a \u2018UAI Seal government and vendors. The policy\nThis document guides the further developments of sector specific requirements\nof Approval\u2019. The UAE AI Seal brand is considers R&D for specific challenges\nand principles, hence every entity within its sector can further amend, use, or\ncurrently under development by the AI that do not currently have an AI solution\ncomplement its current frameworks to properly suit its context and the need of\nOffice. product that is market-ready. Based on\naffected stakeholders.\nglobal standards for AI Procurement, the\npolicy aims to incentivize AI adoption\nIt is a living document and will undergo further future reviews and enhancements.\nwithin government entities and promote\nAI R&D within the private sector for\nspecified government challenges. The\nprocurement policy is currently under\ndevelopment by the AI Office.\n1.3 Strategic alignment\nThis document comes as a fulfillment of the goals and aspirations set in the UAE National AI\nStrategy and the international Sustainable Development Goals (SDGs). The UAE has long been\nat the forefront of sustainable development. These shared global milestones have created global\nmovement and adoption in several disciplines and sectors. As an early adopter, the UAE has\nestablished a national committee on SDGs with a long-term realization of the UAE SDG agenda\n2030. These goals are further engrained and reflected in the AI Ethics Principles under the topics of\nfairness, inclusiveness, equality, human benefit, and rights.\n6 71.4 Responsibility\nThe AI Office will not be responsible for any misuse of the AI Ethics Principles and Guidelines. The\nuser bears all the consequences of their use.\n1.5 Licensing\nThis document is published under the terms of a Creative Commons Attribution 4.0 International\nLicense in order to facilitate its re-use by other governments and private sector organizations.\nIn summary this means you are free to share and adapt the material, including for commercial\npurposes, provided that you give appropriate credit to the UAE AI and Blockchain Council as its\nowner and do not suggest the Minister of State for Artificial Intelligence, Digital Economy and\nRemote Work Applications Office endorses your use.\n1.6 Toolkit\nThe document is part of a toolkit of self-governing nature that boosts awareness and enables\ngovernment and private institutions to pursue innovative use cases while maintaining human\nvalues and principles.\n8 92.1 AI developer organization\n02\nAn organization which does any of the following:\na determines the purpose of an AI system;\nDefinitions\nb designs an AI system;\nc builds an AI system, or:\nd performs technical maintenance or tuning on an AI system\nNote:\nThe definition applies regardless of whether the organization is the ultimate user of the system, or\nwhether they sell it on or give it away.\nExample: A company develops an artificially intelligent facial recognition system and\nsells it to a country\u2019s border force, who use it to identify suspicious personnel. The company\nis an AI developer organization and the border force is an AI operator organization.\n2.2 AI operator organization\nAn organization which does any of the following:\na uses AI systems in operations, backroom processes or decision-making;\nb uses an AI system to provide a service to an AI subject;\nc is a business owner of an AI system;\nd procures and treats data for use in an AI system; or\ne evaluates the use case for an AI system and decides whether to proceed.\nNotes:\n1. This definition applies regardless of whether the AI system was developed in-house or procured.\n2. It is possible for organizations to be both an AI developer organization and an AI operator organization.\n10 112.6 Critical decision\n2.3 Artificial Intelligence (also AI)\nThe capability of a functional unit to perform functions that are generally associated with human An individually significant decision which is deemed to either have a very large impact on an\nintelligence such as reasoning, learning and self-improvement1. individual or to have especially high stakes. These can be especially sensitive, have the potential\nto cause high loss or damage, be societally significant, or set an important precedent.\n2.4 Artificially Intelligent System (also AI system)\nNotes:\nThe types of decisions referred to here are the same as those in the definition of significant-at-\nA product, service, process or decision-making methodology whose operation or outcome is\nscale decisions, except in this case the effects are felt as a result of an individual decision rather\nmaterially influenced by artificially intelligent functional units.\nthan an aggregate of many decisions..\nNotes:\n1. It is not necessary for a system\u2019s outcome to be solely determined by artificially intelligent\nfunctional units in order for the system to be defined as an artificially intelligent system; and Example: A court determines whether a defendant is guilty of a criminal charge, with the\n2. A particular feature of AI systems is that they learn behavior and rules not explicitly punishment for guilt being a life sentence. This is a critical decision because it has a very\nprogrammed in. large impact on the life of the defendant and also sets precedent for similar cases in the\nfuture.\nExample 1: A small claims court uses Example 2: A government entity uses\n2.7 Functional Unit\nan artificially intelligent software package a chatbot which allows customers to ask\nto collect evidence pertaining to a case, routine questions, book appointments\nEntity of hardware or software, or both, capable of accomplishing a specified purpose3 .\ncompare it to similar cases in the past, and conduct minor financial transactions.\nand present a recommended decision The chatbot responds to customer\nto a judge. The judge determines the queries with pre-written responses and is\n2.8 Individually significant decision\nfinal outcome. This decision-making based on pre-programmed decision rules.\nA decision which has the potential for significant impact on at least one individual\u2019s\nmethodology is materially influenced Therefore, the chatbot is not an AI system.\ncircumstances, behavior or choices, or has legal or similarly significant effects on him or her.\nby an artificially intelligent functional If, however, the chatbot autonomously\nunit, and is therefore classified as an AI adjusted its treatment of customers\nsystem. based on the outcome of past cases, it\nwould be an AI system. Example: A company decides to make an employee redundant. This is an individually\nsignificant decision because of its potential impact on the employee\u2019s financial situation.\n2.5 Bias (of a system)\nInclination or prejudice for or against one person or group, especially in a way considered to be\nunfair2.\n2 Oxford Dictionary 2018, Oxford University Press, viewed online 4th October 2018, <https://en.oxforddictionaries.com>\n1 Consistent with ISO/IEC 2382:2015 3 From ISO/IEC 2382:2015\n12 132.9 Non-operational bias (of a system) 2.11 Significant decision\nBias that is either: A decision which is either individually significant or is part of a set of significant-at-scale\ndecisions.\na not a design feature; or\nb not important in achieving the stated purpose of the system. 2.12 Subject of an artificially intelligent system (also AI subject)\nA natural person who is any of the following:\n2.10 Set of significant-at-scale decisions\na an end-user of an AI system\nA set of decisions made by the same system or organization which, when taken in aggregate,\nb directly affected by the operation of or outcomes of an AI system, or:\nhave significant impact on society as a whole or groups within it.\nc a recipient of a service or recommendation provided by an AI system\nNotes:\n1. The decisions need not be individually significant in order to qualify, in aggregate, as a set of\nsignificant-at-scale decisions; and\n2. Examples of areas which have a large impact on society and which include but are not limited\nto: the large-scale allocation of resources or opportunities amongst groups; the structure of\ngovernment; the division of power between large entities or groups; the law, and its interpretation\nand enforcement; conflict and war; international relations, etc.\nExample: An AI system is used by a website to determine which content to show users.\nThis decision is not individually significant, since a user is not greatly affected by whether\na particular piece of media is shown to them. However, if the website is popular then the AI\nsystem may be making a set of significant-at-scale decisions, because any biases in the\nsystem will affect a large number of users.\n14 153.1 Fairness (Principle 1)\n03\n3.1.1 Principle\nPRINCIPLES AND We will make AI systems fair\nData ingested should, where possible, be accurate and representative of the affected\npopulation.\nGUIDELINES\nAlgorithms should avoid non-operational bias.\nSteps should be taken to mitigate and disclose the biases inherent in datasets.\nSignificant decisions should be provably fair.\nAll personnel involved in the development, deployment and use of AI systems have a role and\nresponsibility to operationalize AI fairness and should be educated accordingly.\n3.1.2 Guidelines\n3.1.2.1\nBenefits of AI systems should be available and accessible to all. AI systems should not discriminate\nagainst people or groups in a way that could have an adverse impact. Especially significant and\ncritical decisions made or assisted by AI should be provably fair.\na Considering that fairness can have many different definitions, an organization should\ndocument its own definition of fairness for the context that the AI system is going to be\nimplemented in. Organizations should document what the implemented fairness objective\nstands for and why this choice was considered the most suitable for the given scenario.\nb It is recommended to identify and document demographic groups that may be adversely\nimpacted and mitigate the risk where possible.\nc AI developers and AI operators could consider formal procedures such as Discrimination\nImpact Assessments as a means of ensuring fairness. This assessment should be\ndocumented as well as the mitigation measures that were implemented by the organization.\nThe impact assessment should be conducted pre-release and regularly after as an ongoing\nevaluation, results are advised to be documented.\n16 173.1.2.2 3.1.2.3\nConsideration should be given to whether the data ingested is accurate and representative Consideration should be given to whether decision-making processes introduce bias\nof the affected population\na When subjecting different groups to different decision-making processes, AI developers\nd Fairness has many different definitions in different cultures and for different contexts. should consider whether this will lead to non-operational bias.\nEncouraging a diverse and inclusive AI ecosystem is thus all the more crucial to ensure\nthat one definition of fairness does not contradict another, and that the process of defining b When evaluating the fairness of an AI system, AI developers and AI operators should\nfairness itself is fair, with under-represented groups represented in the discussion. consider whether AI subjects in the same circumstances receive equal treatment.\ne AI developers and AI operators should undertake reasonable data exploration and/or testing\nto identify potentially prejudicial decision-making tendencies in AI systems arising from\nExample: An organization uses an AI tool to automate the pre-screening of candidates\ndata inaccuracy and biases in the data.\nfor a job opening. It is trained on data from the company\u2019s existing employees, the majority\nof whom are from the same ethnic background. Therefore, the system learns to use name\nf AI developers and operators should evaluate all datasets to assess inclusiveness of and nationality as discriminating factors in filtering job applicants. This could have been\nidentified demographic groups and collect data to close any gaps.\nidentified through testing and rectified by, for example, balancing the training data or only\nusing relevant data fields for training.\ng AI developers and AI operators should refrain from training AI systems on data that is not\nlikely to be representative of the affected AI subjects, or is not likely to be accurate, whether\nthat be due to age, omission, method of collection, or other factors.\n3.1.2.4\nSignificant decisions informed by the use of AI should be fair\nExample: Following a natural disaster, a government relief agency uses an AI system to a AI developers and AI operators could consider formal procedures such as Discrimination\ndetect communities in greatest need by analyzing social media data from a range of websites. Impact Assessments as a means of ensuring fairness.\nHowever, those communities where smartphone penetration is lower having less presence on\n3.1.2.4\nsocial media, and so are at risk of receiving less attention. Therefore, the charity complements\nAI operators should consider whether their AI systems are accessible and usable in a fair\ntheir AI tool with traditional techniques to identify needy populations elsewhere.\nmanner across user groups\n3.1.2.5\nh AI developers should consider whether their AI systems can be expected to perform well Consideration should be given to the effect of diversity on the development and deployment\nwhen exposed to previously unseen data, especially when evaluating people who are not processes\nwell-represented in the training data.\nb Organizations should seek to include people from diverse demographic backgrounds across\nthe full lifecyle, to include design, development and deployment processes. Organizations\nshould seek to engage diverse internal and external groups also.\nc AI developers should consider whether the assumptions they make about AI subjects could\nbe wrong or are likely to lead to non-operational bias; if so, they should consider consulting\nthe AI subjects in a representative manner during the design, development and deployment\nto confirm these assumptions.\n18 193.2 Accountable AI (Principle 2)\n3.2.2.2\nPositive efforts should be made to identify and mitigate any significant risks inherent in the AI\nsystems designed\n3.2.1 Principle\na AI operators should only use AI systems that are backed by evidence-based academic or\nindustrial research, and AI developers should base their development on such research.\nWe will make AI systems accountable\nAccountability for the outcomes of an AI system lies not with the system itself but is\napportioned between those who design, develop and deploy it; Example: A foreign country has a government service which identifies parents who owe\nmoney in child maintenance. The data matching process is often incorrect due to misspelled\nDevelopers should make efforts to mitigate the risks inherent in the systems they design; names or missing data which results in some individuals being incorrectly targeted\nautomatically by the system with the result being a large bill, poor credit ratings and even\nAI systems should have built-in appeals procedures whereby users can challenge significant\nfreezing wages. The recourse for individuals who are incorrectly targeted is time-consuming\ndecisions;\nand not straightforward . If the potential impact of incorrect decisions had been assessed,\nmitigation measures such a user-friendly review procedure could have been set up.\nAI systems should be developed by diverse teams which include experts in the area in which\nthe system will be deployed; and\nAI systems should be subject to external audit and decision quality assurance. b AI operators should identify the likely impact of incorrect automated decisions on AI\nsubjects and, in the case where incorrect decisions are likely to cause significant cost or\ninconvenience, consider mitigating measures.\n3.2.2 Guidelines\nc AI operators should consider internal risk assessments or ethics frameworks as a means to\n3.2.2.1 facilitate the identification of risks and mitigating measures.\nAccountability for the outcomes of an AI system should not lie with the system itself\nd In designing AI systems to inform significant decisions, AI developers should consider\na Accountability for loss or damages resulting from the application of AI systems should not measures to maintain data accuracy over time, including:\nbe attributed to the system itself.\n\u2022 the completeness of the data.\nb AI operators and AI developers should consider designating individuals to be responsible for \u2022 timely update of the data, and.\ninvestigating and rectifying the cause of loss or damage arising from the deployment of AI\n\u2022 whether the context in which the data was collected affects its suitability for the\nsystems.\nintended use case.\nExample: A border camera scanning for predictors of risk may misinterpret a \u201ctic\u201d of an\nindividual with Tourette syndrome as suspicious. These can manifest in a diverse fashion, and\nshould not cause this person to undergo secondary inspection every time they pass through\nthe border . If the data is updated after the first case is encountered then it would avoid\ncausing inconvenience on subsequent visits.\n5 Government of Canada. (2018). Responsible AI in the Government of Canada. Digital Disruption White Paper Series. Version 2.0, p.26. Retrieved from:\nhttps://docs.google.com/document/d/1Sn-qBZUXEUG4dVk909eSg5qvfbpNlRhzIefWPtBwbxY/edit\n4 Cabinet Office (UK), Data Science Ethical Framework, Version 1.0, licensed under the Open Government License v3.0, p. 13 6 Stoica, I. et. al.,2017, A Berkeley View of Systems Challenges for AI, p. 2, https://arxiv.org/pdf/1712.05855.pdf\n20 21e AI developers and AI operators should tune AI models periodically to cater for changes to e Be mindful that there might be fundamental tensions between different principles and\ndata and/or models over time. requirements. Continuously identify, evaluate, document and communicate these trade-\noffs and their solutions.\nf AI operators should consider whether AI systems trained in a static environment will display\nmodel instability when deployed in dynamic environments. Adopt a Trustworthy AI assessment list when developing, deploying or using AI systems,\nf\nand adapt it to the specific use case in which the system is being applied.\nExample: AI systems will need to be able to adapt to the changes in the environment that g Keep in mind that such an assessment list will never be exhaustive. Ensuring Trustworthy\nthey are deployed in. For example, a self-driving car would need to quickly adapt to unexpected AI is not about ticking boxes, but about continuously identifying and implementing\nand dangerous road by learning in real time from other cars that have successfully dealt with requirements, evaluating solutions, ensuring improved outcomes throughout the AI\nthese conditions. In addition, such mission-critical applications must handle noisy inputs system\u2019s lifecycle, and involving stakeholders in this.\nand defend against malicious actors .\n3.2.2.4\nAI subjects should be able to challenge significant automated decisions concerning them and,\ng AI developers should collaborate with AI Operators to train models using historical data where appropriate, be able to opt out of such decisions\nfrom the Operator.\na AI operators using AI systems to inform significant decisions should provide procedures by\nwhich affected AI subjects can challenge a specific decision concerning them.\nh AI Operators should consider working with their vendors (AI developers) to continually\nmonitor performance.\nb AI operators should consider such procedures even for non-significant decisions.\ni AI Operators should subject AI systems informing significant decisions to quality checks\nat least as stringent as those that would be required of a human being taking the same c AI operators should make affected AI subjects aware of these procedures and should\ndesign them to be convenient and user-friendly.\ndecision.\nd AI operators should consider employing human case evaluators to review any such\n3.2.2.3\nchallenges and, when appropriate, overturn the challenged decision.\nAI systems informing critical decisions should be subject to appropriate external audit\na When AI systems are used for critical decisions, external auditing of the AI systems in\nquestion should be used as a means to ensure meaningful standards of transparency and Example: A bank allows customers to apply for a loan online by entering their data. The\naccountability are upheld. bank uses an AI system to automatically determine whether to give the loan and what the\ninterest rate should be. They provide users with an option to contest the decision and have\nb In the case that critical decisions are of civic interest, public release of the results of the it reviewed by a human . They also require that customers justify their challenge by filling in\naudit should be considered as a means of ensuring public processes remain accountable a form, which assists the case reviewer and deters customers from challenging a decision\nto those affected by them. without good reason.7\nIn the case that critical decision are life and death decisions, these decisions should be\nc\nsupported by further validation and verification via a human operator.\nd Facilitate traceability and auditability of AI systems, particularly in critical contexts or\nsituations.\n7 Adapted from EU Commission, Can I be subject to automated individual decision-making, including profiling? Retrieved from: https://ec.europa.eu/\ninfo/law/law-topic/data-protection/reform/rights-citizens/my-rights/can-i-be-subject-automated-individual-decision-making-including-profiling_\nen#example\n22 23e AI operators should consider instituting an opt-out mechanism for significant automated 3.2.2.7\ndecisions. AI operators should understand the AI systems they use sufficiently to assess their suitability\nfor the use case and to ensure accountability and transparency\nf AI operators could consider \u201ccrowd challenge\u201d mechanisms whereby a critical number of\ncomplaints triggers an investigation into the fairness and/or accuracy of a decision-making\na In the case of critical decisions, AI operators should avoid using AI systems that cannot be\nprocess as a whole.\nsubjected to meaningful standards of accountability and transparency.\n3.2.2.5 b AI developers should consider notifying customers and AI operators of the use cases for\nAI systems informing significant decisions should not attempt to make value judgements on which the system has been designed, and those for which it is not suitable.\npeople\u2019s behalf without prior consent\na When informing an AI subject about significant choices they will make, AI systems should\nnot unreasonably restrict the available options or otherwise attempt to influence their value\njudgements without the explicit consent of the AI subject in question.\n3.2.2.6\nAI systems informing significant decisions should be developed by diverse teams with\nappropriate backgrounds\na AI developers who develop AI systems which may be used to assist in making critical\ndecisions should involve in the process experts with a background in social science, policy,\nor another subject which prepares them to evaluate the broader societal impact of their\nwork.\nb Development of AI systems informing significant decisions should include consultation\nwith experts in the field in which the system will be deployed.\nExample: An app that uses AI to assess medical symptoms and has a large user base had\nto face regulatory scrutiny because of number of complaints from doctors. They warned\nthat the application can miss signs of serious illness. A number of different shortcomings\nwere identified by doctors, some of which the company could address and resolve.8\n8 Financial Times. 2018. High-profile health app under scrutiny after doctor\u2019s complaints. Retrieved from: https://www.ft.com/content/19dc6b7e-8529-\n11e896-dd-fa565ec55929\n24 253.3 Transparent AI (Principle 3)\nc To facilitate the above, AI developers and AI operators should consider documenting the\nfollowing information during the design, development and deployment phases, and retaining\n3.3.1 Principle this documentation for a length of time appropriate to the decision type or industry:\n\u2022 the provenance of the training data, the methods of collection and treatment, how the data was\nWe will make AI systems transparent\nmoved, and measures taken to maintain its accuracy over time;\n\u2022 the model design and algorithms employed; and\nDevelopers should build systems whose failures can be traced and diagnosed. \u2022 changes to the codebase, and authorship of those changes.\nPeople should be told when significant decisions about them are being made by AI. d Where possible given the model design, AI developers should consider building in a means\nby which the \u201cdecision journey\u201d of a specific outcome (i.e. the component decisions leading\nWithin the limits of privacy and the preservation of intellectual property, those who deploy to it) can be logged.\nAI systems should be transparent about the data and algorithms they use.\nResponsible disclosures should be provided in a timely manner, and provide reasonable\nExample: A technology company has a product which is designed to assist in medical\njustifications for AI systems outcomes. This includes information that helps people\ndiagnosis. It documents each stage of its reasoning and relates it back to the input data.9\nunderstand outcomes, like key factors used in decision making.\n3.3.2 Guidelines\n3.3.2.1\nTraceability should be considered for significant decisions, especially those that have the\npotential to result in loss, harm or damage.\na For AI systems which inform significant decisions, especially those with the potential to\ncause loss, harm or damage, AI developers should consider building in traceability (i.e. the\nability to trace the key factors leading to any specific decision).\nb Organizations should ensure that harms caused through AI systems are investigated and\nredressed, by enacting strong enforcement mechanisms and remedial actions, to make\ncertain that human rights and the rule of law are respected in the digital world and in the\nphysical world.\n9 See IBM WatsonPaths\n26 273.3.2.2 3.4 Explainable AI (Principle 4)\nPeople should be informed of the extent of their interaction with AI systems\n3.4.1 Principle\na AI operators should inform AI subjects when a significant decision affecting them has been\nmade by an AI system.\nWe will make AI systems as explainable as\ntechnically possible\nExample: A small claims court adjudicates minor civil matters such as debt collection and\nevictions. They introduce an AI system to suggest the outcome of a ruling. At the time of the\nDecisions and methodologies of AI systems which have a significant effect on individuals\nruling the plaintiff and defendant are notified that the decision was assisted by an AI system.\nshould be explainable to them, to the extent permitted by available technology.\nThe court also provides an explanation for the decision.\nIt should be possible to ascertain the key factors leading to any specific decision that\ncould have a significant effect on an individual.\nb If an AI system can convincingly impersonate a human being, it should do so only after\nIn the above situation we will provide channels through which people can request such\nnotifying the AI subject that it is an AI system.\nexplanations.\nExample: A technology company produces a conversational AI agent which can make 3.4.2 Guidelines\nsome phone calls on behalf of its users. Those who receive the calls may believe that they\n3.4.2.1\nare speaking to a human. Therefore, the company programs the agent to identify itself at\nAI operators could consider providing affected AI subjects with a high-level explanation of\nthe start of every conversation.\nhow their AI system works\na AI operators could consider informing the affected AI subjects in understandable, non-\ntechnical language of:\n\u2022 the data that is ingested by the system;\n\u2022 the types of algorithms employed;\n\u2022 the categories into which people can be placed; and\n\u2022 the most important features driving the outcomes of decisions\n\u2022 A comprehensive list of feature engineering and models that was considered during\nthe model building phase.\nExample: A person turned down for a credit card might be told that the algorithm took\ntheir credit history, age, and postcode into account, but not learn why their application was\nrejected10 .\n28 29b For non-sensitive public sector use cases designed for the common good, AI operators\ncould consider making source code, together with an explanation of the workings of the AI Example: The UK\u2019s NHS developed a tool called Predict, which allows women with breast\nsystem, available either publicly or upon request (this should be done only if there is low risk cancer to compare their case to other women who have had the same condition in the past,\nof people \u2018gaming the system\u2019). and visualize the expected survival rate under various treatment options. The website has an\nexplanation page which shows the weights behind various factors and contains a description\nc The AI operator should maintain necessary documentation that provides elaboration and of the underlying mathematics12 .\nclarification on how the algorithm works, for example documentation of processes, decision\nmaking flow charts of the system, and how the appeal process is embedded.\nd An individual should have the ability to contest and seek effective redress against decisions c In the case that such explanations are available, they should be easily and quickly accessible,\nmade by AI systems. These must be addressed by the group or team supporting these free of charge and user-friendly.\nmodels.\ne Develop appropriate impact indices for the evaluation of AI system technological 3.5 Robust, Safe and Secure AI (Principle 5)\ninterventions from multiple perspectives.\n3.5.1 Principles\n3.4.2.2\nAI operators should consider providing affected AI subjects with a means to request\nexplanations for specific significant decisions, to the extent possible given the state of present AI systems will be technically robust\nresearch and the choice of model\nAI systems should be technically robust with a preventative approach to risks which\na AI operators should consider providing a means by which people affected by a significant\noperates in a manner such that they reliably behave.\ndecision informed by AI can access the reasoning behind that decision.\nAI Developers should ensure AI systems will not cause any unintentional harm and\nadverse impacts.\nExample: The US Consumer Financial Protection Bureau requires that creditors in the US\nwho reject credit applications must explain to the applicant the principal reason(s) why their\nAI systems should be resilient to attacks and security such as data poisoning and\napplication was rejected (e.g. \u201clength of residence\u201d or \u201cage of automobile\u201d)11 . In particular,\nmodel leakage.\n\u201cstatements that the adverse action was based on the creditor`s internal standards or\npolicies or that the applicant, joint applicant, or similar party failed to achieve a qualifying AI systems should have safeguards that enable a fallback plan in case of problems.\nscore on the creditor`s credit scoring system are insufficient\u201d.\nAI system results should be reproducible.\nb Where such explainability is not possible given available technology, AI operators should\nconsider compromises such as counterfactual reasoning, or listing the most heavily\nweighted factors contributing to the decision.\n11 Consumer Financial Protection Bureau, 12 CFR Part 1002 - Equal Credit Opportunity Act (Regulation B), \u00a7 1002.9 Notifications, Retrieved from https://www.\n10 The Guardian. AI watchdog needed to regulate automated decision-making, say experts. Retrieved from: https://www.theguardian.com/technology/2017/ consumerfinance.gov/policy-compliance/rulemaking/regulations/1002/\njan/27/ai-artificial-intelligence-watchdog-needed-to-prevent-discriminatory-automated-decisions 12 Predict website, accessible at http://www.predict.nhs.uk/predict_v2.1/legal/algorithm\n30 313.5.2 Guidelines\n3.5.2.1 3.5.2.3\nAI Operators should continue conducting vulnerability assessments, verification of AI system AI Operators should assure end users of the system\u2019s reliability through documentation, and\ndifferent behaviors in unexpected situations and for any dual-use case, to include: operationalizing processes for testing and verification of desired outcomes , specifically:\na Putting measures to ensure integrity and resilience of IA system against potential attacks. a AI system should be designed with an approach to continue monitoring if the system meets\ngoals, purposes and intended application.\nb AI Operators should define an approach to ensure results are reproducible through clear\nExample: IBM researchers presented a novel approach to exploiting AI for malicious\nprocess and documentation.\npurposes. Their system, DeepLocker embeds AI capabilities within the malware itself in order\nto improve its evasion techniques. It uses an Artificial Intelligence model to identify its target\nc AI Operators should publish documentation to assure system robustness to end users.\nusing indicators like facial recognition, geolocation and voice recognition.\n3.5.2.4\nAI systems will be safe, secure and controllable by humans\nb Verifying how AI systems behave in unexpected situations and environments.\na Safety and security of people, be they operators, end-users or other parties, will be of\nc Taking preventative measures against any possible system dual-case use. paramount concern in the design of any AI system.\nb AI systems should be verifiably secure and controllable throughout their operational\n3.5.2.2\nlifetime, to the extent permitted by technology.\nAI operators should define a suitable fall back plan and test it to maintain readiness in\nunexpected situations and in high safety risks level, to include:\nc The continued security and privacy of users should be considered when decommissioning\na Developing a plan to measure and assess potential safety risks to you or any third party AI systems.\nfrom technology use accidental or malicious misuse.\nd AI systems that may directly impact people\u2019s lives in a significant way should receive\nb Define thresholds for system acceptable results and governance procure to fall back on commensurate care in their designs.\nalternative defined and tested plans.\ne Such systems should be able to be overridden or their decisions reversed by designated\nc Considering an insurance policy to mitigate risks arising from potential damages. individuals.\nExample: Adopting a risk-based approach to procurement and clearly communicating it to\nvendors can help address such issues by giving the procuring organization advance notice\nof the specific oversight capabilities it will need in future stages of the system lifecycle,\npreventing vendors from presenting intellectual property arguments against required testing,\nmonitoring and auditing of their AI systems going forward and rewarding vendors\u2014of all\nsizes\u2014that are more advanced and responsive in their responsible AI efforts.\n13 IBM. https://documents.trendmicro.com/assets/white_papers/wp-malicious-uses-and-abuses-of-artificial-intelligence.pdf\n32 333.6.2 Guidelines\n3.5.2.5\nAi systems should not be able to autonomously hurt, destroy or deceive humans\n3.6.2.1\na AI systems should be built to serve and inform, and not to deceive and manipulate.\nAI should be developed to align with human values, contribute to human flourishing and\nbenefit society.\nb Nations should collaborate to avoid an arms race in lethal autonomous weapons, and\nsuch weapons should be tightly controlled. a Society should be consulted in a representative fashion to inform the development of AI.\nc Active cooperation should be pursued to avoid corner-cutting on safety standards. b Stakeholders throughout society should be involved in the development of AI and its\ngovernance.\nSystems designed to inform significant decisions should do so impartially.\nd\nc Stakeholders should proactively engage in responsible stewardship of trustworthy AI in\npursuit of beneficial outcomes for people and the planet.\n3.6 Human Centered AI (Principle 6)\nd Organizations should prioritize having all their stakeholders learn about wellbeing metrics\nas a potential determinant of how they create, deploy, market and monitor their AI\n3.6.1 Principles technologies.\nWe will give AI systems human values and\nExample: IEEE P7010 recommended practice14 establishes wellbeing metrics relating to\nmake them beneficial to society human factors directly affected by intelligent and autonomous systems and establishes a\nbaseline for the types of objective and subjective data these systems should analyze and\nGovernment will support the research of the beneficial use of AI. include (in their programming and functioning) to proactively increase human wellbeing.\nStakeholders throughout society should be involved in the development of AI and its\ngovernance.\n3.6.2.2\nSystematic analyses that examine the ethics of designing affective systems to nudge human\nDesign AI systems to adopt, learn and follow the norms and values of the community\nbeings prior to deployment are needed.\nthey serve.\nSystematic analyses that examine the ethics of designing affective systems to nudge a Nudging in AI systems should have an opt-in system policy with explicit consent.\nhuman beings prior to deployment are needed.\nb We recommend that when appropriate, an affective system that nudges human beings\nDecisions related to lethal force, life and death should not be delegated to AI systems. should have the ability to accurately distinguish between users, including detecting\nRules and standards should be adopted to ensure effective human control over those characteristics such as whether the user is an adult or a child. Additional protections must\ndecisions. be put in place for vulnerable populations, such as children, when informed consent cannot\nbe obtained, or when it may not be a sufficient safeguard.\nc AI systems with nudging strategies must be carefully evaluated, monitored, and controlled.\n14 IEEE. https://sagroups.ieee.org/7010/\n34 353.6.2.3 f In situations where it is needed, human rights impact assessments and human rights due\nHumanity should retain the power to govern itself and make the final decision, with AI in an diligence, human determination codes of ethical conduct, or quality labels and certifications\nassisting role. intended to promote human centered values and fairness should be considered.\ng Mechanisms should be put into place to receive external feedback regarding AI systems\na Decisions related to lethal force, life and death should not be delegated to AI systems. Rules\nthat potentially infringe on fundamental rights.\nand standards should be adopted to ensure effective human control over those decisions.\nb Responsible parties (e.g., parents, nurse practitioners, social workers, and governments)\nshould be trained to detect the influence due to AI and ineffective mitigation techniques.\nIn the most extreme cases it should always be possible to shut down harmful AI system.\nc Those actions undertaken by an affective system that are most likely to generate an\nemotional response should be designed to be easily changed.\nd Users should be able to make informed autonomous decisions regarding AI systems. They\nshould be given appropriate knowledge and tools to comprehend and interact with AI\nsystems to satisfactory degree and, where possible to reasonably self-assess or challenge\nthe system.\n3.6.2.4\nAI systems should be designed in a way that respects the rule of law, human rights, society\nvalues, and should include appropriate safeguards to ensure a fair and just society.\na AI Developers should design AI systems to adopt, learn and follow the norms and values of\nthe community they serve.\nb Organizations could identify the norms of the specific community in which AI systems are\nto be deployed in. In particular, pay attention to the norms relevant to the kinds of tasks that\nthe AI systems are designed to perform. These could be documented as well as how these\nnorms are addressed by the AI system.\nc To respond to the dynamic change of norms in society the AI system could be able to adjust\nits existing norms and learn new ones, while being transparent about these changes.\nd Designers should consider forms and metrics for assessing an AI system\u2019s norm conformity\nover the lifetime of the system (e.g. human-machine agreement on moral decisions,\nverifiability of AI decisions, justified trust).\ne The norm identification process must document the similarities and differences between\nthe norms that humans apply to other humans and the norms they apply to AI systems.\nNorm implementations should be evaluated specifically against the norms that the\ncommunity expects the AI system to follow.\n36 373.7 Sustainable and Environmentally Friendly AI (Principle 7)\nd While there is no standardized means of reporting on the environmental impact of AI\nsystems, where possible, model development and commissioning should make carbon\n3.7.1 Principle impacts a core consideration alongside functional and business requirements.\ne Smaller models should be considered \u2013 shrinking down the model size and using fewer\nWe will promote sustainable and environmentally compute cycles (to balance financial and performance costs with the end performance of\nthe model).\nfriendly AI\nf Carbon awareness should be considered \u2013 adjusting the operational parameters of the AI\nAI systems should be used to benefit all human beings, including future generations. The\nsystem to dynamically select the best time and location for energy use from the grid can\nenvironment is fundamental to this.\nreduce its carbon footprint.\nThroughout the AI lifecycle, implementations should only be carried out on the basis of a full\nunderstanding and acknowledgement of implications for sustainability and environment.\nExample: In agriculture, AI can transform Example: Google`s DeepMind division\nAn AI system\u2019s development, deployment and use, should be assessed via critical examination production by better monitoring and has developed AI that teaches itself to\nof resource usage and energy consumption.\nmanaging environmental conditions minimize the use of energy to cool Google`s\nand crop yields. AI can help reduce both data centers. As a result, Google reduced\nMechanisms to measure environmental impact due to type of energy use and processing\nfertilizer and water, all while improving its data center energy requirements by\npower provided by data centers should be established.\ncrop yields. 40% 15.\n3.7.2 Guidelines\n3.7.2.1 Throughout the AI lifecycle, implementations should always be carried out after full\nunderstanding and acknowledgement of AI implications on sustainability and environment.\na The application of Artificial Intelligence to address sustainability challenges are well\nunderstood. Building and running green, sustainable AI systems is of increasing importance,\ngiven the large carbon footprint that they can generate and the wider context of addressing\nclimate change.\nb AI model development, and by extension commissioning, should therefore seek to balance\ntechnical performance with energy consumption and environmental impact.\nc Efforts should be made to estimate and understand sustainability and environmental\nimpact across the AI lifecycle (e.g. the energy consumption costs associated with model\ntraining, and CO2 emissions and cloud compute costs associated with the deployment\nand running of the system).\n15 https://www.deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-by-40\n38 393.8 Privacy Preserving AI (Principle 8)\n3.8.2.2\nEstablish an oversight mechanism for data collection, storage and processing and use across\nyour organization\n3.8.1 Principle\na Assess who can access data and under which conditions.\nWe will respect people\u2019s privacy\nb Assign specific responsibilities and role for Data Protection Officers.\nAI systems should respect privacy and use the minimum intrusion necessary.\nc Prevention of harm to privacy necessitates adequate data governance that covers the\nquality and integrity of the data used, its relevance in light of the domain in which the\nAI systems should uphold high standards of data governance and security,\nprotecting personal information. AI systems will be deployed, its access protocols and the capability to process data in a\nmanner that protects privacy.\nSurveillance or other AI-driven technologies should not be deployed to the extent\nof violating internationally and/or UAE\u2019s accepted standards of privacy and human d AI systems must guarantee privacy and data protection throughout a system\u2019s entire\ndignity and people rights. lifecycle. This includes the information initially provided by the user, as well as the\ninformation generated about the user over the course of their interaction with the system\nPrivacy by design should be embedded in AI systems and where possible AI\n(e.g. outputs that the AI systems generated for specific users or how users responded to\nalgorithm should have adequate privacy impact assessments.\nparticular recommendations). Digital records of human behavior may allow AI systems\nto infer not only individuals\u2019 preferences, but also their age, gender, religious or political\nAdequate data protection frameworks and governance mechanisms should be\nviews. To allow individuals to trust the data gathering process, it must be ensured that data\nestablished in a multi-stakeholder approach and ensured throughout the life cycle\ncollected about them will not be used to unlawfully or unfairly discriminate against them.\nof AI systems.\ne In any given organization that handles individuals\u2019 data (whether someone is a user of the\nAI developers and operators should strive to strike the balance between privacy\nsystem or not), data protocols governing data access should be put in place in line with\nrequirements, individual rights and innovation growth and society benefits.\nnational privacy legislation (be these universal or sector-specific). These protocols should\noutline who can access data and under which circumstances. Only duly qualified personnel\n3.8.2 Guidelines\nwith the competence and need to access individual\u2019s data should be allowed to do so.\n3.8.2.1\nEstablish mechanism for users to flag issues related to privacy or data protection\na AI Operators should review the system for proper consent logging, ability of users to revoke\npermission whenever applicable.\nb Consider training AI model without or with minimal user of potentially sensitive or personal\ndata.\nc Use measures to enhance privacy such as encryption, anonymization and aggregation.\n40 41f Algorithmic systems require adequate privacy impact assessments, which also include\nsocietal and ethical considerations of their use and an innovative use of the privacy by\ndesign approach. AI actors need to ensure that they are accountable for the design and\nimplementation of AI systems in such a way as to ensure that personal information is\nprotected throughout the life cycle of the AI system.\ng Establish data policies or equivalent frameworks, or reinforce existing ones, to ensure full\nsecurity for personal data and sensitive data, which, if disclosed, may cause exceptional\ndamage, injury or hardship to individuals. Examples include data relating to offences,\ncriminal proceedings and convictions, and related security measures; biometric, genetic\nand health data; and personal data such as that relating to race, descent, gender, age,\nlanguage, religion, political opinion, national origin, ethnic origin, social origin, economic or\nsocial condition of birth, or disability and any other characteristics.\nh Promote mechanisms, such as open repositories for publicly funded or publicly held data,\nsource code and data trusts, to support the safe, fair, legal and ethical sharing of data,\namong others.\ni Promote and facilitate the use of quality and robust datasets for training, development and\nuse of AI systems, and exercise vigilance in overseeing their collection and use.\nExample: with the help of AI & ML algorithms16 , Synthetic data can be created to enhance\nprivacy. The data created will have the same statistical characteristics for the testing\nenvironment and where a third party can have access.\n16 https://research.aimultiple.com/privacy-enhancing-technologies/\n42 43Microsoft. 2022. Microsoft Responsible AI Standard, v2. General Requirements. June 2022.\n04\nRetrieved from:\nhttps://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4ZPmV\nISO. 2022. Proposed ISO Standard on Risk Management of AI: What Businesses Should Know.\nBibliography\nMay 2022. Retrieved from:\nhttps://unesdoc.unesco.org/ark:/48223/pf0000381137/PDF/381137eng.pdf.multi\nGSMA 2022. The AI Ethics Playbook. Implementing ethical principles into everyday business.\nFebruary 2022. Retrieved from:\nhttps://www.gsma.com/betterfuture/wp-content/uploads/202201//The-Mobile-Industry-Ethics-\nPlaybook_Feb-2022.pdf\nOECD. 2022. AI Policy Observatory. Retrieved from:\nhttps://standards.ieee.org/wp-content/uploads/import/documents/other/ead_v2.pdf\nUNESCO. 2022. Recommendation on the Ethics of Artificial Intelligence. Adopted in November\n2021. Retrieved from:\nhttps://unesdoc.unesco.org/ark:/48223/pf0000381137/PDF/381137eng.pdf.multi\nEuropean Commission. 2021. Proposal for a Regulation of the European Parliament and of the\nCouncil Laying Down Harmonized Rules on Artificial Intelligence (Artificial Intelligence Act) and\nAmending Certain Union Legislative Acts. Retrieved from:\nhttps://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A52021PC0206\nEuropean Commission. High Level Expert Group on Artificial Intelligence ( AI HLEG ). Ethics\nGuidelines for Trustworthy AI . April 2019. Retrieved from:\nhttps://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai\nCentre for Data Ethics and Innovation. UK Government: The roadmap to an effective AI assurance\necosystem. December 2021. Retrieved from:\nhttps://www.gov.uk/government/publications/the-roadmap-to-an-effective-ai-assurance-\necosystem\nThe New York City Council. 2021. A Local Law to amend the administrative code of the city of New\nYork, in relation to automated employment decision tools. Retrieved from:\nhttps://legistar.council.nyc.gov/LegislationDetail.aspx?ID=4344524&GUID=%20B051915D-A9AC-\n451E-81F86596032-FA3F9&\nUNESCO, Recommendation on the Ethics of Artificial Intelligence. 2021. Retrieved from:\nhttps://unesdoc.unesco.org/ark:/48223/pf0000381137\n44 45AI NOW Institute. Algorithmic accountability for the public sector. August 2021. Retrieved from: CNIL. (2017). How can humans keep the upper hand? The ethical matters raised by algorithms\nhttps://www.opengovpartnership.org/documents/algorithmic-accountability-public-sector/ and artificial intelligence. Retrieved from:\nhttps://www.cnil.fr/sites/default/files/atoms/files/cnil_rapport_ai_gb_web.pdf\nIEEE 2019. Ethically Aligned Design. Version 2 \u2013 For Public Discussion. Retrieved from:\nhttps://standards.ieee.org/wp-content/uploads/import/documents/other/ead_v2.pdf Executive Office of the President of the United States. National Science and Technology Council.\nNetworking and Information Technology Research and Development Subcommittee. (2016). The\nnational artificial intelligence research and development strategic plan. Retrieved from:\nIEEE 2017. Ethically Aligned Designed : A vision for Prioritizing Human Well-being with\nhttps://www.nitrd.gov/PUBS/national_ai_rd_strategic_plan.pdf\nAutonomous and Intelligent Systems. Retrieved from:\nhttps://standards.ieee.org/wp-content/uploads/import/documents/other/ead_v2.pdf\nExecutive Office of the President of the United States National Science and Technology Council.\nCommittee on Technology. (2016). Preparing for the future of artificial intelligence. Retrieved from:\nGoogle. AI Principles 2021. Retrieved from:\nhttps://obamawhitehouse.archives.gov/sites/default/files/whitehouse_files/microsites/ostp\nhttps://ai.google/principles/\nNSTC/preparing_for_the_future_of_ai.pdf\nMicrosoft. Responsible AI Standard v2 (June 2022). Retrieved from:\nThe Headquarters for Japan\u2019s Economic Revitalization. (2015). New Robot Strategy. Japan\u2019s\nhttps://www.microsoft.com/en-us/ai/responsible-ai\nRobot Strategy. Vision, Strategy, Action Plan. Retrieved from:\nhttp://www.meti.go.jp/english/press/2015/pdf/0123_01b.pdf\nThe ECONOMIST Group 2022 . Pushing forward: the future of AI in the Middle East and North\nAfrica Report. Retrieved from:\nTreasury Board of Canada Secretariat. (2018). Responsible Artificial Intelligence in the\nhttps://impact.economist.com/perspectives/sites/default/files/google_ai_mena_report.pdf\nGovernment of Canada. Digital Disruption White Paper Series. Version 2.0.\nhttps://docs.google.com/document/d/1Sn-qBZUXEUG4dVk909eSg5qvfbpNlRhzIefWPtBwbxY/\nPDPC. (2018, June 5). Discussion paper on Artificial Intelligence (AI) and Personal Data.\nedit\nSingapore: Personal Data Protection Commission Singapore (PDPC). Retrieved from:\nhttps://www.pdpc.gov.sg/-/media/Files/PDPC/PDF-Files/Resource-for-Organisation/AI/\nThe Toronto Declaration: Protecting the right to equality and non-discrimination in machine\nDiscussion-Paper-on-AI-and-PD---050618.pdf\nlearning systems. Toronto, Canada: Amnesty International and Access Now. Retrieved from:\nhttps://www.accessnow.org/cms/assets/uploads/201808//The-Toronto-Declaration_ENG_08-\nITI. AI Policy Principles. Retrieved from:\n2018.pdf\nhttps://www.itic.org/public-policy/ITIAIPolicyPrinciplesFINAL.pdf\nhttp://data.parliament.uk/writtenevidence/committeeevidence.svc/evidencedocument/artificial-\nCabinet Office.(2016, May 19). Data Science Ethical Framework. Retrieved from: intelligence-committee/artificial-intelligence/oral/73546.html (house of lords select committee\nhttps://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/ oral evidence, Q61)oral evidence, Q61)\nfile/524298/Data_science_ethics_framework_v1.0_for_publication__1_.pdf\nHouse of Lords Select Committee on Artificial Intelligence. (2018). AI in the UK: ready, willing and\nEuropean Parliament. (2017, February 16). European Parliament resolution of 16 February 2017 able?\nwith recommendations to the Commission on Civil Law Rules on Robotics (20152103/(INL)). https://publications.parliament.uk/pa/ld201719/ldselect/ldai/100100/.pdf\nRetrieved from:\nhttp://www.europarl.europa.eu/sides/getDoc.do?pubRef=-//EP//TEXT+TA+P8-TA-2017- House of Commons Science and Technology Select Committee. Algorithms in Decision Making.\n0++0051DOC+XML+V0//EN https://publications.parliament.uk/pa/cm201719/cmselect/cmsctech/351351/.pdf\nVillani, C. (2018). For a meaningful Artificial Intelligence towards a French and European strategy.\nRetrieved from:\nhttps://www.aiforhumanity.fr/pdfs/MissionVillani_Report_ENG-VF.pdf\n46 47", "metadata": {"country": "Saudi Arabia", "year": "2024", "legally_binding": "no", "binding_proof": "None", "date": "09/01/2024", "regulator": "Saudi Data and Artificial Intelligence Authority (SDAIA)", "type": "non-binding guidelines", "status": "active", "language": "English, Arabic", "use_cases": "[1, 2, 3, 5, 6]"}}
{"_id": "68695c6b5520a4cc7989de83", "title": "Generative AI Guideline for Government", "source": "https://sdaia.gov.sa/en/SDAIA/about/Files/GenAIGuidelinesForGovernmentENCompressed.pdf", "text": "AI ETHICS\nARTIFICIAL INTELLIGENCE ETHICS\nPRINCIPLES & GUIDELINES\nPRINCIPLES & GUIDELINES\nDEC 2022Table of Content\nPage Page\n1. Overview 4 3.4 Explainable 29\n1.1 Introduction 5 3.4.1 Principle 29\nWe will make AI systems as explainable as technically possible\n1.2 Scope 6 3.4.2 Guidelines 29\n1.3 Strategic alignment 6 3.5 Robust, Safe and Secure AI 31\n1.4 Responsibility 8 3.5.1 Principles 31\nAI systems will be technically robust\n3.5.2 Guidelines 32\n1.5 Licensing 8\n3.6 Human Centred AI 34\n1.6 Toolkit 8\n3.6.1 Principles 34\nWe will give AI systems human values and make them beneficial to society\n3.6.2 Guidelines 35\n2. Definitions 10\n3.7 Sustainable and Environmentally Friendly AI 38\n3.7.1 Principle 38\n3. Principles and Guidelines 16\nWe will promote sustainabity and environemntly friendly AI 38\n3.7.2 Guidelines 38\n3.1 Fairness 17\n3.8 Privacy Preserving AI 38\n3.1.1 Principle 17\nWe will make AI systems fair\n3.8.1 Principle 40\n3.1.2 Guidelines 20\nWe will respect people\u2019s privacy\n3.8.2 Guidelines 40\n3.2 Accountable AI 20\n3.2.1 Principle 20\nWe will make AI systems accountable\n4. Bibliography 44\n3.2.2 Guidelines 20\n3.3 Transparent AI 26\n3.3.1 Principle 26\nWe will make AI systems transparent\n3.3.2 Guidelines 26\n2 301 1.1 Introduction\nAI\u2019s rapid advancement and innovation potential across a range of fields is incredibly exciting.\nYet a thorough and open discussion around AI ethics, and the principles organizations using this\nOverview\ntechnology must consider, is needed.\nThis document - AI Ethics: Principles and Guidelines \u2013 intends to meet this need to balance these\ntwo central considerations of AI. It is designed to offer detailed guidance to help AI actors adhere\nto eight principles of AI ethics.\nWe will make AI systems that are:\nexplainable transparent accountable fair\nprivacy preserving sustainable and human centered robust, safe and\nenvironmentally secure\nfriendly\nThe guidelines are non-binding, and are drafted as a collaborative multi-\nstakeholder effort, with full awareness of organizations\u2019 needs to innovate\nand protect their intellectual property. This is a collaborative process in\nwhich all stakeholders are invited to be part of an ongoing dialogue. We\nwould like to see the AI Ethics Guidelines evolve into a universal, practical\nand applicable framework informing ethical requirements for AI design and\nuse. The eventual goal is to reach widespread agreement and adoption of\ncommonly agreed policies to inform the ethical use of AI nationally and\naround the world.\n4 51.2 Scope Additionally, the AI Office is currently developing additional policies to be taken into consideration\nalong with this document:\nThis document gives non-mandatory guidelines for achieving the ethical design\nand deployment of AI systems in both the public and private sectors. AI already\nAI Seal AI Procurement Guidelines\nsurrounds us, but some applications are more visible and sensitive than others.\nThe UAE AI Seal brand (UAI) will be used The AI Procurement Guidelines will be\nThis document is applicable only to those AI systems which make or inform\nto attract talent and business from used to guide Federal Government entities\n\u2018significant decisions\u2019 \u2013 that is, those decisions which have the potential for\nacross the globe to come to the UAE to in the procurement of AI systems. They\nsignificant impact either on individuals or on society as a whole. They also apply to\ntest and develop AI. This includes a UAI will list the general principles, proposed\n\u2018critical decisions\u2019, which are a subset of significant decisions and are of especially\nmark recognizing high quality, ethical AI mechanisms for procurement and review\ncritical nature.\ncompanies. It would reward safe, efficient, of AI products between the federal\nverified AI technology with a \u2018UAI Seal government and vendors. The policy\nThis document guides the further developments of sector specific requirements\nof Approval\u2019. The UAE AI Seal brand is considers R&D for specific challenges\nand principles, hence every entity within its sector can further amend, use, or\ncurrently under development by the AI that do not currently have an AI solution\ncomplement its current frameworks to properly suit its context and the need of\nOffice. product that is market-ready. Based on\naffected stakeholders.\nglobal standards for AI Procurement, the\npolicy aims to incentivize AI adoption\nIt is a living document and will undergo further future reviews and enhancements.\nwithin government entities and promote\nAI R&D within the private sector for\nspecified government challenges. The\nprocurement policy is currently under\ndevelopment by the AI Office.\n1.3 Strategic alignment\nThis document comes as a fulfillment of the goals and aspirations set in the UAE National AI\nStrategy and the international Sustainable Development Goals (SDGs). The UAE has long been\nat the forefront of sustainable development. These shared global milestones have created global\nmovement and adoption in several disciplines and sectors. As an early adopter, the UAE has\nestablished a national committee on SDGs with a long-term realization of the UAE SDG agenda\n2030. These goals are further engrained and reflected in the AI Ethics Principles under the topics of\nfairness, inclusiveness, equality, human benefit, and rights.\n6 71.4 Responsibility\nThe AI Office will not be responsible for any misuse of the AI Ethics Principles and Guidelines. The\nuser bears all the consequences of their use.\n1.5 Licensing\nThis document is published under the terms of a Creative Commons Attribution 4.0 International\nLicense in order to facilitate its re-use by other governments and private sector organizations.\nIn summary this means you are free to share and adapt the material, including for commercial\npurposes, provided that you give appropriate credit to the UAE AI and Blockchain Council as its\nowner and do not suggest the Minister of State for Artificial Intelligence, Digital Economy and\nRemote Work Applications Office endorses your use.\n1.6 Toolkit\nThe document is part of a toolkit of self-governing nature that boosts awareness and enables\ngovernment and private institutions to pursue innovative use cases while maintaining human\nvalues and principles.\n8 92.1 AI developer organization\n02\nAn organization which does any of the following:\na determines the purpose of an AI system;\nDefinitions\nb designs an AI system;\nc builds an AI system, or:\nd performs technical maintenance or tuning on an AI system\nNote:\nThe definition applies regardless of whether the organization is the ultimate user of the system, or\nwhether they sell it on or give it away.\nExample: A company develops an artificially intelligent facial recognition system and\nsells it to a country\u2019s border force, who use it to identify suspicious personnel. The company\nis an AI developer organization and the border force is an AI operator organization.\n2.2 AI operator organization\nAn organization which does any of the following:\na uses AI systems in operations, backroom processes or decision-making;\nb uses an AI system to provide a service to an AI subject;\nc is a business owner of an AI system;\nd procures and treats data for use in an AI system; or\ne evaluates the use case for an AI system and decides whether to proceed.\nNotes:\n1. This definition applies regardless of whether the AI system was developed in-house or procured.\n2. It is possible for organizations to be both an AI developer organization and an AI operator organization.\n10 112.6 Critical decision\n2.3 Artificial Intelligence (also AI)\nThe capability of a functional unit to perform functions that are generally associated with human An individually significant decision which is deemed to either have a very large impact on an\nintelligence such as reasoning, learning and self-improvement1. individual or to have especially high stakes. These can be especially sensitive, have the potential\nto cause high loss or damage, be societally significant, or set an important precedent.\n2.4 Artificially Intelligent System (also AI system)\nNotes:\nThe types of decisions referred to here are the same as those in the definition of significant-at-\nA product, service, process or decision-making methodology whose operation or outcome is\nscale decisions, except in this case the effects are felt as a result of an individual decision rather\nmaterially influenced by artificially intelligent functional units.\nthan an aggregate of many decisions..\nNotes:\n1. It is not necessary for a system\u2019s outcome to be solely determined by artificially intelligent\nfunctional units in order for the system to be defined as an artificially intelligent system; and Example: A court determines whether a defendant is guilty of a criminal charge, with the\n2. A particular feature of AI systems is that they learn behavior and rules not explicitly punishment for guilt being a life sentence. This is a critical decision because it has a very\nprogrammed in. large impact on the life of the defendant and also sets precedent for similar cases in the\nfuture.\nExample 1: A small claims court uses Example 2: A government entity uses\n2.7 Functional Unit\nan artificially intelligent software package a chatbot which allows customers to ask\nto collect evidence pertaining to a case, routine questions, book appointments\nEntity of hardware or software, or both, capable of accomplishing a specified purpose3 .\ncompare it to similar cases in the past, and conduct minor financial transactions.\nand present a recommended decision The chatbot responds to customer\nto a judge. The judge determines the queries with pre-written responses and is\n2.8 Individually significant decision\nfinal outcome. This decision-making based on pre-programmed decision rules.\nA decision which has the potential for significant impact on at least one individual\u2019s\nmethodology is materially influenced Therefore, the chatbot is not an AI system.\ncircumstances, behavior or choices, or has legal or similarly significant effects on him or her.\nby an artificially intelligent functional If, however, the chatbot autonomously\nunit, and is therefore classified as an AI adjusted its treatment of customers\nsystem. based on the outcome of past cases, it\nwould be an AI system. Example: A company decides to make an employee redundant. This is an individually\nsignificant decision because of its potential impact on the employee\u2019s financial situation.\n2.5 Bias (of a system)\nInclination or prejudice for or against one person or group, especially in a way considered to be\nunfair2.\n2 Oxford Dictionary 2018, Oxford University Press, viewed online 4th October 2018, <https://en.oxforddictionaries.com>\n1 Consistent with ISO/IEC 2382:2015 3 From ISO/IEC 2382:2015\n12 132.9 Non-operational bias (of a system) 2.11 Significant decision\nBias that is either: A decision which is either individually significant or is part of a set of significant-at-scale\ndecisions.\na not a design feature; or\nb not important in achieving the stated purpose of the system. 2.12 Subject of an artificially intelligent system (also AI subject)\nA natural person who is any of the following:\n2.10 Set of significant-at-scale decisions\na an end-user of an AI system\nA set of decisions made by the same system or organization which, when taken in aggregate,\nb directly affected by the operation of or outcomes of an AI system, or:\nhave significant impact on society as a whole or groups within it.\nc a recipient of a service or recommendation provided by an AI system\nNotes:\n1. The decisions need not be individually significant in order to qualify, in aggregate, as a set of\nsignificant-at-scale decisions; and\n2. Examples of areas which have a large impact on society and which include but are not limited\nto: the large-scale allocation of resources or opportunities amongst groups; the structure of\ngovernment; the division of power between large entities or groups; the law, and its interpretation\nand enforcement; conflict and war; international relations, etc.\nExample: An AI system is used by a website to determine which content to show users.\nThis decision is not individually significant, since a user is not greatly affected by whether\na particular piece of media is shown to them. However, if the website is popular then the AI\nsystem may be making a set of significant-at-scale decisions, because any biases in the\nsystem will affect a large number of users.\n14 153.1 Fairness (Principle 1)\n03\n3.1.1 Principle\nPRINCIPLES AND We will make AI systems fair\nData ingested should, where possible, be accurate and representative of the affected\npopulation.\nGUIDELINES\nAlgorithms should avoid non-operational bias.\nSteps should be taken to mitigate and disclose the biases inherent in datasets.\nSignificant decisions should be provably fair.\nAll personnel involved in the development, deployment and use of AI systems have a role and\nresponsibility to operationalize AI fairness and should be educated accordingly.\n3.1.2 Guidelines\n3.1.2.1\nBenefits of AI systems should be available and accessible to all. AI systems should not discriminate\nagainst people or groups in a way that could have an adverse impact. Especially significant and\ncritical decisions made or assisted by AI should be provably fair.\na Considering that fairness can have many different definitions, an organization should\ndocument its own definition of fairness for the context that the AI system is going to be\nimplemented in. Organizations should document what the implemented fairness objective\nstands for and why this choice was considered the most suitable for the given scenario.\nb It is recommended to identify and document demographic groups that may be adversely\nimpacted and mitigate the risk where possible.\nc AI developers and AI operators could consider formal procedures such as Discrimination\nImpact Assessments as a means of ensuring fairness. This assessment should be\ndocumented as well as the mitigation measures that were implemented by the organization.\nThe impact assessment should be conducted pre-release and regularly after as an ongoing\nevaluation, results are advised to be documented.\n16 173.1.2.2 3.1.2.3\nConsideration should be given to whether the data ingested is accurate and representative Consideration should be given to whether decision-making processes introduce bias\nof the affected population\na When subjecting different groups to different decision-making processes, AI developers\nd Fairness has many different definitions in different cultures and for different contexts. should consider whether this will lead to non-operational bias.\nEncouraging a diverse and inclusive AI ecosystem is thus all the more crucial to ensure\nthat one definition of fairness does not contradict another, and that the process of defining b When evaluating the fairness of an AI system, AI developers and AI operators should\nfairness itself is fair, with under-represented groups represented in the discussion. consider whether AI subjects in the same circumstances receive equal treatment.\ne AI developers and AI operators should undertake reasonable data exploration and/or testing\nto identify potentially prejudicial decision-making tendencies in AI systems arising from\nExample: An organization uses an AI tool to automate the pre-screening of candidates\ndata inaccuracy and biases in the data.\nfor a job opening. It is trained on data from the company\u2019s existing employees, the majority\nof whom are from the same ethnic background. Therefore, the system learns to use name\nf AI developers and operators should evaluate all datasets to assess inclusiveness of and nationality as discriminating factors in filtering job applicants. This could have been\nidentified demographic groups and collect data to close any gaps.\nidentified through testing and rectified by, for example, balancing the training data or only\nusing relevant data fields for training.\ng AI developers and AI operators should refrain from training AI systems on data that is not\nlikely to be representative of the affected AI subjects, or is not likely to be accurate, whether\nthat be due to age, omission, method of collection, or other factors.\n3.1.2.4\nSignificant decisions informed by the use of AI should be fair\nExample: Following a natural disaster, a government relief agency uses an AI system to a AI developers and AI operators could consider formal procedures such as Discrimination\ndetect communities in greatest need by analyzing social media data from a range of websites. Impact Assessments as a means of ensuring fairness.\nHowever, those communities where smartphone penetration is lower having less presence on\n3.1.2.4\nsocial media, and so are at risk of receiving less attention. Therefore, the charity complements\nAI operators should consider whether their AI systems are accessible and usable in a fair\ntheir AI tool with traditional techniques to identify needy populations elsewhere.\nmanner across user groups\n3.1.2.5\nh AI developers should consider whether their AI systems can be expected to perform well Consideration should be given to the effect of diversity on the development and deployment\nwhen exposed to previously unseen data, especially when evaluating people who are not processes\nwell-represented in the training data.\nb Organizations should seek to include people from diverse demographic backgrounds across\nthe full lifecyle, to include design, development and deployment processes. Organizations\nshould seek to engage diverse internal and external groups also.\nc AI developers should consider whether the assumptions they make about AI subjects could\nbe wrong or are likely to lead to non-operational bias; if so, they should consider consulting\nthe AI subjects in a representative manner during the design, development and deployment\nto confirm these assumptions.\n18 193.2 Accountable AI (Principle 2)\n3.2.2.2\nPositive efforts should be made to identify and mitigate any significant risks inherent in the AI\nsystems designed\n3.2.1 Principle\na AI operators should only use AI systems that are backed by evidence-based academic or\nindustrial research, and AI developers should base their development on such research.\nWe will make AI systems accountable\nAccountability for the outcomes of an AI system lies not with the system itself but is\napportioned between those who design, develop and deploy it; Example: A foreign country has a government service which identifies parents who owe\nmoney in child maintenance. The data matching process is often incorrect due to misspelled\nDevelopers should make efforts to mitigate the risks inherent in the systems they design; names or missing data which results in some individuals being incorrectly targeted\nautomatically by the system with the result being a large bill, poor credit ratings and even\nAI systems should have built-in appeals procedures whereby users can challenge significant\nfreezing wages. The recourse for individuals who are incorrectly targeted is time-consuming\ndecisions;\nand not straightforward . If the potential impact of incorrect decisions had been assessed,\nmitigation measures such a user-friendly review procedure could have been set up.\nAI systems should be developed by diverse teams which include experts in the area in which\nthe system will be deployed; and\nAI systems should be subject to external audit and decision quality assurance. b AI operators should identify the likely impact of incorrect automated decisions on AI\nsubjects and, in the case where incorrect decisions are likely to cause significant cost or\ninconvenience, consider mitigating measures.\n3.2.2 Guidelines\nc AI operators should consider internal risk assessments or ethics frameworks as a means to\n3.2.2.1 facilitate the identification of risks and mitigating measures.\nAccountability for the outcomes of an AI system should not lie with the system itself\nd In designing AI systems to inform significant decisions, AI developers should consider\na Accountability for loss or damages resulting from the application of AI systems should not measures to maintain data accuracy over time, including:\nbe attributed to the system itself.\n\u2022 the completeness of the data.\nb AI operators and AI developers should consider designating individuals to be responsible for \u2022 timely update of the data, and.\ninvestigating and rectifying the cause of loss or damage arising from the deployment of AI\n\u2022 whether the context in which the data was collected affects its suitability for the\nsystems.\nintended use case.\nExample: A border camera scanning for predictors of risk may misinterpret a \u201ctic\u201d of an\nindividual with Tourette syndrome as suspicious. These can manifest in a diverse fashion, and\nshould not cause this person to undergo secondary inspection every time they pass through\nthe border . If the data is updated after the first case is encountered then it would avoid\ncausing inconvenience on subsequent visits.\n5 Government of Canada. (2018). Responsible AI in the Government of Canada. Digital Disruption White Paper Series. Version 2.0, p.26. Retrieved from:\nhttps://docs.google.com/document/d/1Sn-qBZUXEUG4dVk909eSg5qvfbpNlRhzIefWPtBwbxY/edit\n4 Cabinet Office (UK), Data Science Ethical Framework, Version 1.0, licensed under the Open Government License v3.0, p. 13 6 Stoica, I. et. al.,2017, A Berkeley View of Systems Challenges for AI, p. 2, https://arxiv.org/pdf/1712.05855.pdf\n20 21e AI developers and AI operators should tune AI models periodically to cater for changes to e Be mindful that there might be fundamental tensions between different principles and\ndata and/or models over time. requirements. Continuously identify, evaluate, document and communicate these trade-\noffs and their solutions.\nf AI operators should consider whether AI systems trained in a static environment will display\nmodel instability when deployed in dynamic environments. Adopt a Trustworthy AI assessment list when developing, deploying or using AI systems,\nf\nand adapt it to the specific use case in which the system is being applied.\nExample: AI systems will need to be able to adapt to the changes in the environment that g Keep in mind that such an assessment list will never be exhaustive. Ensuring Trustworthy\nthey are deployed in. For example, a self-driving car would need to quickly adapt to unexpected AI is not about ticking boxes, but about continuously identifying and implementing\nand dangerous road by learning in real time from other cars that have successfully dealt with requirements, evaluating solutions, ensuring improved outcomes throughout the AI\nthese conditions. In addition, such mission-critical applications must handle noisy inputs system\u2019s lifecycle, and involving stakeholders in this.\nand defend against malicious actors .\n3.2.2.4\nAI subjects should be able to challenge significant automated decisions concerning them and,\ng AI developers should collaborate with AI Operators to train models using historical data where appropriate, be able to opt out of such decisions\nfrom the Operator.\na AI operators using AI systems to inform significant decisions should provide procedures by\nwhich affected AI subjects can challenge a specific decision concerning them.\nh AI Operators should consider working with their vendors (AI developers) to continually\nmonitor performance.\nb AI operators should consider such procedures even for non-significant decisions.\ni AI Operators should subject AI systems informing significant decisions to quality checks\nat least as stringent as those that would be required of a human being taking the same c AI operators should make affected AI subjects aware of these procedures and should\ndesign them to be convenient and user-friendly.\ndecision.\nd AI operators should consider employing human case evaluators to review any such\n3.2.2.3\nchallenges and, when appropriate, overturn the challenged decision.\nAI systems informing critical decisions should be subject to appropriate external audit\na When AI systems are used for critical decisions, external auditing of the AI systems in\nquestion should be used as a means to ensure meaningful standards of transparency and Example: A bank allows customers to apply for a loan online by entering their data. The\naccountability are upheld. bank uses an AI system to automatically determine whether to give the loan and what the\ninterest rate should be. They provide users with an option to contest the decision and have\nb In the case that critical decisions are of civic interest, public release of the results of the it reviewed by a human . They also require that customers justify their challenge by filling in\naudit should be considered as a means of ensuring public processes remain accountable a form, which assists the case reviewer and deters customers from challenging a decision\nto those affected by them. without good reason.7\nIn the case that critical decision are life and death decisions, these decisions should be\nc\nsupported by further validation and verification via a human operator.\nd Facilitate traceability and auditability of AI systems, particularly in critical contexts or\nsituations.\n7 Adapted from EU Commission, Can I be subject to automated individual decision-making, including profiling? Retrieved from: https://ec.europa.eu/\ninfo/law/law-topic/data-protection/reform/rights-citizens/my-rights/can-i-be-subject-automated-individual-decision-making-including-profiling_\nen#example\n22 23e AI operators should consider instituting an opt-out mechanism for significant automated 3.2.2.7\ndecisions. AI operators should understand the AI systems they use sufficiently to assess their suitability\nfor the use case and to ensure accountability and transparency\nf AI operators could consider \u201ccrowd challenge\u201d mechanisms whereby a critical number of\ncomplaints triggers an investigation into the fairness and/or accuracy of a decision-making\na In the case of critical decisions, AI operators should avoid using AI systems that cannot be\nprocess as a whole.\nsubjected to meaningful standards of accountability and transparency.\n3.2.2.5 b AI developers should consider notifying customers and AI operators of the use cases for\nAI systems informing significant decisions should not attempt to make value judgements on which the system has been designed, and those for which it is not suitable.\npeople\u2019s behalf without prior consent\na When informing an AI subject about significant choices they will make, AI systems should\nnot unreasonably restrict the available options or otherwise attempt to influence their value\njudgements without the explicit consent of the AI subject in question.\n3.2.2.6\nAI systems informing significant decisions should be developed by diverse teams with\nappropriate backgrounds\na AI developers who develop AI systems which may be used to assist in making critical\ndecisions should involve in the process experts with a background in social science, policy,\nor another subject which prepares them to evaluate the broader societal impact of their\nwork.\nb Development of AI systems informing significant decisions should include consultation\nwith experts in the field in which the system will be deployed.\nExample: An app that uses AI to assess medical symptoms and has a large user base had\nto face regulatory scrutiny because of number of complaints from doctors. They warned\nthat the application can miss signs of serious illness. A number of different shortcomings\nwere identified by doctors, some of which the company could address and resolve.8\n8 Financial Times. 2018. High-profile health app under scrutiny after doctor\u2019s complaints. Retrieved from: https://www.ft.com/content/19dc6b7e-8529-\n11e896-dd-fa565ec55929\n24 253.3 Transparent AI (Principle 3)\nc To facilitate the above, AI developers and AI operators should consider documenting the\nfollowing information during the design, development and deployment phases, and retaining\n3.3.1 Principle this documentation for a length of time appropriate to the decision type or industry:\n\u2022 the provenance of the training data, the methods of collection and treatment, how the data was\nWe will make AI systems transparent\nmoved, and measures taken to maintain its accuracy over time;\n\u2022 the model design and algorithms employed; and\nDevelopers should build systems whose failures can be traced and diagnosed. \u2022 changes to the codebase, and authorship of those changes.\nPeople should be told when significant decisions about them are being made by AI. d Where possible given the model design, AI developers should consider building in a means\nby which the \u201cdecision journey\u201d of a specific outcome (i.e. the component decisions leading\nWithin the limits of privacy and the preservation of intellectual property, those who deploy to it) can be logged.\nAI systems should be transparent about the data and algorithms they use.\nResponsible disclosures should be provided in a timely manner, and provide reasonable\nExample: A technology company has a product which is designed to assist in medical\njustifications for AI systems outcomes. This includes information that helps people\ndiagnosis. It documents each stage of its reasoning and relates it back to the input data.9\nunderstand outcomes, like key factors used in decision making.\n3.3.2 Guidelines\n3.3.2.1\nTraceability should be considered for significant decisions, especially those that have the\npotential to result in loss, harm or damage.\na For AI systems which inform significant decisions, especially those with the potential to\ncause loss, harm or damage, AI developers should consider building in traceability (i.e. the\nability to trace the key factors leading to any specific decision).\nb Organizations should ensure that harms caused through AI systems are investigated and\nredressed, by enacting strong enforcement mechanisms and remedial actions, to make\ncertain that human rights and the rule of law are respected in the digital world and in the\nphysical world.\n9 See IBM WatsonPaths\n26 273.3.2.2 3.4 Explainable AI (Principle 4)\nPeople should be informed of the extent of their interaction with AI systems\n3.4.1 Principle\na AI operators should inform AI subjects when a significant decision affecting them has been\nmade by an AI system.\nWe will make AI systems as explainable as\ntechnically possible\nExample: A small claims court adjudicates minor civil matters such as debt collection and\nevictions. They introduce an AI system to suggest the outcome of a ruling. At the time of the\nDecisions and methodologies of AI systems which have a significant effect on individuals\nruling the plaintiff and defendant are notified that the decision was assisted by an AI system.\nshould be explainable to them, to the extent permitted by available technology.\nThe court also provides an explanation for the decision.\nIt should be possible to ascertain the key factors leading to any specific decision that\ncould have a significant effect on an individual.\nb If an AI system can convincingly impersonate a human being, it should do so only after\nIn the above situation we will provide channels through which people can request such\nnotifying the AI subject that it is an AI system.\nexplanations.\nExample: A technology company produces a conversational AI agent which can make 3.4.2 Guidelines\nsome phone calls on behalf of its users. Those who receive the calls may believe that they\n3.4.2.1\nare speaking to a human. Therefore, the company programs the agent to identify itself at\nAI operators could consider providing affected AI subjects with a high-level explanation of\nthe start of every conversation.\nhow their AI system works\na AI operators could consider informing the affected AI subjects in understandable, non-\ntechnical language of:\n\u2022 the data that is ingested by the system;\n\u2022 the types of algorithms employed;\n\u2022 the categories into which people can be placed; and\n\u2022 the most important features driving the outcomes of decisions\n\u2022 A comprehensive list of feature engineering and models that was considered during\nthe model building phase.\nExample: A person turned down for a credit card might be told that the algorithm took\ntheir credit history, age, and postcode into account, but not learn why their application was\nrejected10 .\n28 29b For non-sensitive public sector use cases designed for the common good, AI operators\ncould consider making source code, together with an explanation of the workings of the AI Example: The UK\u2019s NHS developed a tool called Predict, which allows women with breast\nsystem, available either publicly or upon request (this should be done only if there is low risk cancer to compare their case to other women who have had the same condition in the past,\nof people \u2018gaming the system\u2019). and visualize the expected survival rate under various treatment options. The website has an\nexplanation page which shows the weights behind various factors and contains a description\nc The AI operator should maintain necessary documentation that provides elaboration and of the underlying mathematics12 .\nclarification on how the algorithm works, for example documentation of processes, decision\nmaking flow charts of the system, and how the appeal process is embedded.\nd An individual should have the ability to contest and seek effective redress against decisions c In the case that such explanations are available, they should be easily and quickly accessible,\nmade by AI systems. These must be addressed by the group or team supporting these free of charge and user-friendly.\nmodels.\ne Develop appropriate impact indices for the evaluation of AI system technological 3.5 Robust, Safe and Secure AI (Principle 5)\ninterventions from multiple perspectives.\n3.5.1 Principles\n3.4.2.2\nAI operators should consider providing affected AI subjects with a means to request\nexplanations for specific significant decisions, to the extent possible given the state of present AI systems will be technically robust\nresearch and the choice of model\nAI systems should be technically robust with a preventative approach to risks which\na AI operators should consider providing a means by which people affected by a significant\noperates in a manner such that they reliably behave.\ndecision informed by AI can access the reasoning behind that decision.\nAI Developers should ensure AI systems will not cause any unintentional harm and\nadverse impacts.\nExample: The US Consumer Financial Protection Bureau requires that creditors in the US\nwho reject credit applications must explain to the applicant the principal reason(s) why their\nAI systems should be resilient to attacks and security such as data poisoning and\napplication was rejected (e.g. \u201clength of residence\u201d or \u201cage of automobile\u201d)11 . In particular,\nmodel leakage.\n\u201cstatements that the adverse action was based on the creditor`s internal standards or\npolicies or that the applicant, joint applicant, or similar party failed to achieve a qualifying AI systems should have safeguards that enable a fallback plan in case of problems.\nscore on the creditor`s credit scoring system are insufficient\u201d.\nAI system results should be reproducible.\nb Where such explainability is not possible given available technology, AI operators should\nconsider compromises such as counterfactual reasoning, or listing the most heavily\nweighted factors contributing to the decision.\n11 Consumer Financial Protection Bureau, 12 CFR Part 1002 - Equal Credit Opportunity Act (Regulation B), \u00a7 1002.9 Notifications, Retrieved from https://www.\n10 The Guardian. AI watchdog needed to regulate automated decision-making, say experts. Retrieved from: https://www.theguardian.com/technology/2017/ consumerfinance.gov/policy-compliance/rulemaking/regulations/1002/\njan/27/ai-artificial-intelligence-watchdog-needed-to-prevent-discriminatory-automated-decisions 12 Predict website, accessible at http://www.predict.nhs.uk/predict_v2.1/legal/algorithm\n30 313.5.2 Guidelines\n3.5.2.1 3.5.2.3\nAI Operators should continue conducting vulnerability assessments, verification of AI system AI Operators should assure end users of the system\u2019s reliability through documentation, and\ndifferent behaviors in unexpected situations and for any dual-use case, to include: operationalizing processes for testing and verification of desired outcomes , specifically:\na Putting measures to ensure integrity and resilience of IA system against potential attacks. a AI system should be designed with an approach to continue monitoring if the system meets\ngoals, purposes and intended application.\nb AI Operators should define an approach to ensure results are reproducible through clear\nExample: IBM researchers presented a novel approach to exploiting AI for malicious\nprocess and documentation.\npurposes. Their system, DeepLocker embeds AI capabilities within the malware itself in order\nto improve its evasion techniques. It uses an Artificial Intelligence model to identify its target\nc AI Operators should publish documentation to assure system robustness to end users.\nusing indicators like facial recognition, geolocation and voice recognition.\n3.5.2.4\nAI systems will be safe, secure and controllable by humans\nb Verifying how AI systems behave in unexpected situations and environments.\na Safety and security of people, be they operators, end-users or other parties, will be of\nc Taking preventative measures against any possible system dual-case use. paramount concern in the design of any AI system.\nb AI systems should be verifiably secure and controllable throughout their operational\n3.5.2.2\nlifetime, to the extent permitted by technology.\nAI operators should define a suitable fall back plan and test it to maintain readiness in\nunexpected situations and in high safety risks level, to include:\nc The continued security and privacy of users should be considered when decommissioning\na Developing a plan to measure and assess potential safety risks to you or any third party AI systems.\nfrom technology use accidental or malicious misuse.\nd AI systems that may directly impact people\u2019s lives in a significant way should receive\nb Define thresholds for system acceptable results and governance procure to fall back on commensurate care in their designs.\nalternative defined and tested plans.\ne Such systems should be able to be overridden or their decisions reversed by designated\nc Considering an insurance policy to mitigate risks arising from potential damages. individuals.\nExample: Adopting a risk-based approach to procurement and clearly communicating it to\nvendors can help address such issues by giving the procuring organization advance notice\nof the specific oversight capabilities it will need in future stages of the system lifecycle,\npreventing vendors from presenting intellectual property arguments against required testing,\nmonitoring and auditing of their AI systems going forward and rewarding vendors\u2014of all\nsizes\u2014that are more advanced and responsive in their responsible AI efforts.\n13 IBM. https://documents.trendmicro.com/assets/white_papers/wp-malicious-uses-and-abuses-of-artificial-intelligence.pdf\n32 333.6.2 Guidelines\n3.5.2.5\nAi systems should not be able to autonomously hurt, destroy or deceive humans\n3.6.2.1\na AI systems should be built to serve and inform, and not to deceive and manipulate.\nAI should be developed to align with human values, contribute to human flourishing and\nbenefit society.\nb Nations should collaborate to avoid an arms race in lethal autonomous weapons, and\nsuch weapons should be tightly controlled. a Society should be consulted in a representative fashion to inform the development of AI.\nc Active cooperation should be pursued to avoid corner-cutting on safety standards. b Stakeholders throughout society should be involved in the development of AI and its\ngovernance.\nSystems designed to inform significant decisions should do so impartially.\nd\nc Stakeholders should proactively engage in responsible stewardship of trustworthy AI in\npursuit of beneficial outcomes for people and the planet.\n3.6 Human Centered AI (Principle 6)\nd Organizations should prioritize having all their stakeholders learn about wellbeing metrics\nas a potential determinant of how they create, deploy, market and monitor their AI\n3.6.1 Principles technologies.\nWe will give AI systems human values and\nExample: IEEE P7010 recommended practice14 establishes wellbeing metrics relating to\nmake them beneficial to society human factors directly affected by intelligent and autonomous systems and establishes a\nbaseline for the types of objective and subjective data these systems should analyze and\nGovernment will support the research of the beneficial use of AI. include (in their programming and functioning) to proactively increase human wellbeing.\nStakeholders throughout society should be involved in the development of AI and its\ngovernance.\n3.6.2.2\nSystematic analyses that examine the ethics of designing affective systems to nudge human\nDesign AI systems to adopt, learn and follow the norms and values of the community\nbeings prior to deployment are needed.\nthey serve.\nSystematic analyses that examine the ethics of designing affective systems to nudge a Nudging in AI systems should have an opt-in system policy with explicit consent.\nhuman beings prior to deployment are needed.\nb We recommend that when appropriate, an affective system that nudges human beings\nDecisions related to lethal force, life and death should not be delegated to AI systems. should have the ability to accurately distinguish between users, including detecting\nRules and standards should be adopted to ensure effective human control over those characteristics such as whether the user is an adult or a child. Additional protections must\ndecisions. be put in place for vulnerable populations, such as children, when informed consent cannot\nbe obtained, or when it may not be a sufficient safeguard.\nc AI systems with nudging strategies must be carefully evaluated, monitored, and controlled.\n14 IEEE. https://sagroups.ieee.org/7010/\n34 353.6.2.3 f In situations where it is needed, human rights impact assessments and human rights due\nHumanity should retain the power to govern itself and make the final decision, with AI in an diligence, human determination codes of ethical conduct, or quality labels and certifications\nassisting role. intended to promote human centered values and fairness should be considered.\ng Mechanisms should be put into place to receive external feedback regarding AI systems\na Decisions related to lethal force, life and death should not be delegated to AI systems. Rules\nthat potentially infringe on fundamental rights.\nand standards should be adopted to ensure effective human control over those decisions.\nb Responsible parties (e.g., parents, nurse practitioners, social workers, and governments)\nshould be trained to detect the influence due to AI and ineffective mitigation techniques.\nIn the most extreme cases it should always be possible to shut down harmful AI system.\nc Those actions undertaken by an affective system that are most likely to generate an\nemotional response should be designed to be easily changed.\nd Users should be able to make informed autonomous decisions regarding AI systems. They\nshould be given appropriate knowledge and tools to comprehend and interact with AI\nsystems to satisfactory degree and, where possible to reasonably self-assess or challenge\nthe system.\n3.6.2.4\nAI systems should be designed in a way that respects the rule of law, human rights, society\nvalues, and should include appropriate safeguards to ensure a fair and just society.\na AI Developers should design AI systems to adopt, learn and follow the norms and values of\nthe community they serve.\nb Organizations could identify the norms of the specific community in which AI systems are\nto be deployed in. In particular, pay attention to the norms relevant to the kinds of tasks that\nthe AI systems are designed to perform. These could be documented as well as how these\nnorms are addressed by the AI system.\nc To respond to the dynamic change of norms in society the AI system could be able to adjust\nits existing norms and learn new ones, while being transparent about these changes.\nd Designers should consider forms and metrics for assessing an AI system\u2019s norm conformity\nover the lifetime of the system (e.g. human-machine agreement on moral decisions,\nverifiability of AI decisions, justified trust).\ne The norm identification process must document the similarities and differences between\nthe norms that humans apply to other humans and the norms they apply to AI systems.\nNorm implementations should be evaluated specifically against the norms that the\ncommunity expects the AI system to follow.\n36 373.7 Sustainable and Environmentally Friendly AI (Principle 7)\nd While there is no standardized means of reporting on the environmental impact of AI\nsystems, where possible, model development and commissioning should make carbon\n3.7.1 Principle impacts a core consideration alongside functional and business requirements.\ne Smaller models should be considered \u2013 shrinking down the model size and using fewer\nWe will promote sustainable and environmentally compute cycles (to balance financial and performance costs with the end performance of\nthe model).\nfriendly AI\nf Carbon awareness should be considered \u2013 adjusting the operational parameters of the AI\nAI systems should be used to benefit all human beings, including future generations. The\nsystem to dynamically select the best time and location for energy use from the grid can\nenvironment is fundamental to this.\nreduce its carbon footprint.\nThroughout the AI lifecycle, implementations should only be carried out on the basis of a full\nunderstanding and acknowledgement of implications for sustainability and environment.\nExample: In agriculture, AI can transform Example: Google`s DeepMind division\nAn AI system\u2019s development, deployment and use, should be assessed via critical examination production by better monitoring and has developed AI that teaches itself to\nof resource usage and energy consumption.\nmanaging environmental conditions minimize the use of energy to cool Google`s\nand crop yields. AI can help reduce both data centers. As a result, Google reduced\nMechanisms to measure environmental impact due to type of energy use and processing\nfertilizer and water, all while improving its data center energy requirements by\npower provided by data centers should be established.\ncrop yields. 40% 15.\n3.7.2 Guidelines\n3.7.2.1 Throughout the AI lifecycle, implementations should always be carried out after full\nunderstanding and acknowledgement of AI implications on sustainability and environment.\na The application of Artificial Intelligence to address sustainability challenges are well\nunderstood. Building and running green, sustainable AI systems is of increasing importance,\ngiven the large carbon footprint that they can generate and the wider context of addressing\nclimate change.\nb AI model development, and by extension commissioning, should therefore seek to balance\ntechnical performance with energy consumption and environmental impact.\nc Efforts should be made to estimate and understand sustainability and environmental\nimpact across the AI lifecycle (e.g. the energy consumption costs associated with model\ntraining, and CO2 emissions and cloud compute costs associated with the deployment\nand running of the system).\n15 https://www.deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-by-40\n38 393.8 Privacy Preserving AI (Principle 8)\n3.8.2.2\nEstablish an oversight mechanism for data collection, storage and processing and use across\nyour organization\n3.8.1 Principle\na Assess who can access data and under which conditions.\nWe will respect people\u2019s privacy\nb Assign specific responsibilities and role for Data Protection Officers.\nAI systems should respect privacy and use the minimum intrusion necessary.\nc Prevention of harm to privacy necessitates adequate data governance that covers the\nquality and integrity of the data used, its relevance in light of the domain in which the\nAI systems should uphold high standards of data governance and security,\nprotecting personal information. AI systems will be deployed, its access protocols and the capability to process data in a\nmanner that protects privacy.\nSurveillance or other AI-driven technologies should not be deployed to the extent\nof violating internationally and/or UAE\u2019s accepted standards of privacy and human d AI systems must guarantee privacy and data protection throughout a system\u2019s entire\ndignity and people rights. lifecycle. This includes the information initially provided by the user, as well as the\ninformation generated about the user over the course of their interaction with the system\nPrivacy by design should be embedded in AI systems and where possible AI\n(e.g. outputs that the AI systems generated for specific users or how users responded to\nalgorithm should have adequate privacy impact assessments.\nparticular recommendations). Digital records of human behavior may allow AI systems\nto infer not only individuals\u2019 preferences, but also their age, gender, religious or political\nAdequate data protection frameworks and governance mechanisms should be\nviews. To allow individuals to trust the data gathering process, it must be ensured that data\nestablished in a multi-stakeholder approach and ensured throughout the life cycle\ncollected about them will not be used to unlawfully or unfairly discriminate against them.\nof AI systems.\ne In any given organization that handles individuals\u2019 data (whether someone is a user of the\nAI developers and operators should strive to strike the balance between privacy\nsystem or not), data protocols governing data access should be put in place in line with\nrequirements, individual rights and innovation growth and society benefits.\nnational privacy legislation (be these universal or sector-specific). These protocols should\noutline who can access data and under which circumstances. Only duly qualified personnel\n3.8.2 Guidelines\nwith the competence and need to access individual\u2019s data should be allowed to do so.\n3.8.2.1\nEstablish mechanism for users to flag issues related to privacy or data protection\na AI Operators should review the system for proper consent logging, ability of users to revoke\npermission whenever applicable.\nb Consider training AI model without or with minimal user of potentially sensitive or personal\ndata.\nc Use measures to enhance privacy such as encryption, anonymization and aggregation.\n40 41f Algorithmic systems require adequate privacy impact assessments, which also include\nsocietal and ethical considerations of their use and an innovative use of the privacy by\ndesign approach. AI actors need to ensure that they are accountable for the design and\nimplementation of AI systems in such a way as to ensure that personal information is\nprotected throughout the life cycle of the AI system.\ng Establish data policies or equivalent frameworks, or reinforce existing ones, to ensure full\nsecurity for personal data and sensitive data, which, if disclosed, may cause exceptional\ndamage, injury or hardship to individuals. Examples include data relating to offences,\ncriminal proceedings and convictions, and related security measures; biometric, genetic\nand health data; and personal data such as that relating to race, descent, gender, age,\nlanguage, religion, political opinion, national origin, ethnic origin, social origin, economic or\nsocial condition of birth, or disability and any other characteristics.\nh Promote mechanisms, such as open repositories for publicly funded or publicly held data,\nsource code and data trusts, to support the safe, fair, legal and ethical sharing of data,\namong others.\ni Promote and facilitate the use of quality and robust datasets for training, development and\nuse of AI systems, and exercise vigilance in overseeing their collection and use.\nExample: with the help of AI & ML algorithms16 , Synthetic data can be created to enhance\nprivacy. The data created will have the same statistical characteristics for the testing\nenvironment and where a third party can have access.\n16 https://research.aimultiple.com/privacy-enhancing-technologies/\n42 43Microsoft. 2022. Microsoft Responsible AI Standard, v2. General Requirements. June 2022.\n04\nRetrieved from:\nhttps://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4ZPmV\nISO. 2022. Proposed ISO Standard on Risk Management of AI: What Businesses Should Know.\nBibliography\nMay 2022. Retrieved from:\nhttps://unesdoc.unesco.org/ark:/48223/pf0000381137/PDF/381137eng.pdf.multi\nGSMA 2022. The AI Ethics Playbook. Implementing ethical principles into everyday business.\nFebruary 2022. Retrieved from:\nhttps://www.gsma.com/betterfuture/wp-content/uploads/202201//The-Mobile-Industry-Ethics-\nPlaybook_Feb-2022.pdf\nOECD. 2022. AI Policy Observatory. Retrieved from:\nhttps://standards.ieee.org/wp-content/uploads/import/documents/other/ead_v2.pdf\nUNESCO. 2022. Recommendation on the Ethics of Artificial Intelligence. Adopted in November\n2021. Retrieved from:\nhttps://unesdoc.unesco.org/ark:/48223/pf0000381137/PDF/381137eng.pdf.multi\nEuropean Commission. 2021. Proposal for a Regulation of the European Parliament and of the\nCouncil Laying Down Harmonized Rules on Artificial Intelligence (Artificial Intelligence Act) and\nAmending Certain Union Legislative Acts. Retrieved from:\nhttps://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A52021PC0206\nEuropean Commission. High Level Expert Group on Artificial Intelligence ( AI HLEG ). Ethics\nGuidelines for Trustworthy AI . April 2019. Retrieved from:\nhttps://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai\nCentre for Data Ethics and Innovation. UK Government: The roadmap to an effective AI assurance\necosystem. December 2021. Retrieved from:\nhttps://www.gov.uk/government/publications/the-roadmap-to-an-effective-ai-assurance-\necosystem\nThe New York City Council. 2021. A Local Law to amend the administrative code of the city of New\nYork, in relation to automated employment decision tools. Retrieved from:\nhttps://legistar.council.nyc.gov/LegislationDetail.aspx?ID=4344524&GUID=%20B051915D-A9AC-\n451E-81F86596032-FA3F9&\nUNESCO, Recommendation on the Ethics of Artificial Intelligence. 2021. Retrieved from:\nhttps://unesdoc.unesco.org/ark:/48223/pf0000381137\n44 45AI NOW Institute. Algorithmic accountability for the public sector. August 2021. Retrieved from: CNIL. (2017). How can humans keep the upper hand? The ethical matters raised by algorithms\nhttps://www.opengovpartnership.org/documents/algorithmic-accountability-public-sector/ and artificial intelligence. Retrieved from:\nhttps://www.cnil.fr/sites/default/files/atoms/files/cnil_rapport_ai_gb_web.pdf\nIEEE 2019. Ethically Aligned Design. Version 2 \u2013 For Public Discussion. Retrieved from:\nhttps://standards.ieee.org/wp-content/uploads/import/documents/other/ead_v2.pdf Executive Office of the President of the United States. National Science and Technology Council.\nNetworking and Information Technology Research and Development Subcommittee. (2016). The\nnational artificial intelligence research and development strategic plan. Retrieved from:\nIEEE 2017. Ethically Aligned Designed : A vision for Prioritizing Human Well-being with\nhttps://www.nitrd.gov/PUBS/national_ai_rd_strategic_plan.pdf\nAutonomous and Intelligent Systems. Retrieved from:\nhttps://standards.ieee.org/wp-content/uploads/import/documents/other/ead_v2.pdf\nExecutive Office of the President of the United States National Science and Technology Council.\nCommittee on Technology. (2016). Preparing for the future of artificial intelligence. Retrieved from:\nGoogle. AI Principles 2021. Retrieved from:\nhttps://obamawhitehouse.archives.gov/sites/default/files/whitehouse_files/microsites/ostp\nhttps://ai.google/principles/\nNSTC/preparing_for_the_future_of_ai.pdf\nMicrosoft. Responsible AI Standard v2 (June 2022). Retrieved from:\nThe Headquarters for Japan\u2019s Economic Revitalization. (2015). New Robot Strategy. Japan\u2019s\nhttps://www.microsoft.com/en-us/ai/responsible-ai\nRobot Strategy. Vision, Strategy, Action Plan. Retrieved from:\nhttp://www.meti.go.jp/english/press/2015/pdf/0123_01b.pdf\nThe ECONOMIST Group 2022 . Pushing forward: the future of AI in the Middle East and North\nAfrica Report. Retrieved from:\nTreasury Board of Canada Secretariat. (2018). Responsible Artificial Intelligence in the\nhttps://impact.economist.com/perspectives/sites/default/files/google_ai_mena_report.pdf\nGovernment of Canada. Digital Disruption White Paper Series. Version 2.0.\nhttps://docs.google.com/document/d/1Sn-qBZUXEUG4dVk909eSg5qvfbpNlRhzIefWPtBwbxY/\nPDPC. (2018, June 5). Discussion paper on Artificial Intelligence (AI) and Personal Data.\nedit\nSingapore: Personal Data Protection Commission Singapore (PDPC). Retrieved from:\nhttps://www.pdpc.gov.sg/-/media/Files/PDPC/PDF-Files/Resource-for-Organisation/AI/\nThe Toronto Declaration: Protecting the right to equality and non-discrimination in machine\nDiscussion-Paper-on-AI-and-PD---050618.pdf\nlearning systems. Toronto, Canada: Amnesty International and Access Now. Retrieved from:\nhttps://www.accessnow.org/cms/assets/uploads/201808//The-Toronto-Declaration_ENG_08-\nITI. AI Policy Principles. Retrieved from:\n2018.pdf\nhttps://www.itic.org/public-policy/ITIAIPolicyPrinciplesFINAL.pdf\nhttp://data.parliament.uk/writtenevidence/committeeevidence.svc/evidencedocument/artificial-\nCabinet Office.(2016, May 19). Data Science Ethical Framework. Retrieved from: intelligence-committee/artificial-intelligence/oral/73546.html (house of lords select committee\nhttps://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/ oral evidence, Q61)oral evidence, Q61)\nfile/524298/Data_science_ethics_framework_v1.0_for_publication__1_.pdf\nHouse of Lords Select Committee on Artificial Intelligence. (2018). AI in the UK: ready, willing and\nEuropean Parliament. (2017, February 16). European Parliament resolution of 16 February 2017 able?\nwith recommendations to the Commission on Civil Law Rules on Robotics (20152103/(INL)). https://publications.parliament.uk/pa/ld201719/ldselect/ldai/100100/.pdf\nRetrieved from:\nhttp://www.europarl.europa.eu/sides/getDoc.do?pubRef=-//EP//TEXT+TA+P8-TA-2017- House of Commons Science and Technology Select Committee. Algorithms in Decision Making.\n0++0051DOC+XML+V0//EN https://publications.parliament.uk/pa/cm201719/cmselect/cmsctech/351351/.pdf\nVillani, C. (2018). For a meaningful Artificial Intelligence towards a French and European strategy.\nRetrieved from:\nhttps://www.aiforhumanity.fr/pdfs/MissionVillani_Report_ENG-VF.pdf\n46 47", "metadata": {"country": "Saudi Arabia", "year": "2023", "legally_binding": "no", "binding_proof": "None", "date": "09/01/2023", "regulator": "Saudi Data and Artificial Intelligence Authority (SDAIA)", "type": "non-binding guidelines", "status": "active", "language": "English, Arabic", "use_cases": "[3, 5, 6]"}}
{"_id": "68695c755520a4cc7989de84", "title": "Generative AI Guideline for the Public", "source": "https://sdaia.gov.sa/en/SDAIA/about/Files/GenerativeAIPublicEN.pdf", "text": "AI ETHICS\nARTIFICIAL INTELLIGENCE ETHICS\nPRINCIPLES & GUIDELINES\nPRINCIPLES & GUIDELINES\nDEC 2022Table of Content\nPage Page\n1. Overview 4 3.4 Explainable 29\n1.1 Introduction 5 3.4.1 Principle 29\nWe will make AI systems as explainable as technically possible\n1.2 Scope 6 3.4.2 Guidelines 29\n1.3 Strategic alignment 6 3.5 Robust, Safe and Secure AI 31\n1.4 Responsibility 8 3.5.1 Principles 31\nAI systems will be technically robust\n3.5.2 Guidelines 32\n1.5 Licensing 8\n3.6 Human Centred AI 34\n1.6 Toolkit 8\n3.6.1 Principles 34\nWe will give AI systems human values and make them beneficial to society\n3.6.2 Guidelines 35\n2. Definitions 10\n3.7 Sustainable and Environmentally Friendly AI 38\n3.7.1 Principle 38\n3. Principles and Guidelines 16\nWe will promote sustainabity and environemntly friendly AI 38\n3.7.2 Guidelines 38\n3.1 Fairness 17\n3.8 Privacy Preserving AI 38\n3.1.1 Principle 17\nWe will make AI systems fair\n3.8.1 Principle 40\n3.1.2 Guidelines 20\nWe will respect people\u2019s privacy\n3.8.2 Guidelines 40\n3.2 Accountable AI 20\n3.2.1 Principle 20\nWe will make AI systems accountable\n4. Bibliography 44\n3.2.2 Guidelines 20\n3.3 Transparent AI 26\n3.3.1 Principle 26\nWe will make AI systems transparent\n3.3.2 Guidelines 26\n2 301 1.1 Introduction\nAI\u2019s rapid advancement and innovation potential across a range of fields is incredibly exciting.\nYet a thorough and open discussion around AI ethics, and the principles organizations using this\nOverview\ntechnology must consider, is needed.\nThis document - AI Ethics: Principles and Guidelines \u2013 intends to meet this need to balance these\ntwo central considerations of AI. It is designed to offer detailed guidance to help AI actors adhere\nto eight principles of AI ethics.\nWe will make AI systems that are:\nexplainable transparent accountable fair\nprivacy preserving sustainable and human centered robust, safe and\nenvironmentally secure\nfriendly\nThe guidelines are non-binding, and are drafted as a collaborative multi-\nstakeholder effort, with full awareness of organizations\u2019 needs to innovate\nand protect their intellectual property. This is a collaborative process in\nwhich all stakeholders are invited to be part of an ongoing dialogue. We\nwould like to see the AI Ethics Guidelines evolve into a universal, practical\nand applicable framework informing ethical requirements for AI design and\nuse. The eventual goal is to reach widespread agreement and adoption of\ncommonly agreed policies to inform the ethical use of AI nationally and\naround the world.\n4 51.2 Scope Additionally, the AI Office is currently developing additional policies to be taken into consideration\nalong with this document:\nThis document gives non-mandatory guidelines for achieving the ethical design\nand deployment of AI systems in both the public and private sectors. AI already\nAI Seal AI Procurement Guidelines\nsurrounds us, but some applications are more visible and sensitive than others.\nThe UAE AI Seal brand (UAI) will be used The AI Procurement Guidelines will be\nThis document is applicable only to those AI systems which make or inform\nto attract talent and business from used to guide Federal Government entities\n\u2018significant decisions\u2019 \u2013 that is, those decisions which have the potential for\nacross the globe to come to the UAE to in the procurement of AI systems. They\nsignificant impact either on individuals or on society as a whole. They also apply to\ntest and develop AI. This includes a UAI will list the general principles, proposed\n\u2018critical decisions\u2019, which are a subset of significant decisions and are of especially\nmark recognizing high quality, ethical AI mechanisms for procurement and review\ncritical nature.\ncompanies. It would reward safe, efficient, of AI products between the federal\nverified AI technology with a \u2018UAI Seal government and vendors. The policy\nThis document guides the further developments of sector specific requirements\nof Approval\u2019. The UAE AI Seal brand is considers R&D for specific challenges\nand principles, hence every entity within its sector can further amend, use, or\ncurrently under development by the AI that do not currently have an AI solution\ncomplement its current frameworks to properly suit its context and the need of\nOffice. product that is market-ready. Based on\naffected stakeholders.\nglobal standards for AI Procurement, the\npolicy aims to incentivize AI adoption\nIt is a living document and will undergo further future reviews and enhancements.\nwithin government entities and promote\nAI R&D within the private sector for\nspecified government challenges. The\nprocurement policy is currently under\ndevelopment by the AI Office.\n1.3 Strategic alignment\nThis document comes as a fulfillment of the goals and aspirations set in the UAE National AI\nStrategy and the international Sustainable Development Goals (SDGs). The UAE has long been\nat the forefront of sustainable development. These shared global milestones have created global\nmovement and adoption in several disciplines and sectors. As an early adopter, the UAE has\nestablished a national committee on SDGs with a long-term realization of the UAE SDG agenda\n2030. These goals are further engrained and reflected in the AI Ethics Principles under the topics of\nfairness, inclusiveness, equality, human benefit, and rights.\n6 71.4 Responsibility\nThe AI Office will not be responsible for any misuse of the AI Ethics Principles and Guidelines. The\nuser bears all the consequences of their use.\n1.5 Licensing\nThis document is published under the terms of a Creative Commons Attribution 4.0 International\nLicense in order to facilitate its re-use by other governments and private sector organizations.\nIn summary this means you are free to share and adapt the material, including for commercial\npurposes, provided that you give appropriate credit to the UAE AI and Blockchain Council as its\nowner and do not suggest the Minister of State for Artificial Intelligence, Digital Economy and\nRemote Work Applications Office endorses your use.\n1.6 Toolkit\nThe document is part of a toolkit of self-governing nature that boosts awareness and enables\ngovernment and private institutions to pursue innovative use cases while maintaining human\nvalues and principles.\n8 92.1 AI developer organization\n02\nAn organization which does any of the following:\na determines the purpose of an AI system;\nDefinitions\nb designs an AI system;\nc builds an AI system, or:\nd performs technical maintenance or tuning on an AI system\nNote:\nThe definition applies regardless of whether the organization is the ultimate user of the system, or\nwhether they sell it on or give it away.\nExample: A company develops an artificially intelligent facial recognition system and\nsells it to a country\u2019s border force, who use it to identify suspicious personnel. The company\nis an AI developer organization and the border force is an AI operator organization.\n2.2 AI operator organization\nAn organization which does any of the following:\na uses AI systems in operations, backroom processes or decision-making;\nb uses an AI system to provide a service to an AI subject;\nc is a business owner of an AI system;\nd procures and treats data for use in an AI system; or\ne evaluates the use case for an AI system and decides whether to proceed.\nNotes:\n1. This definition applies regardless of whether the AI system was developed in-house or procured.\n2. It is possible for organizations to be both an AI developer organization and an AI operator organization.\n10 112.6 Critical decision\n2.3 Artificial Intelligence (also AI)\nThe capability of a functional unit to perform functions that are generally associated with human An individually significant decision which is deemed to either have a very large impact on an\nintelligence such as reasoning, learning and self-improvement1. individual or to have especially high stakes. These can be especially sensitive, have the potential\nto cause high loss or damage, be societally significant, or set an important precedent.\n2.4 Artificially Intelligent System (also AI system)\nNotes:\nThe types of decisions referred to here are the same as those in the definition of significant-at-\nA product, service, process or decision-making methodology whose operation or outcome is\nscale decisions, except in this case the effects are felt as a result of an individual decision rather\nmaterially influenced by artificially intelligent functional units.\nthan an aggregate of many decisions..\nNotes:\n1. It is not necessary for a system\u2019s outcome to be solely determined by artificially intelligent\nfunctional units in order for the system to be defined as an artificially intelligent system; and Example: A court determines whether a defendant is guilty of a criminal charge, with the\n2. A particular feature of AI systems is that they learn behavior and rules not explicitly punishment for guilt being a life sentence. This is a critical decision because it has a very\nprogrammed in. large impact on the life of the defendant and also sets precedent for similar cases in the\nfuture.\nExample 1: A small claims court uses Example 2: A government entity uses\n2.7 Functional Unit\nan artificially intelligent software package a chatbot which allows customers to ask\nto collect evidence pertaining to a case, routine questions, book appointments\nEntity of hardware or software, or both, capable of accomplishing a specified purpose3 .\ncompare it to similar cases in the past, and conduct minor financial transactions.\nand present a recommended decision The chatbot responds to customer\nto a judge. The judge determines the queries with pre-written responses and is\n2.8 Individually significant decision\nfinal outcome. This decision-making based on pre-programmed decision rules.\nA decision which has the potential for significant impact on at least one individual\u2019s\nmethodology is materially influenced Therefore, the chatbot is not an AI system.\ncircumstances, behavior or choices, or has legal or similarly significant effects on him or her.\nby an artificially intelligent functional If, however, the chatbot autonomously\nunit, and is therefore classified as an AI adjusted its treatment of customers\nsystem. based on the outcome of past cases, it\nwould be an AI system. Example: A company decides to make an employee redundant. This is an individually\nsignificant decision because of its potential impact on the employee\u2019s financial situation.\n2.5 Bias (of a system)\nInclination or prejudice for or against one person or group, especially in a way considered to be\nunfair2.\n2 Oxford Dictionary 2018, Oxford University Press, viewed online 4th October 2018, <https://en.oxforddictionaries.com>\n1 Consistent with ISO/IEC 2382:2015 3 From ISO/IEC 2382:2015\n12 132.9 Non-operational bias (of a system) 2.11 Significant decision\nBias that is either: A decision which is either individually significant or is part of a set of significant-at-scale\ndecisions.\na not a design feature; or\nb not important in achieving the stated purpose of the system. 2.12 Subject of an artificially intelligent system (also AI subject)\nA natural person who is any of the following:\n2.10 Set of significant-at-scale decisions\na an end-user of an AI system\nA set of decisions made by the same system or organization which, when taken in aggregate,\nb directly affected by the operation of or outcomes of an AI system, or:\nhave significant impact on society as a whole or groups within it.\nc a recipient of a service or recommendation provided by an AI system\nNotes:\n1. The decisions need not be individually significant in order to qualify, in aggregate, as a set of\nsignificant-at-scale decisions; and\n2. Examples of areas which have a large impact on society and which include but are not limited\nto: the large-scale allocation of resources or opportunities amongst groups; the structure of\ngovernment; the division of power between large entities or groups; the law, and its interpretation\nand enforcement; conflict and war; international relations, etc.\nExample: An AI system is used by a website to determine which content to show users.\nThis decision is not individually significant, since a user is not greatly affected by whether\na particular piece of media is shown to them. However, if the website is popular then the AI\nsystem may be making a set of significant-at-scale decisions, because any biases in the\nsystem will affect a large number of users.\n14 153.1 Fairness (Principle 1)\n03\n3.1.1 Principle\nPRINCIPLES AND We will make AI systems fair\nData ingested should, where possible, be accurate and representative of the affected\npopulation.\nGUIDELINES\nAlgorithms should avoid non-operational bias.\nSteps should be taken to mitigate and disclose the biases inherent in datasets.\nSignificant decisions should be provably fair.\nAll personnel involved in the development, deployment and use of AI systems have a role and\nresponsibility to operationalize AI fairness and should be educated accordingly.\n3.1.2 Guidelines\n3.1.2.1\nBenefits of AI systems should be available and accessible to all. AI systems should not discriminate\nagainst people or groups in a way that could have an adverse impact. Especially significant and\ncritical decisions made or assisted by AI should be provably fair.\na Considering that fairness can have many different definitions, an organization should\ndocument its own definition of fairness for the context that the AI system is going to be\nimplemented in. Organizations should document what the implemented fairness objective\nstands for and why this choice was considered the most suitable for the given scenario.\nb It is recommended to identify and document demographic groups that may be adversely\nimpacted and mitigate the risk where possible.\nc AI developers and AI operators could consider formal procedures such as Discrimination\nImpact Assessments as a means of ensuring fairness. This assessment should be\ndocumented as well as the mitigation measures that were implemented by the organization.\nThe impact assessment should be conducted pre-release and regularly after as an ongoing\nevaluation, results are advised to be documented.\n16 173.1.2.2 3.1.2.3\nConsideration should be given to whether the data ingested is accurate and representative Consideration should be given to whether decision-making processes introduce bias\nof the affected population\na When subjecting different groups to different decision-making processes, AI developers\nd Fairness has many different definitions in different cultures and for different contexts. should consider whether this will lead to non-operational bias.\nEncouraging a diverse and inclusive AI ecosystem is thus all the more crucial to ensure\nthat one definition of fairness does not contradict another, and that the process of defining b When evaluating the fairness of an AI system, AI developers and AI operators should\nfairness itself is fair, with under-represented groups represented in the discussion. consider whether AI subjects in the same circumstances receive equal treatment.\ne AI developers and AI operators should undertake reasonable data exploration and/or testing\nto identify potentially prejudicial decision-making tendencies in AI systems arising from\nExample: An organization uses an AI tool to automate the pre-screening of candidates\ndata inaccuracy and biases in the data.\nfor a job opening. It is trained on data from the company\u2019s existing employees, the majority\nof whom are from the same ethnic background. Therefore, the system learns to use name\nf AI developers and operators should evaluate all datasets to assess inclusiveness of and nationality as discriminating factors in filtering job applicants. This could have been\nidentified demographic groups and collect data to close any gaps.\nidentified through testing and rectified by, for example, balancing the training data or only\nusing relevant data fields for training.\ng AI developers and AI operators should refrain from training AI systems on data that is not\nlikely to be representative of the affected AI subjects, or is not likely to be accurate, whether\nthat be due to age, omission, method of collection, or other factors.\n3.1.2.4\nSignificant decisions informed by the use of AI should be fair\nExample: Following a natural disaster, a government relief agency uses an AI system to a AI developers and AI operators could consider formal procedures such as Discrimination\ndetect communities in greatest need by analyzing social media data from a range of websites. Impact Assessments as a means of ensuring fairness.\nHowever, those communities where smartphone penetration is lower having less presence on\n3.1.2.4\nsocial media, and so are at risk of receiving less attention. Therefore, the charity complements\nAI operators should consider whether their AI systems are accessible and usable in a fair\ntheir AI tool with traditional techniques to identify needy populations elsewhere.\nmanner across user groups\n3.1.2.5\nh AI developers should consider whether their AI systems can be expected to perform well Consideration should be given to the effect of diversity on the development and deployment\nwhen exposed to previously unseen data, especially when evaluating people who are not processes\nwell-represented in the training data.\nb Organizations should seek to include people from diverse demographic backgrounds across\nthe full lifecyle, to include design, development and deployment processes. Organizations\nshould seek to engage diverse internal and external groups also.\nc AI developers should consider whether the assumptions they make about AI subjects could\nbe wrong or are likely to lead to non-operational bias; if so, they should consider consulting\nthe AI subjects in a representative manner during the design, development and deployment\nto confirm these assumptions.\n18 193.2 Accountable AI (Principle 2)\n3.2.2.2\nPositive efforts should be made to identify and mitigate any significant risks inherent in the AI\nsystems designed\n3.2.1 Principle\na AI operators should only use AI systems that are backed by evidence-based academic or\nindustrial research, and AI developers should base their development on such research.\nWe will make AI systems accountable\nAccountability for the outcomes of an AI system lies not with the system itself but is\napportioned between those who design, develop and deploy it; Example: A foreign country has a government service which identifies parents who owe\nmoney in child maintenance. The data matching process is often incorrect due to misspelled\nDevelopers should make efforts to mitigate the risks inherent in the systems they design; names or missing data which results in some individuals being incorrectly targeted\nautomatically by the system with the result being a large bill, poor credit ratings and even\nAI systems should have built-in appeals procedures whereby users can challenge significant\nfreezing wages. The recourse for individuals who are incorrectly targeted is time-consuming\ndecisions;\nand not straightforward . If the potential impact of incorrect decisions had been assessed,\nmitigation measures such a user-friendly review procedure could have been set up.\nAI systems should be developed by diverse teams which include experts in the area in which\nthe system will be deployed; and\nAI systems should be subject to external audit and decision quality assurance. b AI operators should identify the likely impact of incorrect automated decisions on AI\nsubjects and, in the case where incorrect decisions are likely to cause significant cost or\ninconvenience, consider mitigating measures.\n3.2.2 Guidelines\nc AI operators should consider internal risk assessments or ethics frameworks as a means to\n3.2.2.1 facilitate the identification of risks and mitigating measures.\nAccountability for the outcomes of an AI system should not lie with the system itself\nd In designing AI systems to inform significant decisions, AI developers should consider\na Accountability for loss or damages resulting from the application of AI systems should not measures to maintain data accuracy over time, including:\nbe attributed to the system itself.\n\u2022 the completeness of the data.\nb AI operators and AI developers should consider designating individuals to be responsible for \u2022 timely update of the data, and.\ninvestigating and rectifying the cause of loss or damage arising from the deployment of AI\n\u2022 whether the context in which the data was collected affects its suitability for the\nsystems.\nintended use case.\nExample: A border camera scanning for predictors of risk may misinterpret a \u201ctic\u201d of an\nindividual with Tourette syndrome as suspicious. These can manifest in a diverse fashion, and\nshould not cause this person to undergo secondary inspection every time they pass through\nthe border . If the data is updated after the first case is encountered then it would avoid\ncausing inconvenience on subsequent visits.\n5 Government of Canada. (2018). Responsible AI in the Government of Canada. Digital Disruption White Paper Series. Version 2.0, p.26. Retrieved from:\nhttps://docs.google.com/document/d/1Sn-qBZUXEUG4dVk909eSg5qvfbpNlRhzIefWPtBwbxY/edit\n4 Cabinet Office (UK), Data Science Ethical Framework, Version 1.0, licensed under the Open Government License v3.0, p. 13 6 Stoica, I. et. al.,2017, A Berkeley View of Systems Challenges for AI, p. 2, https://arxiv.org/pdf/1712.05855.pdf\n20 21e AI developers and AI operators should tune AI models periodically to cater for changes to e Be mindful that there might be fundamental tensions between different principles and\ndata and/or models over time. requirements. Continuously identify, evaluate, document and communicate these trade-\noffs and their solutions.\nf AI operators should consider whether AI systems trained in a static environment will display\nmodel instability when deployed in dynamic environments. Adopt a Trustworthy AI assessment list when developing, deploying or using AI systems,\nf\nand adapt it to the specific use case in which the system is being applied.\nExample: AI systems will need to be able to adapt to the changes in the environment that g Keep in mind that such an assessment list will never be exhaustive. Ensuring Trustworthy\nthey are deployed in. For example, a self-driving car would need to quickly adapt to unexpected AI is not about ticking boxes, but about continuously identifying and implementing\nand dangerous road by learning in real time from other cars that have successfully dealt with requirements, evaluating solutions, ensuring improved outcomes throughout the AI\nthese conditions. In addition, such mission-critical applications must handle noisy inputs system\u2019s lifecycle, and involving stakeholders in this.\nand defend against malicious actors .\n3.2.2.4\nAI subjects should be able to challenge significant automated decisions concerning them and,\ng AI developers should collaborate with AI Operators to train models using historical data where appropriate, be able to opt out of such decisions\nfrom the Operator.\na AI operators using AI systems to inform significant decisions should provide procedures by\nwhich affected AI subjects can challenge a specific decision concerning them.\nh AI Operators should consider working with their vendors (AI developers) to continually\nmonitor performance.\nb AI operators should consider such procedures even for non-significant decisions.\ni AI Operators should subject AI systems informing significant decisions to quality checks\nat least as stringent as those that would be required of a human being taking the same c AI operators should make affected AI subjects aware of these procedures and should\ndesign them to be convenient and user-friendly.\ndecision.\nd AI operators should consider employing human case evaluators to review any such\n3.2.2.3\nchallenges and, when appropriate, overturn the challenged decision.\nAI systems informing critical decisions should be subject to appropriate external audit\na When AI systems are used for critical decisions, external auditing of the AI systems in\nquestion should be used as a means to ensure meaningful standards of transparency and Example: A bank allows customers to apply for a loan online by entering their data. The\naccountability are upheld. bank uses an AI system to automatically determine whether to give the loan and what the\ninterest rate should be. They provide users with an option to contest the decision and have\nb In the case that critical decisions are of civic interest, public release of the results of the it reviewed by a human . They also require that customers justify their challenge by filling in\naudit should be considered as a means of ensuring public processes remain accountable a form, which assists the case reviewer and deters customers from challenging a decision\nto those affected by them. without good reason.7\nIn the case that critical decision are life and death decisions, these decisions should be\nc\nsupported by further validation and verification via a human operator.\nd Facilitate traceability and auditability of AI systems, particularly in critical contexts or\nsituations.\n7 Adapted from EU Commission, Can I be subject to automated individual decision-making, including profiling? Retrieved from: https://ec.europa.eu/\ninfo/law/law-topic/data-protection/reform/rights-citizens/my-rights/can-i-be-subject-automated-individual-decision-making-including-profiling_\nen#example\n22 23e AI operators should consider instituting an opt-out mechanism for significant automated 3.2.2.7\ndecisions. AI operators should understand the AI systems they use sufficiently to assess their suitability\nfor the use case and to ensure accountability and transparency\nf AI operators could consider \u201ccrowd challenge\u201d mechanisms whereby a critical number of\ncomplaints triggers an investigation into the fairness and/or accuracy of a decision-making\na In the case of critical decisions, AI operators should avoid using AI systems that cannot be\nprocess as a whole.\nsubjected to meaningful standards of accountability and transparency.\n3.2.2.5 b AI developers should consider notifying customers and AI operators of the use cases for\nAI systems informing significant decisions should not attempt to make value judgements on which the system has been designed, and those for which it is not suitable.\npeople\u2019s behalf without prior consent\na When informing an AI subject about significant choices they will make, AI systems should\nnot unreasonably restrict the available options or otherwise attempt to influence their value\njudgements without the explicit consent of the AI subject in question.\n3.2.2.6\nAI systems informing significant decisions should be developed by diverse teams with\nappropriate backgrounds\na AI developers who develop AI systems which may be used to assist in making critical\ndecisions should involve in the process experts with a background in social science, policy,\nor another subject which prepares them to evaluate the broader societal impact of their\nwork.\nb Development of AI systems informing significant decisions should include consultation\nwith experts in the field in which the system will be deployed.\nExample: An app that uses AI to assess medical symptoms and has a large user base had\nto face regulatory scrutiny because of number of complaints from doctors. They warned\nthat the application can miss signs of serious illness. A number of different shortcomings\nwere identified by doctors, some of which the company could address and resolve.8\n8 Financial Times. 2018. High-profile health app under scrutiny after doctor\u2019s complaints. Retrieved from: https://www.ft.com/content/19dc6b7e-8529-\n11e896-dd-fa565ec55929\n24 253.3 Transparent AI (Principle 3)\nc To facilitate the above, AI developers and AI operators should consider documenting the\nfollowing information during the design, development and deployment phases, and retaining\n3.3.1 Principle this documentation for a length of time appropriate to the decision type or industry:\n\u2022 the provenance of the training data, the methods of collection and treatment, how the data was\nWe will make AI systems transparent\nmoved, and measures taken to maintain its accuracy over time;\n\u2022 the model design and algorithms employed; and\nDevelopers should build systems whose failures can be traced and diagnosed. \u2022 changes to the codebase, and authorship of those changes.\nPeople should be told when significant decisions about them are being made by AI. d Where possible given the model design, AI developers should consider building in a means\nby which the \u201cdecision journey\u201d of a specific outcome (i.e. the component decisions leading\nWithin the limits of privacy and the preservation of intellectual property, those who deploy to it) can be logged.\nAI systems should be transparent about the data and algorithms they use.\nResponsible disclosures should be provided in a timely manner, and provide reasonable\nExample: A technology company has a product which is designed to assist in medical\njustifications for AI systems outcomes. This includes information that helps people\ndiagnosis. It documents each stage of its reasoning and relates it back to the input data.9\nunderstand outcomes, like key factors used in decision making.\n3.3.2 Guidelines\n3.3.2.1\nTraceability should be considered for significant decisions, especially those that have the\npotential to result in loss, harm or damage.\na For AI systems which inform significant decisions, especially those with the potential to\ncause loss, harm or damage, AI developers should consider building in traceability (i.e. the\nability to trace the key factors leading to any specific decision).\nb Organizations should ensure that harms caused through AI systems are investigated and\nredressed, by enacting strong enforcement mechanisms and remedial actions, to make\ncertain that human rights and the rule of law are respected in the digital world and in the\nphysical world.\n9 See IBM WatsonPaths\n26 273.3.2.2 3.4 Explainable AI (Principle 4)\nPeople should be informed of the extent of their interaction with AI systems\n3.4.1 Principle\na AI operators should inform AI subjects when a significant decision affecting them has been\nmade by an AI system.\nWe will make AI systems as explainable as\ntechnically possible\nExample: A small claims court adjudicates minor civil matters such as debt collection and\nevictions. They introduce an AI system to suggest the outcome of a ruling. At the time of the\nDecisions and methodologies of AI systems which have a significant effect on individuals\nruling the plaintiff and defendant are notified that the decision was assisted by an AI system.\nshould be explainable to them, to the extent permitted by available technology.\nThe court also provides an explanation for the decision.\nIt should be possible to ascertain the key factors leading to any specific decision that\ncould have a significant effect on an individual.\nb If an AI system can convincingly impersonate a human being, it should do so only after\nIn the above situation we will provide channels through which people can request such\nnotifying the AI subject that it is an AI system.\nexplanations.\nExample: A technology company produces a conversational AI agent which can make 3.4.2 Guidelines\nsome phone calls on behalf of its users. Those who receive the calls may believe that they\n3.4.2.1\nare speaking to a human. Therefore, the company programs the agent to identify itself at\nAI operators could consider providing affected AI subjects with a high-level explanation of\nthe start of every conversation.\nhow their AI system works\na AI operators could consider informing the affected AI subjects in understandable, non-\ntechnical language of:\n\u2022 the data that is ingested by the system;\n\u2022 the types of algorithms employed;\n\u2022 the categories into which people can be placed; and\n\u2022 the most important features driving the outcomes of decisions\n\u2022 A comprehensive list of feature engineering and models that was considered during\nthe model building phase.\nExample: A person turned down for a credit card might be told that the algorithm took\ntheir credit history, age, and postcode into account, but not learn why their application was\nrejected10 .\n28 29b For non-sensitive public sector use cases designed for the common good, AI operators\ncould consider making source code, together with an explanation of the workings of the AI Example: The UK\u2019s NHS developed a tool called Predict, which allows women with breast\nsystem, available either publicly or upon request (this should be done only if there is low risk cancer to compare their case to other women who have had the same condition in the past,\nof people \u2018gaming the system\u2019). and visualize the expected survival rate under various treatment options. The website has an\nexplanation page which shows the weights behind various factors and contains a description\nc The AI operator should maintain necessary documentation that provides elaboration and of the underlying mathematics12 .\nclarification on how the algorithm works, for example documentation of processes, decision\nmaking flow charts of the system, and how the appeal process is embedded.\nd An individual should have the ability to contest and seek effective redress against decisions c In the case that such explanations are available, they should be easily and quickly accessible,\nmade by AI systems. These must be addressed by the group or team supporting these free of charge and user-friendly.\nmodels.\ne Develop appropriate impact indices for the evaluation of AI system technological 3.5 Robust, Safe and Secure AI (Principle 5)\ninterventions from multiple perspectives.\n3.5.1 Principles\n3.4.2.2\nAI operators should consider providing affected AI subjects with a means to request\nexplanations for specific significant decisions, to the extent possible given the state of present AI systems will be technically robust\nresearch and the choice of model\nAI systems should be technically robust with a preventative approach to risks which\na AI operators should consider providing a means by which people affected by a significant\noperates in a manner such that they reliably behave.\ndecision informed by AI can access the reasoning behind that decision.\nAI Developers should ensure AI systems will not cause any unintentional harm and\nadverse impacts.\nExample: The US Consumer Financial Protection Bureau requires that creditors in the US\nwho reject credit applications must explain to the applicant the principal reason(s) why their\nAI systems should be resilient to attacks and security such as data poisoning and\napplication was rejected (e.g. \u201clength of residence\u201d or \u201cage of automobile\u201d)11 . In particular,\nmodel leakage.\n\u201cstatements that the adverse action was based on the creditor`s internal standards or\npolicies or that the applicant, joint applicant, or similar party failed to achieve a qualifying AI systems should have safeguards that enable a fallback plan in case of problems.\nscore on the creditor`s credit scoring system are insufficient\u201d.\nAI system results should be reproducible.\nb Where such explainability is not possible given available technology, AI operators should\nconsider compromises such as counterfactual reasoning, or listing the most heavily\nweighted factors contributing to the decision.\n11 Consumer Financial Protection Bureau, 12 CFR Part 1002 - Equal Credit Opportunity Act (Regulation B), \u00a7 1002.9 Notifications, Retrieved from https://www.\n10 The Guardian. AI watchdog needed to regulate automated decision-making, say experts. Retrieved from: https://www.theguardian.com/technology/2017/ consumerfinance.gov/policy-compliance/rulemaking/regulations/1002/\njan/27/ai-artificial-intelligence-watchdog-needed-to-prevent-discriminatory-automated-decisions 12 Predict website, accessible at http://www.predict.nhs.uk/predict_v2.1/legal/algorithm\n30 313.5.2 Guidelines\n3.5.2.1 3.5.2.3\nAI Operators should continue conducting vulnerability assessments, verification of AI system AI Operators should assure end users of the system\u2019s reliability through documentation, and\ndifferent behaviors in unexpected situations and for any dual-use case, to include: operationalizing processes for testing and verification of desired outcomes , specifically:\na Putting measures to ensure integrity and resilience of IA system against potential attacks. a AI system should be designed with an approach to continue monitoring if the system meets\ngoals, purposes and intended application.\nb AI Operators should define an approach to ensure results are reproducible through clear\nExample: IBM researchers presented a novel approach to exploiting AI for malicious\nprocess and documentation.\npurposes. Their system, DeepLocker embeds AI capabilities within the malware itself in order\nto improve its evasion techniques. It uses an Artificial Intelligence model to identify its target\nc AI Operators should publish documentation to assure system robustness to end users.\nusing indicators like facial recognition, geolocation and voice recognition.\n3.5.2.4\nAI systems will be safe, secure and controllable by humans\nb Verifying how AI systems behave in unexpected situations and environments.\na Safety and security of people, be they operators, end-users or other parties, will be of\nc Taking preventative measures against any possible system dual-case use. paramount concern in the design of any AI system.\nb AI systems should be verifiably secure and controllable throughout their operational\n3.5.2.2\nlifetime, to the extent permitted by technology.\nAI operators should define a suitable fall back plan and test it to maintain readiness in\nunexpected situations and in high safety risks level, to include:\nc The continued security and privacy of users should be considered when decommissioning\na Developing a plan to measure and assess potential safety risks to you or any third party AI systems.\nfrom technology use accidental or malicious misuse.\nd AI systems that may directly impact people\u2019s lives in a significant way should receive\nb Define thresholds for system acceptable results and governance procure to fall back on commensurate care in their designs.\nalternative defined and tested plans.\ne Such systems should be able to be overridden or their decisions reversed by designated\nc Considering an insurance policy to mitigate risks arising from potential damages. individuals.\nExample: Adopting a risk-based approach to procurement and clearly communicating it to\nvendors can help address such issues by giving the procuring organization advance notice\nof the specific oversight capabilities it will need in future stages of the system lifecycle,\npreventing vendors from presenting intellectual property arguments against required testing,\nmonitoring and auditing of their AI systems going forward and rewarding vendors\u2014of all\nsizes\u2014that are more advanced and responsive in their responsible AI efforts.\n13 IBM. https://documents.trendmicro.com/assets/white_papers/wp-malicious-uses-and-abuses-of-artificial-intelligence.pdf\n32 333.6.2 Guidelines\n3.5.2.5\nAi systems should not be able to autonomously hurt, destroy or deceive humans\n3.6.2.1\na AI systems should be built to serve and inform, and not to deceive and manipulate.\nAI should be developed to align with human values, contribute to human flourishing and\nbenefit society.\nb Nations should collaborate to avoid an arms race in lethal autonomous weapons, and\nsuch weapons should be tightly controlled. a Society should be consulted in a representative fashion to inform the development of AI.\nc Active cooperation should be pursued to avoid corner-cutting on safety standards. b Stakeholders throughout society should be involved in the development of AI and its\ngovernance.\nSystems designed to inform significant decisions should do so impartially.\nd\nc Stakeholders should proactively engage in responsible stewardship of trustworthy AI in\npursuit of beneficial outcomes for people and the planet.\n3.6 Human Centered AI (Principle 6)\nd Organizations should prioritize having all their stakeholders learn about wellbeing metrics\nas a potential determinant of how they create, deploy, market and monitor their AI\n3.6.1 Principles technologies.\nWe will give AI systems human values and\nExample: IEEE P7010 recommended practice14 establishes wellbeing metrics relating to\nmake them beneficial to society human factors directly affected by intelligent and autonomous systems and establishes a\nbaseline for the types of objective and subjective data these systems should analyze and\nGovernment will support the research of the beneficial use of AI. include (in their programming and functioning) to proactively increase human wellbeing.\nStakeholders throughout society should be involved in the development of AI and its\ngovernance.\n3.6.2.2\nSystematic analyses that examine the ethics of designing affective systems to nudge human\nDesign AI systems to adopt, learn and follow the norms and values of the community\nbeings prior to deployment are needed.\nthey serve.\nSystematic analyses that examine the ethics of designing affective systems to nudge a Nudging in AI systems should have an opt-in system policy with explicit consent.\nhuman beings prior to deployment are needed.\nb We recommend that when appropriate, an affective system that nudges human beings\nDecisions related to lethal force, life and death should not be delegated to AI systems. should have the ability to accurately distinguish between users, including detecting\nRules and standards should be adopted to ensure effective human control over those characteristics such as whether the user is an adult or a child. Additional protections must\ndecisions. be put in place for vulnerable populations, such as children, when informed consent cannot\nbe obtained, or when it may not be a sufficient safeguard.\nc AI systems with nudging strategies must be carefully evaluated, monitored, and controlled.\n14 IEEE. https://sagroups.ieee.org/7010/\n34 353.6.2.3 f In situations where it is needed, human rights impact assessments and human rights due\nHumanity should retain the power to govern itself and make the final decision, with AI in an diligence, human determination codes of ethical conduct, or quality labels and certifications\nassisting role. intended to promote human centered values and fairness should be considered.\ng Mechanisms should be put into place to receive external feedback regarding AI systems\na Decisions related to lethal force, life and death should not be delegated to AI systems. Rules\nthat potentially infringe on fundamental rights.\nand standards should be adopted to ensure effective human control over those decisions.\nb Responsible parties (e.g., parents, nurse practitioners, social workers, and governments)\nshould be trained to detect the influence due to AI and ineffective mitigation techniques.\nIn the most extreme cases it should always be possible to shut down harmful AI system.\nc Those actions undertaken by an affective system that are most likely to generate an\nemotional response should be designed to be easily changed.\nd Users should be able to make informed autonomous decisions regarding AI systems. They\nshould be given appropriate knowledge and tools to comprehend and interact with AI\nsystems to satisfactory degree and, where possible to reasonably self-assess or challenge\nthe system.\n3.6.2.4\nAI systems should be designed in a way that respects the rule of law, human rights, society\nvalues, and should include appropriate safeguards to ensure a fair and just society.\na AI Developers should design AI systems to adopt, learn and follow the norms and values of\nthe community they serve.\nb Organizations could identify the norms of the specific community in which AI systems are\nto be deployed in. In particular, pay attention to the norms relevant to the kinds of tasks that\nthe AI systems are designed to perform. These could be documented as well as how these\nnorms are addressed by the AI system.\nc To respond to the dynamic change of norms in society the AI system could be able to adjust\nits existing norms and learn new ones, while being transparent about these changes.\nd Designers should consider forms and metrics for assessing an AI system\u2019s norm conformity\nover the lifetime of the system (e.g. human-machine agreement on moral decisions,\nverifiability of AI decisions, justified trust).\ne The norm identification process must document the similarities and differences between\nthe norms that humans apply to other humans and the norms they apply to AI systems.\nNorm implementations should be evaluated specifically against the norms that the\ncommunity expects the AI system to follow.\n36 373.7 Sustainable and Environmentally Friendly AI (Principle 7)\nd While there is no standardized means of reporting on the environmental impact of AI\nsystems, where possible, model development and commissioning should make carbon\n3.7.1 Principle impacts a core consideration alongside functional and business requirements.\ne Smaller models should be considered \u2013 shrinking down the model size and using fewer\nWe will promote sustainable and environmentally compute cycles (to balance financial and performance costs with the end performance of\nthe model).\nfriendly AI\nf Carbon awareness should be considered \u2013 adjusting the operational parameters of the AI\nAI systems should be used to benefit all human beings, including future generations. The\nsystem to dynamically select the best time and location for energy use from the grid can\nenvironment is fundamental to this.\nreduce its carbon footprint.\nThroughout the AI lifecycle, implementations should only be carried out on the basis of a full\nunderstanding and acknowledgement of implications for sustainability and environment.\nExample: In agriculture, AI can transform Example: Google`s DeepMind division\nAn AI system\u2019s development, deployment and use, should be assessed via critical examination production by better monitoring and has developed AI that teaches itself to\nof resource usage and energy consumption.\nmanaging environmental conditions minimize the use of energy to cool Google`s\nand crop yields. AI can help reduce both data centers. As a result, Google reduced\nMechanisms to measure environmental impact due to type of energy use and processing\nfertilizer and water, all while improving its data center energy requirements by\npower provided by data centers should be established.\ncrop yields. 40% 15.\n3.7.2 Guidelines\n3.7.2.1 Throughout the AI lifecycle, implementations should always be carried out after full\nunderstanding and acknowledgement of AI implications on sustainability and environment.\na The application of Artificial Intelligence to address sustainability challenges are well\nunderstood. Building and running green, sustainable AI systems is of increasing importance,\ngiven the large carbon footprint that they can generate and the wider context of addressing\nclimate change.\nb AI model development, and by extension commissioning, should therefore seek to balance\ntechnical performance with energy consumption and environmental impact.\nc Efforts should be made to estimate and understand sustainability and environmental\nimpact across the AI lifecycle (e.g. the energy consumption costs associated with model\ntraining, and CO2 emissions and cloud compute costs associated with the deployment\nand running of the system).\n15 https://www.deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-by-40\n38 393.8 Privacy Preserving AI (Principle 8)\n3.8.2.2\nEstablish an oversight mechanism for data collection, storage and processing and use across\nyour organization\n3.8.1 Principle\na Assess who can access data and under which conditions.\nWe will respect people\u2019s privacy\nb Assign specific responsibilities and role for Data Protection Officers.\nAI systems should respect privacy and use the minimum intrusion necessary.\nc Prevention of harm to privacy necessitates adequate data governance that covers the\nquality and integrity of the data used, its relevance in light of the domain in which the\nAI systems should uphold high standards of data governance and security,\nprotecting personal information. AI systems will be deployed, its access protocols and the capability to process data in a\nmanner that protects privacy.\nSurveillance or other AI-driven technologies should not be deployed to the extent\nof violating internationally and/or UAE\u2019s accepted standards of privacy and human d AI systems must guarantee privacy and data protection throughout a system\u2019s entire\ndignity and people rights. lifecycle. This includes the information initially provided by the user, as well as the\ninformation generated about the user over the course of their interaction with the system\nPrivacy by design should be embedded in AI systems and where possible AI\n(e.g. outputs that the AI systems generated for specific users or how users responded to\nalgorithm should have adequate privacy impact assessments.\nparticular recommendations). Digital records of human behavior may allow AI systems\nto infer not only individuals\u2019 preferences, but also their age, gender, religious or political\nAdequate data protection frameworks and governance mechanisms should be\nviews. To allow individuals to trust the data gathering process, it must be ensured that data\nestablished in a multi-stakeholder approach and ensured throughout the life cycle\ncollected about them will not be used to unlawfully or unfairly discriminate against them.\nof AI systems.\ne In any given organization that handles individuals\u2019 data (whether someone is a user of the\nAI developers and operators should strive to strike the balance between privacy\nsystem or not), data protocols governing data access should be put in place in line with\nrequirements, individual rights and innovation growth and society benefits.\nnational privacy legislation (be these universal or sector-specific). These protocols should\noutline who can access data and under which circumstances. Only duly qualified personnel\n3.8.2 Guidelines\nwith the competence and need to access individual\u2019s data should be allowed to do so.\n3.8.2.1\nEstablish mechanism for users to flag issues related to privacy or data protection\na AI Operators should review the system for proper consent logging, ability of users to revoke\npermission whenever applicable.\nb Consider training AI model without or with minimal user of potentially sensitive or personal\ndata.\nc Use measures to enhance privacy such as encryption, anonymization and aggregation.\n40 41f Algorithmic systems require adequate privacy impact assessments, which also include\nsocietal and ethical considerations of their use and an innovative use of the privacy by\ndesign approach. AI actors need to ensure that they are accountable for the design and\nimplementation of AI systems in such a way as to ensure that personal information is\nprotected throughout the life cycle of the AI system.\ng Establish data policies or equivalent frameworks, or reinforce existing ones, to ensure full\nsecurity for personal data and sensitive data, which, if disclosed, may cause exceptional\ndamage, injury or hardship to individuals. Examples include data relating to offences,\ncriminal proceedings and convictions, and related security measures; biometric, genetic\nand health data; and personal data such as that relating to race, descent, gender, age,\nlanguage, religion, political opinion, national origin, ethnic origin, social origin, economic or\nsocial condition of birth, or disability and any other characteristics.\nh Promote mechanisms, such as open repositories for publicly funded or publicly held data,\nsource code and data trusts, to support the safe, fair, legal and ethical sharing of data,\namong others.\ni Promote and facilitate the use of quality and robust datasets for training, development and\nuse of AI systems, and exercise vigilance in overseeing their collection and use.\nExample: with the help of AI & ML algorithms16 , Synthetic data can be created to enhance\nprivacy. The data created will have the same statistical characteristics for the testing\nenvironment and where a third party can have access.\n16 https://research.aimultiple.com/privacy-enhancing-technologies/\n42 43Microsoft. 2022. Microsoft Responsible AI Standard, v2. General Requirements. June 2022.\n04\nRetrieved from:\nhttps://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4ZPmV\nISO. 2022. Proposed ISO Standard on Risk Management of AI: What Businesses Should Know.\nBibliography\nMay 2022. Retrieved from:\nhttps://unesdoc.unesco.org/ark:/48223/pf0000381137/PDF/381137eng.pdf.multi\nGSMA 2022. The AI Ethics Playbook. Implementing ethical principles into everyday business.\nFebruary 2022. Retrieved from:\nhttps://www.gsma.com/betterfuture/wp-content/uploads/202201//The-Mobile-Industry-Ethics-\nPlaybook_Feb-2022.pdf\nOECD. 2022. AI Policy Observatory. Retrieved from:\nhttps://standards.ieee.org/wp-content/uploads/import/documents/other/ead_v2.pdf\nUNESCO. 2022. Recommendation on the Ethics of Artificial Intelligence. Adopted in November\n2021. Retrieved from:\nhttps://unesdoc.unesco.org/ark:/48223/pf0000381137/PDF/381137eng.pdf.multi\nEuropean Commission. 2021. Proposal for a Regulation of the European Parliament and of the\nCouncil Laying Down Harmonized Rules on Artificial Intelligence (Artificial Intelligence Act) and\nAmending Certain Union Legislative Acts. Retrieved from:\nhttps://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A52021PC0206\nEuropean Commission. High Level Expert Group on Artificial Intelligence ( AI HLEG ). Ethics\nGuidelines for Trustworthy AI . April 2019. Retrieved from:\nhttps://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai\nCentre for Data Ethics and Innovation. UK Government: The roadmap to an effective AI assurance\necosystem. December 2021. Retrieved from:\nhttps://www.gov.uk/government/publications/the-roadmap-to-an-effective-ai-assurance-\necosystem\nThe New York City Council. 2021. A Local Law to amend the administrative code of the city of New\nYork, in relation to automated employment decision tools. Retrieved from:\nhttps://legistar.council.nyc.gov/LegislationDetail.aspx?ID=4344524&GUID=%20B051915D-A9AC-\n451E-81F86596032-FA3F9&\nUNESCO, Recommendation on the Ethics of Artificial Intelligence. 2021. Retrieved from:\nhttps://unesdoc.unesco.org/ark:/48223/pf0000381137\n44 45AI NOW Institute. Algorithmic accountability for the public sector. August 2021. Retrieved from: CNIL. (2017). How can humans keep the upper hand? The ethical matters raised by algorithms\nhttps://www.opengovpartnership.org/documents/algorithmic-accountability-public-sector/ and artificial intelligence. Retrieved from:\nhttps://www.cnil.fr/sites/default/files/atoms/files/cnil_rapport_ai_gb_web.pdf\nIEEE 2019. Ethically Aligned Design. Version 2 \u2013 For Public Discussion. Retrieved from:\nhttps://standards.ieee.org/wp-content/uploads/import/documents/other/ead_v2.pdf Executive Office of the President of the United States. National Science and Technology Council.\nNetworking and Information Technology Research and Development Subcommittee. (2016). The\nnational artificial intelligence research and development strategic plan. Retrieved from:\nIEEE 2017. Ethically Aligned Designed : A vision for Prioritizing Human Well-being with\nhttps://www.nitrd.gov/PUBS/national_ai_rd_strategic_plan.pdf\nAutonomous and Intelligent Systems. Retrieved from:\nhttps://standards.ieee.org/wp-content/uploads/import/documents/other/ead_v2.pdf\nExecutive Office of the President of the United States National Science and Technology Council.\nCommittee on Technology. (2016). Preparing for the future of artificial intelligence. Retrieved from:\nGoogle. AI Principles 2021. Retrieved from:\nhttps://obamawhitehouse.archives.gov/sites/default/files/whitehouse_files/microsites/ostp\nhttps://ai.google/principles/\nNSTC/preparing_for_the_future_of_ai.pdf\nMicrosoft. Responsible AI Standard v2 (June 2022). Retrieved from:\nThe Headquarters for Japan\u2019s Economic Revitalization. (2015). New Robot Strategy. Japan\u2019s\nhttps://www.microsoft.com/en-us/ai/responsible-ai\nRobot Strategy. Vision, Strategy, Action Plan. Retrieved from:\nhttp://www.meti.go.jp/english/press/2015/pdf/0123_01b.pdf\nThe ECONOMIST Group 2022 . Pushing forward: the future of AI in the Middle East and North\nAfrica Report. Retrieved from:\nTreasury Board of Canada Secretariat. (2018). Responsible Artificial Intelligence in the\nhttps://impact.economist.com/perspectives/sites/default/files/google_ai_mena_report.pdf\nGovernment of Canada. Digital Disruption White Paper Series. Version 2.0.\nhttps://docs.google.com/document/d/1Sn-qBZUXEUG4dVk909eSg5qvfbpNlRhzIefWPtBwbxY/\nPDPC. (2018, June 5). Discussion paper on Artificial Intelligence (AI) and Personal Data.\nedit\nSingapore: Personal Data Protection Commission Singapore (PDPC). Retrieved from:\nhttps://www.pdpc.gov.sg/-/media/Files/PDPC/PDF-Files/Resource-for-Organisation/AI/\nThe Toronto Declaration: Protecting the right to equality and non-discrimination in machine\nDiscussion-Paper-on-AI-and-PD---050618.pdf\nlearning systems. Toronto, Canada: Amnesty International and Access Now. Retrieved from:\nhttps://www.accessnow.org/cms/assets/uploads/201808//The-Toronto-Declaration_ENG_08-\nITI. AI Policy Principles. Retrieved from:\n2018.pdf\nhttps://www.itic.org/public-policy/ITIAIPolicyPrinciplesFINAL.pdf\nhttp://data.parliament.uk/writtenevidence/committeeevidence.svc/evidencedocument/artificial-\nCabinet Office.(2016, May 19). Data Science Ethical Framework. Retrieved from: intelligence-committee/artificial-intelligence/oral/73546.html (house of lords select committee\nhttps://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/ oral evidence, Q61)oral evidence, Q61)\nfile/524298/Data_science_ethics_framework_v1.0_for_publication__1_.pdf\nHouse of Lords Select Committee on Artificial Intelligence. (2018). AI in the UK: ready, willing and\nEuropean Parliament. (2017, February 16). European Parliament resolution of 16 February 2017 able?\nwith recommendations to the Commission on Civil Law Rules on Robotics (20152103/(INL)). https://publications.parliament.uk/pa/ld201719/ldselect/ldai/100100/.pdf\nRetrieved from:\nhttp://www.europarl.europa.eu/sides/getDoc.do?pubRef=-//EP//TEXT+TA+P8-TA-2017- House of Commons Science and Technology Select Committee. Algorithms in Decision Making.\n0++0051DOC+XML+V0//EN https://publications.parliament.uk/pa/cm201719/cmselect/cmsctech/351351/.pdf\nVillani, C. (2018). For a meaningful Artificial Intelligence towards a French and European strategy.\nRetrieved from:\nhttps://www.aiforhumanity.fr/pdfs/MissionVillani_Report_ENG-VF.pdf\n46 47", "metadata": {"country": "Saudi Arabia", "year": "2023", "legally_binding": "no", "binding_proof": "None", "date": "09/01/2023", "regulator": "Saudi Data and Artificial Intelligence Authority (SDAIA)", "type": "non-binding guidelines", "status": "active", "language": "English, Arabic", "use_cases": "[2, 3, 5]"}}
{"_id": "68695c7f5520a4cc7989de85", "title": "Personal Data Protection Law", "source": "https://sdaia.gov.sa/en/SDAIA/about/Documents/Personal%20Data%20English%20V2-23April2023-%20Reviewed-.pdf", "text": "AI ETHICS\nARTIFICIAL INTELLIGENCE ETHICS\nPRINCIPLES & GUIDELINES\nPRINCIPLES & GUIDELINES\nDEC 2022Table of Content\nPage Page\n1. Overview 4 3.4 Explainable 29\n1.1 Introduction 5 3.4.1 Principle 29\nWe will make AI systems as explainable as technically possible\n1.2 Scope 6 3.4.2 Guidelines 29\n1.3 Strategic alignment 6 3.5 Robust, Safe and Secure AI 31\n1.4 Responsibility 8 3.5.1 Principles 31\nAI systems will be technically robust\n3.5.2 Guidelines 32\n1.5 Licensing 8\n3.6 Human Centred AI 34\n1.6 Toolkit 8\n3.6.1 Principles 34\nWe will give AI systems human values and make them beneficial to society\n3.6.2 Guidelines 35\n2. Definitions 10\n3.7 Sustainable and Environmentally Friendly AI 38\n3.7.1 Principle 38\n3. Principles and Guidelines 16\nWe will promote sustainabity and environemntly friendly AI 38\n3.7.2 Guidelines 38\n3.1 Fairness 17\n3.8 Privacy Preserving AI 38\n3.1.1 Principle 17\nWe will make AI systems fair\n3.8.1 Principle 40\n3.1.2 Guidelines 20\nWe will respect people\u2019s privacy\n3.8.2 Guidelines 40\n3.2 Accountable AI 20\n3.2.1 Principle 20\nWe will make AI systems accountable\n4. Bibliography 44\n3.2.2 Guidelines 20\n3.3 Transparent AI 26\n3.3.1 Principle 26\nWe will make AI systems transparent\n3.3.2 Guidelines 26\n2 301 1.1 Introduction\nAI\u2019s rapid advancement and innovation potential across a range of fields is incredibly exciting.\nYet a thorough and open discussion around AI ethics, and the principles organizations using this\nOverview\ntechnology must consider, is needed.\nThis document - AI Ethics: Principles and Guidelines \u2013 intends to meet this need to balance these\ntwo central considerations of AI. It is designed to offer detailed guidance to help AI actors adhere\nto eight principles of AI ethics.\nWe will make AI systems that are:\nexplainable transparent accountable fair\nprivacy preserving sustainable and human centered robust, safe and\nenvironmentally secure\nfriendly\nThe guidelines are non-binding, and are drafted as a collaborative multi-\nstakeholder effort, with full awareness of organizations\u2019 needs to innovate\nand protect their intellectual property. This is a collaborative process in\nwhich all stakeholders are invited to be part of an ongoing dialogue. We\nwould like to see the AI Ethics Guidelines evolve into a universal, practical\nand applicable framework informing ethical requirements for AI design and\nuse. The eventual goal is to reach widespread agreement and adoption of\ncommonly agreed policies to inform the ethical use of AI nationally and\naround the world.\n4 51.2 Scope Additionally, the AI Office is currently developing additional policies to be taken into consideration\nalong with this document:\nThis document gives non-mandatory guidelines for achieving the ethical design\nand deployment of AI systems in both the public and private sectors. AI already\nAI Seal AI Procurement Guidelines\nsurrounds us, but some applications are more visible and sensitive than others.\nThe UAE AI Seal brand (UAI) will be used The AI Procurement Guidelines will be\nThis document is applicable only to those AI systems which make or inform\nto attract talent and business from used to guide Federal Government entities\n\u2018significant decisions\u2019 \u2013 that is, those decisions which have the potential for\nacross the globe to come to the UAE to in the procurement of AI systems. They\nsignificant impact either on individuals or on society as a whole. They also apply to\ntest and develop AI. This includes a UAI will list the general principles, proposed\n\u2018critical decisions\u2019, which are a subset of significant decisions and are of especially\nmark recognizing high quality, ethical AI mechanisms for procurement and review\ncritical nature.\ncompanies. It would reward safe, efficient, of AI products between the federal\nverified AI technology with a \u2018UAI Seal government and vendors. The policy\nThis document guides the further developments of sector specific requirements\nof Approval\u2019. The UAE AI Seal brand is considers R&D for specific challenges\nand principles, hence every entity within its sector can further amend, use, or\ncurrently under development by the AI that do not currently have an AI solution\ncomplement its current frameworks to properly suit its context and the need of\nOffice. product that is market-ready. Based on\naffected stakeholders.\nglobal standards for AI Procurement, the\npolicy aims to incentivize AI adoption\nIt is a living document and will undergo further future reviews and enhancements.\nwithin government entities and promote\nAI R&D within the private sector for\nspecified government challenges. The\nprocurement policy is currently under\ndevelopment by the AI Office.\n1.3 Strategic alignment\nThis document comes as a fulfillment of the goals and aspirations set in the UAE National AI\nStrategy and the international Sustainable Development Goals (SDGs). The UAE has long been\nat the forefront of sustainable development. These shared global milestones have created global\nmovement and adoption in several disciplines and sectors. As an early adopter, the UAE has\nestablished a national committee on SDGs with a long-term realization of the UAE SDG agenda\n2030. These goals are further engrained and reflected in the AI Ethics Principles under the topics of\nfairness, inclusiveness, equality, human benefit, and rights.\n6 71.4 Responsibility\nThe AI Office will not be responsible for any misuse of the AI Ethics Principles and Guidelines. The\nuser bears all the consequences of their use.\n1.5 Licensing\nThis document is published under the terms of a Creative Commons Attribution 4.0 International\nLicense in order to facilitate its re-use by other governments and private sector organizations.\nIn summary this means you are free to share and adapt the material, including for commercial\npurposes, provided that you give appropriate credit to the UAE AI and Blockchain Council as its\nowner and do not suggest the Minister of State for Artificial Intelligence, Digital Economy and\nRemote Work Applications Office endorses your use.\n1.6 Toolkit\nThe document is part of a toolkit of self-governing nature that boosts awareness and enables\ngovernment and private institutions to pursue innovative use cases while maintaining human\nvalues and principles.\n8 92.1 AI developer organization\n02\nAn organization which does any of the following:\na determines the purpose of an AI system;\nDefinitions\nb designs an AI system;\nc builds an AI system, or:\nd performs technical maintenance or tuning on an AI system\nNote:\nThe definition applies regardless of whether the organization is the ultimate user of the system, or\nwhether they sell it on or give it away.\nExample: A company develops an artificially intelligent facial recognition system and\nsells it to a country\u2019s border force, who use it to identify suspicious personnel. The company\nis an AI developer organization and the border force is an AI operator organization.\n2.2 AI operator organization\nAn organization which does any of the following:\na uses AI systems in operations, backroom processes or decision-making;\nb uses an AI system to provide a service to an AI subject;\nc is a business owner of an AI system;\nd procures and treats data for use in an AI system; or\ne evaluates the use case for an AI system and decides whether to proceed.\nNotes:\n1. This definition applies regardless of whether the AI system was developed in-house or procured.\n2. It is possible for organizations to be both an AI developer organization and an AI operator organization.\n10 112.6 Critical decision\n2.3 Artificial Intelligence (also AI)\nThe capability of a functional unit to perform functions that are generally associated with human An individually significant decision which is deemed to either have a very large impact on an\nintelligence such as reasoning, learning and self-improvement1. individual or to have especially high stakes. These can be especially sensitive, have the potential\nto cause high loss or damage, be societally significant, or set an important precedent.\n2.4 Artificially Intelligent System (also AI system)\nNotes:\nThe types of decisions referred to here are the same as those in the definition of significant-at-\nA product, service, process or decision-making methodology whose operation or outcome is\nscale decisions, except in this case the effects are felt as a result of an individual decision rather\nmaterially influenced by artificially intelligent functional units.\nthan an aggregate of many decisions..\nNotes:\n1. It is not necessary for a system\u2019s outcome to be solely determined by artificially intelligent\nfunctional units in order for the system to be defined as an artificially intelligent system; and Example: A court determines whether a defendant is guilty of a criminal charge, with the\n2. A particular feature of AI systems is that they learn behavior and rules not explicitly punishment for guilt being a life sentence. This is a critical decision because it has a very\nprogrammed in. large impact on the life of the defendant and also sets precedent for similar cases in the\nfuture.\nExample 1: A small claims court uses Example 2: A government entity uses\n2.7 Functional Unit\nan artificially intelligent software package a chatbot which allows customers to ask\nto collect evidence pertaining to a case, routine questions, book appointments\nEntity of hardware or software, or both, capable of accomplishing a specified purpose3 .\ncompare it to similar cases in the past, and conduct minor financial transactions.\nand present a recommended decision The chatbot responds to customer\nto a judge. The judge determines the queries with pre-written responses and is\n2.8 Individually significant decision\nfinal outcome. This decision-making based on pre-programmed decision rules.\nA decision which has the potential for significant impact on at least one individual\u2019s\nmethodology is materially influenced Therefore, the chatbot is not an AI system.\ncircumstances, behavior or choices, or has legal or similarly significant effects on him or her.\nby an artificially intelligent functional If, however, the chatbot autonomously\nunit, and is therefore classified as an AI adjusted its treatment of customers\nsystem. based on the outcome of past cases, it\nwould be an AI system. Example: A company decides to make an employee redundant. This is an individually\nsignificant decision because of its potential impact on the employee\u2019s financial situation.\n2.5 Bias (of a system)\nInclination or prejudice for or against one person or group, especially in a way considered to be\nunfair2.\n2 Oxford Dictionary 2018, Oxford University Press, viewed online 4th October 2018, <https://en.oxforddictionaries.com>\n1 Consistent with ISO/IEC 2382:2015 3 From ISO/IEC 2382:2015\n12 132.9 Non-operational bias (of a system) 2.11 Significant decision\nBias that is either: A decision which is either individually significant or is part of a set of significant-at-scale\ndecisions.\na not a design feature; or\nb not important in achieving the stated purpose of the system. 2.12 Subject of an artificially intelligent system (also AI subject)\nA natural person who is any of the following:\n2.10 Set of significant-at-scale decisions\na an end-user of an AI system\nA set of decisions made by the same system or organization which, when taken in aggregate,\nb directly affected by the operation of or outcomes of an AI system, or:\nhave significant impact on society as a whole or groups within it.\nc a recipient of a service or recommendation provided by an AI system\nNotes:\n1. The decisions need not be individually significant in order to qualify, in aggregate, as a set of\nsignificant-at-scale decisions; and\n2. Examples of areas which have a large impact on society and which include but are not limited\nto: the large-scale allocation of resources or opportunities amongst groups; the structure of\ngovernment; the division of power between large entities or groups; the law, and its interpretation\nand enforcement; conflict and war; international relations, etc.\nExample: An AI system is used by a website to determine which content to show users.\nThis decision is not individually significant, since a user is not greatly affected by whether\na particular piece of media is shown to them. However, if the website is popular then the AI\nsystem may be making a set of significant-at-scale decisions, because any biases in the\nsystem will affect a large number of users.\n14 153.1 Fairness (Principle 1)\n03\n3.1.1 Principle\nPRINCIPLES AND We will make AI systems fair\nData ingested should, where possible, be accurate and representative of the affected\npopulation.\nGUIDELINES\nAlgorithms should avoid non-operational bias.\nSteps should be taken to mitigate and disclose the biases inherent in datasets.\nSignificant decisions should be provably fair.\nAll personnel involved in the development, deployment and use of AI systems have a role and\nresponsibility to operationalize AI fairness and should be educated accordingly.\n3.1.2 Guidelines\n3.1.2.1\nBenefits of AI systems should be available and accessible to all. AI systems should not discriminate\nagainst people or groups in a way that could have an adverse impact. Especially significant and\ncritical decisions made or assisted by AI should be provably fair.\na Considering that fairness can have many different definitions, an organization should\ndocument its own definition of fairness for the context that the AI system is going to be\nimplemented in. Organizations should document what the implemented fairness objective\nstands for and why this choice was considered the most suitable for the given scenario.\nb It is recommended to identify and document demographic groups that may be adversely\nimpacted and mitigate the risk where possible.\nc AI developers and AI operators could consider formal procedures such as Discrimination\nImpact Assessments as a means of ensuring fairness. This assessment should be\ndocumented as well as the mitigation measures that were implemented by the organization.\nThe impact assessment should be conducted pre-release and regularly after as an ongoing\nevaluation, results are advised to be documented.\n16 173.1.2.2 3.1.2.3\nConsideration should be given to whether the data ingested is accurate and representative Consideration should be given to whether decision-making processes introduce bias\nof the affected population\na When subjecting different groups to different decision-making processes, AI developers\nd Fairness has many different definitions in different cultures and for different contexts. should consider whether this will lead to non-operational bias.\nEncouraging a diverse and inclusive AI ecosystem is thus all the more crucial to ensure\nthat one definition of fairness does not contradict another, and that the process of defining b When evaluating the fairness of an AI system, AI developers and AI operators should\nfairness itself is fair, with under-represented groups represented in the discussion. consider whether AI subjects in the same circumstances receive equal treatment.\ne AI developers and AI operators should undertake reasonable data exploration and/or testing\nto identify potentially prejudicial decision-making tendencies in AI systems arising from\nExample: An organization uses an AI tool to automate the pre-screening of candidates\ndata inaccuracy and biases in the data.\nfor a job opening. It is trained on data from the company\u2019s existing employees, the majority\nof whom are from the same ethnic background. Therefore, the system learns to use name\nf AI developers and operators should evaluate all datasets to assess inclusiveness of and nationality as discriminating factors in filtering job applicants. This could have been\nidentified demographic groups and collect data to close any gaps.\nidentified through testing and rectified by, for example, balancing the training data or only\nusing relevant data fields for training.\ng AI developers and AI operators should refrain from training AI systems on data that is not\nlikely to be representative of the affected AI subjects, or is not likely to be accurate, whether\nthat be due to age, omission, method of collection, or other factors.\n3.1.2.4\nSignificant decisions informed by the use of AI should be fair\nExample: Following a natural disaster, a government relief agency uses an AI system to a AI developers and AI operators could consider formal procedures such as Discrimination\ndetect communities in greatest need by analyzing social media data from a range of websites. Impact Assessments as a means of ensuring fairness.\nHowever, those communities where smartphone penetration is lower having less presence on\n3.1.2.4\nsocial media, and so are at risk of receiving less attention. Therefore, the charity complements\nAI operators should consider whether their AI systems are accessible and usable in a fair\ntheir AI tool with traditional techniques to identify needy populations elsewhere.\nmanner across user groups\n3.1.2.5\nh AI developers should consider whether their AI systems can be expected to perform well Consideration should be given to the effect of diversity on the development and deployment\nwhen exposed to previously unseen data, especially when evaluating people who are not processes\nwell-represented in the training data.\nb Organizations should seek to include people from diverse demographic backgrounds across\nthe full lifecyle, to include design, development and deployment processes. Organizations\nshould seek to engage diverse internal and external groups also.\nc AI developers should consider whether the assumptions they make about AI subjects could\nbe wrong or are likely to lead to non-operational bias; if so, they should consider consulting\nthe AI subjects in a representative manner during the design, development and deployment\nto confirm these assumptions.\n18 193.2 Accountable AI (Principle 2)\n3.2.2.2\nPositive efforts should be made to identify and mitigate any significant risks inherent in the AI\nsystems designed\n3.2.1 Principle\na AI operators should only use AI systems that are backed by evidence-based academic or\nindustrial research, and AI developers should base their development on such research.\nWe will make AI systems accountable\nAccountability for the outcomes of an AI system lies not with the system itself but is\napportioned between those who design, develop and deploy it; Example: A foreign country has a government service which identifies parents who owe\nmoney in child maintenance. The data matching process is often incorrect due to misspelled\nDevelopers should make efforts to mitigate the risks inherent in the systems they design; names or missing data which results in some individuals being incorrectly targeted\nautomatically by the system with the result being a large bill, poor credit ratings and even\nAI systems should have built-in appeals procedures whereby users can challenge significant\nfreezing wages. The recourse for individuals who are incorrectly targeted is time-consuming\ndecisions;\nand not straightforward . If the potential impact of incorrect decisions had been assessed,\nmitigation measures such a user-friendly review procedure could have been set up.\nAI systems should be developed by diverse teams which include experts in the area in which\nthe system will be deployed; and\nAI systems should be subject to external audit and decision quality assurance. b AI operators should identify the likely impact of incorrect automated decisions on AI\nsubjects and, in the case where incorrect decisions are likely to cause significant cost or\ninconvenience, consider mitigating measures.\n3.2.2 Guidelines\nc AI operators should consider internal risk assessments or ethics frameworks as a means to\n3.2.2.1 facilitate the identification of risks and mitigating measures.\nAccountability for the outcomes of an AI system should not lie with the system itself\nd In designing AI systems to inform significant decisions, AI developers should consider\na Accountability for loss or damages resulting from the application of AI systems should not measures to maintain data accuracy over time, including:\nbe attributed to the system itself.\n\u2022 the completeness of the data.\nb AI operators and AI developers should consider designating individuals to be responsible for \u2022 timely update of the data, and.\ninvestigating and rectifying the cause of loss or damage arising from the deployment of AI\n\u2022 whether the context in which the data was collected affects its suitability for the\nsystems.\nintended use case.\nExample: A border camera scanning for predictors of risk may misinterpret a \u201ctic\u201d of an\nindividual with Tourette syndrome as suspicious. These can manifest in a diverse fashion, and\nshould not cause this person to undergo secondary inspection every time they pass through\nthe border . If the data is updated after the first case is encountered then it would avoid\ncausing inconvenience on subsequent visits.\n5 Government of Canada. (2018). Responsible AI in the Government of Canada. Digital Disruption White Paper Series. Version 2.0, p.26. Retrieved from:\nhttps://docs.google.com/document/d/1Sn-qBZUXEUG4dVk909eSg5qvfbpNlRhzIefWPtBwbxY/edit\n4 Cabinet Office (UK), Data Science Ethical Framework, Version 1.0, licensed under the Open Government License v3.0, p. 13 6 Stoica, I. et. al.,2017, A Berkeley View of Systems Challenges for AI, p. 2, https://arxiv.org/pdf/1712.05855.pdf\n20 21e AI developers and AI operators should tune AI models periodically to cater for changes to e Be mindful that there might be fundamental tensions between different principles and\ndata and/or models over time. requirements. Continuously identify, evaluate, document and communicate these trade-\noffs and their solutions.\nf AI operators should consider whether AI systems trained in a static environment will display\nmodel instability when deployed in dynamic environments. Adopt a Trustworthy AI assessment list when developing, deploying or using AI systems,\nf\nand adapt it to the specific use case in which the system is being applied.\nExample: AI systems will need to be able to adapt to the changes in the environment that g Keep in mind that such an assessment list will never be exhaustive. Ensuring Trustworthy\nthey are deployed in. For example, a self-driving car would need to quickly adapt to unexpected AI is not about ticking boxes, but about continuously identifying and implementing\nand dangerous road by learning in real time from other cars that have successfully dealt with requirements, evaluating solutions, ensuring improved outcomes throughout the AI\nthese conditions. In addition, such mission-critical applications must handle noisy inputs system\u2019s lifecycle, and involving stakeholders in this.\nand defend against malicious actors .\n3.2.2.4\nAI subjects should be able to challenge significant automated decisions concerning them and,\ng AI developers should collaborate with AI Operators to train models using historical data where appropriate, be able to opt out of such decisions\nfrom the Operator.\na AI operators using AI systems to inform significant decisions should provide procedures by\nwhich affected AI subjects can challenge a specific decision concerning them.\nh AI Operators should consider working with their vendors (AI developers) to continually\nmonitor performance.\nb AI operators should consider such procedures even for non-significant decisions.\ni AI Operators should subject AI systems informing significant decisions to quality checks\nat least as stringent as those that would be required of a human being taking the same c AI operators should make affected AI subjects aware of these procedures and should\ndesign them to be convenient and user-friendly.\ndecision.\nd AI operators should consider employing human case evaluators to review any such\n3.2.2.3\nchallenges and, when appropriate, overturn the challenged decision.\nAI systems informing critical decisions should be subject to appropriate external audit\na When AI systems are used for critical decisions, external auditing of the AI systems in\nquestion should be used as a means to ensure meaningful standards of transparency and Example: A bank allows customers to apply for a loan online by entering their data. The\naccountability are upheld. bank uses an AI system to automatically determine whether to give the loan and what the\ninterest rate should be. They provide users with an option to contest the decision and have\nb In the case that critical decisions are of civic interest, public release of the results of the it reviewed by a human . They also require that customers justify their challenge by filling in\naudit should be considered as a means of ensuring public processes remain accountable a form, which assists the case reviewer and deters customers from challenging a decision\nto those affected by them. without good reason.7\nIn the case that critical decision are life and death decisions, these decisions should be\nc\nsupported by further validation and verification via a human operator.\nd Facilitate traceability and auditability of AI systems, particularly in critical contexts or\nsituations.\n7 Adapted from EU Commission, Can I be subject to automated individual decision-making, including profiling? Retrieved from: https://ec.europa.eu/\ninfo/law/law-topic/data-protection/reform/rights-citizens/my-rights/can-i-be-subject-automated-individual-decision-making-including-profiling_\nen#example\n22 23e AI operators should consider instituting an opt-out mechanism for significant automated 3.2.2.7\ndecisions. AI operators should understand the AI systems they use sufficiently to assess their suitability\nfor the use case and to ensure accountability and transparency\nf AI operators could consider \u201ccrowd challenge\u201d mechanisms whereby a critical number of\ncomplaints triggers an investigation into the fairness and/or accuracy of a decision-making\na In the case of critical decisions, AI operators should avoid using AI systems that cannot be\nprocess as a whole.\nsubjected to meaningful standards of accountability and transparency.\n3.2.2.5 b AI developers should consider notifying customers and AI operators of the use cases for\nAI systems informing significant decisions should not attempt to make value judgements on which the system has been designed, and those for which it is not suitable.\npeople\u2019s behalf without prior consent\na When informing an AI subject about significant choices they will make, AI systems should\nnot unreasonably restrict the available options or otherwise attempt to influence their value\njudgements without the explicit consent of the AI subject in question.\n3.2.2.6\nAI systems informing significant decisions should be developed by diverse teams with\nappropriate backgrounds\na AI developers who develop AI systems which may be used to assist in making critical\ndecisions should involve in the process experts with a background in social science, policy,\nor another subject which prepares them to evaluate the broader societal impact of their\nwork.\nb Development of AI systems informing significant decisions should include consultation\nwith experts in the field in which the system will be deployed.\nExample: An app that uses AI to assess medical symptoms and has a large user base had\nto face regulatory scrutiny because of number of complaints from doctors. They warned\nthat the application can miss signs of serious illness. A number of different shortcomings\nwere identified by doctors, some of which the company could address and resolve.8\n8 Financial Times. 2018. High-profile health app under scrutiny after doctor\u2019s complaints. Retrieved from: https://www.ft.com/content/19dc6b7e-8529-\n11e896-dd-fa565ec55929\n24 253.3 Transparent AI (Principle 3)\nc To facilitate the above, AI developers and AI operators should consider documenting the\nfollowing information during the design, development and deployment phases, and retaining\n3.3.1 Principle this documentation for a length of time appropriate to the decision type or industry:\n\u2022 the provenance of the training data, the methods of collection and treatment, how the data was\nWe will make AI systems transparent\nmoved, and measures taken to maintain its accuracy over time;\n\u2022 the model design and algorithms employed; and\nDevelopers should build systems whose failures can be traced and diagnosed. \u2022 changes to the codebase, and authorship of those changes.\nPeople should be told when significant decisions about them are being made by AI. d Where possible given the model design, AI developers should consider building in a means\nby which the \u201cdecision journey\u201d of a specific outcome (i.e. the component decisions leading\nWithin the limits of privacy and the preservation of intellectual property, those who deploy to it) can be logged.\nAI systems should be transparent about the data and algorithms they use.\nResponsible disclosures should be provided in a timely manner, and provide reasonable\nExample: A technology company has a product which is designed to assist in medical\njustifications for AI systems outcomes. This includes information that helps people\ndiagnosis. It documents each stage of its reasoning and relates it back to the input data.9\nunderstand outcomes, like key factors used in decision making.\n3.3.2 Guidelines\n3.3.2.1\nTraceability should be considered for significant decisions, especially those that have the\npotential to result in loss, harm or damage.\na For AI systems which inform significant decisions, especially those with the potential to\ncause loss, harm or damage, AI developers should consider building in traceability (i.e. the\nability to trace the key factors leading to any specific decision).\nb Organizations should ensure that harms caused through AI systems are investigated and\nredressed, by enacting strong enforcement mechanisms and remedial actions, to make\ncertain that human rights and the rule of law are respected in the digital world and in the\nphysical world.\n9 See IBM WatsonPaths\n26 273.3.2.2 3.4 Explainable AI (Principle 4)\nPeople should be informed of the extent of their interaction with AI systems\n3.4.1 Principle\na AI operators should inform AI subjects when a significant decision affecting them has been\nmade by an AI system.\nWe will make AI systems as explainable as\ntechnically possible\nExample: A small claims court adjudicates minor civil matters such as debt collection and\nevictions. They introduce an AI system to suggest the outcome of a ruling. At the time of the\nDecisions and methodologies of AI systems which have a significant effect on individuals\nruling the plaintiff and defendant are notified that the decision was assisted by an AI system.\nshould be explainable to them, to the extent permitted by available technology.\nThe court also provides an explanation for the decision.\nIt should be possible to ascertain the key factors leading to any specific decision that\ncould have a significant effect on an individual.\nb If an AI system can convincingly impersonate a human being, it should do so only after\nIn the above situation we will provide channels through which people can request such\nnotifying the AI subject that it is an AI system.\nexplanations.\nExample: A technology company produces a conversational AI agent which can make 3.4.2 Guidelines\nsome phone calls on behalf of its users. Those who receive the calls may believe that they\n3.4.2.1\nare speaking to a human. Therefore, the company programs the agent to identify itself at\nAI operators could consider providing affected AI subjects with a high-level explanation of\nthe start of every conversation.\nhow their AI system works\na AI operators could consider informing the affected AI subjects in understandable, non-\ntechnical language of:\n\u2022 the data that is ingested by the system;\n\u2022 the types of algorithms employed;\n\u2022 the categories into which people can be placed; and\n\u2022 the most important features driving the outcomes of decisions\n\u2022 A comprehensive list of feature engineering and models that was considered during\nthe model building phase.\nExample: A person turned down for a credit card might be told that the algorithm took\ntheir credit history, age, and postcode into account, but not learn why their application was\nrejected10 .\n28 29b For non-sensitive public sector use cases designed for the common good, AI operators\ncould consider making source code, together with an explanation of the workings of the AI Example: The UK\u2019s NHS developed a tool called Predict, which allows women with breast\nsystem, available either publicly or upon request (this should be done only if there is low risk cancer to compare their case to other women who have had the same condition in the past,\nof people \u2018gaming the system\u2019). and visualize the expected survival rate under various treatment options. The website has an\nexplanation page which shows the weights behind various factors and contains a description\nc The AI operator should maintain necessary documentation that provides elaboration and of the underlying mathematics12 .\nclarification on how the algorithm works, for example documentation of processes, decision\nmaking flow charts of the system, and how the appeal process is embedded.\nd An individual should have the ability to contest and seek effective redress against decisions c In the case that such explanations are available, they should be easily and quickly accessible,\nmade by AI systems. These must be addressed by the group or team supporting these free of charge and user-friendly.\nmodels.\ne Develop appropriate impact indices for the evaluation of AI system technological 3.5 Robust, Safe and Secure AI (Principle 5)\ninterventions from multiple perspectives.\n3.5.1 Principles\n3.4.2.2\nAI operators should consider providing affected AI subjects with a means to request\nexplanations for specific significant decisions, to the extent possible given the state of present AI systems will be technically robust\nresearch and the choice of model\nAI systems should be technically robust with a preventative approach to risks which\na AI operators should consider providing a means by which people affected by a significant\noperates in a manner such that they reliably behave.\ndecision informed by AI can access the reasoning behind that decision.\nAI Developers should ensure AI systems will not cause any unintentional harm and\nadverse impacts.\nExample: The US Consumer Financial Protection Bureau requires that creditors in the US\nwho reject credit applications must explain to the applicant the principal reason(s) why their\nAI systems should be resilient to attacks and security such as data poisoning and\napplication was rejected (e.g. \u201clength of residence\u201d or \u201cage of automobile\u201d)11 . In particular,\nmodel leakage.\n\u201cstatements that the adverse action was based on the creditor`s internal standards or\npolicies or that the applicant, joint applicant, or similar party failed to achieve a qualifying AI systems should have safeguards that enable a fallback plan in case of problems.\nscore on the creditor`s credit scoring system are insufficient\u201d.\nAI system results should be reproducible.\nb Where such explainability is not possible given available technology, AI operators should\nconsider compromises such as counterfactual reasoning, or listing the most heavily\nweighted factors contributing to the decision.\n11 Consumer Financial Protection Bureau, 12 CFR Part 1002 - Equal Credit Opportunity Act (Regulation B), \u00a7 1002.9 Notifications, Retrieved from https://www.\n10 The Guardian. AI watchdog needed to regulate automated decision-making, say experts. Retrieved from: https://www.theguardian.com/technology/2017/ consumerfinance.gov/policy-compliance/rulemaking/regulations/1002/\njan/27/ai-artificial-intelligence-watchdog-needed-to-prevent-discriminatory-automated-decisions 12 Predict website, accessible at http://www.predict.nhs.uk/predict_v2.1/legal/algorithm\n30 313.5.2 Guidelines\n3.5.2.1 3.5.2.3\nAI Operators should continue conducting vulnerability assessments, verification of AI system AI Operators should assure end users of the system\u2019s reliability through documentation, and\ndifferent behaviors in unexpected situations and for any dual-use case, to include: operationalizing processes for testing and verification of desired outcomes , specifically:\na Putting measures to ensure integrity and resilience of IA system against potential attacks. a AI system should be designed with an approach to continue monitoring if the system meets\ngoals, purposes and intended application.\nb AI Operators should define an approach to ensure results are reproducible through clear\nExample: IBM researchers presented a novel approach to exploiting AI for malicious\nprocess and documentation.\npurposes. Their system, DeepLocker embeds AI capabilities within the malware itself in order\nto improve its evasion techniques. It uses an Artificial Intelligence model to identify its target\nc AI Operators should publish documentation to assure system robustness to end users.\nusing indicators like facial recognition, geolocation and voice recognition.\n3.5.2.4\nAI systems will be safe, secure and controllable by humans\nb Verifying how AI systems behave in unexpected situations and environments.\na Safety and security of people, be they operators, end-users or other parties, will be of\nc Taking preventative measures against any possible system dual-case use. paramount concern in the design of any AI system.\nb AI systems should be verifiably secure and controllable throughout their operational\n3.5.2.2\nlifetime, to the extent permitted by technology.\nAI operators should define a suitable fall back plan and test it to maintain readiness in\nunexpected situations and in high safety risks level, to include:\nc The continued security and privacy of users should be considered when decommissioning\na Developing a plan to measure and assess potential safety risks to you or any third party AI systems.\nfrom technology use accidental or malicious misuse.\nd AI systems that may directly impact people\u2019s lives in a significant way should receive\nb Define thresholds for system acceptable results and governance procure to fall back on commensurate care in their designs.\nalternative defined and tested plans.\ne Such systems should be able to be overridden or their decisions reversed by designated\nc Considering an insurance policy to mitigate risks arising from potential damages. individuals.\nExample: Adopting a risk-based approach to procurement and clearly communicating it to\nvendors can help address such issues by giving the procuring organization advance notice\nof the specific oversight capabilities it will need in future stages of the system lifecycle,\npreventing vendors from presenting intellectual property arguments against required testing,\nmonitoring and auditing of their AI systems going forward and rewarding vendors\u2014of all\nsizes\u2014that are more advanced and responsive in their responsible AI efforts.\n13 IBM. https://documents.trendmicro.com/assets/white_papers/wp-malicious-uses-and-abuses-of-artificial-intelligence.pdf\n32 333.6.2 Guidelines\n3.5.2.5\nAi systems should not be able to autonomously hurt, destroy or deceive humans\n3.6.2.1\na AI systems should be built to serve and inform, and not to deceive and manipulate.\nAI should be developed to align with human values, contribute to human flourishing and\nbenefit society.\nb Nations should collaborate to avoid an arms race in lethal autonomous weapons, and\nsuch weapons should be tightly controlled. a Society should be consulted in a representative fashion to inform the development of AI.\nc Active cooperation should be pursued to avoid corner-cutting on safety standards. b Stakeholders throughout society should be involved in the development of AI and its\ngovernance.\nSystems designed to inform significant decisions should do so impartially.\nd\nc Stakeholders should proactively engage in responsible stewardship of trustworthy AI in\npursuit of beneficial outcomes for people and the planet.\n3.6 Human Centered AI (Principle 6)\nd Organizations should prioritize having all their stakeholders learn about wellbeing metrics\nas a potential determinant of how they create, deploy, market and monitor their AI\n3.6.1 Principles technologies.\nWe will give AI systems human values and\nExample: IEEE P7010 recommended practice14 establishes wellbeing metrics relating to\nmake them beneficial to society human factors directly affected by intelligent and autonomous systems and establishes a\nbaseline for the types of objective and subjective data these systems should analyze and\nGovernment will support the research of the beneficial use of AI. include (in their programming and functioning) to proactively increase human wellbeing.\nStakeholders throughout society should be involved in the development of AI and its\ngovernance.\n3.6.2.2\nSystematic analyses that examine the ethics of designing affective systems to nudge human\nDesign AI systems to adopt, learn and follow the norms and values of the community\nbeings prior to deployment are needed.\nthey serve.\nSystematic analyses that examine the ethics of designing affective systems to nudge a Nudging in AI systems should have an opt-in system policy with explicit consent.\nhuman beings prior to deployment are needed.\nb We recommend that when appropriate, an affective system that nudges human beings\nDecisions related to lethal force, life and death should not be delegated to AI systems. should have the ability to accurately distinguish between users, including detecting\nRules and standards should be adopted to ensure effective human control over those characteristics such as whether the user is an adult or a child. Additional protections must\ndecisions. be put in place for vulnerable populations, such as children, when informed consent cannot\nbe obtained, or when it may not be a sufficient safeguard.\nc AI systems with nudging strategies must be carefully evaluated, monitored, and controlled.\n14 IEEE. https://sagroups.ieee.org/7010/\n34 353.6.2.3 f In situations where it is needed, human rights impact assessments and human rights due\nHumanity should retain the power to govern itself and make the final decision, with AI in an diligence, human determination codes of ethical conduct, or quality labels and certifications\nassisting role. intended to promote human centered values and fairness should be considered.\ng Mechanisms should be put into place to receive external feedback regarding AI systems\na Decisions related to lethal force, life and death should not be delegated to AI systems. Rules\nthat potentially infringe on fundamental rights.\nand standards should be adopted to ensure effective human control over those decisions.\nb Responsible parties (e.g., parents, nurse practitioners, social workers, and governments)\nshould be trained to detect the influence due to AI and ineffective mitigation techniques.\nIn the most extreme cases it should always be possible to shut down harmful AI system.\nc Those actions undertaken by an affective system that are most likely to generate an\nemotional response should be designed to be easily changed.\nd Users should be able to make informed autonomous decisions regarding AI systems. They\nshould be given appropriate knowledge and tools to comprehend and interact with AI\nsystems to satisfactory degree and, where possible to reasonably self-assess or challenge\nthe system.\n3.6.2.4\nAI systems should be designed in a way that respects the rule of law, human rights, society\nvalues, and should include appropriate safeguards to ensure a fair and just society.\na AI Developers should design AI systems to adopt, learn and follow the norms and values of\nthe community they serve.\nb Organizations could identify the norms of the specific community in which AI systems are\nto be deployed in. In particular, pay attention to the norms relevant to the kinds of tasks that\nthe AI systems are designed to perform. These could be documented as well as how these\nnorms are addressed by the AI system.\nc To respond to the dynamic change of norms in society the AI system could be able to adjust\nits existing norms and learn new ones, while being transparent about these changes.\nd Designers should consider forms and metrics for assessing an AI system\u2019s norm conformity\nover the lifetime of the system (e.g. human-machine agreement on moral decisions,\nverifiability of AI decisions, justified trust).\ne The norm identification process must document the similarities and differences between\nthe norms that humans apply to other humans and the norms they apply to AI systems.\nNorm implementations should be evaluated specifically against the norms that the\ncommunity expects the AI system to follow.\n36 373.7 Sustainable and Environmentally Friendly AI (Principle 7)\nd While there is no standardized means of reporting on the environmental impact of AI\nsystems, where possible, model development and commissioning should make carbon\n3.7.1 Principle impacts a core consideration alongside functional and business requirements.\ne Smaller models should be considered \u2013 shrinking down the model size and using fewer\nWe will promote sustainable and environmentally compute cycles (to balance financial and performance costs with the end performance of\nthe model).\nfriendly AI\nf Carbon awareness should be considered \u2013 adjusting the operational parameters of the AI\nAI systems should be used to benefit all human beings, including future generations. The\nsystem to dynamically select the best time and location for energy use from the grid can\nenvironment is fundamental to this.\nreduce its carbon footprint.\nThroughout the AI lifecycle, implementations should only be carried out on the basis of a full\nunderstanding and acknowledgement of implications for sustainability and environment.\nExample: In agriculture, AI can transform Example: Google`s DeepMind division\nAn AI system\u2019s development, deployment and use, should be assessed via critical examination production by better monitoring and has developed AI that teaches itself to\nof resource usage and energy consumption.\nmanaging environmental conditions minimize the use of energy to cool Google`s\nand crop yields. AI can help reduce both data centers. As a result, Google reduced\nMechanisms to measure environmental impact due to type of energy use and processing\nfertilizer and water, all while improving its data center energy requirements by\npower provided by data centers should be established.\ncrop yields. 40% 15.\n3.7.2 Guidelines\n3.7.2.1 Throughout the AI lifecycle, implementations should always be carried out after full\nunderstanding and acknowledgement of AI implications on sustainability and environment.\na The application of Artificial Intelligence to address sustainability challenges are well\nunderstood. Building and running green, sustainable AI systems is of increasing importance,\ngiven the large carbon footprint that they can generate and the wider context of addressing\nclimate change.\nb AI model development, and by extension commissioning, should therefore seek to balance\ntechnical performance with energy consumption and environmental impact.\nc Efforts should be made to estimate and understand sustainability and environmental\nimpact across the AI lifecycle (e.g. the energy consumption costs associated with model\ntraining, and CO2 emissions and cloud compute costs associated with the deployment\nand running of the system).\n15 https://www.deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-by-40\n38 393.8 Privacy Preserving AI (Principle 8)\n3.8.2.2\nEstablish an oversight mechanism for data collection, storage and processing and use across\nyour organization\n3.8.1 Principle\na Assess who can access data and under which conditions.\nWe will respect people\u2019s privacy\nb Assign specific responsibilities and role for Data Protection Officers.\nAI systems should respect privacy and use the minimum intrusion necessary.\nc Prevention of harm to privacy necessitates adequate data governance that covers the\nquality and integrity of the data used, its relevance in light of the domain in which the\nAI systems should uphold high standards of data governance and security,\nprotecting personal information. AI systems will be deployed, its access protocols and the capability to process data in a\nmanner that protects privacy.\nSurveillance or other AI-driven technologies should not be deployed to the extent\nof violating internationally and/or UAE\u2019s accepted standards of privacy and human d AI systems must guarantee privacy and data protection throughout a system\u2019s entire\ndignity and people rights. lifecycle. This includes the information initially provided by the user, as well as the\ninformation generated about the user over the course of their interaction with the system\nPrivacy by design should be embedded in AI systems and where possible AI\n(e.g. outputs that the AI systems generated for specific users or how users responded to\nalgorithm should have adequate privacy impact assessments.\nparticular recommendations). Digital records of human behavior may allow AI systems\nto infer not only individuals\u2019 preferences, but also their age, gender, religious or political\nAdequate data protection frameworks and governance mechanisms should be\nviews. To allow individuals to trust the data gathering process, it must be ensured that data\nestablished in a multi-stakeholder approach and ensured throughout the life cycle\ncollected about them will not be used to unlawfully or unfairly discriminate against them.\nof AI systems.\ne In any given organization that handles individuals\u2019 data (whether someone is a user of the\nAI developers and operators should strive to strike the balance between privacy\nsystem or not), data protocols governing data access should be put in place in line with\nrequirements, individual rights and innovation growth and society benefits.\nnational privacy legislation (be these universal or sector-specific). These protocols should\noutline who can access data and under which circumstances. Only duly qualified personnel\n3.8.2 Guidelines\nwith the competence and need to access individual\u2019s data should be allowed to do so.\n3.8.2.1\nEstablish mechanism for users to flag issues related to privacy or data protection\na AI Operators should review the system for proper consent logging, ability of users to revoke\npermission whenever applicable.\nb Consider training AI model without or with minimal user of potentially sensitive or personal\ndata.\nc Use measures to enhance privacy such as encryption, anonymization and aggregation.\n40 41f Algorithmic systems require adequate privacy impact assessments, which also include\nsocietal and ethical considerations of their use and an innovative use of the privacy by\ndesign approach. AI actors need to ensure that they are accountable for the design and\nimplementation of AI systems in such a way as to ensure that personal information is\nprotected throughout the life cycle of the AI system.\ng Establish data policies or equivalent frameworks, or reinforce existing ones, to ensure full\nsecurity for personal data and sensitive data, which, if disclosed, may cause exceptional\ndamage, injury or hardship to individuals. Examples include data relating to offences,\ncriminal proceedings and convictions, and related security measures; biometric, genetic\nand health data; and personal data such as that relating to race, descent, gender, age,\nlanguage, religion, political opinion, national origin, ethnic origin, social origin, economic or\nsocial condition of birth, or disability and any other characteristics.\nh Promote mechanisms, such as open repositories for publicly funded or publicly held data,\nsource code and data trusts, to support the safe, fair, legal and ethical sharing of data,\namong others.\ni Promote and facilitate the use of quality and robust datasets for training, development and\nuse of AI systems, and exercise vigilance in overseeing their collection and use.\nExample: with the help of AI & ML algorithms16 , Synthetic data can be created to enhance\nprivacy. The data created will have the same statistical characteristics for the testing\nenvironment and where a third party can have access.\n16 https://research.aimultiple.com/privacy-enhancing-technologies/\n42 43Microsoft. 2022. Microsoft Responsible AI Standard, v2. General Requirements. June 2022.\n04\nRetrieved from:\nhttps://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4ZPmV\nISO. 2022. Proposed ISO Standard on Risk Management of AI: What Businesses Should Know.\nBibliography\nMay 2022. Retrieved from:\nhttps://unesdoc.unesco.org/ark:/48223/pf0000381137/PDF/381137eng.pdf.multi\nGSMA 2022. The AI Ethics Playbook. Implementing ethical principles into everyday business.\nFebruary 2022. Retrieved from:\nhttps://www.gsma.com/betterfuture/wp-content/uploads/202201//The-Mobile-Industry-Ethics-\nPlaybook_Feb-2022.pdf\nOECD. 2022. AI Policy Observatory. Retrieved from:\nhttps://standards.ieee.org/wp-content/uploads/import/documents/other/ead_v2.pdf\nUNESCO. 2022. Recommendation on the Ethics of Artificial Intelligence. Adopted in November\n2021. Retrieved from:\nhttps://unesdoc.unesco.org/ark:/48223/pf0000381137/PDF/381137eng.pdf.multi\nEuropean Commission. 2021. Proposal for a Regulation of the European Parliament and of the\nCouncil Laying Down Harmonized Rules on Artificial Intelligence (Artificial Intelligence Act) and\nAmending Certain Union Legislative Acts. Retrieved from:\nhttps://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A52021PC0206\nEuropean Commission. High Level Expert Group on Artificial Intelligence ( AI HLEG ). Ethics\nGuidelines for Trustworthy AI . April 2019. Retrieved from:\nhttps://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai\nCentre for Data Ethics and Innovation. UK Government: The roadmap to an effective AI assurance\necosystem. December 2021. Retrieved from:\nhttps://www.gov.uk/government/publications/the-roadmap-to-an-effective-ai-assurance-\necosystem\nThe New York City Council. 2021. A Local Law to amend the administrative code of the city of New\nYork, in relation to automated employment decision tools. Retrieved from:\nhttps://legistar.council.nyc.gov/LegislationDetail.aspx?ID=4344524&GUID=%20B051915D-A9AC-\n451E-81F86596032-FA3F9&\nUNESCO, Recommendation on the Ethics of Artificial Intelligence. 2021. Retrieved from:\nhttps://unesdoc.unesco.org/ark:/48223/pf0000381137\n44 45AI NOW Institute. Algorithmic accountability for the public sector. August 2021. Retrieved from: CNIL. (2017). How can humans keep the upper hand? The ethical matters raised by algorithms\nhttps://www.opengovpartnership.org/documents/algorithmic-accountability-public-sector/ and artificial intelligence. Retrieved from:\nhttps://www.cnil.fr/sites/default/files/atoms/files/cnil_rapport_ai_gb_web.pdf\nIEEE 2019. Ethically Aligned Design. Version 2 \u2013 For Public Discussion. Retrieved from:\nhttps://standards.ieee.org/wp-content/uploads/import/documents/other/ead_v2.pdf Executive Office of the President of the United States. National Science and Technology Council.\nNetworking and Information Technology Research and Development Subcommittee. (2016). The\nnational artificial intelligence research and development strategic plan. Retrieved from:\nIEEE 2017. Ethically Aligned Designed : A vision for Prioritizing Human Well-being with\nhttps://www.nitrd.gov/PUBS/national_ai_rd_strategic_plan.pdf\nAutonomous and Intelligent Systems. Retrieved from:\nhttps://standards.ieee.org/wp-content/uploads/import/documents/other/ead_v2.pdf\nExecutive Office of the President of the United States National Science and Technology Council.\nCommittee on Technology. (2016). Preparing for the future of artificial intelligence. Retrieved from:\nGoogle. AI Principles 2021. Retrieved from:\nhttps://obamawhitehouse.archives.gov/sites/default/files/whitehouse_files/microsites/ostp\nhttps://ai.google/principles/\nNSTC/preparing_for_the_future_of_ai.pdf\nMicrosoft. Responsible AI Standard v2 (June 2022). Retrieved from:\nThe Headquarters for Japan\u2019s Economic Revitalization. (2015). New Robot Strategy. Japan\u2019s\nhttps://www.microsoft.com/en-us/ai/responsible-ai\nRobot Strategy. Vision, Strategy, Action Plan. Retrieved from:\nhttp://www.meti.go.jp/english/press/2015/pdf/0123_01b.pdf\nThe ECONOMIST Group 2022 . Pushing forward: the future of AI in the Middle East and North\nAfrica Report. Retrieved from:\nTreasury Board of Canada Secretariat. (2018). Responsible Artificial Intelligence in the\nhttps://impact.economist.com/perspectives/sites/default/files/google_ai_mena_report.pdf\nGovernment of Canada. Digital Disruption White Paper Series. Version 2.0.\nhttps://docs.google.com/document/d/1Sn-qBZUXEUG4dVk909eSg5qvfbpNlRhzIefWPtBwbxY/\nPDPC. (2018, June 5). Discussion paper on Artificial Intelligence (AI) and Personal Data.\nedit\nSingapore: Personal Data Protection Commission Singapore (PDPC). Retrieved from:\nhttps://www.pdpc.gov.sg/-/media/Files/PDPC/PDF-Files/Resource-for-Organisation/AI/\nThe Toronto Declaration: Protecting the right to equality and non-discrimination in machine\nDiscussion-Paper-on-AI-and-PD---050618.pdf\nlearning systems. Toronto, Canada: Amnesty International and Access Now. Retrieved from:\nhttps://www.accessnow.org/cms/assets/uploads/201808//The-Toronto-Declaration_ENG_08-\nITI. AI Policy Principles. Retrieved from:\n2018.pdf\nhttps://www.itic.org/public-policy/ITIAIPolicyPrinciplesFINAL.pdf\nhttp://data.parliament.uk/writtenevidence/committeeevidence.svc/evidencedocument/artificial-\nCabinet Office.(2016, May 19). Data Science Ethical Framework. Retrieved from: intelligence-committee/artificial-intelligence/oral/73546.html (house of lords select committee\nhttps://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/ oral evidence, Q61)oral evidence, Q61)\nfile/524298/Data_science_ethics_framework_v1.0_for_publication__1_.pdf\nHouse of Lords Select Committee on Artificial Intelligence. (2018). AI in the UK: ready, willing and\nEuropean Parliament. (2017, February 16). European Parliament resolution of 16 February 2017 able?\nwith recommendations to the Commission on Civil Law Rules on Robotics (20152103/(INL)). https://publications.parliament.uk/pa/ld201719/ldselect/ldai/100100/.pdf\nRetrieved from:\nhttp://www.europarl.europa.eu/sides/getDoc.do?pubRef=-//EP//TEXT+TA+P8-TA-2017- House of Commons Science and Technology Select Committee. Algorithms in Decision Making.\n0++0051DOC+XML+V0//EN https://publications.parliament.uk/pa/cm201719/cmselect/cmsctech/351351/.pdf\nVillani, C. (2018). For a meaningful Artificial Intelligence towards a French and European strategy.\nRetrieved from:\nhttps://www.aiforhumanity.fr/pdfs/MissionVillani_Report_ENG-VF.pdf\n46 47", "metadata": {"country": "Saudi Arabia", "year": "2021", "legally_binding": "yes", "binding_proof": "Law", "date": "09/16/2021", "regulator": "Saudi Data and Artificial Intelligence Authority (SDAIA)", "type": "legislation", "status": "active", "language": "English, Arabic", "use_cases": "[3, 5, 6]"}}
{"_id": "686b66546e4e9653b2a68b1d", "title": "AI Ethics Principles", "source": "https://sdaia.gov.sa/en/SDAIA/about/Documents/ai-principles.pdf", "text": "AI Ethics Principles\nSeptember 2023\nVersion 1.0\nTable of Contents\nIntroduction\n..............................................................................................................................................\n3\nDefinitions\n.................................................................................................................................................\n5\nScope 8\n...........................................................................................................................................................\nAI Risks 8\n.......................................................................................................................................................\nAI System Lifecycle 9\n................................................................................................................................\nAI Ethics Principles and Controls ....................................................................................................\n11\nPrinciple 1 \u2013 Fairness 12\n.............................................................................................................................\nPrinciple 2 \u2013 Privacy & Security ..........................................................................................................\n15\nPrinciple 3 \u2013 Humanity 17\n...........................................................................................................................\nPrinciple 4 \u2013 Social & Environmental Benefits ................................................................................\n19\nPrinciple 5 \u2013 Reliability & Safety .........................................................................................................\n21\nPrinciple 6 \u2013 Transparency & Explainability ....................................................................................\n23\nPrinciple 7 \u2013 Accountability & Responsibility ..................................................................................\n25\nRoles and Responsibilities 28\n.................................................................................................................\nNational Level 29\n...........................................................................................................................................\nSDAIA 29\n......................................................................................................................................................\n.....................................................................................................................................\nAdopting Entities 30\n............................................................................................................................\nOptional Registration 32\n...............................................................................................................................................\nCompliance 32\n.............................................................................................................................\nMotivational Badges 32\nAnnexure 33\n....................................................................................................................................................\nAnnexure A: AI Ethics Tools ...............................................................................................................\n34\nAnnexure B: AI Ethics Tools Mapping to AI System Lifecycle .............................................\n40\nAnnexure C: AI Ethics Checklist .......................................................................................................\n41\nAI Ethics Principles\n2\nIntroduction\nDue to the fast growth of practices and technologies around Artificial Intelligence (AI), the\nuse of AI has expanded to several industries such as health, education, entertainment, etc.\nAI helps make entities decision-making processes more efficient, accurate and faster by\npredicting future patterns. AI can be used to analyze data, including big data, by developing\nand operating systems with advanced models and algorithms that help improve the quality\nof processes. Due to the increasing interest in these technologies, many entities in both the\npublic and private sectors, in addition to non-profit entities, have developed digital solutions\nbased on AI to address existing challenges using creative and innovative methods, thus\nmaking the role of AI more essential to maintain the competitive capabilities of these entities.\nAI Ethics Principles\n3\nIn accordance with KSA's commitment to human rights and its cultural values, as well as\naligning to international standards and recommendations on the ethics of Artificial Intelligence\nand with reference to the Council of Ministers\u2019 Resolution No. (292) dated 27/04/1441 AH.,\nstating in Paragraph (1) of Article 10 that SDAIA is mandated to develop policies, governance\nmechanisms, standards, and controls related to data and artificial intelligence and monitor\ncompliance therewith upon issuance, SDAIA has analyzed global practices and standards to\ndevelop this AI Ethics Framework which aims to:\nSupport the Kingdom\u2019s efforts towards\nachieving its vision and national\nstrategies related to adopting AI\ntechnology, encouraging research and\ninnovation, and driving economic\ngrowth for prosperity and development.\nDevelop and establish AI ethics\npolicies, guidelines, regulations, and\nframeworks.\nGovern data and AI models to limit the\nnegative implications of AI systems\n(economically, psychologically,\nsocially, etc.) and potential threats\n(security, political, etc.).\nHelp entities adopt standards and\nethics when building and developing\nAI-based solutions to ensure\nresponsible use thereof.\nProtect the privacy of data subjects\nand their rights with respect to the\ncollection and processing of their data.\nAI Ethics Principles\n4\nDefinitions\nFor the purposes of this document, the following words and phrases, wherever mentioned\nherein, shall have meanings ascribed thereto, unless the context requires otherwise:\nAdopting Entity\nAny public entity, business, or individual\nthat is required to comply with the present\ndocument.\nAI Ethics\nA set of values, principles, and techniques to\nguide moral conduct in developing and using\nAI technologies.\nArtificial Intelligence\nArtificial intelligence or AI is a collection of\ntechnologies that can enable a machine\nor system to sense, comprehend, act,\nand learn.\nAI System\nA set of predictive models and advanced\nalgorithms that can be used to analyze data\nand predict the future or facilitate decision\n-making for projected future events.\nAI System Assessor\nAny natural or legal person that audits AI\nsystems to achieve certain goals.\nAI System Developer\nAny natural or legal person that develops AI\nsystems by building predictive models using\ndata and algorithms to achieve certain goals.\nAI System Lifecycle\nThe cyclical process that AI projects are\nexpected follow to be able to design, build,\nand produce a robust and safe system that\ndelivers business value and insights through\nadhering to a standard and structured way\nof managing AI model delivery and\nimplementation.\nAI System Owner\nAny natural or legal person that applies or uses\nAI systems to achieve certain goals.\nCDO\nThe Chief Data Officer (CDO) is responsible\nfor the development and execution of the\nData Management & Data Governance and\noversee the implementation practices across\nthe Public Entities.\nAuthorized User\nAn individual that is permitted, appropriately\ncleared, and has a requirement to access an\ninformation system to perform or assist an\nestablished and preset role or responsibility\non the system\u2019s functionalities and\ncomponents.\nAI Ethics Principles\n6\nData\nA collection of facts in a raw or unorganized\nform such as numbers, characters, images,\nvideo, voice recordings, or symbols.\nData Sample\nThe data used to build, train and test predictive\nmodels and AI algorithms to reach specific\nresults.\nEnd-User\nAny natural or legal person that consumes or\nmakes use of the goods or services produced\nby AI systems.\nPersonal Data\nEvery data \u2013 of whatever source or form \u2013 that\nwould lead to the identification of the individual\nspecifically, or make it possible to identify him\ndirectly or indirectly, including: name, personal\nidentification number, addresses, contact\nnumbers, license numbers, records, personal\nproperty, bank account and credit card\nnumbers, fixed or removing pictures of the\nindividual, and other data of personal nature.\nSDAIA\nSaudi Data and Artificial Intelligence Authority\nValidity (Accuracy)\nHow accurately a method measures what it\nis intended to measure.\nAI Ethics Principles\nData Governance\nData governance is the process of managing\nthe availability, usability, integrity, and security\nof data in organizations and systems, based\non data standards and policies that also\ncontrol data usage.\nData Subject\nAn individual to whom the personal data\nbelongs, his representative, or whoever has\nlegal guardianship over him.\nNational Regulatory Authority\nAny independent governmental or public\nentity assuming regulatory duties and\nresponsibilities for a specific sector in the\nKingdom of Saudi Arabia under a legal\ninstrument.\nReliability\nThe property of intended consistency in\nbehavior and results.\nSensitive Data\nPersonal data that indicates or includes\na reference to a person\u2019s ethnic or tribal\norigin; religious, intellectual or political\nbeliefs; membership of civil associations\nor institutions; criminal and security data;\nbiometric identifying data; genetic data;\ncredit data; health data; location data; and\ndata that indicates that one or both of an\nindividual\u2019s parents are unknown.\n7\nScope\nThis AI Ethics Framework shall apply to all AI stakeholders designing, developing, deploying,\nimplementing, using, or being affected by AI systems within KSA, including but not limited to\npublic entities, private entities, non-profit entities, researchers, public services, institutions, civil\nsociety organizations, individuals, workers, and consumers.\nAI Risks\nThe categories and levels of risks associated with the development and/or use of artificial\nintelligence are classified into the following:\nLittle or no risk: There are no restrictions on AI systems that pose little or no risk such as\nspam filters, but it is recommended that these systems be ethically compliant.\nLimited risk: AI systems that pose limited risks, such as technical programs related to\nfunction, development, and performance, are subject to the application of the AI ethics\nprinciples mentioned in this document.\nHigh risk: AI systems that pose \u201chigh risks\u201d to basic rights must undergo pre- and\npost-conformity assessments, and in addition to adhering to ethics, the relevant statutory\nrequirements must be considered.\nUnacceptable risk: AI systems that pose an \u201cunacceptable risk\u201d to people\u2019s safety,\nlivelihood, and rights such as those related to social profiling, exploitation of children, or\ndistortion of behavior that are likely to occur are not allowed.\nRisk management should be directly interlinked into AI initiatives, so that oversight is concurrent\nwith internal development of AI technology. Risk management of AI systems a\ufb00ects a wide\nrange of risk types including data, algorithm, compliance, operational, legal, reputational, and\nregulatory risks. Risk management subcomponents, such as model interpretability, bias\ndetection, performance monitoring, are built in so that oversight is constant and consistent with\nAI development activities. In this approach, standards, testing, and controls are embedded into\nvarious stages of the AI System Lifecycle, from design to development and post-deployment.\nAI Ethics Principles\n8\nAI System Lifecycle\nThe AI System Lifecycle is the cyclical process that AI projects follow. It defines each step that an\norganization is expected to follow to take advantage of AI to derive practical business value. It is\na standard way of representing the tasks based on best practices in implementing and managing\nAI models which makes it a great candidate for embedding AI ethics.\nThe AI System Lifecycle is split into four steps, all of which have equal importance, and the\nrelevant activities are explained below.\nPlan and Design:\nDefine the problem\nSupport your problem with a data-driven approach\nSelect a framing approach on technology and system\nwhich governs AI\nConduct feasibility assessment for the selected approach\nDefine KPIs\nPlan and\nDesign\n01 02\nPrepare\nInput\nData\nDeploy\nand\nMonitor\n03\n04\nBuild and\nValidate\nPrepare Input Data:\nGather data\nDiscover and assess data\nCleanse and validate data\nTransform data into AI model input features\nAI Ethics Principles\n9\nBuild and Validate:\nTrain and test the model\nTune the hyperparameters of the model\nValidate model performance\nRisk Evaluation\nDeploy and Monitor:\nDeploy the model to the AI system\nCreate versioning structure\nMonitor the production model performance periodically\nAssess if there is a need to change the design according to results of periodic reviews\nAI Ethics Principles\n10\nAI Ethics Principles\nand Controls\nPrinciple 1 \u2013 Fairness\nThe fairness principle requires taking necessary actions to eliminate bias, discrimination\nor stigmatization of individuals, communities, or groups in the design, data, development,\ndeployment and use of AI systems. Bias may occur due to data, representation or algorithms\nand could lead to discrimination against the historically disadvantaged groups.\nWhen designing, selecting, and developing AI systems, it is essential to ensure just, fair,\nnon-biased, non-discriminatory and objective standards that are inclusive, diverse, and\nrepresentative of all or targeted segments of society. The functionality of an AI system should\nnot be limited to a specific group based on gender, race, religion, disability, age, or sexual\norientation. In addition, the potential risks, overall benefits, and purpose of utilizing sensitive\npersonal data should be well-motivated and defined or articulated by the AI System Owner.\nTo ensure consistent AI systems that are based on fairness and inclusiveness, AI systems should\nbe trained on data that are cleansed from bias and is representative of affected minority groups.\nAl algorithms should be built and developed in a manner that makes their composition free from\nbias and correlation fallacy.\nPlan and Design:\n1- At the initial stages of setting out the purpose of the AI system, the design team shall\ncollaborate to pinpoint the objectives and how to reach them in an efficient and optimized\nmanner. Planning the design of the AI system is an essential stage to translate the system\u2019s\nintended goals and outcomes. During this phase, it is important to implement a fairness-aware\ndesign that takes appropriate precautions across the AI system algorithm, processes, and\nmechanisms to prevent biases from having a discriminatory effect or lead to skewed and\nunwanted results or outcomes.\n2- Fairness-aware design should start at the beginning of the AI System Lifecycle with a\ncollaborative effort from technical and non-technical members to identify potential harm and\nbenefits, affected individuals and vulnerable groups and evaluate how they are impacted by\nthe results and whether the impact is justifiable given the general purpose of the AI system.\nAI Ethics Principles\n12\n3- A fairness assessment of the AI system is crucial, and the metrics should be selected at this\nstage of the AI System Lifecycle. The metrics should be chosen based on the algorithm type\n(rule-based, classification, regression, etc.), the effect of the decision (punitive, selective, etc.),\nand the harm and benefit on correctly and incorrectly predicted samples.\n4- Sensitive personal data attributes relating to persons or groups which are systematically or\nhistorically disadvantaged should be identified and defined at this stage. The allowed threshold\nwhich makes the assessment fair or unfair should be defined. The fairness assessment metrics\nto be applied to sensitive features should be measured during future steps.\nPrepare Input Data:\n1- Following the best practice of responsible data acquisition, handling, classification, and\nmanagement must be a priority to ensure that results and outcomes align with the AI system\u2019s\nset goals and objectives. Effective data quality soundness and procurement begin by ensuring\nthe integrity of the data source and data accuracy in representing all observations to avoid the\nsystematic disadvantaging of under-represented or advantaging over-represented groups. The\nquantity and quality of the data sets should be sufficient and accurate to serve the purpose of\nthe system. The sample size of the data collected or procured has a significant impact on the\naccuracy and fairness of the outputs of a trained model.\n2- Sensitive personal data attributes which are defined in the plan and design phase should\nnot be included in the model data not to feed the existing bias on them. Also, the proxies of\nthe sensitive features should be analyzed and not included in the input data. In some cases,\nthis may not be possible due to the accuracy or objective of the AI system. In this case, the\njustification of the usage of the sensitive personal data attributes or their proxies should be\nprovided.\nAI Ethics Principles\n13\nBuild and Validate:\n1- At the build and validate stage of the AI System Lifecycle, it is essential to take into\nconsideration implementation fairness as a common theme when building, testing, and\nimplementing the AI system. Model building and feature selection will require engineers and\ndesigners to be aware that the choices made about grouping or separating and including or\nexcluding features as well as more general judgments about the reliability and security of the\ntotal set of features may have significant consequences for vulnerable or protected groups.\n2- During the selection of the champion model, the fairness metric assessment should be\nconsidered. The champion model fairness metrics should be within the defined threshold for the\nsensitive features. The optimization approach of fairness and performance metrics should be\nclearly set throughout this phase. The fairness assessment should be justified if the champion\nmodel does not pass the assessment.\n3- Causality-based feature selection should be ensured. Selected features should be verified\nwith business owners and non-technical teams.\n4- Automated decision-support technologies present major risks of bias and unwanted\napplication at the deployment phase, so it is critical to set out mechanisms to prevent harmful\nand discriminatory results at this phase.\nDeploy and Monitor:\n1- Well-defined mechanisms and protocols should be set in place when deploying the AI system\nto measure the fairness and performance of the outcomes and how it impacts individuals and\ncommunities. When analyzing the outcomes of the predictive model, it should be assessed if\nrepresented groups in the data sample receive benefits in equal or similar portions and if the AI\nsystem disproportionately harms specific members based on demographic differences to ensure\noutcome fairness.\n2- The predefined fairness metrics should be monitored in production. If there is any deviation\nfrom the allowed threshold, it should be investigated whether there is a need to renew the model.\n3- The overall harm and benefit of the system should be quantified and materialized on the\nsensitive groups.\nAI Ethics Principles\n14\nPrinciple 2 \u2013 Privacy & Security\nThe privacy and security principle represents overarching values that require AI systems;\nthroughout the AI System Lifecycle; to be built in a safe way that respects the privacy of the\ndata collected as well as upholds the highest levels of data security processes and procedures\nto keep the data confidential preventing data and system breaches which could lead to\nreputational, psychological, financial, professional, or other types of harm. AI systems should\nbe designed with mechanisms and controls that provide the possibility to govern and monitor\ntheir outcomes and progress throughout their lifecycle to ensure continuous monitoring within\nthe privacy and security principles and protocols set in place.\nPlan and Design:\n1- The planning and design of the AI system and its associated algorithm must be configured\nand modelled in a manner such that there is respect for the protection of the privacy of\nindividuals, personal data is not misused and exploited, and the decision criteria of the\nautomated technology is not based on personally identifying characteristics or information.\n2- The use of personal information should be limited only to that which is necessary for the\nproper functioning of the system. The design of AI systems resulting in the profiling of individuals\nor communities may only occur if approved by Chief Compliance and Ethics Officer, Compliance\nOfficer or in compliance with a code of ethics and conduct developed by a national regulatory\nauthority for the specific sector or industry.\n3- The security and protection blueprint of the AI system, including the data to be processed\nand the algorithm to be used, should be aligned to best practices to be able to withstand\ncyberattacks and data breach attempts.\n4- Privacy and security legal frameworks and standards should be followed and customized for\nthe particular use case or organization.\n5- An important aspect of privacy and security is data architecture; consequently, data\nclassification and profiling should be planned to define the levels of protection and usage of\npersonal data.\n6- Security mechanisms for de-identification should be planned for the sensitive or personal\ndata in the system. Furthermore, read/write/update actions should be authorized for the relevant\ngroups.\nAI Ethics Principles\n15\nPrepare Input Data:\n1- The exercise of data procurement, management, and organization should uphold the legal\nframeworks and standards of data privacy. Data privacy and security protect information from\na wide range of threats.\n2- The confidentiality of data ensures that information is accessible only to those who are\nauthorized to access the information and that there are specific controls that manage the\ndelegation of authority.\n3- Designers and engineers of the AI system must exhibit the appropriate levels of integrity to\nsafeguard the accuracy and completeness of information and processing methods to ensure\nthat the privacy and security legal framework and standards are followed. They should also\nensure that the availability and storage of data are protected through suitable security database\nsystems.\n4- All processed data should be classified to ensure that it receives the appropriate level\nof protection in accordance with its sensitivity or security classification and that AI system\ndevelopers and owners are aware of the classification or sensitivity of the information they are\nhandling and the associated requirements to keep it secure. All data shall be classified in terms\nof business requirements, criticality, and sensitivity in order to prevent unauthorized disclosure\nor modification. Data classification should be conducted in a contextual manner that does\nnot result in the inference of personal information. Furthermore, de-identification mechanisms\nshould be employed based on data classification as well as requirements relating to data\nprotection laws.\n5- Data backups and archiving actions should be taken in this stage to align with business\ncontinuity, disaster recovery and risk mitigation policies.\nBuild and Validate:\n1- Privacy and security by design should be implemented while building the AI system. The\nsecurity mechanisms should include the protection of various architectural dimensions of an AI\nmodel from malicious attacks. The structure and modules of the AI system should be protected\nfrom unauthorized modification or damage to any of its components.\nAI Ethics Principles\n16\n2- The AI system should be secure to ensure and maintain the integrity of the information it\nprocesses. This ensures that the system remains continuously functional and accessible to\nauthorized users. It is crucial that the system safeguards confidential and private information,\neven under hostile or adversarial conditions. Furthermore, appropriate measures should be\nin place to ensure that AI systems with automated decision-making capabilities uphold the\nnecessary data privacy and security standards.\n3- The AI System should be tested to ensure that the combination of available data does not\nreveal the sensitive data or break the anonymity of the observation.\nDeploy and Monitor:\n1- After the deployment of the AI system, when its outcomes are realized, there must be\ncontinuous monitoring to ensure that the AI system is privacy-preserving, safe and secure.\nThe privacy impact assessment and risk management assessment should be continuously\nrevisited to ensure that societal and ethical considerations are regularly evaluated.\n2- AI System Owners should be accountable for the design and implementation of AI systems in\nsuch a way as to ensure that personal information is protected throughout the life cycle of the AI\nsystem. The components of the AI system should be updated based on continuous monitoring\nand privacy impact assessment.\nPrinciple 3 \u2013 Humanity\nThe humanity principle highlights that AI systems should be built using an ethical methodology\nto be just and ethically permissible, based on intrinsic and fundamental human rights and\ncultural values to generate a beneficial impact on individual stakeholders and communities,\nin both the long and short-term goals and objectives to be used for the good of humanity.\nPredictive models should not be designed to deceive, manipulate, or condition behavior that is\nnot meant to empower, aid, or augment human skills but should adopt a more human-centric\ndesign approach that allows for human choice and determination.\nAI Ethics Principles\n17\nPlan and Design:\n1- It is essential to design and build a model that is based on the fundamental human rights and\ncultural values and principles that are applied within and on the AI system\u2019s decisions, processes,\nand functionalities.\n2- The designers of the AI model should define how the AI system will align with fundamental\nhuman rights and KSA\u2019s cultural values while designing, building, and testing the technology;\nas well as how the AI system and its outcomes will strive to achieve and positively contribute\nto augment and complement human skills and capabilities.\nPrepare Input Data:\n1- To ensure that AI models embody a human-centric build and design that requires adhering to\npractices of responsible and ethical data management frameworks and processes to be followed\naccording to best practices and data regulations within KSA.\n2- Data must be properly acquired, classified, processed, and accessible to ensure respect for\nhuman rights, and KSA\u2019s cultural values and preferences.\nBuild and Validate:\n1- When constructing AI systems, designers and engineers should prioritize building AI systems\nand algorithms that allow and facilitate decision-making with an outlook of aligning with human\nrights and KSA\u2019s cultural values. The automated decisions that result from AI systems should not\nact in a partial and standalone manner without considering broader human rights and cultural\nvalues in their final outcomes and results.\n2- Designers and Engineers should enable AI systems with the appropriate parameters and\nalgorithm training to attain outcomes that advance humanity.\nAI Ethics Principles\n18\nDeploy and Monitor:\n1- Periodic assessments of the deployed AI system should be conducted to ensure that its\nresults are aligned with human rights and cultural values, accuracy key performance indicators\n(KPIs), and impact on individuals or communities to ensure the continuous improvement of the\ntechnology.\n2- Designers of AI models should establish mechanisms of assessing AI systems against\nfundamental human rights and cultural values to mitigate any negative and harmful outcomes\nresulting from the use of the AI system. If any negative and harmful outcomes are found,\nthe owner of the AI system should identify the areas that need to be addressed and apply\ncorrective measures to recursively improve the functioning and outcomes of the AI system.\nPrinciple 4 \u2013 Social & Environmental Benefits\nThe social and environmental benefit principle embraces the beneficial and positive impact of\nsocial and environmental priorities that should benefit individuals and the wider community that\nfocus on sustainable goals and objectives. AI systems should neither cause nor accelerate\nharm or otherwise adversely affect human beings but rather contribute to empowering\nand complementing social and environmental progress while addressing associated social\nand environmental ills. This entails the protection of social good as well as environmental\nsustainability.\nPlan and Design:\n1- AI systems have a significant impact on communities and the ecosystems that they live in;\nhence AI System Owners should have a high sense of awareness that these technologies may\nhave disruptive and transformative effects on society and the environment. The design of AI\nsystems should be approached in an ethical and sensitive manner in line with the values of\nprevention of harm to both human beings and the environment.\n2- When planning and designing AI systems, due consideration should be given to preventing\nand helping address social and environmental issues in a way that will ensure sustainable social\nand ecological responsibility.\nAI Ethics Principles\n19\nPrepare Input Data:\n1- The processes and policies that govern data management should be followed when preparing\nthe categorization and structuring of data that will feed into the AI system.\n2- The data pertaining to the social and environmental topics should be accessible to the public\ndata infrastructure and must clearly articulate the social benefit of the data presented.\nBuild and Validate:\n1- The models and algorithms must have, as their ultimate goal, a result linked to a socially\nrecognized end, with the ability to demonstrate how the expected results relate to that social\nor environmental purpose through transformative and impactful benefits where applicable.\n2- It is best practice to measure and maintain acceptable levels of resource usage and energy\nconsumption during this phase setting the tone that AI systems not only strive to foster AI\nsolutions that address global concerns relating to social and environmental issues but also\npractice sustainable and ecological responsibilities.\nDeploy and Monitor:\n1- After the deployment of the AI system, the AI System Owner should ensure that continuous\nassessment of the human, social, cultural, economic and environmental impact of AI\ntechnologies are carried out with full cognizance of the implications of the AI system for\nsustainability as a set of constantly evolving goals across a range of dimensions against the\npriority objectives that were set at the Plan and Design phase.\n2- The AI System Owner should also foster and encourage the power of AI solutions in\naddressing areas of global concern aligning with sustainable development goals.\nAI Ethics Principles\n20\nPrinciple 5 \u2013 Reliability & Safety\nThe reliability and safety principle ensures that the AI system adheres to the set specifications\nand that the AI system behaves exactly as its designers intended and anticipated. Reliability is\na measure of consistency and provides confidence in how robust a system is. It is a measure of\ndependability with which it operationally conforms to its intended functionality and the outcomes\nit produces. On the other hand, safety is a measure of how the AI system does not pose a risk\nof harm or danger to society and individuals. As an illustration, AI systems such as autonomous\nvehicles can pose a risk to people\u2019s lives if living organisms are not properly recognized, certain\nscenarios are not trained for or if the system malfunctions. A reliable working system should be\nsafe by not posing a danger to society and should have built-in mechanisms to prevent harm.\nThe risk mitigation framework is closely related to this principle. Potential risks and unintended\nharms should be minimized in this aspect.\nThe predictive model should be monitored and controlled in a periodic and continuous manner to\ncheck if its operations and functionality are aligned with the designed structure and frameworks\nin place. The AI system should be technically sound, robust, and developed to prevent malicious\nusage to exploit its data and outcomes to harm entities, individuals or communities. A continuous\nimplementation/continuous development approach is essential to ensure reliability.\nPlan and Design:\n1- Designing and developing an AI system that can withstand the uncertainty, instability, and\nvolatility that it might encounter is crucial.\n2- Planning to set out a robust and reliable AI system that works with different sets of inputs\nand situations is essential to prevent unintended harm and mitigate risks of system failures\nwhen positioned against unknown and unforeseen events.\n3- Establishing a set of standards and protocols for assessing the reliability of an AI system\nis necessary to secure the safety of the system\u2019s algorithm and data output. It is essential to\nkeep a sustainable technical outlay and outcomes generated from the system to maintain the\npublic\u2019s trust and confidence in the AI system.\nAI Ethics Principles\n21\n4- The documentation standards are essential to track the evolution of the system, foresee\npossible risks and fix vulnerabilities.\n5- All critical decision points in the system design should be subject to sign-off by relevant\nstakeholders to minimize risks and make stakeholders accountable for the decisions.\nPrepare Input Data:\n1- Adequate steps and actions should be taken to measure the data sample\u2019s quality, accuracy,\nsuitability, and credibility when dealing with the data sets of an AI model. This is essential\nto ensure the accuracy of data interpretation by the AI system, the consistency of avoiding\nmisleading measurements, as well as ensuring the relevance of the AI system\u2019s outcomes to\nthe purpose of the model.\n2- It is crucial for the build and validate step to test how the system behaves under outlier events,\nextreme parameters, etc. In this step, stress test data should be prepared for extreme scenarios.\nBuild and Validate:\n1- To develop a sound and functional AI system that is both reliable and safe, the AI system\u2019s\ntechnical construct should be accompanied by a comprehensive methodology to test the quality\nof the predictive data-based systems and models according to standard policies and protocols.\n2- To ensure the technical robustness of an AI system rigorous testing, validation, and\nre-assessment as well as the integration of adequate mechanisms of oversight and controls\ninto its development is required. System integration test sign-off should be done with relevant\nstakeholders to minimize risks and liability.\n3- Automated AI systems involving scenarios where decisions are understood to have an impact\nthat is irreversible or difficult to reverse or may involve life-and-death decisions should trigger\nhuman oversight and final determination. Furthermore, AI systems should not be used for social\nscoring or mass surveillance purposes.\nAI Ethics Principles\n22\nDeploy and Monitor:\n1- Monitoring the robustness of the AI system should be adopted and undertaken in a periodic\nand continuous manner to measure and assess any risks related to the technicalities of the AI\nsystem (an inward perspective) as well as the magnitude of the risk posed by the system and\nits capabilities (an outward perspective).\n2- The model must also be monitored in a periodic and continuous manner to verify whether\nits operations and functions are compatible with the designed structure and frameworks. The\nAI system must also be safe to prevent destructive use to exploit its data and results to harm\nentities, individuals, or groups. It is necessary to continuously work on implementation and\ndevelopment to ensure system reliability.\nPrinciple 6 \u2013 Transparency & Explainability\nThe transparency and explainability principle is crucial for building and maintaining trust in AI\nsystems and technologies. AI systems must be built with a high level of clarity and explainability\nas well as features to track the stages of automated decision-making, particularly those that\nmay lead to detrimental effects on data subjects. It follows that data, algorithms, capabilities,\nprocesses, and purpose of the AI system need to be transparent and communicated as well as\nexplainable to those who are directly and indirectly affected. The degree to which the system is\ntraceable, auditable, transparent, and explainable is dependent on the context and purpose of\nthe AI system and the severity of the outcomes that may result from the technology. AI systems\nand their designers should be able to justify how the rationale behind their design, practices,\nprocesses, algorithms, and decisions or behaviors are ethically permissible, nondiscriminatory,\nand nonharmful to the public.\nPlan and Design:\n1- When designing a transparent and trusted AI system, it is vital to ensure that stakeholders\naffected by AI systems are fully aware and informed of how outcomes are processed. They\nshould further be given access to and an explanation of the rationale for decisions made by the\nAI technology in an understandable and contextual manner. Decisions should be traceable. AI\nsystem owners must define the level of transparency for different stakeholders on the technology\nbased on data privacy, sensitivity, and authorization of the stakeholders.\nAI Ethics Principles\n23\n2- The AI system should be designed to include an information section in the platform to give\nan overview of the AI model decisions as part of the overall transparency application of the\ntechnology. Information sharing as a sub-principle should be adhered to with end-users and\nstakeholders of the AI system upon request or open to the public, depending on the nature\nof the AI system and target market. The model should establish a process mechanism to log\nand address issues and complaints that arise to be able to resolve them in a transparent and\nexplainable manner.\nPrepare Input Data:\n1- The data sets and the processes that yield the AI system\u2019s decision should be documented\nto the best possible standard to allow for traceability and an increase in transparency.\n2- The data sets should be assessed in the context of their accuracy, suitability, validity, and\nsource. This has a direct effect on the training and implementation of these systems since\nthe criteria for the data\u2019s organization, and structuring must be transparent and explainable in\ntheir acquisition and collection adhering to data privacy regulations and intellectual property\nstandards and controls.\nBuild and Validate:\n1- Transparency in AI is thought about from two perspectives, the first is the process behind it\n(the design and implementation practices that lead to an algorithmically supported outcome) and\nthe second is in terms of its product (the content and justification of that outcome). Algorithms\nshould be developed in a transparent way to ensure that input transparency is evident and\nexplainable to the end-users of the AI system to be able to provide evidence and information on\nthe data used to process the decisions that have been processed.\n2- Transparent and explainable algorithms ensure that stakeholders affected by AI systems,\nboth individuals and communities, are fully informed when an outcome is processed by the\nAI system by providing the opportunity to request explanatory information from the AI system\nowner. This enables the identification of the AI decision and its respective analysis which\nfacilitates its auditability as well as its explainability.\n3- If the AI system is built by a third party, AI system owners should make sure that an AI Ethics\ndue diligence is carried out and all the documentation are accessible and traceable before\nprocurement or sign-off.\nAI Ethics Principles\n24\nDeploy and Monitor:\n1- Upon deployment of the AI system, performance metrics relating the AI system\u2019s output,\naccuracy and alignment to priorities and objectives, as well as its measured impact on individuals\nand communities should be documented, available and accessible to stakeholders of the AI\ntechnology.\n2- Information on any system failures, data breaches, system breakdowns, etc. should be\nlogged and stakeholders should be informed about these instances keeping the performance\nand execution of the AI system transparent. Periodic UI and UX testing should be conducted\nto avoid the risk of confusion, confirmation of biases, or cognitive fatigue of the AI system.\nPrinciple 7 \u2013 Accountability & Responsibility\nThe accountability and responsibility principle holds designers, vendors, procurers, developers,\nowners and assessors of AI systems and the technology itself ethically responsible and liable for\nthe decisions and actions that may result in potential risk and negative effects on individuals and\ncommunities. Human oversight, governance, and proper management should be demonstrated\nacross the entire AI System Lifecycle to ensure that proper mechanisms are in place to avoid\nharm and misuse of this technology. AI systems should never lead to people being deceived\nor unjustifiably impaired in their freedom of choice. The designers, developers, and people who\nimplement the AI system should be identifiable and assume responsibility and accountability\nfor any potential damage the technology has on individuals or communities, even if the adverse\nimpact is unintended. The liable parties should take necessary preventive actions as well as\nset risk assessment and mitigation strategy to minimize the harm due to the AI system. The\naccountability and responsibility principle is closely related to the fairness principle. The parties\nresponsible for the AI system should ensure that the fairness of the system is maintained and\nsustained through control mechanisms. All parties involved in the AI System Lifecycle should\nconsider and action these values in their decisions and execution.\nAI Ethics Principles\n25\nPlan and Design:\n1- This step is crucial to design or procure an AI System in an accountable and responsible\nmanner. The ethical responsibility and liability for the outcomes of the AI system should be\nattributable to stakeholders who are responsible for certain actions in the AI System Lifecycle. It\nis essential to set a robust governance structure that defines the authorization and responsibility\nareas of the internal and external stakeholders without leaving any areas of uncertainty to\nachieve this principle. The design approach of the AI system should respect human rights, and\nfundamental freedoms as well as the national laws and cultural values of the kingdom.\n2- Organizations can put in place additional instruments such as impact assessments, risk\nmitigation frameworks, audit and due diligence mechanisms, redress, and disaster recovery\nplans.\n3- It is essential to build and design a human-controlled AI system where decisions on the\nprocesses and functionality of the technology are monitored and executed, and are susceptible\nto intervention from authorized users. Human governance and oversight establish the necessary\ncontrol and levels of autonomy through set mechanisms.\nPrepare Input Data:\n1- An important aspect of the Accountability and Responsibility principle during Prepare Input\nData step in the AI System Lifecycle is data quality as it affects the outcome of the AI model\nand decisions accordingly. It is, therefore, important to do necessary data quality checks, clean\ndata and ensure the integrity of the data in order to get accurate results and capture intended\nbehavior in supervised and unsupervised models.\n2- Data sets should be approved and signed-off before commencing with developing the AI\nmodel. Furthermore, the data should be cleansed from societal biases. In parallel with the\nfairness principle, the sensitive features should not be included in the model data. In the event\nthat sensitive features need to be included, the rationale or trade-off behind the decision for\nsuch inclusion should be clearly explained. The data preparation process and data quality\nchecks should be documented and validated by responsible parties.\n3- The documentation of the process is necessary for auditing and risk mitigation. Data must be\nproperly acquired, classified, processed, and accessible to ease human intervention and control\nat later stages when needed.\nAI Ethics Principles\n26\nBuild and Validate:\n1- Model development of the AI system and algorithm should consist of the selection of\nfeatures, hyperparameter tuning and performance metric selection. To achieve this, the technical\nstakeholders who build and validate models should be responsible for these decisions.\n2- Assigning the appropriate ownership and communicating responsibilities will set the tone for\naccountability that would aid in steering the development of the AI system on good reasons,\nsolid interference, and will allow the intervention of human critical judgement and expertise.\n3- The decisions should be supported with quantitative (performance measures on train/test\ndatasets, consistency of the performance on different sensitive groups, performance comparison\nfor each set of hyperparameters, etc.) and qualitative indicators (decisions to mitigate and correct\nunintended risks from inaccurate predictions).\n4- The appropriate stakeholders and owners of the AI technology should review and sign o\ufb00 the\nmodel after successful testing and validation of user acceptance testing rounds have been\nconducted and completed before the AI models can be productionized.\nDeploy and Monitor:\n1- The responsibility and associated liability in the Deploy and Monitor step should be set\nclearly. The outcomes and decisions set in the build and validate step should be monitored\ncontinuously and should result in periodic performance reports.\n2- Predefined triggers/alerts should be defined for this step on the data and performance\nmetrics. Setting these triggers is a rigorous process and each trigger should be assigned to the\nappropriate stakeholder. These triggers/alerts can be defined as part of the risk mitigation or\ndisaster recovery procedure and may need human oversight.\nAI Ethics Principles\n27\nRoles and\nResponsibilities\nRoles and Responsibilities\nThe AI Ethics Framework defines the following roles and responsibilities both at the national and\nentity level.\nNational Level\nSDAIA\nSDAIA works to review and update the principles of artificial intelligence ethics and monitor\ncompliance with them. SDAIA also prepares national guides, standards, and directives that\nensure the effective management and dissemination of artificial intelligence ethics at the\nKingdom level and achieve the desired goal. In order to implement its objectives, the Authority\nmay carry out the following tasks:\nAI Ethics Development: Develop, issue, and update this AI Ethics principles and framework.\nThis document should be revisited regularly to address possible changes affecting AI Ethics,\nassociated regulations, communities, and the environment.\nAI Ethics Adoption Plan Development: Develop supporting material and provide\ncontinuous guidance to Adopting Entities to facilitate the adoption of this Framework.\nAI Ethics Advisory: Support Adopting Entities in complying with this Framework and answer\nany queries related to the AI Ethics and compliance with this document.\nAI Ethics Compliance Measurement: Measure compliance of Adopting Entities on a\nregular basis based on the defined compliance mechanism directly or through sector\nregulators (Refer to the \u2018Compliance\u2019 section for more details) and audit AI Ethics activities\nwhen required.\nAI Ethics Compliance Monitoring: Conduct investigations and audits and monitor\ncompliance with this Framework with the support of the National Regulatory Authorities.\nAI Ethics Principles\n29\nAdopting Entities\nAll Adopting Entities shall have the primary responsibility for ensuring that their AI Ethics\ndocuments are published in compliance with these AI Ethics Principles. As such, Entities shall\ndesignate individuals who will be responsible for carrying out the AI Ethics activities as outlined\nbelow.\n1- Head of the Entity / Chief Data O\ufb03cer (CDO): Responsible for the AI Ethics practice within\nan Entity. Responsibilities include:\nApprove and oversee the implementation of the AI Ethics Plan within the Entity.\nDesignate di\ufb00erent roles with regard to AI Ethics.\nApprove the AI Ethics annual report.\nTake or delegate the necessary actions for the resolution of the issues that are raised by the\nCCO/CO.\nAct as the first point of contact between the Entity and the Authority. The Head of the Entity\nor CDO shall resolve any pending issues around AI Ethics for their respective Entity and\nescalate them to SDAIA whenever necessary.\n2- Chief Compliance Officer (CCO) / Compliance Officer (CO): The strategic lead of the\nAI Ethics practice, the CO is positioned under the CDO and reports directly to the CDO. The\nresponsibilities of the CCO/CO shall include:\nOversee the development of the AI Ethics Plan and present it to the head of the Entity. The\nCCO/CO shall also review the performance of AI Ethics to identify improvement opportunities\nand feed into the AI Ethics Plan.\nReview the AI Ethics identification and prioritization activities, monitor AI Ethics KPIs for\nin-house and third-party systems, and ensure maintenance activities are being performed.\nEnsure compliance of the Entity\u2019s AI Ethics activities with national regulations, including but\nnot limited to Data Classification, Data Privacy, and Freedom of Information. Make sure that\nthird-party systems comply with these Principles through contractual guarantees.\nAI Ethics Principles\n30\n3- Responsible AI Officer (RAIO): The operational lead of Responsible AI within the entity\nand works collaboratively with other officers from the Data Management team. Responsibilities\ninclude:\nDevelop the AI Ethics Plan, including the AI Ethics prioritization methodology, and set targets\nand KPIs to be agreed on with the CCO/CO or head of Entity/CDO.\nCollaborate with other o\ufb03cers in managing, governing, and protecting data.\nDefining, updating, maintaining, and reviewing AI ethics priorities and procedures.\nEducate and raise awareness across the Entity\u2019s employees on AI Ethics and support national\nawareness campaigns in coordination with the CCO/CO.\n4- AI System Assessor: Responsible for auditing AI systems to achieve certain goals and has\nthe following responsibilities:\nReview communications channels and interactions with stakeholders to provide disclosure\nand e\ufb00ective feedback channels.\nConduct periodical reviews on the AI Ethics process and documentation.\nContinuously review AI Ethics KPI\u2019s.\nPublish audit reports on the AI Ethics Assessment of the organization which covers the AI\ndevelopment and deployment process as well as the third-party AI product procurement\nprocess.\nAI Ethics Principles\n31\nOptional Registration\nOptional Registration aims to motivate target entities to consider AI ethics when building and\ndeveloping AI-based solutions to ensure a responsible use.\nCompliance\nThe Authority may follow up and measure the level of commitment and compliance of\nregistered entities and support them in evaluating compliance with the application of their\nAI ethics and submitting optional reports.\nThe level of compliance is measured according to the following:\nDisplaying the progress of the product or entity in complying with the checklist\nmentioned in the attachments of this document.\nResults of internal or external evaluation of AI ethics.\nObjectives of the product or entity and indicators for measuring the performance of AI\nethics.\nThe level of compliance to AI ethics and the fulfillment of its requirements and the badges\nobtained by the product or entity.\nThe authority can assist the entities in reviewing annual reports and make recommendations\nregarding general compliance with AI ethics.\nMotivational Badges\nTo motivate the implementing agencies to register and work with the principles of AI ethics, the\nAuthority may provide motivational badges that will reflect the level of compliance and progress\non AI ethics adoption. The Authority will issue a guide explaining the mechanism and controls\nfor awarding them.\nAI Ethics Principles\n32\nAnnexure\nAnnexure A: AI Ethics Tools\nAI Fairness Position Statement: A fairness position statement allows the owner of the AI\ntechnology to clearly state the fairness criteria that have been employed by the AI system and\nexplain the rationale and logic behind it in a direct and non-technical language. To implement\na fair AI system in a sustainable way, choosing the right fairness objectives is key to setting\nthe tone of the AI model in terms of its ethical standards and regulatory requirements. This is\ndone by sharing the reasons and underlying fairness values expressed throughout the model\nas well as the decision-making process of the AI model to communicate and reach the wider\naudience. This document would be made accessible and available to the public and affected\nindividuals and communities.\nEthical Impact Assessment: AI has accelerated innovation in how business is conducted\nand executed by practitioners, and therefore it is imperative to evolve AI system ethical\nimpact assessments to identify areas that need adjustment and recalibrating to design\nthe AI model into an ethically accepted technology to maximize its positive impact on\ncomplimenting human capabilities and skillsets. The objective of impact assessments is\nto evaluate and analyze the level of ethical impact of the AI technology on individuals or\ncommunities in both direct and indirect manners which enables the owner of the AI system\nto address identified issues and strengthen areas where improvements and adjustments\nare required. It is also essential to be able to assess the ethical risks that the AI system\nis projecting, analyze the discriminatory harm impact and accurate representation of the\nsystem on the ethical impact through a diversified and multi-stakeholder analysis, as well as\nact as a facilitator to address whether a model should move to production or deployment.\nOne of the purposes of the ethical impact assessment is to help build public confidence\naround the AI system and demonstrate consideration and due diligence to wider the public\naudience.\nAI Ethics Principles\n34\nPrivacy and Security Standards: Privacy and security standards are in place to help\ncompanies improve their information security strategy by providing guidelines and best\npractices based on the company\u2019s industry and the type of data they maintain. In the\nfollowing table, some examples are given for privacy and security standards:\nStandard Name Link\nISO standards for artificial intelligence, such as the\nISO 23894 risk standard and the standards issued\nby the Saudi Standards and Metrology Organization\nhttps://www.iso.org/standard/77304.html\nStandards of the Institute of Electrical and\nElectronics Engineers\nhttps://www.standards.ieee.org\nNIST Cybersecurity Framework (National\nInstitute of Standards and Technology)\nhttps://www.nist.gov/cyberframework/framework\nNIST AI Risk Management Framework (National\nInstitute of Standards and Technology) \u2013 Work\nin Progress\nhttps://www.nist.gov/itl/ai-risk-management-framework\nCIS Controls (Center for Internet Security\nControls)\nhttps://www.cisecurity.org/controls/\nPCI-DSS (Payment Card Industry Data\nSecurity Standard)\nhttps://www.pcisecuritystandards.org/pci_security/\nCOBIT (Control Objectives for Information\nand Related Technologies)\nhttp://www.isaca.org/resources/cobit\nAI Ethics Principles\n35\nArchitecture for Trustworthy AI: The requirements that are stated in the AI Ethics\nPrinciples and Controls section should be reflected in the design of the AI system\narchitecture. The AI system architecture should set the rules and restrictions across the\nAI System Lifecycle. The principles and controls are generic and addressing them to the\nparticular use cases or AI systems should be done with the sense-plan-act theoretical\napproach. Adapting the architecture to AI Ethics entails the integration of the three steps\nof this approach:\nSense: The system should be developed such that it recognizes all environmental\nelements necessary to ensure adherence to the requirements\nPlan: The system should only consider plans that adhere to the requirements\nAct: The system\u2019s actions should be restricted to behaviors that realize the requirements.\nThe technical objectives of accuracy, reliability, safety, and robustness must be prioritized to\nensure that the AI System functions safely. AI System Developers should build a system that will\noperate accurately and reliably, in accordance with the intended design, even when confronted\nwith unexpected changes, anomalies, and disturbances.\nAlgorithm Assessment: The objective of the algorithm assessment is to ensure that\nindividuals or communities are informed about the use of algorithms and the weights\nand counterweights that exist to manage their use. The AI maturity of enterprises and\ngovernment entities are different, and this assessment will show the improvement areas for\nthe respective entities or government entities to improve descriptions on how algorithms\ninform or impact decision making, particularly in those cases where there is a degree of\nautomatic decision making or where the algorithms support decisions that have a significant\nimpact on individuals or groups. This assessment will balance the human oversight for\nefficient AI systems.\nAI Ethics Principles\n36\nFairness Assessment: It is a set of diagnostic methods that helps you compare how fair\nmodels and label markers perform for specific groups. It checks whether the model\u2019s result\nis regularly overestimated or underestimated for one or more groups compared to others.\nAdditionally, it evaluates how well the diversity of data is represented for each group. In the\nfollowing table, fairness assessment tool examples are included\nTool Name Link\nGoogle Model Card Toolkit https://github.com/tensorflow/model-card-toolkit\nAI Fairness 360 https://github.com/Trusted-AI/AIF360\nMicrosoft Fairlearn https://fairlearn.org/\nGoogle What-if Tool https://pair-code.github.io/what-if-tool/\nAequitas Bias and Fairness Audit Toolkit http://aequitas.dssg.io/\nVeritas Fairness Assessment Tool https://github.com/mas-veritas2/veritastool\nTensorFlow Fairness Indicators https://www.tensorflow.org/tfx/guide/fairness_indicators\nAI Explainability 360 https://github.com/Trusted-AI/AIX360\nAI Ethics Principles\n37\nAI Methods Explanation Report: As explained under Transparency and Explainability\nsection, it should be explained why the system behaves the way it does and how it takes\ndecisions. Although some training methods have superior performance, they work as\nblack-box, and it is a challenge to interpret the results. Small deviations in the data could\nlead to dramatic deviations and changes in the outcome. The report should explain the\nbehavior of the system as well as ensure the deployment of reliable technology. The report\nshould help to better understand the AI system\u2019s underlying mechanism as well as the\ninterpretation of the outcomes.\nAI System stakeholders should consider the trade-off between performance, materialization,\nand explanation methods. In some cases, explanation methods increase complexity or\nrequire sacrificing performance. A cost-benefit analysis should be conducted and the level\nof explainability should be justified based on this analysis.\nAlgorithm Auditing: Unexpected algorithmic behaviors can be detected with algorithm\naudits. In general, algorithm audits are done on an ad-hoc basis, and it is important to\nstandardize algorithm auditing process with supporting AI algorithms. The process should\nbe systematic and continuous. Regulating and auditing AI systems for ethical compliance\nis more complicated than regulating and auditing human decision-making or processes. AI\nsystems should be designed with due consideration of AI Ethics principles and controls.\nAuditing mechanisms should follow the same principles and controls in alignment with\nthese principles.\nSafety Self-Assessment: Safety considerations of accuracy, reliability, security, and\nrobustness should be considered at every step of the AI System Lifecycle. AI system\nsafety self-assessments should be continuously logged and documented in a way that\nallows review and re-assessment. The performance of AI system safety self-assessments\nshould be conducted by relevant stakeholders at each stage of the workflow. They should\nevaluate how the design and implementation practices line up with the safety objectives\nof accuracy, reliability, security, and robustness.\nAI Ethics Principles\n38\nData Protection Methods: These methods help to protect data by applying data\ntransformation methods, especially for the data that is classified as sensitive. The following\ndata protection methods are given as examples and they should be after data classification.\nData De-Identification is the process of eliminating Personally Identifiable Data (PII) from\nany document or other media, including an individual\u2019s Protected Health Information (PHI).\nData Anonymization is a kind of data sanitization process that intends to protect the\nprivacy of individuals. It is the process of removing PII from data sets to maintain the\nanonymity of individuals whom the data describes. It is often the preferred method for\nmaking structured medical datasets secure for sharing.\nData Masking is a technique that removes or hides information, replacing it with realistic\nreplacement data or fake information. The objective is to create a version that can\u2019t\nbe decoded or reverse engineered. There are a number of ways to change the data,\nincluding encryption, character shuffling, and word or character replacement.\nData Pseudonymization is a way of masking data that ensures it is not possible\nto attribute personal data to a specific person, without using additional information\nsubject to security measures. It is an integral part of the EU General Data Protection\nRegulation (GDPR), which has several recitals specifying how and when data should\nbe pseudonymized.\nData Encryption is a method of data masking, used to protect it from cybercriminals,\nothers with malicious intent, or accidental exposure. The data might be the contents of\na database, an email note, an instant message, or a file retained on a computer.\nData Tokenization is a process of substituting personal data with a random token.\nOften, a link is maintained between the original information and the token (such as for\npayment processing on sites). Tokens can be completely random numbers or generated\nby one-way functions (such as salted hashes).\nData Loss Prevention (DLP): Data Loss Prevention (DLP) is used to detect and prevent\ndata breaches. This involves monitoring network activity, identifying and blocking\nsuspicious behavior, and implementing encryption and access controls.\nAI Ethics Principles\n39\nData Governance: Data Governance encompasses all aspects of data management\nthroughout its entire life cycle, including security, usability, availability, and privacy. This\ninvolves defining data handling policies and processes and assigning data management\nauthority and responsibilities.\nData Minimization: Data minimization refers to the practice of collecting just the\npersonal information required to satisfy a specified objective. This helps to mitigate the\nrisks connected with data breaches and the abuse of personal information\nAnnexure B: AI Ethics Tools Mapping to\nAI System Lifecycle\nTool Plan & Design\nPrepare Input\nData\nBuild & Validate\nDeploy & Monitor\nFairness Position Statement\nEthical Impact Assessment\nPrivacy and Security Standards\nArchitecture for Trustworthy AI\nAlgorithm Assessment\nFairness Assessment\nAI Methods Explanation Report\nAlgorithm Auditing\nSafety Self-Assessment\nData Protection Methods\nAI Ethics Principles\n40\nAnnexure C: AI Ethics Checklist\nAI System Lifecycle Phase 1: Plan & Design\nPhase Question Did you design the appropriate level of human oversight for the AI\nPD.1 Yes system and use case?\nBinding for\nThird-party PD.2\nDoes your AI system design prevent overconfidence in or overreliance\non the AI system with necessary human intervention mechanisms?\nYes\nPD.3\nDid you define human oversight processes with the appropriate KPIs\nand assign responsibility to the relevant parties?\nNo\nPD.4\nDid you design an operation and governance strategy to abort or\nintervene in the system when the system doesn't work in an\nintended way?\nNo\nPD.5\nDid you consider the liability and Data Subject protection\nrequirements and take them into account?\nYes\nPD.6\nDid you define thresholds of the KPIs and did you put governance\nprocedures or autonomous actions in place to trigger\nalternative/fallback plans?\nNo\nPD.7\nDid you provide training and education to help develop\naccountability practices?\nNo\nPD.8\nDid you ensure that the AI Ethics Governance structure is\ncompliant with the proposed governance mechanism in the\nNational AI Ethics Policy?\nNo\nPD.9\nDid you ensure that the AI Ethics Governance structure includes\ninternal or external audit mechanisms?\nNo\nPD.10\nDid you establish a strategy or a set of procedures to avoid creating\nor reinforcing unfair bias in the AI system, covering both input data as\nwell as for the algorithm design?\nYes\nPD.11\nDid you identify sensitive personal data attributes relating to persons\nor groups that are systematically or historically disadvantaged? If so,\nthe permissible limit that makes the assessment fair or unfair must\nbe determined.\nNo\nPD.12\nDid you define fairness assessment KPIs?\nNo\nPD.13\nDid you consider a mechanism to include the participation of different\nstakeholders in the AI system\u2019s development and use?\nNo\nPD.14\nDid you conduct an impact analysis on how the AI system affects\nfundamental human rights and cultural values? Did you list any\npotential negative effects on fundamental human rights and cultural\nvalues and the solutions or recovery mechanisms?\nYes\nAI Ethics Principles\nPrinciples\nAccountability &\nResponsibility\nAccountability &\nResponsibility\nAccountability &\nResponsibility\nAccountability &\nResponsibility\nAccountability &\nResponsibility\nAccountability &\nResponsibility\nAccountability &\nResponsibility\nAccountability &\nResponsibility\nAccountability &\nResponsibility\nFairness\nFairness\nFairness\nFairness\nHumanity\n41\nPhase\nPD.15\nQuestion Did you put measures in place to ensure that the AI system does\nnot lead to people being deceived or unjustifiably impaired in their\nfreedom of choice?\nBinding for\nThird-party Yes\nPrinciples\nHumanity\nPD.16\nDid you align your AI system with relevant standards or policies (for\nexample, ISO, IEEE, Data Privacy Law) or widely adopted protocols\nfor daily data management and governance?\nYes\nPrivacy &\nSecurity\nPD.17\nDid you follow established protocols, processes, and procedures to\nmanage and ensure proper data governance? Did you ensure that\nNational Data Management and Personal Data Protection\nStandards are followed?\nYes\nPrivacy &\nSecurity\nPD.18\nDid you ensure that data access control meets security, privacy,\nand compliance requirements? Did you design a log mechanism for\naudit and debug purposes?\nYes\nPrivacy &\nSecurity\nPD.19\nDid you design a risk management strategy for your AI system? Did you\ninclude risk tiers, KPIs, risk assessment, and mitigation procedures?\nNo\nReliability &\nSafety\nPD.20\nDid you assess whether there is a likelihood that the AI system may\ncause damage or harm to users or third parties? Did you assess\nthe potential damage, impacted audience, and severity?\nYes\nReliability &\nSafety\nPD.21\nDid you assess whether there is a likelihood that the AI system may\nunintentionally give wrong results or inaccurate predictions, fail or\nfeed societal biases?\nYes\nReliability &\nSafety\nPD.22\nDid you consider the potential impact or safety risk to the environment,\nliving creatures, or society in addition to the Data Subjects?\nYes\nSocial &\nEnvironmental\nBenefits\nPD.23\nDid you assess whether the system\u2019s business model is aligned with\nthe organization's vision and mission as well as the code of conduct?\nYes\nTransparency\n& Explainability\nPD.24\nDid you design an interpretable AI system where the data, algorithms,\noutcomes, and decisions are transparent and explainable to the\nrelated parties?\nYes\nTransparency\n& Explainability\nPD.25\nDid you design User Experience with human psychology in mind to\navoid the risk of confusion, confirmation bias, or cognitive fatigue?\nYes\nTransparency\n& Explainability\nPD.26\nWas there any trade off assumption?\nYes\nFairness\nPD.27\nHave you established a measurement or assessment mechanism for\nprivacy impact?\nYes\nPrivacy &\nSecurity\nPD.28\nHas the data management approach been reviewed based on\nhuman-centric values and according to data regulations within\nthe KSA?\nYes\nHumanity\nAI Ethics Principles\n42\nAI System Lifecycle Phase 2: Prepare Input Data\nPhase\nPID.1\nQuestion Is there an established mechanism that flags issues related to data\nprivacy or protection in the process of data collection and processing?\nBinding for\nThird-party Yes\nPrinciples\nPrivacy &\nSecurity\nPID.2\nHas the data been reviewed in terms of scope and categorization?\nNo\nPrivacy &\nSecurity\nPID.3\nHas the data been reviewed to check if personal data is evident\nwithin the dataset? Is there an established mechanism that allows\nthe AI model to train without or with minimal use of personal or\nsensitive data?\nNo\nPrivacy &\nSecurity\nPID.4\nIs there an established mechanism that controls the usage of\npersonal data (such as valid consent and the possibility to revoke,\nwhen applicable)?\nYes\nPrivacy &\nSecurity\nPID.5\nAre there processes to ensure that AI systems are secure and keep\ninformation safe, confidential, and private, as well as the integrity of the\nprocessed information even under hostile or adversarial conditions?\nYes\nPrivacy &\nSecurity\nPID.6\nHas the quality and source of the acquired data been assessed\nthrough set processes?\nNo\nPrivacy &\nSecurity\nPID.7\nHas there been an assessment on whether an analysis can be\nperformed post training and testing the data?\nNo\nTransparency\n& Explainability\nPID.8\nHas diversity and inclusion of the dataset at hand been considered\nor reviewed?\nFairness\nNo\nPID.9\nIs there an established mechanism that measures whether the\nintegrity, quality, and accuracy of data collection and its sources\nhave been evaluated and data is up to date?\nNo\nAccountability &\nResponsibility\nPID.10\nHas an analysis process been developed for the proxies of the\nsensitive features?\nYes\nFairness\nPID.11\nHas the team evaluated the classification, processing, and access\nto data to ensure that it has been properly acquired?\nYes\nHumanity\nPID.12\nHave the data and AI models been validated to include respect\nfor human rights, values, and cultural preferences in the Kingdom\nof Saudi Arabia?\nYes\nHumanity\nPID.13\nDid you classify the data using SDAIA recommendations? If you\nuse other standards, please mention them.\nYes\nSocial &\nEnvironmental\nBenefits\nPID.14\nAre there appropriate procedures to measure the quality, accuracy,\nrelevance, and credibility of a data sample when dealing with data\nsets for an AI model?\nYes\nReliability &\nSafety\nAI Ethics Principles\n43\nAI System Lifecycle Phase 3: Build & Validate\nPhase\nBV.1\nQuestion Has the behavior of the system been tested against unexpected\nsituations and environments? Is there a defined fallback plan if the\nAI model encounters adversarial attacks or other unexpected\nsituations? Have the fallback plans been tested and confirmed?\nBinding for\nThird-party Yes\nPrinciples\nReliability &\nSafety\nBV.2\nAre there defined processes that outline procedures to describe\nactions to be taken when an AI system fails in different contexts?\nHave the processes been tested?\nYes\nReliability &\nSafety\nBV.3\nAre there defined processes that outline procedures to describe\nwhen an AI system fails in different contexts? Have the processes\nbeen tested?\nYes\nReliability &\nSafety\nBV.4\nIs there an established mechanism of communication to assure\nthe end-users of the system\u2019s reliability?\nYes\nReliability &\nSafety\nBV.5\nAre there clear and understandable definitions explaining why the\noutcomes of the AI system took a certain decision?\nNo\nTransparency\n& Explainability\nBV.6\nHas the model been built in a simple and interpretable manner?\nNo\nTransparency\n& Explainability\nBV.7\nHas an examination of the AI model's interpretability been\nsuccessfully completed after the model's training?\nNo\nTransparency\n& Explainability\nBV.8\nHas there been a research exercise done relating to the use of\navailable technical tools to be able to improve the understanding of\nthe data, model, and its performance?\nNo\nFairness\nBV.9\nAre there established processes and quantitative analysis to test and\nmonitor for potential biases and the overall fairness of the system\nduring the development of the system? Are there mechanisms in place\nto protect any individuals or groups who might be disproportionately\naffected by negative implications?\nNo\nFairness\nBV.10\nAre there any established mechanisms that assess whether the AI\nsystem encourages humans to develop attachment and empathy\ntowards the system? Are there mechanisms that ensure that the AI\nsystems' social interaction is simulated and that it has no capacity\nfor \"feelings\"?\nNo\nSocial &\nEnvironmental\nBenefits\nBV.11\nHave the stakeholders approved the successful tests and\nvalidated rounds of user acceptance testing prior to the\nproduction of AI models?\nYes\nAccountability &\nResponsibility\nBV.12\nDid you use any sensitive data/ attributes in the model? If so,\njustify the use of sensitive personal data attributes or their proxies?\nYes\nFairness\nBV.13\nAre there AI approaches and algorithms that allow and facilitate\ndecision-making alignment with human rights and KSA\u2019s\ncultural values?\nYes\nHumanity\nAI Ethics Principles\n44\nAI System Lifecycle Phase 4: Deploy & Monitor\nPhase\nDM.1\nQuestion In case of a chatbot or other communication systems, are the end-users\naware that they are interacting with a non-human correspondent?\nBinding for\nThird-party Yes\nPrinciples\nAccountability &\nResponsibility\nDM.2\nHas the team assessed the AI system's vulnerabilities to potential\nattacks, revelation of sensitive data, or breaking the confidentiality?\nYes\nPrivacy &\nSecurity\nDM.3\nAre there mechanisms to measure if the system is producing an\nunacceptable amount of inaccurate predictions?\nNo\nAccountability &\nResponsibility\nDM.4\nIs there a set strategy in place to monitor and measure if the AI\nsystem is meeting the goals, purposes, and intended applications?\nNo\nReliability &\nSafety\nDM.5\nAre the persons who are accessing the data qualified with the\nnecessary competences to understand the details of data\nprotection requirements?\nNo\nPrivacy &\nSecurity\nDM.6\nAre there mechanisms in place to assess the level of influence the\nAI system may have on end-users\u2019 decision-making?\nNo\nTransparency\n& Explainability\nDM.7\nIs there a process set in place, which is clear and explainable, to\ninform end-users of the reasons, criteria, and benefits behind the\noutcomes and results of the AI system? Are there clear steps of\ncommunication on how and to whom issues can be raised?\nNo\nTransparency\n& Explainability\nDM.8\nIs there a process set in place to collect and consider the end-users'\nfeedback and adopt it into to the system?\nYes\nTransparency\n& Explainability\nDM.9\nAre there established processes and quantitative analysis to monitor\nbiases and the overall fairness of the system during the deployment\nof the system?\nYes\nFairness\nDM.10\nIn the case of variability, did you establish a measurement or\nassessment mechanism of the potential impact of such variability\non fundamental rights?\nNo\nFairness\nDM.11\nAre there established mechanisms to ensure fairness in your\nAI systems?\nNo\nFairness\nDM.12\nIs the information about the AI system accessible to end-users of\nassistive technologies?\nNo\nFairness\nDM.13\nAre there established mechanisms to measure the social and\nenvironmental impact of the AI system\u2019s deployment and use?\nYes\nSocial &\nEnvironmental\nBenefits\nDM.14\nAre there established mechanisms to ensure the application of\nfundamental human rights?\nYes\nAccountability &\nResponsibility\nDM.15\nAre there established processes for third parties or workers to\nreport potential vulnerabilities, risks, or biases in the AI system?\nYes\nAccountability &\nResponsibility\nAI Ethics Principles\n45\nPhase\nQuestion Binding for\nThird-party DM.16\nAre there established mechanisms to demonstrate your compliance\nwith the principles set out in this document?\nNo\nPrinciples\nAccountability &\nResponsibility\nDM.17\nAre there established mechanisms that allow for redress in case\nof the occurrence of any harm or adverse impact?\nYes\nAccountability &\nResponsibility\nDM.18\nAre there established mechanisms that provide information to\nend-users/third parties about opportunities for redress?\nYes\nAccountability &\nResponsibility\nDM.19\nAre there continuous monitoring techniques to ensure that the AI\nsystem maintains privacy and security?\nYes\nPrivacy &\nSecurity\nDM.20\nAre there periodic assessments of the deployed artificial intelligence\nsystems to ensure the implementation of fundamental human rights\nand cultural values of the Kingdom?\nYes\nHumanity\nAI Ethics Principles\n46\nSDAIA.GOV.SA SDAIA_SA SDAIA.SAUDI SDAIA-KSA ", "metadata": {"country": "Saudi Arabia", "year": "2023", "legally_binding": "no", "binding_proof": "None", "date": "09-01", "regulator": "Saudi Data and Artificial Intelligence Authority (SDAIA)", "type": "non-binding guidelines", "status": "active", "language": "English, Arabic", "use_cases": "[UC3 - Risk, UC5 - Public, UC6 - Policy]"}}
{"_id": "686b66aa6e4e9653b2a68b1e", "title": "AI Adoption Framework", "source": "https://sdaia.gov.sa/en/SDAIA/about/Files/AIAdoptionFramework.pdf", "text": "Saudi Data & AI Authority\nAI Adoption Framework\nSeptember 2024\nContent\n2\n1. Introduction\nEstablishment\nActivation and Adoption of AI\nAI Enablers\nCommunication and Change Management\nMonitoring and Continuous Improvement\nTargeted Audience\nDefining Direction and Priorities\nEstablishing AI Unit\nAssessing Maturity and Readiness\nData\nTechnology\nHuman Capabilities\nAI Responsible Use\n3. Definitions\n2. AI Adoption Framework\n.............................................................................................................................................................................. 3\n............................................................................................................................................................................... 5\n.............................................................................................................................................................................................. 9\n......................................................................................................................... 11\n........................................................................................................................... 12\n................................................................................................................................................................................. 9\n................................................................................................................................................................. 10\n................................................................................................................................................................. 10\n......................................................................................................................................... 5\n.................................................................................................................................................................. 3\n.................................................................................................................................................................................... 9\n................................................................................................................................................................................. 13\n.................................................................................................................................... 6\n................................................................................................................................................................. 5\n.................................................................................................................................................. 4\n................................................................................................................................................... 8\n2\nIntroduction\nThe rapid advancement in artificial intelligence (AI) technologies has transformed the\nbusiness landscape across various sectors globally. These technologies have played a vital\nrole in enhancing economic performance, improving the quality of services, and offering\ninnovative solutions to current challenges, all while increasing business efficiency and\nproductivity. This has driven many countries to compete in achieving the necessary technical\nmaturity for these technologies and adopt them in a structured and responsible manner, with\nthe aim of leading the AI field both nationally and globally.\nRecognizing the importance of AI and its role in realizing Vision 2030, the Kingdom of Saudi\nArabia is making diligent efforts to implement its strategy that seeks to integrate AI across all\nsectors and direct efforts in a comprehensive and well-considered manner. It also aims to\npromote the ethical and responsible use of this technology.\nIn this context, the Saudi Data & AI Authority (SDAIA) acknowledges the importance of taking\nproactive steps to promote the adoption of AI across the Kingdom. SDAIA offers this document\nas a guiding framework that provides a comprehensive roadmap for the adoption of AI in all\nsectors. This framework represents a strategic step toward building a knowledge-based\nsociety founded on innovation and continuous development. Its goal is to provide necessary\nguidance and instructions, outline critical steps and procedures, and align with best practices\nto ensure optimal and responsible AI adoption, thus achieving successful milestones in the\ntransformation toward AI within the ecosystem.\nIn reference to the regulatory arrangements for the Authority (SDAIA) issued by the Council of\nMinisters Resolution No. (292) dated 1441/4/27 AH, which stipulates in Paragraph (2) of\nArticle (3) that the Authority (SDAIA) is the competent entity in the Kingdom for data (including\nbig data) and AI, and the national reference in all matters related to their regulation,\ndevelopment, and management.\n3\nAI Adoption Framework\nActivation and Adoption of AI\nAI Enablers\nEstablishment\nTargeted Audience\nLeaders and Officials\nSpecialists Responsible\nfor or Involved in the AI\nTransformation\nExecutives and\nBusiness Unit\nManagers\nAssessment and\nShortlisting the AI\nUse Cases\nDefining the AI\nPrioity Use\nCases\nExecute AI\nInitiatives and\nProjects\nMonitoring the\nDeployment of AI\nProducts Defining Direction\nand Priorities\nEstablishment of\nAI Unit\nAssessing Maturity and Readiness\nData Technology Human Capabilities AI Responsible Use\nCommunication and Change Management\nMonitoring and Continuous Improvement\n4\nEstablishment\nA series of meetings with officials is to be held to assess the current situation and explain\nthe role and importance of the AI Unit. During these meetings, the comprehensive vision and\nstrategic goals will be clarified, and the key enablers of AI technologies will be identified.\nThe meetings will also focus on pinpointing the current challenges and needs within the\nwork environment that AI technologies can effectively address. Identifying potential AI use\ncases based on actual business needs will help clarify the overall objectives for adopting AI\ntechnologies, such as enhancing operational efficiency and improving the quality of services.\nThese meetings will include open discussions on how the entity can benefit from AI\ncapabilities while considering current challenges and future needs. The exchange of ideas\nand sharing of perspectives will contribute to shaping the directions and priorities of the AI\nUnit in alignment with the organization\u2019s short- and long-term goals.\nDefining the overall objective of the unit and the services it will provide, as well as identifying\nthe key objectives and tasks that the AI Unit will undertake, along with the roles,\nresponsibilities, job titles, the expected number of employees, and the unit's interaction model\nwith related sectors. The following proposal can be utilized to assist entities in establishing\nand activating the AI Unit.\n1 Defining Direction and Priorities\n2 Establishment of AI Unit\n5\nIdentifying opportunities for applying AI in various departments within the organization and\nworking closely with these departments to understand their specific needs and objectives.\nThis includes designing customized and innovative AI solutions, proposing updates to internal\npolicies and procedures as needed, and then monitoring the performance of AI project\ndevelopment and implementation, including machine learning applications, data analysis, and\nautomation. Additionally, it involves evaluating their impact and ensuring they achieve\nstrategic goals and responsible use.\nYou can refer to the National Framework for Professional Standards, which outlines the\nprofessional standards, activities, skills, and competencies for the proposed jobs in data and\nAI.\nMeasuring the organization's maturity and assessing its ability to transition to AI, integrate,\nand use this technology within the ecosystem, thereby helping the organization understand its\nexact position before starting its transformation journey. This contributes to identifying the\nnext steps to achieve the desired goals. Readiness is measured based on five key aspects:\n2.1 Key Tasks\n2.2 Roles and Responsibilities\n3 Assessing Maturity and Readiness\n6\nMeasuring the leadership's overall understanding of the importance of AI, its potential\nimpact on business, and the existence of a plan to leverage it.\nKey Priorities/Institutional Readiness\nEvaluating the availability and readiness of data, with a commitment to relevant regulations\nand policies.\nData\nMeasuring the availability of the necessary expertise to adopt AI, along with a plan to\ndevelop skills in AI fields.\nHuman Capabilities\nAssessing the readiness of the infrastructure for AI technologies and the availability of tools\nnecessary to process AI models (e.g., GPU, TPU), including feasibility and scalability for\ncloud-based AI products.\nTechnology and Infrastructure\nEvaluating the current adherence to regulations and policies and the maturity of internal\nprocesses that govern the use of AI technologies and applications.\nAI Governance\n7\nAI Maturity Levels\nProficient Advanced\nAn advanced entity in the field of AI, often\nfollowing best practices and contributing\nto AI development.\nA proficient entity with skills in most\naspects of AI and working to adopt best\npractices.\nEmerging Developing\nA developing entity that recognizes areas\nfor growth and is planning to implement\ninitiatives focused on effectively adopting\nAI.\nAn emerging entity facing challenges in\nmultiple key areas and needing to work\non various aspects to develop and\nimprove its readiness to adopt AI.\nActivation and Adoption of AI\nEntities can adopt a flexible approach that begins with defining a clear vision for data and AI\ntechnologies while identifying a set of specific use cases that support their core operations.\nThis is done by evaluating the current environment and infrastructure to help develop a\nroadmap for gradually implementing these use cases based on their feasibility, rather than\nwaiting for the environment and infrastructure to be fully prepared.\nAdopting a use case-driven methodology in services enables the achievement of incremental\nvalue, maintains the support of key stakeholders, and enhances confidence in AI adoption\nprograms.\n8\nAI Enablers\nData is the main input for AI models, and the data used to create AI models differs in format\nfrom that used in operational programs or data analysis. Therefore, data is one of the most\nimportant assets that contribute to improving performance and productivity, facilitating\nservice delivery, and enhancing its value in strategic decision-making and future forecasting.\nData plays a pivotal role in AI technologies and is the fundamental element upon which all AI\napplications and technologies are built, such as model training, pattern recognition, and future\nprediction. Organizations collect and process vast amounts of data, which can be leveraged to\nenhance AI innovations.\nSaudi Arabia possesses a robust digital infrastructure that helps accelerate digital\ntransformation. The Kingdom has achieved a ranking that places it among the top countries in\ndigital transactions. The infrastructure is considered one of the key enablers necessary for\nbuilding a successful and effective ecosystem that supports innovation and research in the\nfield of AI. Organizations have adopted AI technologies and enhanced their infrastructure to\ninclude computing platforms that accelerate the training process and the implementation of\nAI applications.\nIt is worth noting that developing AI applications that involve large and complex models has\nbecome easier thanks to the use of smaller algorithms capable of performing complex tasks.\nPreviously, this was limited to large algorithms with high computational demands, which\nwere more complex and required significant computing power. It is now easier to train and\ndeploy AI models on affordable compute power\nAdditionally, organizations can adopt open-source AI algorithms with commercial licenses,\nsuch as the MIT License and Apache License, while adhering to the restrictions and conditions\nassociated with these licenses when making improvements and modifications. These\nalgorithms are characterized by their ability to be continuously modified and improved by the\nglobal community of programmers and AI scientists. The open nature of these algorithms\nensures that they remain up-to-date and effective, providing organizations with access to the\nlatest innovations in this field.\n1 Data\n2 Technology\n9\nDeveloping and enhancing internal capabilities within the organization is one of the key\nenablers of AI. Specialists oversee the development, implementation, and use of AI, in addition\nto studying the options of contracting or internally building systems. They also monitor the\nethical and responsible use of AI, supervise service improvements, and highlight the\nimportance of continuous development in data, technology, and ongoing training to support\nthe organization's goals.\nAI Responsible Use\nIn 2023, the Saudi Data & AI Authority (SDAIA) issued the AI Ethics Principles aimed at guiding\norganizations in the responsible use of these technologies.\nAI developers are obligated to take the necessary steps to prevent bias and discrimination in\ndata, algorithms, and outcomes, ensuring equality and fairness.\nPrinciples:\n1 Integrity and Fairness\nAI systems must be securely protected in a way that respects data privacy and complies with\ncybersecurity standards to prevent unauthorized access and potential harm.\n2 Privacy and Security\nAI systems should be reliable and safe and should operate according to the specifications\ndesigned to achieve the desired outcomes.\n3 Reliability and Safety\nAI systems should be transparent and interpretable to build trust, with clear tracking of\ndecision-making stages and justifying practices and ethics.\n4 Transparency and Interpretability\nDesigners, developers, and implementers of AI systems should be identified and reachable,\nwith appropriate mechanisms in place to avoid harm and misuse of the technology. AI\nsystems should not deceive individuals or unjustifiably infringe on their freedom.\n5 Accountability and Responsibility\n3 Human Capabilities\n10\nAI systems must be designed to serve humanity.\n6 Humanity\nAI systems should enhance sustainable social and environmental benefits for individuals and\nsociety, contributing to technical, social, and environmental progress while addressing\nrelevant challenges.\n7 Social and Environmental Benefits\nCommunication and Change Management\nRaising awareness about AI and its importance is a crucial step to ensure the effective\nacceptance and application of this technology. Below are the details of the communication and\noutreach plan activities designed to increase awareness:\nHolding interactive sessions and\nworkshops to explain AI concepts and\ntheir impact on work. These workshops\ncan be tailored to suit different\ndepartments and job levels, ensuring a\ncomprehensive understanding of the\nsubject.\nAwareness Sessions and\nDedicated Workshops\nInternal awareness campaigns that\ninclude the vision, objectives, and\nstrategies related to the implementation\nof AI, raising awareness about AI\ninitiatives, while highlighting its benefits,\nimportance, and related changes.\nInternal Campaigns\nProducing short and engaging videos that\nexplain AI concepts in simple language,\nfeaturing testimonials from employees\nwho have successfully used this\ntechnology.\nEducational Videos\nPublishing awareness articles on the\ninternal portal that explain successful AI\nuse cases, best practices, and\nachievements within the organization.\nAwareness Articles\n11\nChange Management\nDeveloping a comprehensive change management plan for the AI Unit, focusing on cultural\nand organizational impact to ensure the success of AI initiatives. The change management\nplan aims to facilitate the transition towards AI adoption, address any concerns or resistance\nto change, and encourage collaboration from all employees.\nMonitoring and Continuous Improvement\nContinuous evaluation of the implementation and impact of AI use cases against objectives is\none of the key factors for the success of AI transformation and ensuring ongoing alignment\nwith and contribution to the organization's overall strategic goals. The most important\nmonitoring methods include:\nIdentifying lessons learned and\nareas for improvement during the\nimplementation of various\ninitiatives, projects, and programs,\nand applying these improvements.\nContinuous Improvement\nPreparing periodic reports on the\nprogress in achieving key performance\nindicators based on the organization's\napproved performance management\nframework. This includes, for example,\nthe percentage of AI use case\nimplementation and the level of\n.readiness\nPreparing KPI reports for the\nAI Unit\n12\nDefinitions\n13\nA field of computer science focused on building systems capable of performing tasks that\ntypically require human intelligence, such as learning, reasoning, and self-development. It is\nalso referred to as \"machine intelligence.\"\n1 Artificial Intelligence (AI)\nA subfield of AI that focuses on learning patterns from available data to make predictions or\ndecisions based on new data without explicit programming.\n2 Machine Learning (ML)\nPractices that combine machine learning, DevOps, and data engineering to deploy and\nmaintain machine learning models in production reliably and efficiently.\n3 Machine Learning Operations (MLOps)\nA type of AI algorithm designed to perform specific, complex tasks using fewer computational\nresources compared to large algorithms.\n4 Small Algorithms\nA set of values, principles, and approaches guiding ethical behavior in the development and\nuse of AI technologies.\n5 AI Ethics\n13\nSDAIA.GOV.SA SDAIA_SA SDAIA.SAUDI SDAIA-KSA ", "metadata": {"country": "Saudi Arabia", "year": "2024", "legally_binding": "no", "binding_proof": "None", "date": "09-01", "regulator": "Saudi Data and Artificial Intelligence Authority (SDAIA)", "type": "non-binding guidelines", "status": "active", "language": "English, Arabic", "use_cases": "[UC2 - Operational, UC3 - Risk, UC6 - Policy]"}}
{"_id": "686b670c6e4e9653b2a68b1f", "title": "Generative AI Guideline for Government", "source": "https://sdaia.gov.sa/en/SDAIA/about/Files/GenAIGuidelinesForGovernmentENCompressed.pdf", "text": "Version 1\nJanuary 2024\nContents\nIntroduction 1\n\u0019\u0018 Definitions 2\n$\u0018 The Scope 3\n6\u0018 Rules Governing the Use of Generative Artificial Intelligence Tools 3\n3.1 - Fairness 4\n3.2 - Reliability and Safety 5\n3.3 - Transparency and Interpretability 6\n3.4 - Accountability and Responsibility 7\n3.5 - Privacy and Security 8\n3.6 - Humanity 10\n3.7 - Social and Environmental Benefits 10\n\u00a2\u0018 Roles and Responsibilities within the Entity 11\nAppendix 1 - Guidelines for the Use of Generative Artificial\nIntelligence in Government Entities 12\nHow can government entities benefit from GenAI? 12\nPotential Challenges or Risks Associated with the Use of\nGenAI and Challenges 15\nPractical Guidelines - with Examples 17\nAppendix 2 - Checklist for Utilizing Generative Artificial Intelligence\nTools 23\nIntroduction\nThe field of artificial intelligence has witnessed rapid development in generative\nartificial intelligence services and tools. It is extensively utilized by governments,\ncompanies, and individuals worldwide. While this technology offers numerous\nadvantages to its users, entities, and employees must be aware of the associated\nrisks and limitations. They should apply critical thinking methodologies and\nexercise caution when utilizing the outputs of generative artificial intelligence or\nwhen providing information in the form of commands to generative artificial\nintelligence. Accordingly, government employees should use generative artificial\nintelligence only in cases where they can adequately identify risks and avoid them\nor mitigate their severity.\n\n\nThis document provides regulatory guidelines for government employees\nregarding the use and processing of government data in GenAI tools and includes\nexamples based on common scenarios that entities may address. It also highlights\nthe challenges and considerations associated with the use of generative artificial\nintelligence, proposes principles for responsible use, and presents recommended\npractices. The Saudi Data & AI Authority (SDAIA) recognizes the need to keep pace\nwith regulatory and technological changes, given the evolving nature of generative\nartificial intelligence. Therefore, this document may be updated as needed to keep\npace with any changes.\n\n\nThis document aligns with and supports compliance with current regulations and\npolicies in the Kingdom of Saudi Arabia, including regulations and policies related\nto data governance, privacy, security, intellectual property, and human rights. \n1\n\t\b Definitions\nFor the purposes of this document, the following terms and expressions, wherever\nthey appear in this document, shall have the meanings indicated next to each of\nthem unless the context requires otherwise:\nGenerative Artificial Intelligence\nThe generative model is a machine learning model that can create new examples\nsimilar to the training dataset. This model is also a sub-part of artificial\nintelligence that can create new content (including text, images, sounds, icons,\nvideos, etc.) and works by interpreting commands given by users. Generative\nartificial intelligence can perform tasks that require human cognitive abilities,\nincluding responding to and formulating verbal or written commands, \u201clearning,\u201d\nand \u201cproblem-solving.\u201d\nGovernment Data\nGovernment data encompasses all data, irrespective of its form, source, or nature,\nthat is generated or acquired by government entities during the course of their\nofficial duties. It also includes data collected by entities responsible for\ninfrastructure, involved in the management, operation, or maintenance of public\nfacilities or national infrastructure, or engaged in public services related to the\noversight of such facilities or infrastructure.\nGovernment Data Users\nAny person who works on issuing, preparing, dealing with, processing, or storing\ngovernment data. This also includes employees of government entities,\ncontractors, and others.\n2\n\t\b The Scope\nThis document was created with the objective of promoting the responsible and\neffective utilization of GenAI tools and services in government transactions. It\nemphasizes the proper use and processing of government data in adherence to\nrelevant laws and regulations in the Kingdom of Saudi Arabia.\n;\b Generative Artificial Intelligence Guidelines\nGovernment entities are required to adhere to the AI Ethics Principles when\nengaging with generative artificial intelligence tools across all stages of the tools'\nlife cycle. This involves maximizing their benefits while mitigating associated risks.\nAchieving this goal necessitates the formulation of tool-specific policies, ethical\nstandards, and professional responsibilities. Government entities must also refrain\nfrom using generative artificial intelligence in critical decision-making processes\nthat affect individuals or the vital interests of the Kingdom. All results should be\nhuman-reviewed to ensure the integrity of the GenAI output, prevent harm, as well\nas strictly adhere to accuracy and eliminate bias and discrimination within the\nalgorithms. This approach ensures the reliability of the results and eliminates any\nnegative consequences. \n\n\n\nThe AI Ethics Principles, as outlined by SDAIA, are meant to be applied to\nindividuals across diverse sectors in the Kingdom of Saudi Arabia. This includes,\nbut is not limited to, workers in public, private, and non-profit entities, researchers,\nemployees in both public and private sectors, and consumers. Notably, these\nprinciples are intended to be universally applicable, encompassing all artificial\nintelligence tools used, and are not exclusive to generative artificial intelligence.\n\n\nBelow is a breakdown of the mandatory regulations governing the use of\ngovernment data in generative artificial intelligence tools: \n3\n3.1 - Fairness\nIt is crucial to implement measures to identify groups affected by the system and\nto prevent or minimize biases, discrimination, or profiling that individuals or\ngroups may encounter due to data or algorithms. This is essential to avoid\nnegative discrimination against specific groups. Generative artificial intelligence\ntools possess the capability to generate content that may exhibit discriminatory\ntendencies, lack representation, or contain biases and stereotypes. These biases\ncan be diverse and linked to various intersecting identity factors, such as gender\nand race. A number of models are trained using vast datasets from the Internet,\nwhich is often the source of these biases.\n\n\n.\u001c Carefully evaluate the created content to ensure alignment with the Kingdom of\nSaudi Arabia\u2019s regulations, values, and ethics. This evaluation must also\ninclude comprehensive and careful scrutiny of biases or stereotypical\nassociations.1\n*\u001c Learn to write commands (for GenAI tools) in ways that ensure the creation of\ncorrect and free of biased content.1\n:\u001c Make efforts to obtain a comprehensive understanding of the data used to train\nthe tool, including knowing the source of the data, its contents, and how it is\nselected and prepared.1\n\u001c Enhance users\u2019 understanding and awareness of bias, the importance of\ndiversity and inclusion, anti-racism, values and ethics, as this knowledge will\ncontribute to improving their ability to identify biased content.\nUsers must: C\n4\n5\n3.2 - Reliability and Safety\nA generative artificial intelligence system must ensure adherence to\nspecifications, operating consistently as intended and expected by its designers\nthroughout all stages of its life. Reliability serves as a measure of the credibility\nand dependability of the system in carrying out its specific functions and achieving\nintended results. Generative artificial intelligence tools are often susceptible to\ninaccuracies, as they learn from internet data containing errors and outdated\ninformation. While content generated with generative artificial intelligence may\ninitially seem consistent, it frequently lacks the logical consistency or context\nnecessary. Recognizing both strengths and limitations is crucial in the responsible\nuse of generative artificial intelligence within government entities. Ensuring quality\nnecessitates a multi-faceted approach encompassing monitoring, human\nsupervision, and user education to effectively harness the capabilities of\ngenerative artificial intelligence and reduce associated risks.\n\n\n2\u0018 Identify content generated with generative artificial intelligence to ensure that\nusers are prepared to address potential reliability issues and have the ability to\nverify content using other sources\u0018\n8\u0018 Proofread content for factual accuracy and relevance to context to prevent the\nspread of misinformation\u0018\n,\u0018 Seek to understand the quality and sources of training data used by a\ngenerative artificial intelligence system to enhance content reliability\u0018\n\u0012\u0018 Encourage supplementing content created with generative artificial intelligence\nwith information from trusted sources to ensure accuracy.\nUsers must: C\n6\n3.3 - Transparency and Interpretability\nIt is crucial to construct generative artificial intelligence tools with a high degree of\nclarity and interpretability. These tools should incorporate features that allow for\nthe tracking of automated decision-making stages, ensuring transparency and\nexplainability to both those directly and indirectly affected by them.\n\nTransparency is an important consideration when building trust in interactions\nbetween humans and generative artificial intelligence in government entities, and\nalerts help demystify content generated by generative artificial intelligence,\nhelping users distinguish between automated and human responses.\n\n\n+\u001d Communicate clearly and meaningfully when using generative artificial\nintelligence to interact with the public\u001d\n#\u001d Notify beneficiaries when messages or content are created with generative\nartificial intelligence\u001d\n8\u001d Provide alternative, non-automated communication channels for beneficiaries\nwho prefer human interactions\u001d\n\u0014\u001d Use watermarks to help beneficiaries identify content created with generative\nartificial intelligence..\nUsers must: B\n7\n3.4 - Accountability and Responsibility\nDesigners, developers, users, and evaluators of generative artificial intelligence\ntools bear ethical responsibility for decisions and actions that may lead to potential\nrisks and negative impacts on individuals and communities.\n\nAdopting a generative artificial intelligence system may carry legal and ethical\nimplications, requiring comprehensive consideration including, but not limited to,\nthe risk of infringement of intellectual property rights, concerns about data\nprotection, and the potential for human rights violations.\n\n\n0\u001b Consult with relevant legal professionals to assess the risks associated with\nthe use of generative artificial intelligence tools and find a way to avoid or\nreduce these risks\u001b\n5\u001b Comply with relevant legislation, including the Personal Data Protection Law,\nand protect user rights\u001b\n+\u001b Ensure that content created with generative artificial intelligence tools\nrespects intellectual property rights and adheres to copyright legislation.\nUsers must: B\n8\n3.5 - Privacy and Security\nIt is important to develop and use generative artificial intelligence tools in a safe\nmanner, taking into account the relevant regulatory requirements, including the\nregulatory requirements related to the protection of personal data subjects, the\nrelevant cybersecurity standards issued by the National Cybersecurity Authority,\nand the Data Classification Policy. With the aim of preventing illegal access to data\nand the system, which may lead to damage to reputation or psychological,\nfinancial, or professional harm, generative artificial intelligence tools are designed\nusing mechanisms and controls that enable managing and monitoring the results\nthroughout the system\u2019s life cycle. \n\n\n& Employees in government entities must refrain from entering any data\nclassified as restricted or higher (restricted, secret, and top secret)\n7 Implement strict data protection measures in accordance with the provisions of\nthe Personal Data Protection Law\n9 The use of generative artificial intelligence tools should be limited to data\nclassified as \u201cpublic\u201d data\n\u0015 You should check the privacy policies of these tools, including their disclosure\nand sharing terms\n$ The risks arising from the use of tools must be taken into account, including\ndata leakage, misinformation, deepfake, bias, and decisions affecting the safety\nof individuals. It is crucial not to enter any data or information that is not\nproperly classified. Assessing the risks involves estimating and addressing the\npotential risks resulting from the development or use of generative artificial\nintelligence tools. Subsequently, the risk levels in the tools are classified at any\nof the following levels in accordance with the AI Ethics Principles issued by\nSDAIA\n* The risks resulting from the use of generative artificial intelligence tools must\nbe assessed in accordance with the AI Ethics Principles:\nUsers must: N\n9\n\u0017 Little or no risk:\n\u0017 Limited risk:\n\u0017 High risk:\n\u0017 Unacceptable risk:\n There are no restrictions on AI systems that pose little or no\nrisk, such as spam filters but, it is recommended that these systems be\nethically compliant. +\n AI systems that pose limited risks, such as technical programs\nrelated to function, development, and performance, are subject to the\napplication of the AI Ethics principles mentioned in this document. +\nAI systems that pose \u201chigh risks\u201d to basic rights must undergo preand post-conformity assessments, and in addition to adhering to ethics, the\nrelevant statuary requirements must be considered.+\n AI systems that pose an 'unacceptable risk' to people\u2019s\nsafety, livelihoods, and rights, such as those related to social profiling,\nexploitation of children, or distorted behavior, that are likely to occur are not\nallowed. \n10\n3.6 - Humanity\nIt is essential to build and use GenAI tools according to a fair and ethical\nmethodology grounded in human rights and fundamental cultural values. This\napproach aims to create a positive impact on the involved parties and local\ncommunities, contributing to the realization of both short and long-term goals for\nthe benefit and prosperity of humanity. It is necessary to design and use GenAI\ntools in a manner that avoids deception, manipulation, or the establishment of\nbehavior contrary to the empowerment, enhancement, or augmentation of human\nskills. Instead, the design and use approach should be centered on providing\nchoices and decision-making capabilities for the benefit of humanity.\n3.7 - Social and Environmental Benefits\nPromote the positive and beneficial impact of social and environmental priorities\nthat are designed to benefit individuals and society as a whole, which focus on\nsustainable goals and objectives. GenAI tools should not cause or accelerate harm\nor negatively impact humans but rather should contribute to enabling and\ncomplementing technological, social, and environmental advancement while\nseeking to address the challenges associated with it.\n11\n\f\u0013 Roles and Responsibilities within the Entity\n This section defines roles and responsibilities to ensure that generative artificial\nintelligence tools are used responsibly in the Kingdom of Saudi Arabia.\n4.1 - Data Management Office\nThe Data Management Office plays a pivotal role in ensuring the ethical and legal\nuse of GenAI tools. It is responsible for approving the tools that are used after\nreviewing the privacy, disclosure, and data-sharing policies to ensure their\ncompliance with the relevant regulations and policies of the Kingdom of Saudi\nArabia. Additionally, it is responsible for ensuring robust data management\npractices, including obtaining, processing, and maintaining data. This involves\nverifying the legitimacy of data sources and obtaining necessary approvals. The\nData Management Office must conduct audits to evaluate the quality of generative\nartificial intelligence training data and identify any potential bias. In addition, the\nData Management Office has been tasked with liaising with legal experts to address\ndata privacy concerns and ensure compliance with relevant legislation, such as the\nPersonal Data Protection Law and policies issued by the Saudi Data & AI Authority.\n4.2 - Users\nUsers of generative artificial intelligence tools in government entities must apply a\ncritical thinking methodology and exercise caution when dealing with content\ncreated using these tools. Despite their value, such content is not inherently reliable\nand should be supplemented with information from trustworthy sources. It is the\nusers' responsibility to inform the government entity about any errors, biases, or\ninappropriate content generated with generative artificial intelligence tools.\nFurthermore, staying informed about the latest developments in the capabilities and\nlimitations of generative artificial intelligence tools is crucial for making informed\njudgments. When utilizing content created with generative artificial intelligence, it is\nimperative to ensure that relevant individuals are informed, and watermarks may be\nused when necessary to indicate that. \n12\n4.3 - Government Entities\nIt is the responsibility of government entities in the Kingdom of Saudi Arabia to\nestablish clear policies and guidelines for the responsible use of generative artificial\nintelligence. This involves consulting experts to assess legal risks and ensure\ncompliance with all applicable laws and regulations. Government entities must\nprioritize transparency by disclosing content created with generative artificial\nintelligence. They should provide users with easy methods to identify content\ngenerated with this technology and offer alternatives to non-automated\ncommunication when users prefer human interactions.\n\nAdditionally, government entities must actively promote educational initiatives to\nenhance users' understanding of generative artificial intelligence. This includes\nimparting knowledge on how to critically evaluate content created with these tools\nand how to use these tools responsibly. \nAppendix 1 - Guidelines for the Use of Generative\nArtificial Intelligence in Government Entities\nHow can government entities benefit from GenAI?\nGenerative artificial intelligence offers a range of potential benefits to government\nentities that can be leveraged. However, it is crucial for these entities to ensure that\nthey do not violate the guidelines mentioned in this document.\nox Enhancing Operational Efficiency\nGenerative artificial intelligence can help government employees complete time-consuming\ntasks more efficiently, enabling them to redirect their focus toward more complex or strategic\nactivities.\n\n\nExample: The department manager can request generative artificial intelligence to draft job\ndescriptions for newly created department functions and subsequently edit them, rather than\ncreating the descriptions from scratch.\n13\n\u0013\u0012 Make Informed Decisions\nGenerative artificial intelligence services can assist users in making informed decisions by\nprocessing large amounts of previous data, creating reports, and providing an overview of\nimportant information.\n\n\nExample: In the event of a new disease spreading, generative artificial intelligence can swiftly\noffer an overview of the most effective measures taken by authorities in different countries\npreviously, which will provide support to guide Saudi Arabia's response.\nL\u0012 Quality of Service and Citizen Support\nCitizens' experiences with government services can be enriched by deploying chatbots, for\nexample, generative artificial intelligence can be used to simplify user inquiries and support, as\nwell as to create personalized content and services to meet individual needs.\n\n\nExample: A government entity can use a chatbot to provide a detailed explanation to users on\nhow to apply for a caf\u00e9 license, enabling the user to avoid spending time and effort researching\nthe required legislation.\n\u0082\u0012 Accelerate the Research and Analysis Process\nThe research process across sectors, including healthcare, economics, and environmental\nstudies, can be accelerated through generative artificial intelligence, as it can help analyze data,\ncreate summaries and reports, and streamline searches.\n\n\nExample: Public Relations employees can leverage generative artificial intelligence to generate\nsummaries of news from media outlets worldwide, to gain insights into how the world perceives\nSaudi Arabia.\n5.\tCrisis Response\nDuring times of crisis or emergency, generative artificial intelligence can process and distribute\nvital information and help employees allocate resources efficiently.\n\n\nExample: Generative artificial intelligence can process unstructured data from multiple sources\nsimultaneously, such as weather data, social media feeds, and emergency services reports, to\nsupport Crisis Control Center operations.\n14\n6.\tContent Creation\nGenerative artificial intelligence simplifies the complex task of content creation, as the\ntechnology can generate high-quality text, audio and video content, as well as reports and\nsummaries, reducing time and effort.\n\n\nExample: Marketing employees can use generative artificial intelligence to prepare weekly\nawareness posts about open data.\n;9 Translation\nGenerative artificial intelligence has the ability to overcome language barriers, facilitating\neffective communication between diverse groups of people. Unlike basic translation tools, which\noffer literal translations and often lack nuance, generative artificial intelligence employs\nadvanced natural language processing technologies. This enables it to generate more accurate\nand fluent translations that consider context and incorporate subtle linguistic details, such as\nidiomatic expressions. It also reflects differences across cultures, resulting in more accurate\nand \u201chuman-like\u201d output. Additionally, generative artificial intelligence can be leveraged to create\naudio and video content as well as translation, accurately conveying the tone of voice,\npersonality traits, and intent of the native speaker.\n\n\nExample: Generative artificial intelligence can automatically create audio and video guides for\nmuseums in multiple languages.\n15\nPotential Challenges or Risks Associated with the Use of GenAI\nGenerative artificial intelligence has the potential to transform government\noperations when adopted responsibly, empowering employees and delivering highquality services to citizens. While discovering the benefits of generative artificial\nintelligence is essential, it is crucial to confront challenges and risks to maximize\nthe benefits from the capabilities of such advanced technology. \n\n\n The complexities involved in the process of formulating policies,\nregulations, and international cooperation appropriate for the governance of\nartificial intelligence technologies, including generative artificial intelligence\ntools.3\n Financial constraints, employment disruptions, and income inequality\nresulting from the widespread adoption of artificial intelligence technologies.3\n Social issues, such as public perception, ethical concerns, and societal\nattitudes toward artificial intelligence, including generative artificial intelligence,\naffect the level of its acceptance and integration into various aspects of life.3\n These include infrastructure constraints, access to cutting-edge\nresearch and talent, and cybersecurity risks arising from the advancement and\nwidespread adoption of artificial intelligence and generative artificial\nintelligence.3\n Coping with the increased energy consumption and e-waste\ngeneration due to the computationally intensive requirements of artificial\nintelligence tools, including generative artificial intelligence tools.3\n It includes all laws and regulations in force in the Kingdom of Saudi\nArabia, including intellectual property rights, AI Ethics Principles, and data laws\nand policies, to ensure the responsible use of artificial intelligence technologies\nin general and generative artificial intelligence in particular.\nGeneral Challenges and Risks: 3\nP Political:\nP Economic:\nP Social:\nP Technology:\nP Environmental:\nP Legal:\n16\nCommon Risks and Challenges\nIn this section, we explore the challenges or risks that employees of government\nentities may face. Addressing these issues requires adopting a proactive approach,\nand the advantages of generative artificial intelligence can be harnessed to mitigate\nthe associated risks through a cautious approach and the adoption of preventive\nmeasures.\nData Leaks Cases\nGovernment employees may unintentionally share restricted information with these services,\nresulting in data leakage, as generative artificial intelligence services typically store userprovided data to expand their training dataset. These services also pose a risk of unintentional\ndisclosure of classified data to other users.\nMisinformation\nGenerative artificial intelligence can present misleading information, potentially leading users to\nbe tricked into providing partially or wholly incorrect information (often called a \u201challucination\u201d) \nas a result of the technology.\nDeepfake\nMisinformation and hallucinations are among the side effects of generative artificial intelligence,\nbut the most significant threat arises when scammers exploit the technology to generate hyperrealistic yet fake content, often called deepfake. Deepfake can be used to commit financial fraud\nor disseminate false yet realistic-looking footage intended to manipulate public opinion or\ntarnish individual or national reputation.\nPrejudice and Injustice\nGenerative artificial intelligence models trained on biased datasets may produce content that\nreflects those biases, potentially entrenching stereotypes and leading to discrimination against\nspecific individuals and communities.\nSafety Concerns\nSafety risks when using generative artificial intelligence may arise from relying on incorrect or\nmisleading content, leading to adverse consequences. For example: if content generated with\ngenerative artificial intelligence is used in critical decision-making processes without proper\nverification, it could pose significant safety risks. \n17\nPractical Guidelines - with Examples\nCreating Documents or Messages\nGenerative artificial intelligence can assist government employees in drafting\nmemorandums, letters, job descriptions, and other written materials.\n\n\nBest practices:>\n, Provide specific context in command format to receive more relevant and\naccurate responses.>\n, Edit and review created content, regardless of its source. The user is\nresponsible for the content used to communicate with the target audience and\nother stakeholders.>\n, Ensure compliance with the Data Classification Policy by excluding information\nclassified as restricted or higher from the command format and refrain from\nusing it to establish communications regarding sensitive topics.\n\n\nExample:\n\n\nGood command format example: Please write a job description for the Chief\nAccountant who will be responsible for auditing budgets.\n\n\nBad command format example: I work for a government entity and would like to\nhire 20 new accountants. Please write the job description for these accountants.\nKeep in mind that they will supervise the audit of budgets.\n\n18\nSimplifying Content\nGenerative artificial intelligence can assist in creating simple, straightforward\ncontent that is easy to understand. You can provide instructions in command\nformat regarding the target audience and desired reading level of the text. \n\n\nBest practices:4\n\u001b Specify if you have a particular audience in mind when writing command\nformat<\n\u001b Test different command format or request multiple versions of the same\nsentence until you achieve the desired result<\n\u001b Ensure compliance with the Data Classification Policy by excluding classified\ninformation from command format<\n\u001b Review the text to ensure it is conveyed into inclusive and respectful language.\nWhile models may suggest common linguistic patterns, it's important to be\naware that they may also exhibit biases (for example. exclude individuals based\non age, race, or gender).\n\n\nExample:\n\n\nGood command format example: Simplify agricultural incentive texts published\nusing clear language to be used in social media campaigns for farmers. The target\naudience is farmers who grow tomato crops within the Kingdom. The objective is\nto develop a simple awareness campaign on social media.\n\n\nBad command format example: Below is a plan outlining undisclosed agricultural\nincentives. Summarize the internally developed draft of incentives that we can\nutilize in external communications.\n19\nSummarizing Texts\nGenerative artificial intelligence excels at summarizing long texts into concise\nsummaries, making it a valuable tool for various tasks. For example, you can\nsummarize a 5-page report in one paragraph. \n\n\nBest practices:9\n\u001c Specify the desired format (for example: bulleted list or text))\n\u001c Thoroughly review the original document(s) to avoid missing or distorting any\nimportant information)\n\u001c Ensure compliance with the Data Classification Policy by excluding information\nclassified as restricted and higher from command format)\n\u001c Make sure to remove any sensitive data from the notes or entries.\n\n\nExample:\n\n\nGood command format example: Summarize the key findings and insights from\nthis published benchmark study on trends in renewable energy adoption. Focus on\nemerging technologies and potential growth areas, and present the summary in\nbullet point format for clarity.\n\n\nBad command format example: I aim to assess if the budget statement\nappropriately allocates investments to priority sectors. Please provide a summary\nand analysis of the main findings from the benchmark study conducted by a public\nentity on renewable energy adoption trends in the region, and identify the\ninvestment opportunities revealed in the budget corresponding to the identified\ntrends.\n\n20\nCreating Content in a Different Language\nGenerative artificial intelligence can assist in creating communications in\nlanguages other than Arabic, for example: you can use ChatGPT to translate this\ndocument into French or Spanish. \n\n\nBest practices:)\n\u0011 Try different languages: ChatGPT, Bard, and other generative artificial\nintelligence models may have been trained on texts focused on a specific\nlanguage or languages:\n\u0011 Ensure the accuracy of the content by performing a back translation on the\noutput of the translated text:\n\u0011 Ensure compliance with the Data Classification Policy by excluding information\nclassified as restricted and higher from the command format:\n\u0011 Avoid using content created in a language you do not understand without first\nconsulting someone fluent in that language:\n\u0011 Use command format to specify regional dialects when needed.\n\n\nExample:\n\n\nGood command format example: Translate these instructions from Arabic to\nEnglish or French, ensuring that the translated text is clear and accurately conveys\nthe original message.\n\n\nBad command format example: Translate these instructions for Americans to\nunderstand.\n21\nUsing Generative Artificial Intelligence for Project Planning\nGenerative artificial intelligence can be leveraged to create a basic project plan\npresentation that can later be refined. For example, Government employees can\nobtain a project plan template through the use of generative artificial intelligence.\n\n\nBest practices:-\n\u0018 Avoid entering specific project details, such as project name, authority, budget,\nsystem names, employee names, or any classified data@\n\u0018 Manually customize and refine the plan to meet the specific needs of your\nproject@\n\u0018 Ensure that the created project plan aligns with your organization's overall\ngoals and strategies@\n\u0018 Review the plan to ensure it aligns with applicable legal and regulatory\nrequirements for compliance.\n\n\nExample:\n\n\nGood command format example: Provide a general project plan template for\ndeveloping a budget for a public entity, including templates for tracking actual and\nplanned expenses.\n\n\nBad command format example: Develop a planning model for a new public entity\nproject, where four consultants will be appointed for a period of six months to\nwork on allocating the national budget for the next year.\n22\nUsing Generative Artificial Intelligence to Analyze Data\nGenerative artificial intelligence offers the capability to perform numerical\nanalysis on datasets; however, ethical and legal considerations must be adhered to\nwhen engaging in data analysis.\n\n\nBest practices:=\n! Restrict the data analysis by generative artificial intelligence to datasets\nclassified as public or open dataE\n! Ensure that data analysis using generative artificial intelligence complies with\nrelevant provisions, controls, and policiesE\n! Obtain explicit consent from the data subject before utilizing generative\nartificial intelligence to analyze the data (unless the data is made available\nunder an open data license.\u001d\n! Maintain a clear record of the data analysis processE\n! Implement data anonymization techniques before uploading data to the tool to\nensure adherence to privacy principlesE\n! Respect the principles of data ownership and seek consent when possibleE\n! Prioritize data security measures to prevent unauthorized access or breaches.\n\n\nExample:\n\n\nGood command format example: Analyze trends in customer purchasing behavior\nfor the past year.\n\n\n\nBad command format example: As part of the public entity's ongoing work, there is\nan effort to develop incentives for companies responsible for selling popular\nproducts in the Kingdom at the present time. Below is the collected data, which\nincludes information on purchases from shopping centers including the name and\ncredit card number. The data is not anonymized, each customer's best-selling\nproduct is analyzed and identified.\n\n23\nAppendix 2 - Checklist for Utilizing Generative\nArtificial Intelligence Tools\nLegal and Ethical Compliance\nN The generative artificial intelligence tool adheres to the Kingdom\u2019s current\nregulations and policies, the Personal Data Protection Law, and the AI Ethics\nPrinciples.\nData Processing and Policies\nN The use of government data in generative artificial intelligence tools is\nconsistent with the entity's policies, including privacy, data classification, data\nsharing, and relevant terms and conditions.g\nN Establishing and adhering to standards and controls for the use of generative\nartificial intelligence tools in contracts and procurement. \nTraining and Education\nN Employees undergo continuous training courses aimed at enhancing\nawareness of both the potential benefits and risks associated with using\ngenerative artificial intelligence tools.g\nN Employees undergo training to review and comprehend policies and\nprocedures for using tools related to data collection, processing, and\nprotection, enabling them to make informed recommendations. g\nN Employees undergo training courses covering the technical aspects of\ngenerative artificial intelligence tools to enhance workflow efficiency.\n24\nDisclosure\n' Limit the utilization of the generative artificial intelligence tool within the\ngovernment entity to authorized personnel. \u001e\n' Detailed records of employee use of the tool are kept to evaluate its impact on\nworkflow.\nOutputs\n' Users leveraging generative artificial intelligence tools acknowledge that the\noutput generated by the tools may be subject to inaccuracy, contain outdated\ninformation, be biased, or have misleading content/\n' Users consistently verify the validity and accuracy of these outputs, particularly\nwhen preparing official reports and legal documents/\n' Users are notified that the results may not adhere to intellectual property\nprotection regulations and understand the necessity for continuous verification/\n' Verify the accuracy of the generated code before using and executing it. ", "metadata": {"country": "Saudi Arabia", "year": "2024", "legally_binding": "no", "binding_proof": "None", "date": "01-01", "regulator": "Saudi Data and Artificial Intelligence Authority (SDAIA)", "type": "non-binding guidelines", "status": "active", "language": "English, Arabic", "use_cases": "[UC2 - Operational, UC3 - Risk, UC6 - Policy]"}}
{"_id": "686b674f6e4e9653b2a68b20", "title": "Generative AI Guideline for the Public", "source": "https://sdaia.gov.sa/en/SDAIA/about/Files/GenerativeAIPublicEN.pdf", "text": "Version 1\nJanuary 2024\nContents\nIntroduction 1\n\u0013\u0018 Definitions 2\n$\u0018 The Scope 3\n2\u0018 Benefits of Generative AI 3\nI\u0018 Guidelines of Generative AI 5\n4.1 - Fairness 5\n4.2 - Reliability and Safety 6\n4.3 - Transparency & Explainability 7\n4.4 - Accountability and Responsibility 8\n4.5 - Privacy and Security 9\n4.6 - Humanity 10\n4.7 - Social and Environmental Benefits 10\n\u00b5\u0018 Generative AI Risks & Mitigations 11\nIntroduction\nGenerative Artificial Intelligence (GenAI) has gathered significant attention in the\nmedia in recent months. Despite being relatively new, both individuals and\nbusinesses have already begun integrating it into their daily lives and activities.\nWhile the advantages of GenAI are readily apparent, it is also crucial to\nacknowledge the substantial risks and challenges associated with potential\nmisuse.\n\n\nThe Saudi Data and Artificial Intelligence Authority (SDAIA) is comprehensively\nassessing opportunities and related risks in order to promote investment, adoption\nand responsible use to gain benefits of GenAI technologies while reducing related\nrisks. GenAI tools can improve efficiency and create new products to the benefit of\ncitizens. We are committed to facilitate the wide spreading adoption of the\ntechnology in KSA to achieve our national goals.\n\n1\n\b Definitions\nFor the purposes of this document, the following terms and expressions, wherever\nthey appear in this document, shall have the meanings indicated next to each of\nthem unless the context requires otherwise:\nGenerative Artificial Intelligence\nThe generative model is a machine learning model that can create new examples\nsimilar to the training dataset. This model is also a sub-part of artificial\nintelligence that can create new content (including text, images, sounds, icons,\nvideos, etc.) and works by interpreting commands given by users. Generative\nartificial intelligence can perform tasks that require human cognitive abilities,\nincluding responding to and formulating verbal or written commands, \u201clearning,\u201d\nand \u201cproblem-solving.\u201d\nUsers\nAny natural or legal person that consumes or makes use of the goods or services\nproduced by GenAI systems. This is applicable to all stakeholders including\ncompanies, NGOs and individuals.\nDevelopers\nAny natural or legal person that develops GenAI systems to achieve certain goals.\nThis includes AI developers, data scientists and researchers.\n2\n\t\b The Scope\nThis document serves as guidance for the public including developers and users of\nGenAI in the Kingdom of Saudi Arabia.\n\n\nThis guide aims to comprehensively address the responsible and effective\ndevelopment and use of GenAI in KSA. It shall apply to all stakeholders designing,\ndeveloping, deploying, implementing, using, or being affected by GenAI systems\nwithin KSA. \n\n\nAcknowledging the widespread demand for this technology, SDAIA has formulated\nthis document as a robust guideline, fostering both trust and understanding of\nGenAI systems and assisting to avoid negative outcomes. \n9\b Benefits of Generative AI\nGenAI is expected to transform various sectors within the KSA in the following ways:\n3.1 - Increased Efficiency and Productivity\nGenAI can automate a wide range of routine tasks that are currently performed by\nhumans, such as generating summaries of complex documents, transcribing text,\nor generating images and videos. This frees up people to focus on more complex\nstrategic and critical endeavors.\n3.2 - Improved Communication and Collaboration\nThe deployment of GenAI holds promise in areas like the content creation, digital\navatars, optimization of bandwidth for video conferencing, and the formulation of\nsophisticated virtual collaboration environments. These are critical components\nthat can drive enhanced communication and cooperation.\n3\n4\n3.3 - Informed Decision-Making\nGenAI can assist in the decision-making process by analyzing and summarizing\nmultiformat data (e.g., text, audio, and video) and helping to simulate scenarios\nthat could impact strategic choices. When applied appropriately, this capability\nprovides an additional layer of information, fostering a data-driven approach to\ndecision-making in diverse areas such as crisis management, healthcare, and\neducation.\n3.4 - Enhanced Accessibility and Inclusion\nGenAI offers solutions to enhance content accessibility. It can produce materials\nadapted for individuals with special needs or disability, ensuring that platforms\nand communications are universally accessible. Additionally, by generating content\nthat aligns with local dialects and traditions, it promotes clear communication\nbetween people from diverse backgrounds.\n3.5 - Elevated Public Service Quality\nGenAI has the potential to elevate citizen experiences and boost overall\nsatisfaction with public services through the customization of service delivery and\nexpanded availability. It can be used effectively to optimize citizens\u2019 experiences\nacross various public sectors, including healthcare, education, social welfare, and\nlegal assistance.\n5\n\u0013\u0012 Guidelines of Generative AI\nWhen dealing with GenAI tools, developers and users should consistently adhere to\nthe AI ethics and GenAI guidelines during all phases of the system's lifecycle to\nharness their benefits while mitigating risks.\n\n\n\nThe AI Ethics Principles, which were developed by SDAIA, are applicable to\nstakeholders in KSA. They are designed to apply to the use of all AI systems (not just\nGenAI) and are listed below: \n4.1 - Fairness\nRequires stakeholders to take necessary actions to eliminate bias, discrimination,\nor stigmatization of individuals, communities, or groups in the design, data,\ndevelopment, deployment, and use of GenAI systems. Given that GenAI tools have\nthe capability to generate content that may be discriminatory or lack\nrepresentativeness. To ensure consistent systems that are based on fairness and\ninclusiveness, GenAI systems should be trained on data that are cleansed from\nbias and is representative of affected minority groups.\n\n\ns\nZ Carefully test GenAI models to insure no bias has been embedded in the codes\nor algorithms and Ensure that the data used to train the tool are cleansed from\nbias and is representative of affected minority groups. r\nZ Make efforts to gain a thorough understanding of the data used to train the tool\n\u2013 this understanding should encompass the data\u2019s origin, its contents, and how\nit was selected and prepared.r\nZ Enhance knowledge on bias, diversity, inclusion, anti-racism, and values and\nethics \u2013 this knowledge will improve their ability to recognize biased or\ndiscriminatory content.\nAs such, developers and users should:\n6\n4.2 - Reliability & Safety\nEnsures that GenAI systems adhere to set specifications and that they behave as\ndesigners intend. Reliability is a measure of consistency and provides confidence\nin how robust a system is. On the other hand, safety is a measure of how the AI\nsystem does not pose a risk of harm or danger to society and individuals. A\nreliable and safe working system should have built-in mechanisms to prevent\nharm.\n\n\n7\n\u0014 Design and develop GenAI system that can withstand the uncertainty,\ninstability, and volatility that it might encounter\u0019\n\u0014 Establish a set of standards and protocols for assessing the reliability of a\nGenAI system to secure the safety of the system\u2019s algorithm and data output\u0019\n\u0014 Predefined triggers/alerts should be in place based on system risks, GenAI\nsystems should trigger human oversight\u0019\n\u0014 System trigger should be assigned to the appropriate stakeholder. These\ntriggers/alerts can be defined as part of the risk mitigation or disaster\nrecovery procedure and may need human oversight\u0019\n\u0014 Identify AI-generated content to ensure users are alert to potential reliability\nissues and can check it against other sources\u0019\n\u0014 Encourage users to complement AI-generated content with information from\nreputable sources to ensure accuracy\u0019\n\u0014 Strive to comprehend the quality and origins of the training data employed by\nthe AI system to enhance content reliability\u0019\n\u0014 Scrutinize content for factual accuracy and contextual relevance to prevent the\ndissemination of erroneous information.\nAs such, developers and users should:\n7\n4.3 - Transparency & Explainability\nBuilds and maintains the public\u2019s trust in GenAI systems and technologies. In line\nwith this principle, systems must be built to be clearly \u201cexplainable\u201d and have\nfeatures that track how automated decisions are made so that lessons can be\nlearned if these decisions prove to be less than optimal. Transparency is an\nimportant consideration when building trust in human-AI interactions in.\nNotification of AI-generated content eliminates ambiguity, helping users\ndifferentiate between automated and human responses.\n\n\n>\n\u0017 Clearly communicate when GenAI is used in interactions with the public\n\u0017 Notify users when messages or content have been generated by AI\n\u0017 Offer alternative, non-automated communication channels for users who\nprefer human interactions\n\u0017 Employ tools such as watermarks to assist others in identifying content\ngenerated by AI.\nAs such, developers and users should:\n8\n4.4 - Accountability & Responsibility\nHolds designers, vendors, procurers, developers, owners, and assessors of GenAI\nsystems ethically responsible and liable for the decisions/actions that may\nnegatively affect individuals and/or communities.\n\nThe adoption of GenAI systems may entail legal and ethical implications. These\nwarrant thorough consideration and include the risk of infringing intellectual\nproperty rights, concerns about data privacy, and the potential for human rights\nviolations.\n\n\n4\n\u0016 Ensure that data is be properly acquired, classified, processed, and accessible\nto ease human intervention and control at later stages when needed\u0018\n\u0016 Ensure data quality checks, cleanse data and validate the integrity of the data\nin order to get accurate results\u0018\n\u0016 Build and validate models in a responsible manner to achieve intended results\u0018\n\u0016 Adhere to relevant legislation, such as Personal Data Protection Law,\nIntellectual Property Laws to ensure compliance and protect user rights\u0018\n\u0016 Consult with legal professionals to assess and mitigate risks associated with\nthe deployment of GenAI systems.\nAs such, developers and users should:\n9\n4.5 - Privacy & Security\nRequires all GenAI systems to be built and operated in ways that protect the\nprivacy of the data they collect. In line with this, GenAI systems should employ best\npractice data security measures developed by the national authorities to prevent\ndata breaches that could lead to reputational, psychological, financial,\nprofessional, or other types of harm.\n\n\n)\n\u0014 Ensure adequate privacy and security measures in place when using classified\ndata to train GenAI modules\u0016\n\u0014 Implement rigorous data protection measures and consider the principles such\nas, the one outlined in the Personal Data Protection Law\u0016\n\u0014 Assess the risks resulting from the use of the GenAI tool according to the AI\nethics principles; little or no risk, limited risk, high risk, unacceptable risk\u0016\n\u0014 Privacy and security by design should be implemented while building the AI\nsystem. The security mechanisms should include the protection of various\narchitectural dimensions of an AI model from malicious attacks. The structure\nand modules of the AI system should be protected from unauthorized\nmodification or damage to any of its component0\n\u0014 The privacy impact assessment and risk management assessment should be\ncontinuously revisited to ensure that societal and ethical considerations are\nregularly evaluated\nAs such, developers and users should:\n10\n4.6 - Humanity\nAI systems should be built using an ethical methodology to be just and ethically\npermissible, based on intrinsic and fundamental human rights and cultural values\nto generate a beneficial impact on individual stakeholders and communities in both\nthe long and short-term goals and objectives to be used for the good of humanity.\nPredictive models should not be designed to deceive, manipulate, or condition\nbehavior that is not meant to empower, aid, or augment human skills but should\nadopt a more human-centric design approach that allows for human choice and\ndetermination.\n4.7 - Social & Environmental Benefits\nThis principle embraces the beneficial and positive impact of social and\nenvironmental priorities that should benefit individuals and the wider community,\nfocusing on sustainable goals and objectives. GenAI systems should neither cause\nnor accelerate harm or otherwise adversely affect human beings but rather\ncontribute to empowering and complementing social and environmental progress\nwhile addressing associated social and environmental ills. This entails the\nprotection of social good as well as environmental sustainability.\n11\n\u0014\u0013 Generative AI Risks & Mitigations\nWhile GenAI creates significant opportunities, it is important to make sure that it is\nused in a safe, ethical, and legal manner. Based on emerging trends in use of GenAI,\nthe following risks and potential mitigation measures should be prioritized.\n5.1 - Deepfakes and Misrepresentation\nGenAI has transformed the world of digital media through its ability to create\nrealistic text, images, and audio. However, these capabilities can also be used for\nharmful purposes, like scams, financial fraud, blackmail, and sophisticated identity\ntheft. Often, individuals exploit data from social media and other public platforms to\ncreate fake digital representations (i.e., deepfakes). With the technology advancing\nrapidly, distinguishing between real and fake content has become more challenging.\n\n\n All GenAI services should include identifiable\nwatermarks. This measure helps individuals and security services detect and flag\ndeepfakes and AI-synthesized content without affecting legitimate GenAI use.o\nProviders that\ndeliver the intensive computational power needed by GenAI, especially those\nproviding Graphics Processing Units (GPUs) and/or Tensor Processing Units\n(TPUs), should use verification processes similar to those used by financial\ninstitutions. For example, requiring users to provide passport details or other\nproof-of-identity documents to make it more difficult for malevolent actors to\naccess these powerful tools.o\n GenAI services should analyze generated content for any\ninappropriate content (like hate-speech, money transfer request), as well as the\nuse of public figures\u2019 faces and voice samples.o\n Private companies and public\ninstitutions should provide training and have regular awareness campaigns that\ninclude the risks of the deepfakes and tools and methods to identify them, as well\nas information on the importance of limiting personal information exposure online,\nthe dangers of unverified digital communications, and the importance of\nauthenticating information requests through trusted, alternate channels. \nMitigation Measures: \u008f\n\u008a Watermark Implementation:\n\u008a \u2018Know Your Customer' (KYC) Protocols for Cloud Server Providers:\n\u008a Output Verification:\n\u008a Enhanced Digital Literacy and Online Safety:\n12\n5.2 - Safety Threats\nWhile GenAI is a great tool to access information on a broad range of topics, when\nmanipulated with malicious intent, it can compromise public safety and security on\nan unprecedented scale.\n\n\n On platforms where GenAI output\nis accessible to users, deploying robust content moderation and filtering\nsystems is crucial. Such systems should be able to detect, flag, and prevent the\ndissemination of harmful or malicious content generated by the AI. Good\npractice would be to verify both the user prompt and the model\u2019s output.\nRegular updates and refinements to these systems based on feedback and\nemerging trends will ensure they remain effective against evolving threats\u0019\nGenAI developers should minimize the amount of\npotentially dangerous information being used in training datasets for models,\nkeeping in mind that all information used will eventually be available to the\ngeneral public\u0019\nScientists\nworking with models that could potentially create safety threats (e.g., protein\nsynthesis) should not publish such models in the public domain without first\nrequiring users to verify who they are and what research they intend to use the\nmodel for.\nMitigation Measures: @\n> Content Moderation and Filtering Systems:\n> Training Dataset Filtering:\n> Limiting Open Access / Open Source for Scientific GenAI Models: \n13\n5.3 - Misinformation and \u201cHallucination\u201d\nDue to the nature of GenAI text models, some information they provide might be\nincorrect. Sometimes such models can even over-confidently generate \u201cfacts\u201d that\nare complete fiction (also known as \u201cAI hallucination\u201d). Without critical review of\nsuch outputs, there\u2019s a risk of unintentional misinformation. \n\n\n Publicly available GenAI services should\nestimate the accuracy of information prior to displaying it to users alongside\nthe content requested. Such services should also be able to cite sources\nindicating where the information was acquired. If the model cannot produce a\nresponse with a high level of accuracy and identify its source/s, the output\nshould be replaced with a pre-defined message warning users that the\ninformation generated may not be accurate&\nGenAI services should embed watermarks in outputs based\non the type of media used. For photo and video, watermarks can take the form\nof traditional preprint, or be embedded into encoding or decoding. Audio\ncontent can be watermarked through embedded audio snippets or echo\nmodulation. For text, content-specific word and letter combinations can be\nused, as well as non-standard fonts&\n It is users\u2019 responsibility to verify the\ncontent generated by GenAI. This means users need to scrutinize the content\nand fact check it against trustworthy sources. Users should also always look\nfor any indication that the content came from a GenAI tool if this was the way it\nwas produced (see Content Labelling above)?\n Users should be regularly reminded that outputs from\nGenAI can sometimes be inaccurate, outdated, biased, or even deceptive, and\nbe educated about standard fact-checking and content-verification processes.\nMitigation Measures: Y\nW Content Verification and Citation:\nW Content Labeling:\nW User Vigilance and Fact-checking:\nW Raising Awareness:\n14\n5.4 - Classified Data Breaches\nWhen interacting with GenAI, users should be aware that they might\nunintentionally expose sensitive information. This is because GenAI services use\nthe information received through prompts as training data for further development\nof the model \u2013 this can then be exposed to third parties. This risk is complicated by\nthe fact that GenAI models do not store information in traditional document form,\nmeaning it\u2019s harder to identify if any sensitive information was in fact leaked or to\nmake the model \u201cunlearn\u201d this information.\n\n\n Organizations should implement policies for GenAI use\nthat prohibits users from entering classified information into third-party tools.\nThese should detail acceptable content generation practices, actions to prevent\nsensitive data leaks, and steps to support ethical use of the technology.\nOversight and regular reviews are essential to confirm adherence to policies,\nand rapid, organization-wide communication is key to their successful\nimplementation\u001b\nOrganizations must emphasize to their\nworkforce that existing and upcoming legal requirements remain binding when\nusing GenAI tools. This includes adherence to company policies, as well as\nbroader legal mandates encompassing data governance, cybersecurity,\ngovernment data classification, personal data protection, intellectual property\n(IP) rights, and other pertinent legal or policy areas. Targeted training sessions\ncan provide guidance on how to reduce potential legal risks associated with the\ndeployment of GenAI\u001b\n GenAI service providers should provide users with options\nto give \u2013 or refuse \u2013 consent to use their data for AI model training purposes,\nthey should also provide an option to remove all prompt history on request in\naccordance with relevant laws and regulations.\nMitigation Measures: Q\nO GenAI Usage Protocols:\nO Employee Awareness and Training:\nO User Data Control: \n15\n5.5 - Certification Fraud\nHuman certification processes, such as exams and professional evaluations, stand\nas essential benchmarks of individual competence and institutional credibility.\nHowever, with the advent of GenAI, these processes are facing novel threats.\nGenAI, with its ability to craft \u2018human-like\u2019 specialized content, can be misused to\nproduce answers, essays, or even detailed research, effectively undermining\ntraditional educational and professional standards. The sophistication of this AIgenerated content poses a distinct challenge: it is original and not directly copied\nfrom known sources, making it harder to detect with standard anti-plagiarism\ntools. Thus, the unchecked misuse of GenAI could jeopardize the integrity of\nfoundational educational and professional assessments.\n\n\n Review all forms of assessment and evaluation to\nenhance their resilience against AI-aided fraud. Consider adjustments such as\ntransitioning to in-person assessments, revising question formats, or adopting\ninnovative evaluation methodologies. Moreover, organizations should conduct a\nthorough evaluation of critical certification exams, particularly those essential for\npositions like national critical infrastructure access or medical practice licenses,\nwhere certifications determine eligibility. In cases where GenAI could potentially be\nexploited to pass such certifications, organizations must proactively revise the\ncertification processes to reduce the likelihood of GenAI use. This ensures the\nassessments maintain their integrity and security, especially in vital areas\u0019\nConduct comprehensive training to raise awareness and\neducate both educators and students about responsible GenAI use. This includes\nrecognizing misuse, adhering to ethical practices, and identifying suspicious\nbehaviors that may indicate fraud\u0019\n Collaboratively develop and implement clear guidelines\nin conjunction with students and educators to govern GenAI usage in academic\nsettings. These guidelines should align with learning objectives and specify when\nand how GenAI can be applied\u0019\n Provide specialized training to enhance the competency of\neducators, researchers, and students in crafting precise and effective prompts for\nGenAI systems. Emphasize the significance of formulating inputs that yield optimal\nAI outputs.\nMitigation Measures: M\nK Assessment Enhancement:\nK Education and Training:\nK Guidelines and Policies:\nK Query Proficiency: \n16\n5.6 - Intellectual Property Infringement \u2013 and Protection\nThe rise of GenAI has brought to the forefront the issue of unauthorized use or\nreplication of copyrighted material, potentially leading to legal liabilities of IP. This\nrisk extends to both the unauthorized use of existing copyrighted material and the\ncreation of novel content by AI that may inadvertently infringe upon IP rights.\n\n\n GenAI developers should obtain licenses for any IP\nincluded in training data, preventing indiscriminate content scraping. Customers\nshould perform their own due diligence to confirm whether AI models were trained\nwith protected content before using them!\nGenAI services should obtain original\ncontent IP holder permission before generating content based on their IP. GenAI\nservices should establish compensation mechanisms (e.g., contributor funds) to\nfairly remunerate creators if their IP has been used in training sets. For instance,\nShutterstock offers creators the option to opt out of having their work used in AI\ntraining sets and has set up a contributor fund to compensate them when their\nwork is used.\nMitigation Measures: J\nH IP Licensing and Due Diligence:\nH Creator Permission and Compensation: \n17\n5.7 - Variability of Outputs\nGenAI services operate differently from traditional \u201cprogrammed\u201d services where\noutput generation adheres to pre-defined algorithms in 100 percent of cases.\nFurthermore, developers of such services may update their offerings without\ninforming end users. Consequently, users should be very vigilant when relying on\noutputs generated by GenAI.\n\n\n Code developers who leverage GenAI\ncoding tools, such as ChatGPT and GitHub Copilot, should add clear annotations\nwithin the source code to indicate that it was generated by AI. This annotation not\nonly promotes transparency but also ensures accountability for the code\u2019s origin\nand quality\u001b\nUsers should adopt a practice of regularly verifying\nand validating AI\u2013generated code and other content, especially in critical\napplications where accuracy is paramount. Implementing thorough testing and\nvalidation processes can help identify and rectify any inconsistencies or errors in\nthe outputs\u001b\nTo mitigate the risk of unannounced updates to\nGenAI services, users should stay informed about any changes or enhancements\nmade by the service providers. Subscribing to notifications and updates from the\ndevelopers can help users adapt to modifications in the service\u2019s behavior or\noutput quality\u001b\nUsers, particularly those in software development and coding\nroles, should develop expertise in understanding GenAI. This proficiency will help\nwhen evaluating and refining AI-generated outputs to meet specific requirements.\nMitigation Measures: Q\nN Clear Annotation for AI-Generated Code:\nN Regularly Verify and Validate:\nN Stay Informed About Updates:\nN Build Expertise:  ", "metadata": {"country": "Saudi Arabia", "year": "2023", "legally_binding": "no", "binding_proof": "None", "date": "01-01", "regulator": "Saudi Data and Artificial Intelligence Authority (SDAIA)", "type": "non-binding guidelines", "status": "active", "language": "English, Arabic", "use_cases": "[UC1 - Personal, UC3 - Risk, UC6 - Policy]"}}
{"_id": "686b67846e4e9653b2a68b21", "title": "Personal Data Protection Law", "source": "https://sdaia.gov.sa/en/SDAIA/about/Documents/Personal%20Data%20English%20V2-23April2023-%20Reviewed-.pdf", "text": "SDAIA\n\u0627\u0644\u0647\u064a\u0626\u0629 \u0627\u0644\u0633\u0639\u0648\u062f\u064a\u0629 \u0644\u0644\u0628\u064a\u0627\u0646\u0627\u062a\n\u0648\u0627\u0644\u0630\u0643\u0627\u0621 \u0627\u0644\u0627\u0635\u0637\u0646\u0627\u0639\u064a\n\u0160audi Data & Al Authority\nPersonal Data Protection Law\nIssued pursuant to Royal Decree No. (M/19) dated 09/02/1443 AH corresponding to\n16/09/2021 G\nAmended pursuant to Royal Decree No. (M/148) dated 05/09/1444 AH corresponding to\n27/03/2023 G\nSDAIA\n\u0648\u0627\u0644\u0630\u0643\u0627\u0621 \u0627\u0644\u0627\u0635\u0637\u0646\u0627\u0639\u0649 \u0627\u0644\u0647\u064a\u0626\u0629 \u0627\u0644\u0633\u0639\u0648\u062f\u064a\u0629 \u0644\u0644\u0628\u064a\u0627\u0646\u0627\u062a\n\u0160audi Data & Al Authority\nPersonal Data Protection Law\nIssued pursuant to Royal Decree No. (M/19) dated 09/02/1443 AH corresponding to\n16/09/2021 G\nAmended pursuant to Royal Decree No. (M/148) dated 05/09/1444 AH corresponding to\n27/03/2023 G\nPersonal Data Protection Law\nArticle 1\nFor the purpose of implementing this Law, the following terms shall have the meanings\nassigned thereto, unless the context requires otherwise:\n1-Law: The Personal Data Protection Law.\n2-Regulations: The Implementing Regulations of the Law.\n3-Competent Authority: The authority to be determined by a resolution of the Council of\nMinisters.\n4-Personal Data: Any data, regardless of its source or form, that may lead to identifying an\nindividual specifically, or that may directly or indirectly make it possible to identify an\nindividual, including name, personal identification number, addresses, contact numbers,\nlicense numbers, records, personal assets, bank and credit card numbers, photos and\nvideos of an individual, and any other data of personal nature.\n5-Processing: Any operation carried out on Personal Data by any means, whether manual\nor automated, including collecting, recording, saving, indexing, organizing, formatting,\nstoring, modifying, updating, consolidating, retrieving, using, disclosing, transmitting,\npublishing, sharing, linking, blocking, erasing and destroying data.\n6-Collection: The collection of Personal Data by Controller in accordance with the\nprovisions of this Law, either from the Data Subject directly, a representative of the Data\nSubject, any legal guardian over the Data Subject or any other party.\n7-Destruction: Any action taken on Personal Data that makes it unreadable and\nirretrievable, or impossible to identify the related Data Subject.\n8-Disclosure: Enabling any person - other than the Controller or the Processor, as the case\nmay be - to access, collect or use personal data by any means and for any purpose.\n9-Transfer: The transfer of Personal Data from one place to another for Processing.\n10-Publishing: Transmitting or making available any Personal Data using any written, audio\nor visual means.\n11-Sensitive Data: Personal Data revealing racial or ethnic origin, or religious, intellectual\nor political belief, data relating to security criminal convictions and offenses, biometric or\nGenetic Data for the purpose of identifying the person, Health Data, and data that indicates\nthat one or both of the individual\u2019s parents are unknown.\n12-Genetic Data: Any Personal Data related to the hereditary or acquired characteristics of\na natural person that uniquely identifies the physiological or health characteristics of that \nperson, and derived from biological sample analysis of that person, such as DNA or any\nother testing that leads to generating Genetic Data.\n13-Health Data: Any Personal Data related to an individual's health condition, whether their\nphysical, mental or psychological conditions, or related to Health Services received by that\nindividual.\n14-Health Services: Services related to the health of an individual, including preventive,\ncurative, rehabilitative and hospitalizing services, as well as the provision of medications.\n15-Credit Data: Any Personal Data related to an individual's request for, or obtaining of,\nfinancing from a financing entity, whether for a personal or family purpose, including any\ndata relating to that individual\u2019s ability to obtain and repay debts, and the credit history of\nthat person.\n16-Data Subject: The individual to whom the Personal Data relate.\n17-Public Entity: Any ministry, department, public institution or public authority, any\nindependent public entity in the Kingdom, or any affiliated entity therewith.\n18-Controller: Any Public Entity, natural person or private legal person that specifies the\npurpose and manner of Processing Personal Data, whether the data is processed by that\nController or by the Processor.\n19-Processor: Any Public Entity, natural person or private legal person that processes\nPersonal Data for the benefit and on behalf of the Controller.\nArticle 2\n1-The Law applies to any Processing of Personal Data related to individuals that takes\nplace in the Kingdom by any means, including the Processing of Personal Data related to\nindividuals residing in the Kingdom by any means from any party outside the Kingdom. This\nincludes the data of the deceased if it would lead to them or a member of their family being\nidentified specifically.\n2-The scope of applying the Law excludes the individual's Personal Data Processing for\npurposes that do not go beyond personal or family use, as long as the Data Subject did not\npublish or disclose it to others. The Regulations shall define personal and family use\nprovided in this Paragraph.\nArticle 3\nThe provisions and procedures stated in this Law shall not prejudice any provision that\ngrants a right to the Data Subject or confers better protection to Personal Data pursuant to\nany other law or an international agreement to which the Kingdom is a party.\nArticle 4\nData Subject shall have the following rights pursuant to this Law and as set out in the\nRegulations:\n1-The right to be informed about the legal basis and the purpose of the Collection of their\nPersonal Data.\n2-The right to access their Personal Data held by the Controller, in accordance with the\nrules and procedures set out in the Regulations, and without prejudice to the provisions of\nArticle (9) of this Law.\n3-The right to request obtaining their Personal Data held by the Controller in a readable and\nclear format, in accordance with the controls and procedures specified by the Regulations.\n4-The right to request correcting, completing, or updating their Personal Data held by the\nController.\n5-The right to request a Destruction of their Personal Data held by the Controller when such\nPersonal Data is no longer needed by Data Subject, without prejudice to the provisions of\nArticle (18) of this Law.\nArticle 5\n1-Except for the cases stated in this Law, neither Personal Data may be processed nor the\npurpose of Personal Data Processing may be changed without the consent of the Data\nSubject. The Regulations Shall set out the conditions of the consent, the cases in which the\nconsent must be explicit, and the terms and conditions related to obtaining the consent of\nthe legal guardian if the Data Subject fully or partially lacks legal capacity.\n2-In all cases, Data Subject may withdraw the consent mentioned in Paragraph (1) of this\nArticle at any time; the Regulations determines the necessary controls for such case.\nArticle 6\nIn the following cases, Processing of Personal Data shall not be subject to the consent\nreferred to in Paragraph (1) of Article (5) herein:\n1-If the Processing serves actual interests of the Data Subject, but communicating with the\nData Subject is impossible or difficult.\n2-If the Processing is pursuant to another law or in implementation of a previous agreement\nto which the Data Subject is a party.\n3-If the Controller is a Public Entity and the Processing is required for security purposes or\nto satisfy judicial requirements.\n4-If the Processing is necessary for the purpose of legitimate interest of the Controller,\nwithout prejudice to the rights and interests of the Data Subject, and provided that no\nSensitive Data is to be processed. Related provisions and controls are set out in the\nRegulations.\nArticle 7\nThe consent referred to in paragraph (1) of Article (5) of this Law may not form a condition\nof providing a service or a benefit, unless such service or benefit is directly related to the\nProcessing of Personal Data for which the consent is given.\nArticle 8\nSubject to the provisions of this Law and the Regulations regarding the Disclosure of\nPersonal Data, the Controller shall only select Processors providing the necessary\nguarantees to implement the provisions of this Law and the Regulations. The Controller\nshall also monitor the compliance of said Processors with the provisions of this Law and\nthe Regulations. This shall not prejudice the Controller\u2019s responsibilities towards the Data\nSubject or the Competent Authority as the case may be. The Regulations shall set out the\nprovisions necessary in this regard, including provisions related to any subsequent\ncontracts conducted by the Processor.\nArticle 9\n1-The Controller may set time frames for exercising the right to access Personal Data\nstated in Paragraph (2) of Article (4) herein as stipulated in the Regulations. The Controller\nmay limit the exercise of this right in the following cases:\na) If this is necessary to protect the Data Subject or other parties from any harm,\naccording to the provisions set forth the Regulations.\nb) If the Controller is a Public Entity and the restriction is required for security\npurposes, required by another law, or required to fulfill judicial requirements.\n2-The Controller shall prevent the Data Subject from accessing Personal Data in any of the\nsituations stated in Paragraphs (1, 2, 3, 4, 5) and (6) of Article (16) herein.\nArticle 10\nThe Controller may only collect Personal Data directly from the Data Subject and may only\nprocess Personal Data for the purposes for which they have been collected. However, the\nController may collect Personal Data from a source other that the Data Subject and may\nprocess Personal Data for purposes other than the ones for which they have been collected\nin the following situations:\n1- The Data Subject gives their consent in accordance with the provisions of this Law.\n2- Personal Data is publicly available or was collected from a publicly available source.\n3- The Controller is a Public Entity, and the Collection or Processing of the Personal Data is\nrequired for public interest or security purposes, or to implement another law, or to fulfill\njudicial requirements.\n4- Complying with this may harm the Data Subject or affect their vital interests\n5- Personal Data Collection or Processing is necessary to protect public health, public\nsafety, or to protect the life or health of specific individuals.\n6- Personal Data is not to be recorded or stored in a form that makes it possible to directly\nor indirectly identify the Data Subject.\n7- Personal Data Collection is necessary to achieve legitimate interests of the Controller,\nwithout prejudice to the rights and interests of the Data Subject, and provided that no\nSensitive Data is to be processed.\nThe Regulations shall set out the provisions, controls and procedures related to what is\nstated in paragraphs (2) to (7) of this Article.\nArticle 11\n1-The purpose for which Personal Data is collected shall be directly related to the\nController\u2019s purposes, and shall not contravene any legal provisions.\n2-The methods and means of Personal Data Collection shall not conflict with any legal\nprovisions, shall be appropriate for the circumstances of the Data Subject, shall be\ndirect, clear and secure, and shall not involve any deception, misleading or extortion.\n3-The content of the Personal Data shall be appropriate and limited to the minimum\namount necessary to achieve the purpose of the Collection. Content that may lead to\nspecifically identifying Data Subject once the purpose of Collection is achieved shall be\navoided. The Regulations shall set out the necessary controls in this regard.\n4-If the Personal Data collected is no longer necessary for the purpose for which it has\nbeen collected, the Controller shall, without undue delay, cease their Collection and\ndestroy previously collected Personal Data.\nArticle 12\nThe Controller shall use a privacy policy and make it available to Data Subjects for their\ninformation prior to collecting their Personal Data. The policy shall specify the purpose of\nCollection, Personal Data to be collected, the means used for Collection, Processing,\nstorage and Destruction, and information about the Data Subject rights and how to exercise\nsuch rights.\nArticle 13\nWhen collecting Personal Data directly from the Data Subject, the Controller shall take\nappropriate measures to inform the Data Subject of the following upon Collection:\n1-The legal basis for collecting their Personal Data.\n2-The purpose of the Collection, and shall specify the Personal Data whose Collection is\nmandatory and the Personal Data whose Collection is optional. The Data Subject shall be\ninformed that the Personal Data will not be subsequently processed in a manner\ninconsistent with the Collection purpose or in cases other than those stated in Article (10)\nof this Law.\n3-Unless the Collection is for security purposes, the identity of the person collecting the\nPersonal Data and the address of its representative, if necessary.\n4-The entities to which the Personal Data will be disclosed, the capacity of such entities,\nand whether the Personal Data will be transferred, disclosed or processed outside the\nKingdom.\n5-The potential consequences and risks that may result from not collecting the Personal\nData.\n6-The rights of the Data Subject pursuant to Article (4) herein.\n7-Such other elements as set out in the Regulations based on the nature of the activity\ndone by the Controller.\nArticle 14\nThe Controller may not process Personal Data without taking sufficient steps to verify the\nPersonal Data accuracy, completeness, timeliness and relevance to the purpose for which\nit is collected in accordance with the provisions of the Law.\nArticle 15\nThe Controller may not Disclose Personal Data except in the following situations:\n1- Data Subject consents to the Disclosure in accordance with the provisions of the\nLaw.\n2- Personal Data has been collected from a publicly available source.\n3- The entity requesting Disclosure is a Public Entity, and the Collection or Processing\nof the Personal Data is required for public interest or security purposes, or to\nimplement another law, to fulfill judicial requirements.\n4- The Disclosure is necessary to protect public health, public safety, or to protect the\nlives or health of specific individuals.\n5- The Disclosure will only involve subsequent Processing in a form that makes it\nimpossible to directly or indirectly identify the Data Subject.\n6- The Disclosure is necessary to achieve legitimate interests of the Controller, without\nprejudice to the rights and interests of the Data Subject, and provided that no\nSensitive Data is to be processed.\nThe Regulations shall set out the provisions, controls and procedures related to\nwhat is stated in paragraphs (2) to (6) of this Article.\nArticle 16\nThe Controller shall not disclose Personal Data in the situations stated in Paragraphs (1, 2,\n5) and (6) of Article (15) if the Disclosure:\n1- Represents a threat to security, harms the reputation of the Kingdom, or conflicts\nwith the interests of the Kingdom.\n2- Affects the Kingdom\u2019s relations with any other state.\n3- Prevents the detection of a crime, affects the rights of an accused to a fair trial, or\naffects the integrity of existing criminal procedures.\n4- Compromises the safety of an individual.\n5- Results in violating the privacy of an individual other than the Data Subject, as set\nout in the Regulations.\n6- Conflicts with the interests of a person that fully or partially lacks legal capacity.\n7- Violates legally established professional obligations.\n8- Involves a violation of an obligation, procedure, or judicial decision.\n9- Exposes the identity of a confidential source of information in a manner detrimental\nto the public interest.\nArticle 17\n1- If Personal Data is corrected, completed or updated, the Controller shall notify such\namendment to all the other entities to which such Personal Data has been\ntransferred and make the amendment available to such entities.\n2- The Regulations shall set out the time frames for correction and updating of\nPersonal Data, types of correction, and the procedures required to avoid the\nconsequences of Processing incorrect, inaccurate or outdated Personal Data.\nArticle 18\n1- The Controller shall, without undue delay, Destroy the Personal Data when no longer\nnecessary for the purpose for which they were collected. However, the Controller\nmay retain data after the purpose of the Collection ceases to exist; provided that it\ndoes not contain anything that may lead to specifically identifying Data Subject\npursuant to the controls stipulated in the Regulations.\n 2- In the following cases, the Controller shall retain the Personal Data after the purpose\nof the Collection ceases to exist:\na) If there is a legal basis for retaining the Personal Data for a specific period, in which\ncase the Personal Data shall be destroyed upon the lapse of that period or when the\npurpose of the Collection is satisfied, whichever longer.\nb) If the Personal Data is closely related to a case under consideration before a judicial\nauthority and the retention of the Personal Data is required for that purpose, in\nwhich case the Personal Data shall be destroyed once the judicial procedures are\nconcluded.\nArticle 19\nThe Controller shall implement all the necessary organizational, administrative and technical\nmeasures to protect Personal Data, including during the Transfer of Personal Data, in\naccordance with the provisions and controls set out in the Regulations.\nArticle 20\n1-The Controller shall notify the Competent Authority upon knowing of any breach,\ndamage, or illegal access to personal data, in accordance with the Regulations.\n2-The Controller shall notify the Data Subject of any breach, damage or illegal access to\ntheir Personal Data that would cause damage to their data or cause prejudice to their\nrights and interests, in accordance with the Regulations.\nArticle 21\nThe Controller shall respond to the requests of the Data Subject pertaining to their rights\nunder this Law within such period and in such method as set out in the Regulations.\nArticle 22\nThe Controller shall conduct an impact assessment of Personal Data Processing in relation\nto any product or service, based on the nature of the activity carried out by the Controller,\nin accordance with the relevant provisions of the Regulations.\nArticle 23\nWithout prejudice to this Law, the Regulations shall set out additional controls and\nprocedures for the Processing of Health Data in a manner that ensures the privacy of the\nData Subject and protects their rights under this Law. Such additional controls and\nprocedures shall include the following:\n1- Restricting the right to access Health Data, including medical files, to the minimum\nnumber of employees or workers and only to the extent necessary to provide the\nrequired Health Services.\n2- Restricting Health Data Processing procedures and operations to the minimum\nextent possible of employees and workers as necessary to provide Health Services\nor offer health insurance programs.\nArticle 24\nWithout prejudice to this Law, the Regulations shall set out additional controls and\nprocedures for the Processing of Credit Data in a manner that ensures the privacy of the\nData Subject and protects their rights under this Law and the Credit Information Law. Such\ncontrols and procedures shall include the following:\n1- Implementing appropriate measures to verify that the Data Subject has given their\nexplicit consent to the Collection of the Personal Data, changing the purpose of the\nCollection, or Disclosure or Publishing of the Personal Data in accordance with the\nprovisions of this Law and the Credit Information Law.\n2- Requiring that the Data Subject be notified when a request for Disclosure of their\nCredit Data is received from any entity.\nArticle 25\nWith the exception of the awareness-raising materials sent by Public Entities, Controller\nmay not use personal means of communication, including the post and email, of the Data\nSubject to send advertising or awareness-raising materials, unless:\n1- Obtaining the prior consent of the targeted recipient for such materials.\n2- The sender of the material shall provide a clear mechanism, as set out in the\nRegulations, that enables the targeted recipient to request stopping receiving such\nmaterials if they desire so.\n3- The Regulations shall set out the provisions concerning the aforementioned\nadvertising and awareness-raising materials, as well as the conditions and situations\nconcerning the consent of the recipient to receive aforementioned materials.\nArticle 26\nWith the exception of Sensitive Data, Personal Data may be processed for marketing\npurposes, if it is collected directly from the Data Subject and their consent is given in\naccordance with the provisions of Law; the Regulations shall set out the controls in such\nregard.\nArticle 27\nPersonal data may be collected or processed for scientific, research, or statistical purposes\nwithout the consent of the Data Subject in the following situations:\n1-If it does not specifically identify the Data Subject.\n2-If evidence of the Data Subject\u2019s identity will be destroyed during the Processing and\nprior to Disclosure of such data to any other entity, if it is not Sensitive Data.\n3-If personal data is collected or processed for these purposes is required by another law\nor in implementation of a previous agreement to which the Data Subject is a party.\nThe Regulations shall set out the controls required by the provisions of this Article.\nArticle 28\nIt is not permissible to copy official documents where Data Subjects are identifiable, except\nwhere it is required by law, or when a competent public authority requests copying such\ndocuments pursuant to the Regulations.\nArticle 29\n1-Subject to the provisions of Paragraph (2) of this Article, a Controller may Transfer\nPersonal Data outside the Kingdom or disclose it to a party outside the Kingdom, in\norder to achieve any of the following purposes:\nA. If this is relating to performing an obligation under an agreement, to which the\nKingdom is a party.\nB. If it is to serve the interests of the Kingdom.\nC. If this is to the performance of an obligation to which the Data Subject is a party\nD. If this is to fulfill other purposes as set out in the Regulations.\n2-The conditions that must be met when there is a Transfer or Disclosure of\nPersonal Data, according to what is stated in Paragraph (1) of this Article, are as\nfollows:\nA. The Transfer or Disclosure shall not cause any prejudice to national security or\nthe vital interests of the Kingdom.\nB. There is an adequate level of protection for Personal Data outside the Kingdom.\nSuch level of protection shall be at least equivalent to the level of protection\nguaranteed by the Law and Regulations, according to the results of an\nassessment conducted by the Competent Authority in coordination with\nwhomever it deems appropriate from the other relevant authorities.\nC. The Transfer or Disclosure shall be limited to the minimum amount of Personal\nData needed.\n3-Paragraph (2) of this Article shall not apply to cases of extreme necessity to preserve\nthe life or vital interests of the Data Subject or to prevent, examine or treat disease.\n4-The Regulations shall set out the provisions, criteria and procedures related to the\nimplementing this Article, including applicable exceptions for Controllers regarding\nconditions referred to in Subparagraphs (b) and (c) of Paragraph (2) of this Article, as\nwell as controls and procedures for such exemptions.\nArticle 30\n1- Without prejudice to the provisions of this Law and the powers of the Saudi Central\nBank pursuant to applicable legal provisions, the Competent Authority shall be the\nentity in charge of overseeing the implementation of this Law and the Regulations.\n2- The Regulations shall identify the situations where the Controller shall appoint one or\nmore persons as personal data protection officer(s). and shall set the responsibilities\nof any such person in accordance with the provisions of this Law.\n3- The Controller shall cooperate with the Competent Authority in performing its duties\nto supervise the implementation of the provisions of this Law and the Regulations,\nand shall take such steps as necessary in connection with the related matters\nreferred to the Controller by the Competent Authority.\n4- The Competent Authority, in order to carry out its duties related to supervising the\nimplementation of the provisions of the Law and Regulations, may:\nA. Request the necessary documents or information from the Controller to ensure\nits compliance with the provisions of the Law and Regulations.\nB. Request the cooperation of any other party for the purposes of support in\naccomplishing supervisory duties and enforcement of the provisions of the Law\nand Regulations.\nC. Specify the appropriate tools and mechanisms for monitoring Controllers\u2019\ncompliance with the provisions of the Law and the Regulations, including\nmaintaining a national register of Controllers for this purpose.\nD. Provide services related to Personal Data protection through the national register\nreferred to in Subparagraph (c) of this Paragraph or through any other means\ndeemed appropriate. The Competent Authority may collect a fee for the Personal\nData protection services it may provide.\n5- The Competent Authority may, at its discretion, delegate to other authorities the\naccomplishment of some of its duties that are related to supervision or enforcement\nof the provisions of the Law and Regulations.\nArticle 31\nWithout prejudice to Article (18) herein, the Controller shall maintain records, for such a\nperiod as required under the Regulations, of the Personal Data Processing activities, based\non the nature of the activity carried out by the Controller. Such records are to be available\nwhenever requested by the Competent Authority. The records shall contain the following\ninformation at a minimum:\n1-Contact details of the Controller.\n2-The purpose of the Personal Data Processing.\n3-Description of the categories of Personal Data Subjects.\n4-Any other entity to which Personal Data has been, or will be, disclosed.\n5-Whether the Personal Data has been or will be transferred outside the Kingdom or\ndisclosed to an entity outside the Kingdom.\n6-The expected period for which Personal Data shall be retained.\nArticle 32\nRepealed.\nArticle 33\n1-The Competent Authority shall set the requirements for practicing commercial,\nprofessional or non-profit activities related to Personal Data protection in the Kingdom, in\ncoordination with the competent authorities, and without prejudice to the other\nrequirements set by those authorities in their domain of competence.\n2-The Competent Authority may grant licenses to entities that issue accreditation\ncertificates to Controllers and Processors. The Competent Authority shall set the rules to\nregulate the issuance of such certificates.\n3-The Competent Authority may grant licenses to entities that conduct audits or checks of\nPersonal Data Processing activities related to the Controller\u2019s activity, in accordance with\nthe provisions stipulated in the Regulations. The Competent Authority shall set the\nconditions and criteria to grant such licenses, and the rules regulating them.\n4-The Competent Authority shall specify the appropriate tools and mechanisms to monitor\ncompliance of Controllers and Processors outside the Kingdom in regard with their\nobligations as stated in the Law and the Regulations when Processing personal data\nrelated to individuals residing in the Kingdom by any means, and shall define procedures to\nenforce the provisions of the Law and the Regulations outside the Kingdom.\nArticle 34\nA Data Subject may submit to the Competent Authority any complaint that may arise out of\nthe implementation of this Law and the Regulations. The Regulations shall set out the rules\nfor processing the complaints that may arise from implementing this Law and the\nRegulations.\nArticle 35\n1-Without prejudice to any harsher penalty stipulated in another law, any individual\ndiscloses or publishes Sensitive Data, in violation of the provisions of the Law, with the\nintention of harming the Data Subject or achieving a personal benefit shall be punished with\nimprisonment for a period not exceeding (two years), or a fine not exceeding (three million)\nRiyals, or both.\n2-The Public Prosecution is responsible for investigating and prosecuting before the\ncompetent court for the violation stipulated in Paragraph (1) of this Article.\n3-The competent court shall be in charge of lawsuits arising from the implementation of this\nArticle and issuing the prescribed penalties.\n4-The competent court may double the fine penalty stipulated in Paragraph (1) of this\nArticle in the case of recidivism, even if it results in exceeding its maximum limit, provided\nthat it does not exceed double this limit.\nArticle 36\n1-In cases that are not covered in Article (35) herein and without prejudice to any harsher\npenalty stipulated in another law, a warning or a fine not exceeding (five million) Riyals shall\nbe imposed on every person with a special natural or legal capacity - covered by the\nprovisions of the Law - who violates any of the provisions of the Law or the Regulations.\nThe fine penalty may be doubled in the event of a repeat violation, even if it results in\nexceeding its maximum limit, provided that it does not exceed double this limit.\n2-A committee (or more) shall be formed by a decision of the president of the Competent\nAuthority. The number of its members shall not be less than (three), and one of them shall\nbe appointed as the committee head, and there shall be a technical specialist and a legal\nadvisor among them. The committee is to examine violations and issue warnings or impose\nfines as stipulated in Paragraph (1) of this Article, considering the type of violation\ncommitted, its seriousness and the extent of its impact; provided that the decision of the\ncommittee is approved by the president of the Competent Authority or whomever they\ndelegate. The president of the Competent Authority shall issue, by their decision, the rules\nof work of the committee, and the remunerations of its members shall be determined\ntherein.\n3-Anyone against whom a decision has been issued by the committee mentioned in\nParagraph (2) of this Article has the right to appeal against them before the competent\ncourt.\nArticle 37\n1-Employees and workers appointed by a decision of the president of the Competent\nAuthority shall have the powers to control and inspect the violations stated in this Law or\nthe Regulations. The president of the Competent Authority shall issue the rules and\nprocedures in regard to the work of those employees and workers in accordance with the\napplicable laws.\n2-The employees and workers referred to in Paragraph (1) of this Article may seek\nassistance from criminal investigations authorities or other competent authorities to carry\nout their duties concerning control and inspection of violations stipulated in the Law or\nRegulations.\n3-The Competent Authority has the right to seize the means or tools used in committing the\nviolation until a decision is made on it.\nArticle 38\n1-Without prejudice to the rights of bona fide third parties, the competent court may order\nthe confiscation of funds obtained as a result of committing the violations stipulated in the\nLaw.\n2-The competent court, or the committee referred to in paragraph (2) of Article (36), as the\ncase may be, may include in their penalty judgment or decision a provision that a summary\nof such judgment or decision shall be published at the expense of the violator in one (or more)\nlocal newspapers distributed in their area of residence, or using any other proper means. This\nis based on the type, seriousness and impact of the violation; provided that the publishing\nshall be after the judgment becomes final, the lapse of the deadline for appeals, or the\nissuance of a final ruling dismissing the appeal against the judgement.\nArticle 39\nWithout prejudice to the provisions of Article (35) and Paragraph (1) of Article (36) of this\nLaw, the Public Entity shall discipline any of its employees who violate any of the provisions\nof this Law and Regulations, in accordance with the disciplinary provisions and procedures\nprescribed by law.\nArticle 40\nWithout prejudice to the penalties stated in this Law, any individual that suffers a damage\nas a result of any of the violations stated in this Law or the Regulations may apply to a\ncompetent court for proportionate compensation for the material or moral damage.\nArticle 41\nAny person that engages in the Processing of Personal Data shall protect the confidentiality\nof the Personal Data even after the end of such person\u2019s occupational or contractual\nrelationship.\nArticle 42\nThe president of the Competent Authority shall issue the Regulations within a period not\nexceeding (seven hundred and twenty) days commencing on the date of publishing the Law\nprovided that the president must coordinate before issuing the Law with: (Ministry of\nCommunications and Information Technology, Ministry of Foreign Affairs, Communications,\nSpace & Technology Commission, Digital Government Authority, National Cybersecurity\nAuthority, Saudi Health Council, and Saudi Central Bank), each in its own jurisdiction.\nArticle 43\nThis Law shall come into force after (seven hundred and twenty) days commencing on the\ndate of its publication in the Official Gazette. ", "metadata": {"country": "Saudi Arabia", "year": "2021", "legally_binding": "yes", "binding_proof": "Legislation", "date": "09-16", "regulator": "Saudi Data and Artificial Intelligence Authority (SDAIA)", "type": "legislation", "status": "active", "language": "English, Arabic", "use_cases": "[UC3 - Risk, UC4 - Regulatory, UC6 - Policy]"}}
